"""Single-file consolidated FastAPI app for PaperX."""

from __future__ import annotations

import asyncio
import enum
import io
import json
import logging
import os
import random
import re
import requests
import sys
import textwrap
import time
import uuid
import math
import ast
from dataclasses import dataclass, field
from datetime import datetime, date, timezone
from decimal import Decimal
from functools import lru_cache
from pathlib import Path
from typing import Any, AsyncGenerator, Dict, Iterator, List, Literal, Optional, Set, Tuple
from urllib.parse import quote, urlparse
import threading
import concurrent.futures

# New imports for using google.genai as requested
try:
    from google import genai
    from google.genai import types
except ImportError:
    genai = None
    types = None


from autogen_agentchat.agents import AssistantAgent
from autogen_core.models import ModelInfo
from autogen_ext.models.openai import OpenAIChatCompletionClient
from bs4 import BeautifulSoup
from dotenv import load_dotenv
from fastapi import APIRouter, Body, FastAPI, File, Form, Header, HTTPException, Query, UploadFile, WebSocket, WebSocketDisconnect, Request
from starlette.concurrency import run_in_threadpool
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import FileResponse, JSONResponse, Response, StreamingResponse, RedirectResponse
from fastapi.staticfiles import StaticFiles
try:
    import httpx  # Optional: used for catching RemoteProtocolError from underlying HTTP calls
    from httpx import RemoteProtocolError as HTTPXRemoteProtocolError  # type: ignore
except Exception:  # pragma: no cover
    httpx = None
    class HTTPXRemoteProtocolError(Exception):
        pass

try:
    import fitz  # type: ignore
except Exception:  # pragma: no cover
    fitz = None  # type: ignore

try:
    import docx  # type: ignore
except Exception:  # pragma: no cover
    docx = None  # type: ignore

try:
    import PyPDF2  # type: ignore
except Exception:  # pragma: no cover
    PyPDF2 = None  # type: ignore

try:
    from pptx import Presentation  # type: ignore
except Exception:  # pragma: no cover
    Presentation = None  # type: ignore

try:
    import textract  # type: ignore
except Exception:  # pragma: no cover
    textract = None  # type: ignore
from markdownify import markdownify as md
from pydantic import BaseModel, Field, HttpUrl, validator, root_validator
from rapidfuzz import fuzz
from serpapi import GoogleSearch
from supabase import Client, create_client
from postgrest.exceptions import APIError
# Package imports removed - code inlined below for deployment compatibility
# (Original: from packages.yt_transcript, packages.tunex_router, packages.problems_api, packages.youtube_video)
import regex as rx  # For yt_transcript functionality
import socket
import subprocess
import shutil
from concurrent.futures import ThreadPoolExecutor
from threading import Lock
from urllib.error import HTTPError, URLError
from urllib.request import Request as URLRequest, urlopen

try:
    # Used for fetching YouTube video metadata (channel, views, etc.)
    from yt_dlp import YoutubeDL  # type: ignore
except Exception:  # pragma: no cover
    YoutubeDL = None  # type: ignore

# =============================================================================
# INLINED: youtube_video.py
# =============================================================================

def _parse_video_id(url: str) -> Optional[str]:
    """Extract video ID from YouTube URL."""
    if not url:
        return None
    if re.match(r'^[A-Za-z0-9_-]{11}$', url):
        return url
    patterns = [
        r'(?:youtube\.com\/watch\?v=|youtu\.be\/|youtube\.com\/embed\/)([A-Za-z0-9_-]{11})',
        r'youtube\.com\/shorts\/([A-Za-z0-9_-]{11})',
    ]
    for pattern in patterns:
        match = re.search(pattern, url)
        if match:
            return match.group(1)
    return None

def _format_duration(seconds: Optional[int]) -> str:
    """Format duration in seconds to MM:SS or HH:MM:SS."""
    if seconds is None:
        return ""
    try:
        total_seconds = int(round(float(seconds)))
    except (TypeError, ValueError):
        return ""
    if total_seconds <= 0:
        return ""
    hours, remainder = divmod(total_seconds, 3600)
    minutes, secs = divmod(remainder, 60)
    if hours > 0:
        return f"{hours}:{minutes:02d}:{secs:02d}"
    return f"{minutes}:{secs:02d}"

def _format_views(views: Optional[int]) -> str:
    """Format view count with commas."""
    if views is None:
        return ""
    if views >= 1_000_000:
        return f"{views / 1_000_000:.1f}M views"
    elif views >= 1_000:
        return f"{views / 1_000:.1f}K views"
    return f"{views:,} views"

_channel_logo_cache: Dict[str, str] = {}
_channel_logo_lock = Lock()
_DEFAULT_CHANNEL_LOGO = "https://www.youtube.com/s/desktop/94838207/img/favicon_144x144.png"

def _normalize_channel_logo(logo_url: str) -> str:
    """Downscale large channel logo URLs to a smaller size."""
    if not logo_url or not isinstance(logo_url, str):
        return ""
    return re.sub(r"=s\d+-c", "=s88-c", logo_url, count=1)

def _fetch_channel_logo(channel_page_url: Optional[str]) -> str:
    """Fetch the channel avatar URL by scraping the channel page's open graph metadata."""
    if not channel_page_url:
        return ""
    channel_page_url = channel_page_url.strip()
    if not channel_page_url:
        return ""
    with _channel_logo_lock:
        cached = _channel_logo_cache.get(channel_page_url)
    if cached is not None:
        return cached
    try:
        req = Request(
            channel_page_url,
            headers={
                "User-Agent": (
                    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
                    "AppleWebKit/537.36 (KHTML, like Gecko) "
                    "Chrome/116.0 Safari/537.36"
                )
            },
        )
        with urlopen(req, timeout=4) as response:
            charset = response.headers.get_content_charset() or "utf-8"
            html_text = response.read().decode(charset, errors="ignore")
    except (ValueError, HTTPError, URLError, TimeoutError, socket.timeout):
        with _channel_logo_lock:
            _channel_logo_cache[channel_page_url] = ""
        return ""
    logo_match = re.search(
        r'<meta[^>]+property=["\']og:image["\'][^>]*content=["\']([^"\']+)["\']',
        html_text,
        flags=re.IGNORECASE,
    )
    if not logo_match:
        with _channel_logo_lock:
            _channel_logo_cache[channel_page_url] = ""
        return ""
    logo_url = html.unescape(logo_match.group(1))
    with _channel_logo_lock:
        _channel_logo_cache[channel_page_url] = logo_url
    return logo_url

def get_channel_logo(channel_page_url: Optional[str]) -> str:
    """Public helper to resolve and normalize a channel logo."""
    return _normalize_channel_logo(_fetch_channel_logo(channel_page_url)) or ""

def get_default_channel_logo() -> str:
    return _DEFAULT_CHANNEL_LOGO

def search_youtube_videos(query: str, num: int = 8, *, prefetch_logos: bool = False) -> List[Dict[str, str]]:
    """Search YouTube videos using yt-dlp."""
    if not YoutubeDL:
        raise ImportError("yt-dlp is not installed.")
    if not query or not query.strip():
        return []
    num = max(1, min(num, 20))
    ydl_opts = {
        'quiet': True,
        'no_warnings': True,
        'extract_flat': True,
        'force_generic_extractor': False,
        'default_search': 'ytsearch',
        'format': 'best',
        'noplaylist': True,
        'playlistend': num,
        'cachedir': False,
    }
    videos: List[Dict[str, str]] = []
    try:
        with YoutubeDL(ydl_opts) as ydl:
            search_query = f"ytsearch{num}:{query}"
            result = ydl.extract_info(search_query, download=False)
            if not result or 'entries' not in result:
                return []
            entries = result.get('entries', [])
            limited_entries = entries[:num]
            if prefetch_logos:
                unique_pages: List[str] = []
                for entry in limited_entries:
                    channel_page = (entry.get('channel_url') or entry.get('uploader_url') or "").strip()
                    if channel_page and channel_page not in unique_pages:
                        unique_pages.append(channel_page)
                with _channel_logo_lock:
                    cached_pages = set(_channel_logo_cache.keys())
                missing_pages = [page for page in unique_pages if page and page not in cached_pages]
                if missing_pages:
                    max_workers = min(6, len(missing_pages))
                    try:
                        with ThreadPoolExecutor(max_workers=max_workers) as executor:
                            list(executor.map(_fetch_channel_logo, missing_pages))
                    except RuntimeError:
                        for page in missing_pages:
                            _fetch_channel_logo(page)
            for entry in limited_entries:
                if not entry:
                    continue
                video_id = entry.get('id', '')
                title = entry.get('title', '').strip()
                channel = entry.get('channel') or entry.get('uploader') or 'YouTube'
                duration_sec = entry.get('duration')
                duration = _format_duration(duration_sec)
                view_count = entry.get('view_count')
                views = _format_views(view_count)
                thumbnail = entry.get('thumbnail', '')
                if not thumbnail and video_id:
                    thumbnail = f"https://i.ytimg.com/vi/{video_id}/maxresdefault.jpg"
                video_url = entry.get('url', '')
                if not video_url and video_id:
                    video_url = f"https://www.youtube.com/watch?v={video_id}"
                channel_page = (entry.get('channel_url') or entry.get('uploader_url') or "").strip()
                channel_logo = get_channel_logo(channel_page) if prefetch_logos else ""
                final_logo = channel_logo or _DEFAULT_CHANNEL_LOGO
                if title and video_url and thumbnail:
                    videos.append({
                        "title": title,
                        "link": video_url,
                        "channel": channel.strip() if channel else "YouTube",
                        "views": views,
                        "duration": duration,
                        "thumbnail": thumbnail,
                        "channel_logo": final_logo,
                        "channel_logo_is_default": final_logo == _DEFAULT_CHANNEL_LOGO,
                        "channel_page": channel_page,
                    })
    except Exception as e:
        print(f"Error searching YouTube videos: {e}")
        return []
    return videos

# =============================================================================
# INLINED: python_compiler.py
# =============================================================================

def execute_python_code(code: str, timeout: int = 5) -> dict:
    """Executes Python code in a separate process and returns the output."""
    try:
        with tempfile.NamedTemporaryFile(mode='w', suffix='.py', delete=False, encoding='utf-8') as temp_file:
            temp_file.write(code)
            temp_file_path = temp_file.name
    except Exception as e:
        return {"output": "", "error": f"System Error: Failed to create temporary file: {str(e)}", "status": "error"}
    try:
        result = subprocess.run(
            [sys.executable, temp_file_path],
            capture_output=True,
            text=True,
            timeout=timeout
        )
        return {
            "output": result.stdout,
            "error": result.stderr,
            "status": "success" if result.returncode == 0 else "error"
        }
    except subprocess.TimeoutExpired:
        return {"output": "", "error": f"Execution timed out after {timeout} seconds.", "status": "timeout"}
    except Exception as e:
        return {"output": "", "error": f"Execution failed: {str(e)}", "status": "error"}
    finally:
        if os.path.exists(temp_file_path):
            try:
                os.remove(temp_file_path)
            except:
                pass

# =============================================================================
# INLINED: java_compiler.py
# =============================================================================

_CLASS_NAME_RE = re.compile(r"\bpublic\s+class\s+([A-Za-z_][A-Za-z0-9_]*)\b")
_FALLBACK_CLASS_RE = re.compile(r"\bclass\s+([A-Za-z_][A-Za-z0-9_]*)\b")

def _detect_main_class_name(code: str) -> str:
    """Best-effort extraction of Java public class name."""
    m = _CLASS_NAME_RE.search(code or "")
    if m:
        return m.group(1)
    m2 = _FALLBACK_CLASS_RE.search(code or "")
    if m2:
        return m2.group(1)
    return "Main"

def execute_java_code(code: str, timeout: int = 7) -> Dict[str, str]:
    """Compiles and runs Java code."""
    javac_path = shutil.which("javac")
    java_path = shutil.which("java")
    if not javac_path or not java_path:
        return {
            "output": "",
            "error": "Java compiler not found. Please install a JDK.",
            "status": "error",
        }
    code = code or ""
    class_name = _detect_main_class_name(code)
    if "class" not in code:
        code = (
            "public class Main {\n"
            "    public static void main(String[] args) throws Exception {\n"
            + "\n".join("        " + line for line in code.splitlines())
            + "\n    }\n"
            "}\n"
        )
        class_name = "Main"
    temp_dir = None
    try:
        temp_dir = tempfile.mkdtemp(prefix="tunex-java-")
        java_file = os.path.join(temp_dir, f"{class_name}.java")
        with open(java_file, "w", encoding="utf-8") as f:
            f.write(code)
        try:
            compile_res = subprocess.run(
                [javac_path, "-encoding", "UTF-8", java_file],
                cwd=temp_dir,
                capture_output=True,
                text=True,
                timeout=timeout,
            )
        except subprocess.TimeoutExpired:
            return {"output": "", "error": f"Compilation timed out after {timeout} seconds.", "status": "timeout"}
        if compile_res.returncode != 0:
            err = (compile_res.stderr or "") + ("\n" + compile_res.stdout if compile_res.stdout else "")
            return {"output": "", "error": err.strip() or "Compilation failed.", "status": "error"}
        try:
            run_res = subprocess.run(
                [java_path, "-Dfile.encoding=UTF-8", "-cp", temp_dir, class_name],
                cwd=temp_dir,
                capture_output=True,
                text=True,
                timeout=timeout,
            )
        except subprocess.TimeoutExpired:
            return {"output": "", "error": f"Execution timed out after {timeout} seconds.", "status": "timeout"}
        return {
            "output": run_res.stdout or "",
            "error": run_res.stderr or "",
            "status": "success" if run_res.returncode == 0 else "error",
        }
    except Exception as e:
        return {"output": "", "error": f"Execution failed: {e}", "status": "error"}
    finally:
        if temp_dir and os.path.isdir(temp_dir):
            try:
                shutil.rmtree(temp_dir, ignore_errors=True)
            except Exception:
                pass

# =============================================================================
# INLINED: yt_transcript.py 
# =============================================================================

# Regex patterns for cleaning (uses 'regex' module for \p{L} support)
TAG_TS_RE        = rx.compile(r"<\d{2}:\d{2}:\d{2}(?:\.\d{1,3})?>", flags=rx.I)
TAG_C_OPEN_RE    = rx.compile(r"<c(?:\.[^>]*)?>", flags=rx.I)
TAG_C_CLOSE_RE   = rx.compile(r"</c>", flags=rx.I)
BRACKET_NOISE_RE = rx.compile(r"\[(?:music|applause|__|noise|silence)\]", flags=rx.I)
WS_RE            = rx.compile(r"[ \t\u00A0]+")
TOKEN_RE = rx.compile(r"\p{L}+\p{M}*|\d+|[^\s\p{L}\p{N}]", rx.UNICODE)
YOUTUBE_ID_PATTERNS = [
    r"(?:v=|/v/|/embed/|/shorts/|youtu\.be/)([A-Za-z0-9_-]{11})",
    r"^([A-Za-z0-9_-]{11})$",
]

def extract_video_id(url_or_id: str) -> str:
    for pat in YOUTUBE_ID_PATTERNS:
        m = re.search(pat, url_or_id)
        if m:
            return m.group(1)
    token = (url_or_id or "").strip()
    if re.fullmatch(r"[A-Za-z0-9_-]{11}", token):
        return token
    raise HTTPException(400, "Could not parse YouTube video ID from the provided URL or ID.")

def _strip_inline_tags(text: str) -> str:
    t = TAG_TS_RE.sub("", text)
    t = TAG_C_OPEN_RE.sub("", t)
    t = TAG_C_CLOSE_RE.sub("", t)
    t = html.unescape(t)
    t = BRACKET_NOISE_RE.sub("", t)
    t = t.replace("_", " ")
    t = WS_RE.sub(" ", t).strip()
    return t

def _normalize_for_compare(text: str) -> str:
    t = _strip_inline_tags(text)
    t = unicodedata.normalize("NFKC", t)
    t = rx.sub(r"[^\p{L}\p{N}\s.,!?;:']", "", t)
    t = WS_RE.sub(" ", t).strip().lower()
    return t

def _smart_sentence_join(chunks: List[str]) -> str:
    raw = " ".join(chunks)
    raw = WS_RE.sub(" ", raw).strip()
    raw = re.sub(r"\s+([.,!?;:])", r"\1", raw)
    return raw

def _yt_tokens(s: str) -> List[str]:
    return TOKEN_RE.findall(s)

def _yt_untokenize(tokens: List[str]) -> str:
    out = []
    for i, tok in enumerate(tokens):
        if i > 0 and rx.match(r"[\p{L}\p{N}]", tok) and rx.match(r"[\p{L}\p{N}]", tokens[i-1]):
            out.append(" ")
        out.append(tok)
    return "".join(out)

def _compact_repetitions(text: str, max_ngram: int = 12, min_chars_per_span: int = 4) -> str:
    if not text or len(text) < 2:
        return text
    text = rx.sub(r"\b(\p{L}+)\s+\1\b", r"\1", text, flags=rx.IGNORECASE)
    toks = _yt_tokens(text)
    i = 0
    out: List[str] = []
    while i < len(toks):
        matched = False
        max_n = min(max_ngram, (len(toks) - i) // 2)
        for n in range(max_n, 0, -1):
            a = toks[i:i+n]
            b = toks[i+n:i+2*n]
            if not a or not b:
                continue
            if a == b:
                span_txt = _yt_untokenize(a)
                if len(rx.sub(r"\s+", "", span_txt)) >= min_chars_per_span:
                    j = i + n
                    while j + n <= len(toks) and toks[j:j+n] == a:
                        j += n
                    out.extend(a)
                    i = j
                    matched = True
                    break
        if not matched:
            out.append(toks[i])
            i += 1
    s = _yt_untokenize(out)
    s = rx.sub(r"\s+([.,!?;:])", r" \1", s)
    s = rx.sub(r"([(\[{])\s+", r"\1", s)
    s = rx.sub(r"\s+([)\]}])", r"\1", s)
    s = rx.sub(r"\s{2,}", " ", s).strip()
    return s

def _prefer_language_candidates(preferred_langs: List[str]) -> List[str]:
    expanded: List[str] = []
    for lang in preferred_langs:
        expanded.append(lang)
        if "-" in lang:
            base = lang.split("-")[0]
            if base not in expanded:
                expanded.append(base)
    for fb in ["en", "en-US", "en-GB", "en-IN"]:
        if fb not in expanded:
            expanded.append(fb)
    return expanded

def _convert_transcript_to_dicts(transcript_list) -> List[dict]:
    result = []
    for item in transcript_list:
        if hasattr(item, 'text'):
            result.append({"text": item.text, "start": item.start, "duration": item.duration})
        elif isinstance(item, dict):
            result.append(item)
        else:
            result.append(dict(item))
    return result

def _try_youtube_transcript_api(video_id: str, langs: List[str]) -> Tuple[Optional[List[dict]], Optional[str]]:
    try:
        from youtube_transcript_api import YouTubeTranscriptApi, TranscriptsDisabled, NoTranscriptFound, VideoUnavailable
        ytt = YouTubeTranscriptApi()
        listing = ytt.list(video_id)
    except Exception:
        return None, None
    for lang in _prefer_language_candidates(langs):
        try:
            tr = listing.find_manually_created_transcript([lang])
            return _convert_transcript_to_dicts(tr.fetch()), tr.language_code
        except Exception:
            pass
        try:
            tr = listing.find_generated_transcript([lang])
            return _convert_transcript_to_dicts(tr.fetch()), tr.language_code
        except Exception:
            pass
    for tr in listing:
        try:
            if tr.is_translatable:
                for lang in _prefer_language_candidates(langs):
                    try:
                        return _convert_transcript_to_dicts(tr.translate(lang).fetch()), lang
                    except Exception:
                        continue
        except Exception:
            continue
    try:
        first = next(iter(listing))
        return _convert_transcript_to_dicts(first.fetch()), first.language_code
    except Exception:
        return None, None

def _parse_vtt_timestamp(ts: str) -> float:
    parts = ts.split(":")
    if len(parts) == 3:
        h, m, s = parts
    elif len(parts) == 2:
        h = "0"; m, s = parts
    else:
        return 0.0
    if "." in s:
        sec, ms = s.split("."); ms = int(ms.ljust(3, "0")[:3])
    else:
        sec, ms = s, 0
    return int(h) * 3600 + int(m) * 60 + int(sec) + ms / 1000.0

def _load_vtt_to_segments(vtt_path: str) -> List[dict]:
    with open(vtt_path, "r", encoding="utf-8", errors="ignore") as f:
        lines = [ln.rstrip("\n") for ln in f]
    segs: List[dict] = []
    i = 0; n = len(lines)
    while i < n:
        line = lines[i].strip(); i += 1
        if not line or line.upper() == "WEBVTT":
            continue
        if re.match(r"^\d+\s*$", line):
            if i < n:
                line = lines[i].strip(); i += 1
        if "-->" in line:
            times = line.split("-->")
            if len(times) != 2:
                while i < n and lines[i].strip():
                    i += 1
                continue
            start_s = _parse_vtt_timestamp(times[0].strip())
            end_s = _parse_vtt_timestamp(times[1].strip())
            cue = []
            while i < n and lines[i].strip() != "":
                cue.append(lines[i]); i += 1
            while i < n and lines[i].strip() == "":
                i += 1
            text_raw = " ".join(cue)
            text_clean = _strip_inline_tags(text_raw)
            if text_clean:
                segs.append({"start": start_s, "duration": max(0.0, end_s - start_s), "text": text_clean})
    return segs

def _try_yt_dlp_captions(video_id: str, langs: List[str]) -> Tuple[Optional[str], Optional[str]]:
    tempdir = tempfile.mkdtemp(prefix="ytcapt_")
    outtmpl = os.path.join(tempdir, "%(id)s.%(ext)s")
    ydl_opts = {
        "skip_download": True,
        "writesubtitles": True,
        "writeautomaticsub": True,
        "subtitleslangs": _prefer_language_candidates(langs),
        "subtitlesformat": "vtt",
        "outtmpl": outtmpl,
        "quiet": True,
        "no_warnings": True,
    }
    url = f"https://www.youtube.com/watch?v={video_id}"
    with YoutubeDL(ydl_opts) as ydl:
        try:
            ydl.extract_info(url, download=True)
        except Exception:
            return None, None
    vtts = [os.path.join(tempdir, f) for f in os.listdir(tempdir) if f.endswith(".vtt")]
    if not vtts:
        return None, None
    prefs = _prefer_language_candidates(langs)
    chosen = None; chosen_lang = None
    for lang in prefs:
        for p in vtts:
            if re.search(rf"\.{re.escape(lang)}\.vtt$", os.path.basename(p)):
                chosen, chosen_lang = p, lang
                break
        if chosen:
            break
    if not chosen:
        chosen = vtts[0]
        m = re.search(r"\.([a-zA-Z-]{2,})\.vtt$", os.path.basename(chosen))
        chosen_lang = m.group(1) if m else None
    return chosen, chosen_lang

def _dedupe_and_merge_segments(segments: List[dict]) -> List[dict]:
    out: List[dict] = []
    for seg in segments:
        t = (seg.get("text") or "").strip()
        if not t:
            continue
        norm = _normalize_for_compare(t)
        if not norm:
            continue
        if out:
            last = out[-1]
            last_norm = last.get("_norm")
            if norm == last_norm:
                last_end = last["start"] + last["duration"]
                new_end = seg["start"] + seg["duration"]
                if seg["start"] <= last_end + 0.2:
                    last["duration"] = max(last["duration"], new_end - last["start"])
                continue
            if t.lower() == last["text"].lower() and seg["start"] <= (last["start"] + last["duration"] + 0.5):
                last_end = last["start"] + last["duration"]
                new_end = seg["start"] + seg["duration"]
                last["duration"] = max(last["duration"], new_end - last["start"])
                continue
        seg = dict(seg)
        seg["_norm"] = norm
        out.append(seg)
    for s in out:
        s.pop("_norm", None)
    return out

def _render_paragraph(transcript: List[dict]) -> str:
    chunks = [seg["text"].strip() for seg in transcript if seg.get("text")]
    text = _smart_sentence_join(chunks)
    return _compact_repetitions(text) + "\n"

def fetch_transcript_paragraph(
    url_or_id: str,
    lang: str = "en",
    *,
    fallback_ytdlp: bool = True,
    use_whisper: bool = False,
    clean: bool = True,
) -> str:
    """Returns the entire transcript as a single cleaned paragraph."""
    video_id = extract_video_id(url_or_id)
    preferred = [lang] if lang else ["en"]
    transcript, _lang_code = _try_youtube_transcript_api(video_id, preferred)
    if transcript is None and fallback_ytdlp:
        vtt_path, _ytdlp_lang = _try_yt_dlp_captions(video_id, preferred)
        if vtt_path:
            transcript = _load_vtt_to_segments(vtt_path)
    if transcript is None:
        raise HTTPException(404, "No transcript/captions available for this video.")
    if clean:
        cleaned = []
        for seg in transcript:
            t = _strip_inline_tags(seg.get("text", ""))
            if not t:
                continue
            t = _compact_repetitions(t)
            cleaned.append({"start": float(seg.get("start", 0.0)), "duration": float(seg.get("duration", 0.0)), "text": t})
        transcript = _dedupe_and_merge_segments(cleaned)
    else:
        transcript = [
            {"start": float(seg.get("start", 0.0)), "duration": float(seg.get("duration", 0.0)), "text": seg.get("text", "")}
            for seg in (transcript or [])
        ]
    return _render_paragraph(transcript)

# YouTube Transcript Router (inlined)
yt_transcript_router = APIRouter()

@yt_transcript_router.get("/transcript.txt")
def get_transcript_txt(
    url: str = Query(..., description="YouTube video URL (or ID)"),
    lang: str = Query("en"),
    fallback_ytdlp: bool = Query(True),
    use_whisper: bool = Query(False),
    clean: bool = Query(True),
):
    text = fetch_transcript_paragraph(
        url_or_id=url,
        lang=lang,
        fallback_ytdlp=fallback_ytdlp,
        use_whisper=use_whisper,
        clean=clean,
    )
    return PlainTextResponse(text, media_type="text/plain; charset=utf-8")

# Load .env from this module's directory to avoid CWD issues
try:
    _env_path = (Path(__file__).resolve().parent / ".env")
    # override=True so a valid file value isn't shadowed by a stale OS env
    load_dotenv(dotenv_path=str(_env_path), override=True)
except Exception:
    # Fallback to default discovery
    load_dotenv()

# --- Supabase helpers ---


supabase_logger = logging.getLogger("paperx.supabase")
supabase_logger.setLevel(logging.INFO)

# Truncate long uvicorn access log messages
class TruncateLogFilter(logging.Filter):
    def filter(self, record):
        if hasattr(record, 'args') and record.args:
            # Truncate the message if it contains a very long URL
            args = list(record.args) if isinstance(record.args, tuple) else [record.args]
            new_args = []
            for arg in args:
                if isinstance(arg, str) and len(arg) > 100:
                    new_args.append(arg[:97] + "...")
                else:
                    new_args.append(arg)
            record.args = tuple(new_args)
        return True

# Apply filter to uvicorn.access logger
uvicorn_access_logger = logging.getLogger("uvicorn.access")
uvicorn_access_logger.addFilter(TruncateLogFilter())

SUPABASE_URL = os.getenv("SUPABASE_URL", "").strip()
SUPABASE_SERVICE_ROLE_KEY = os.getenv("SUPABASE_SERVICE_ROLE_KEY", "").strip()
SUPABASE_ANON_KEY = os.getenv("SUPABASE_ANON_KEY", "").strip()

if not SUPABASE_URL or not SUPABASE_SERVICE_ROLE_KEY:
    raise RuntimeError("Missing SUPABASE_URL or SUPABASE_SERVICE_ROLE_KEY in environment.")


@lru_cache()
def get_service_client() -> Client:
    return create_client(SUPABASE_URL, SUPABASE_SERVICE_ROLE_KEY)


@lru_cache()
def get_anon_client() -> Optional[Client]:
    if not SUPABASE_ANON_KEY:
        supabase_logger.warning("Missing SUPABASE_ANON_KEY; auth-dependent routes will be disabled.")
        return None
    return create_client(SUPABASE_URL, SUPABASE_ANON_KEY)


def _to_supabase_json(value: Any) -> Any:
    """Recursively coerce common Python types (UUID, datetime, set, etc.) into JSON-serializable forms."""
    if value is None:
        return None
    if isinstance(value, uuid.UUID):
        return str(value)
    if isinstance(value, Decimal):
        return float(value)
    if isinstance(value, (datetime, date)):
        return value.isoformat()
    if isinstance(value, enum.Enum):
        return value.value
    if isinstance(value, BaseModel):
        return _to_supabase_json(value.dict(exclude_none=True))
    if isinstance(value, set):
        return [_to_supabase_json(v) for v in value]
    if isinstance(value, (list, tuple)):
        return [_to_supabase_json(v) for v in value]
    if isinstance(value, dict):
        return {k: _to_supabase_json(v) for k, v in value.items() if v is not None}
    return value


def _supabase_payload(raw: Dict[str, Any]) -> Dict[str, Any]:
    """Return a JSON-serializable dict suitable for Supabase from a Pydantic .dict() payload."""
    return {k: _to_supabase_json(v) for k, v in raw.items() if v is not None}

# --- Auth helpers ---
def _bearer_token_from_header(authorization: Optional[str]) -> Optional[str]:
    if not authorization:
        return None
    try:
        parts = str(authorization).split()
        if len(parts) >= 2 and parts[0].lower() == 'bearer':
            return parts[1]
        return str(authorization)
    except Exception:
        return None

# --- Resiliency helpers for Supabase/httpx transient protocol errors ---
# Some users have observed intermittent httpcore.RemoteProtocolError("Server disconnected") coming
# from underlying HTTP/2 (or connection reuse) when performing rapid successive metadata lookups.
# These are typically transient (connection closed between frames). We add a lightweight retry
# wrapper so endpoint handlers can reattempt idempotent read queries without failing the whole request.
RETRYABLE_EXCEPTIONS: tuple = ()
try:  # HTTPXRemoteProtocolError already imported conditionally at top
    if httpx is not None:
        RETRYABLE_EXCEPTIONS = (HTTPXRemoteProtocolError, httpx.RemoteProtocolError)  # type: ignore
    else:
        RETRYABLE_EXCEPTIONS = (HTTPXRemoteProtocolError,)  # type: ignore
except Exception:  # pragma: no cover
    pass

def _supabase_retry(fn, *, retries: int = 3, base_delay: float = 0.35):
    """Execute a zero-arg callable returning a Supabase response with simple exponential backoff.

    Only catches protocol-level disconnection errors that are safe to retry for idempotent SELECT/IN queries.
    """
    for attempt in range(retries):
        try:
            return fn()
        except RETRYABLE_EXCEPTIONS as e:  # pragma: no cover - network timing dependent
            if attempt == retries - 1:
                raise
            time.sleep(base_delay * (2 ** attempt))



PROJECTS_TABLE = os.getenv("PROJECTS_TABLE") or os.getenv("SUPABASE_PROJECTS_TABLE") or "projects"
PROJECT_APPLICATIONS_TABLE = os.getenv("PROJECT_APPLICATIONS_TABLE") or "project_applications"
PROJECT_COLLAB_TABLE = os.getenv("PROJECT_COLLAB_TABLE") or "project_collab_messages"
SKILL_TESTS_TABLE = os.getenv("SKILL_TESTS_TABLE") or "skill_tests"
SKILL_VERIFICATIONS_TABLE = os.getenv("SKILL_VERIFICATIONS_TABLE") or "skill_verifications"
PROJECTS_BUCKET = (os.getenv("SUPABASE_PROJECTS_BUCKET") or os.getenv("SUPABASE_BUCKET") or "").strip()
PROJECT_MEDIA_EXTENSIONS = {
    "cover": {".png", ".jpg", ".jpeg", ".webp", ".gif"},
    "gallery": {".png", ".jpg", ".jpeg", ".webp", ".gif", ".mp4", ".mov", ".webm"},
}

# --- Learning track table names ---

LEARNING_TRACK_GOALS_TABLE = os.getenv("LEARNING_TRACK_GOALS_TABLE", "learning_track_goals")
LEARNING_TRACK_PLANS_TABLE = os.getenv("LEARNING_TRACK_PLANS_TABLE", "learning_track_plans")
LEARNING_TRACK_PROGRESS_TABLE = os.getenv("LEARNING_TRACK_PROGRESS_TABLE", "learning_track_progress")

# --- Print/Orders table names ---
PRINT_SHOPS_TABLE = os.getenv("PRINT_SHOPS_TABLE") or "print_shops"
PRINT_PRICING_TABLE = os.getenv("PRINT_PRICING_TABLE") or "print_pricing"
PRINT_PRINTERS_TABLE = os.getenv("PRINT_PRINTERS_TABLE") or "print_printers"
PRINT_JOBS_TABLE = os.getenv("PRINT_JOBS_TABLE") or "print_jobs"
PRINT_JOB_EVENTS_TABLE = os.getenv("PRINT_JOB_EVENTS_TABLE") or "print_job_events"

# --- AI model clients ---

openai_model_client = OpenAIChatCompletionClient(
    model="gpt-4o-mini",
    api_key=os.environ.get("OPENAI_API_KEY")
)

deepseek_model_client =  OpenAIChatCompletionClient(
    base_url="https://openrouter.ai/api/v1",
    model="deepseek/deepseek-r1-0528:free",
    api_key = os.environ.get("OPENROUTER_API_KEY"),
    model_info=ModelInfo(
        vision=True,
        function_calling=True,
        json_output=True,
        family="unknown",
        structured_output=True
    )
)

gemini_model_client = OpenAIChatCompletionClient(
    base_url=os.getenv("GEMINI_BASE_URL", "https://generativelanguage.googleapis.com/v1beta/openai/"),
    model="gemini-2.5-flash",
    api_key=(os.getenv("GEMINI_API_KEY", "") or "").strip(),
    model_info=ModelInfo(
        vision=True,
        function_calling=True,
        json_output=True,
        family="unknown",
        structured_output=True
    )
)

# --- Notes storage ---

BASE_DIR = os.path.abspath(os.path.dirname(__file__))
NOTES_DIR = os.path.join(BASE_DIR, "notes")
os.makedirs(NOTES_DIR, exist_ok=True)
os.makedirs(NOTES_DIR, exist_ok=True)

# DB table for AI notes (title + markdown)
AI_NOTES_TABLE = os.getenv("AI_NOTES_TABLE", "ai_notes")
AI_NOTES_CHEATSHEET_TABLE = os.getenv("AI_NOTES_CHEATSHEET_TABLE", "ai_notes_cheatsheet")
AI_NOTES_SIMPLE_TABLE = os.getenv("AI_NOTES_SIMPLE_TABLE", "ai_notes_simple")
AI_NOTES_USER_EDITS_TABLE = os.getenv("AI_NOTES_USER_EDITS_TABLE", "ai_notes_user_edits")

VALID_NOTE_VARIANTS = {"detailed", "cheatsheet", "simple"}


def _normalize_course_type(course_type: Optional[str]) -> Optional[str]:
    if course_type is None:
        return None
    value = str(course_type).strip().lower()
    if value in {"maths", "theorey", "practical"}:
        return value
    return None

def _normalize_variant(variant: Optional[str]) -> str:
    v = (variant or "detailed").strip().lower()
    return v if v in VALID_NOTE_VARIANTS else "detailed"

def _table_for_variant(variant: Optional[str]) -> str:
    v = _normalize_variant(variant)
    if v == "cheatsheet":
        return AI_NOTES_CHEATSHEET_TABLE
    if v == "simple":
        return AI_NOTES_SIMPLE_TABLE
    return AI_NOTES_TABLE

def db_get_ai_note_by_title_exact(title: str) -> Optional[Dict[str, Any]]:
    """Fetch a detailed-variant note by exact title. Backward-compat wrapper."""
    return db_get_ai_note_by_title_exact_variant(title, variant="detailed")

def db_get_ai_note_by_title_exact_variant(title: str, *, variant: Optional[str] = None) -> Optional[Dict[str, Any]]:
    """Fetch a note by exact title (case-insensitive) from the table for the given variant."""
    supabase = get_service_client()
    t = (title or "").strip()
    if not t:
        return None
    table = _table_for_variant(variant)
    try:
        res = (
            supabase.table(table)
            .select("id,title,markdown,image_urls,created_at,updated_at")
            .eq("title_ci", t.lower())
            .limit(1)
            .execute()
        )
        data = getattr(res, 'data', []) or []
        return data[0] if data else None
    except Exception:
        return None

def db_upsert_ai_note_by_title(
    title: str,
    markdown: str,
    image_urls: Optional[List[str]] = None,
) -> Dict[str, Any]:
    """Insert or update a detailed-variant note by title; returns the stored row."""
    return db_upsert_ai_note_by_title_variant(title, markdown, variant="detailed", image_urls=image_urls)

def db_upsert_ai_note_by_title_variant(
    title: str,
    markdown: str,
    *,
    variant: Optional[str] = None,
    image_urls: Optional[List[str]] = None,
) -> Dict[str, Any]:
    """Insert or update a note by title in the variant-specific table; returns the stored row."""
    supabase = get_service_client()
    now = datetime.utcnow().isoformat()
    payload = {
        "title": (title or "").strip() or "Untitled",
        "markdown": markdown or "",
        "updated_at": now,
    }
    if image_urls is not None:
        cleaned_images = [str(url).strip() for url in image_urls if str(url or "").strip()]
        payload["image_urls"] = cleaned_images
    table = _table_for_variant(variant)
    try:
        res = supabase.table(table).upsert(payload, on_conflict="title_ci", returning="representation").execute()
        if getattr(res, 'error', None):
            raise Exception(res.error)
        row = (getattr(res, 'data', []) or [{}])[0]
        if row:
            return row
        # Fallback: refetch by title_ci
        ref = (
            supabase.table(table)
            .select("id,title,markdown,image_urls,created_at,updated_at")
            .eq("title_ci", payload["title"].lower())
            .limit(1)
            .execute()
        )
        data = getattr(ref, 'data', []) or []
        if not data:
            raise RuntimeError("Failed to upsert ai_note")
        return data[0]
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"DB save failed: {e}")

def db_get_ai_note_by_id(note_id: str) -> Optional[Dict[str, Any]]:
    """Backward compat: search only detailed table."""
    return db_get_ai_note_by_id_variant(note_id, variant="detailed")

def db_get_ai_note_by_id_variant(note_id: str, *, variant: Optional[str] = None) -> Optional[Dict[str, Any]]:
    supabase = get_service_client()
    table = _table_for_variant(variant)
    try:
        res = (
            supabase.table(table)
            .select("id,title,markdown,image_urls,created_at,updated_at")
            .eq("id", note_id)
            .limit(1)
            .execute()
        )
        data = getattr(res, 'data', []) or []
        return data[0] if data else None
    except Exception:
        return None

def db_get_ai_note_by_id_any(note_id: str) -> Tuple[Optional[Dict[str, Any]], Optional[str]]:
    """Search all variant tables for the given id. Returns (row, variant)."""
    for v in ("detailed", "cheatsheet", "simple"):
        row = db_get_ai_note_by_id_variant(note_id, variant=v)
        if row:
            return row, v
    return None, None

def db_update_ai_note_markdown(
    note_id: str,
    markdown: str,
    image_urls: Optional[List[str]] = None,
) -> Optional[Dict[str, Any]]:
    """Backward compat: update only detailed table."""
    return db_update_ai_note_markdown_variant(note_id, markdown, variant="detailed", image_urls=image_urls)

def db_update_ai_note_markdown_variant(
    note_id: str,
    markdown: str,
    *,
    variant: Optional[str] = None,
    image_urls: Optional[List[str]] = None,
) -> Optional[Dict[str, Any]]:
    supabase = get_service_client()
    table = _table_for_variant(variant)
    try:
        update_payload: Dict[str, Any] = {"markdown": markdown or "", "updated_at": datetime.utcnow().isoformat()}
        if image_urls is not None:
            update_payload["image_urls"] = [str(url).strip() for url in image_urls if str(url or "").strip()]
        res = supabase.table(table).update(update_payload).eq("id", note_id).execute()
        if getattr(res, 'error', None):
            raise Exception(res.error)
        out = db_get_ai_note_by_id_variant(note_id, variant=variant)
        return out
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"DB update failed: {e}")


# --- Per-user edited notes helpers ---

def db_get_user_edit_by_title(user_id: str, title: str, *, variant: Optional[str] = None) -> Optional[Dict[str, Any]]:
    supabase = get_service_client()
    v = _normalize_variant(variant or "detailed")
    t = (title or "").strip()
    if not user_id or not t:
        return None
    try:
        res = (
            supabase.table(AI_NOTES_USER_EDITS_TABLE)
            .select("id,title,variant,markdown,created_at,updated_at")
            .eq("user_id", str(user_id))
            .eq("title_ci", t.lower())
            .eq("variant", v)
            .limit(1)
            .execute()
        )
        data = getattr(res, 'data', []) or []
        return data[0] if data else None
    except Exception:
        return None


def db_upsert_user_edit(user_id: str, title: str, markdown: str, *, variant: Optional[str] = None) -> Dict[str, Any]:
    supabase = get_service_client()
    v = _normalize_variant(variant or "detailed")
    now = datetime.utcnow().isoformat()
    payload = {
        "user_id": str(user_id),
        "title": (title or "").strip() or "Untitled",
        "variant": v,
        "markdown": markdown or "",
        "updated_at": now,
    }
    try:
        res = (
            supabase
            .table(AI_NOTES_USER_EDITS_TABLE)
            .upsert(payload, on_conflict="user_id,title_ci,variant", returning="representation")
            .execute()
        )
        if getattr(res, 'error', None):
            raise Exception(res.error)
        row = (getattr(res, 'data', []) or [{}])[0]
        if row:
            return row
        # Fallback: refetch by composite key
        ref = (
            supabase.table(AI_NOTES_USER_EDITS_TABLE)
            .select("id,title,variant,markdown,created_at,updated_at")
            .eq("user_id", str(user_id))
            .eq("title_ci", payload["title"].lower())
            .eq("variant", v)
            .limit(1)
            .execute()
        )
        data = getattr(ref, 'data', []) or []
        if not data:
            raise RuntimeError("Failed to upsert user edit")
        return data[0]
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"DB save failed: {e}")


def _slugify_topic(topic: str) -> str:
    s = re.sub(r"[^a-zA-Z0-9_.-]+", "-", (topic or "").strip()).strip("-")
    if not s:
        s = "note"
    return s[:60]


def _note_id(topic: str) -> str:
    slug = _slugify_topic(topic)
    ts = int(time.time() * 1000)
    short = uuid.uuid4().hex[:6]
    return f"{slug}-{ts}-{short}"


def _note_path(note_id: str) -> str:
    return os.path.join(NOTES_DIR, f"{note_id}.md")


def save_note(topic: str, markdown: str) -> Dict[str, str]:
    nid = _note_id(topic)
    path = _note_path(nid)
    with open(path, "w", encoding="utf-8") as f:
        f.write(markdown or "")
    st = os.stat(path)
    return {
        "id": nid,
        "topic": topic,
        "path": path,
        "created_at": datetime.fromtimestamp(st.st_mtime).isoformat(),
        "size": st.st_size,
    }


def read_note(note_id: str) -> Dict[str, str]:
    path = _note_path(note_id)
    if not os.path.isfile(path):
        raise FileNotFoundError("Note not found")
    with open(path, "r", encoding="utf-8") as f:
        md = f.read()
    st = os.stat(path)
    return {
        "id": note_id,
        "markdown": md,
        "updated_at": datetime.fromtimestamp(st.st_mtime).isoformat(),
        "size": st.st_size,
    }


def update_note(note_id: str, markdown: str) -> Dict[str, str]:
    path = _note_path(note_id)
    if not os.path.isfile(path):
        raise FileNotFoundError("Note not found")
    with open(path, "w", encoding="utf-8") as f:
        f.write(markdown or "")
    st = os.stat(path)
    return {
        "id": note_id,
        "updated_at": datetime.fromtimestamp(st.st_mtime).isoformat(),
        "size": st.st_size,
    }


def list_notes() -> List[Dict[str, str]]:
    items: List[Dict[str, str]] = []
    if not os.path.isdir(NOTES_DIR):
        return items
    for name in os.listdir(NOTES_DIR):
        if not name.endswith(".md"):
            continue
        nid = name[:-3]
        p = os.path.join(NOTES_DIR, name)
        st = os.stat(p)
        topic = None
        try:
            with open(p, "r", encoding="utf-8") as f:
                for line in f:
                    if line.startswith("# "):
                        topic = line[2:].strip()
                        break
        except Exception:
            pass
        items.append({
            "id": nid,
            "topic": topic,
            "updated_at": datetime.fromtimestamp(st.st_mtime).isoformat(),
            "size": st.st_size,
        })
    items.sort(key=lambda x: x["updated_at"], reverse=True)
    return items


def note_path(note_id: str) -> str:
    """Expose file path for download endpoints."""
    return _note_path(note_id)


# -------------------- Fuzzy/Semantic search helpers --------------------

def _normalize_text(s: str) -> str:
    s = (s or "").lower()
    s = re.sub(r"[^a-z0-9\s]", " ", s)
    s = re.sub(r"\s+", " ", s).strip()
    return s


def _extract_title_and_headings(md: str) -> Tuple[Optional[str], List[str]]:
    title: Optional[str] = None
    headings: List[str] = []
    for raw in (md or "").splitlines():
        line = raw.strip()
        if line.startswith("# "):
            h = line[2:].strip()
            if not title:
                title = h
            headings.append(h)
        elif line.startswith("## "):
            headings.append(line[3:].strip())
        elif line.startswith("### "):
            headings.append(line[4:].strip())
    return title, headings


def _score_query_against_note(query: str, note_id: str) -> Tuple[int, Dict[str, str]]:
    path = _note_path(note_id)
    try:
        with open(path, "r", encoding="utf-8") as f:
            md = f.read()
    except Exception:
        return 0, {"id": note_id}

    title, headings = _extract_title_and_headings(md)

    # Derive slug part from note_id like: slug-<ts>-<short>
    slug_part = note_id
    parts = note_id.rsplit("-", 2)
    if len(parts) == 3:
        slug_part = parts[0]

    q = _normalize_text(query)
    scores: List[int] = []

    if title:
        scores.append(fuzz.token_set_ratio(q, _normalize_text(title)))
    if headings:
        best_heading = max((fuzz.token_set_ratio(q, _normalize_text(h)) for h in headings), default=0)
        scores.append(best_heading)
    if slug_part:
        scores.append(fuzz.token_set_ratio(q, _normalize_text(slug_part)))

    # Also compare with first 300 chars of body (after dropping headings)
    body = []
    for raw in md.splitlines():
        if raw.strip().startswith("#"):
            continue
        body.append(raw)
        if len(body) > 60:
            break
    body_text = _normalize_text(" ".join(body))[:300]
    if body_text:
        scores.append(fuzz.token_set_ratio(q, body_text))

    score = max(scores) if scores else 0
    return score, {
        "id": note_id,
        "title": title or None,
        "slug": slug_part,
        "path": path,
    }


def search_notes(query: str, limit: int = 5) -> List[Dict[str, object]]:
    """Return best-matching notes for a textual query using fuzzy scoring.

    Each result includes: id, score, title, path, slug.
    """
    results: List[Tuple[int, Dict[str, str]]] = []
    for name in os.listdir(NOTES_DIR):
        if not name.endswith(".md"):
            continue
        nid = name[:-3]
        score, meta = _score_query_against_note(query, nid)
        results.append((score, meta))

    results.sort(key=lambda x: x[0], reverse=True)
    out: List[Dict[str, object]] = []
    for s, meta in results[:limit]:
        m = dict(meta)
        m["score"] = int(s)
        out.append(m)
    return out


def find_existing_note_for_topic(topic: str, threshold: int = 82) -> Optional[Dict[str, object]]:
    """Find a cached note for a topic if similarity exceeds threshold.

    Returns dict with: id, score, title, path; or None if no match.
    """
    candidates = search_notes(topic, limit=3)
    if not candidates:
        return None
    best = candidates[0]
    if best.get("score", 0) >= threshold:
        return best
    return None

# --- PDF rendering ---

try:
    from markdown import markdown as md_to_html
except Exception:  # pragma: no cover - optional if headless is used
    md_to_html = None
try:
    from xhtml2pdf import pisa
except Exception:  # pragma: no cover - optional if headless is used
    pisa = None
try:
    from playwright.sync_api import sync_playwright
except Exception:  # pragma: no cover - optional
    sync_playwright = None


BASE_CSS = """
<style>
  @page { size: A4; margin: 1.2cm; }
  body { font-family: DejaVu Sans, Arial, sans-serif; font-size: 11pt; color: #111827; }
  h1, h2, h3 { color: #0f172a; }
  h1 { font-size: 22pt; margin: 0 0 10px 0; }
  h2 { font-size: 16pt; margin: 14px 0 8px 0; }
  h3 { font-size: 13pt; margin: 10px 0 6px 0; }
  p, li { line-height: 1.4; }
  pre, code { font-family: DejaVu Sans Mono, Consolas, monospace; font-size: 9pt; }
  pre { background: #0b1020; color: #e5e7eb; padding: 10px; border-radius: 6px; }
  code { background: #eef1ff; padding: 1px 3px; border-radius: 4px; }
  table { width: 100%; border-collapse: collapse; margin: 8px 0; }
  th, td { border: 1px solid #e5e7eb; padding: 6px; }
  blockquote { border-left: 3px solid #93c5fd; padding-left: 8px; color: #374151; }
  .small { color: #6b7280; font-size: 9pt; }
</style>
"""


def markdown_to_html(markdown_text: str) -> str:
    if md_to_html is None:
        raise RuntimeError("markdown library not installed")
    html_body = md_to_html(markdown_text or "", extensions=[
        'extra', 'admonition', 'codehilite', 'tables', 'toc'
    ])
    return f"<html><head>{BASE_CSS}</head><body>{html_body}</body></html>"


def render_pdf_from_markdown(markdown_text: str) -> bytes:
    """Render a PDF byte stream from a Markdown string.

    Uses markdownâ†’HTML and xhtml2pdf. Returns PDF bytes or raises RuntimeError on failure.
    """
    if pisa is None:
        raise RuntimeError("xhtml2pdf not installed")
    html = markdown_to_html(markdown_text)
    pdf_io = io.BytesIO()
    result = pisa.CreatePDF(io.StringIO(html), dest=pdf_io, encoding='utf-8')
    if result.err:
        raise RuntimeError("Failed to generate PDF")
    return pdf_io.getvalue()


# ---------------- WYSIWYG via headless Chromium (best for KaTeX/Mermaid) ----------------

def render_pdf_via_headless(url: str, wait_selector: str = "#output", timeout_ms: int = 20000) -> bytes:
    """Open a URL in headless Chromium and print to PDF. Requires playwright + browsers installed.

    Install: pip install playwright; playwright install
    """
    if sync_playwright is None:
        raise RuntimeError("playwright not installed")
    with sync_playwright() as p:
        browser = p.chromium.launch()
        try:
            page = browser.new_page()
            page.goto(url, wait_until="networkidle")
            # wait for primary content to appear
            try:
                page.wait_for_selector(wait_selector, timeout=timeout_ms)
            except Exception:
                pass
            # give KaTeX/mermaid a brief window to render
            try:
                page.wait_for_selector('.katex', timeout=3000)
            except Exception:
                time.sleep(0.5)
            pdf_bytes = page.pdf(format="A4", print_background=True, margin={
                'top': '0.6in', 'bottom': '0.6in', 'left': '0.6in', 'right': '0.6in'
            })
            return pdf_bytes
        finally:
            browser.close()


def render_pdf_from_markdown_via_headless(markdown_text: str, title: str = "Notes") -> bytes:
    """Render a PDF by loading a minimal HTML page that renders MD + KaTeX client-side.

    Uses Playwright to set page content with CDN assets for Marked, DOMPurify, KaTeX.
    """
    if sync_playwright is None:
        raise RuntimeError("playwright not installed")
    md_json = json.dumps(markdown_text or "")
    title_esc = json.dumps(title or "Notes")
    html = f"""
<!doctype html>
<html>
  <head>
    <meta charset=\"utf-8\" />
    <title>{title_esc}</title>
    {BASE_CSS}
    <style>
      body {{ background: #ffffff; }}
      main {{ max-width: 800px; margin: 0 auto; }}
    </style>
    <link rel=\"stylesheet\" href=\"https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/katex.min.css\" />
    <script src=\"https://cdn.jsdelivr.net/npm/marked/marked.min.js\"></script>
    <script src=\"https://cdn.jsdelivr.net/npm/dompurify@3.1.6/dist/purify.min.js\"></script>
    <script defer src=\"https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/katex.min.js\"></script>
    <script defer src=\"https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/contrib/auto-render.min.js\"></script>
  </head>
  <body>
    <main>
      <article id=\"output\"></article>
    </main>
    <script>
      const md = {md_json};
      const html = DOMPurify.sanitize(marked.parse(md));
      const out = document.getElementById('output');
      out.innerHTML = html;
      function doRenderMath() {{
        if (typeof renderMathInElement === 'function') {{
          renderMathInElement(out, {{
            delimiters: [
              {{ left: '$$', right: '$$', display: true }},
              {{ left: '$', right: '$', display: false }},
              {{ left: '\\(', right: '\\)', display: false }},
              {{ left: '\\[', right: '\\]', display: true }}
            ],
            throwOnError: false,
            ignoredTags: ['script','noscript','style','textarea','pre','code']
          }});
        }} else {{ setTimeout(doRenderMath, 50); }}
      }}
      doRenderMath();
    </script>
  </body>
</html>
"""
    with sync_playwright() as p:
        browser = p.chromium.launch()
        try:
            page = browser.new_page()
            page.set_content(html, wait_until="load")
            try:
                page.wait_for_selector('.katex', timeout=5000)
            except Exception:
                time.sleep(0.5)
            pdf_bytes = page.pdf(format="A4", print_background=True, margin={
                'top': '0.6in', 'bottom': '0.6in', 'left': '0.6in', 'right': '0.6in'
            })
            return pdf_bytes
        finally:
            browser.close()

# --- Notes generation ---



notes_logger = logging.getLogger("paperx.notes")
if not notes_logger.handlers:
    handler = logging.StreamHandler(sys.stdout)
    fmt = logging.Formatter("%(asctime)s [%(levelname)s] %(name)s: %(message)s")
    handler.setFormatter(fmt)
    notes_logger.addHandler(handler)
notes_logger.setLevel(logging.INFO)

learning_logger = logging.getLogger("paperx.learning_tracks")
if not learning_logger.handlers:
    l_handler = logging.StreamHandler(sys.stdout)
    l_fmt = logging.Formatter("%(asctime)s [%(levelname)s] %(name)s: %(message)s")
    l_handler.setFormatter(l_fmt)
    learning_logger.addHandler(l_handler)
learning_logger.setLevel(logging.INFO)

OPENAI_MODEL = os.getenv("OPENAI_MODEL", "gpt-4o-mini")
OPENAI_API_KEY = (os.getenv("OPENAI_API_KEY", "") or "").strip()
SERPAPI_API_KEY = (os.getenv("SERPAPI_API_KEY", "") or "").strip()
SERPAPI_ENABLED = os.getenv("ENABLE_SERPAPI", "true").strip().lower() in {"1", "true", "yes", "on"}
SERPAPI_TIMEOUT_SEC = float(os.getenv("SERPAPI_TIMEOUT_SEC", "8"))
GEMINI_API_KEY = (os.getenv("GEMINI_API_KEY", "") or "").strip()
GEMINI_NOTES_MODEL = os.getenv("GEMINI_NOTES_MODEL", "gemini-2.5-flash")
MAX_TRANSCRIPT_CHARS_FOR_NOTES = int(os.getenv("TRANSCRIPT_NOTES_MAX_CHARS", "20000"))

# Default domains for notes/web search when DB has no config yet
DEFAULT_ALLOWED_DOMAINS = [
    "geeksforgeeks.org",
    "tutorialspoint.com",
    "scaler.com",
    "byjus.com",
    "wikipedia.org",
    "tpointtech.com",
]

# Table to store degree-specific allowed domains
DEGREE_ALLOWED_DOMAINS_TABLE = os.getenv("DEGREE_ALLOWED_DOMAINS_TABLE", "degree_allowed_domains")


LEARNING_TRACK_DEFAULT_LANGUAGES = [
    "en",
    "ta",
    "hi",
    "es",
]

LEARNING_TRACK_DEFAULT_STACKS = [
    "python",
    "javascript",
    "java",
    "dsa",
    "react",
]

LEARNING_TRACK_DEFAULT_GOALS = [
    "company_interview",
    "full_stack",
    "semester_prep",
]

LEARNING_TRACK_DEFAULT_COMPANIES = [
    "tcs",
    "zoho",
    "infosys",
    "product",
]

LEARNING_TRACK_DEFAULT_COMPILER_LANGUAGES = [
    {"id": "python", "runtime": "python3", "name": "Python 3"},
    {"id": "javascript", "runtime": "javascript", "name": "JavaScript (Node.js)"},
    {"id": "java", "runtime": "java", "name": "Java 17"},
    {"id": "cpp", "runtime": "cpp", "name": "C++ 17"},
    {"id": "go", "runtime": "go", "name": "Go"},
]


def _parse_env_list(name: str, default: List[str]) -> List[str]:
    raw = os.getenv(name, "")
    if not raw:
        return list(default)
    try:
        parts = [item.strip() for item in re.split(r"[,|]", raw) if item.strip()]
    except Exception:
        return list(default)
    return parts or list(default)


def _parse_env_json_array(name: str, default: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    raw = os.getenv(name, "").strip()
    if not raw:
        return [dict(item) for item in default]
    try:
        data = json.loads(raw)
        if isinstance(data, list):
            cleaned: List[Dict[str, Any]] = []
            for entry in data:
                if isinstance(entry, dict) and entry.get("id") and entry.get("runtime"):
                    cleaned.append({
                        "id": str(entry["id"]).strip(),
                        "runtime": str(entry["runtime"]).strip(),
                        "name": str(entry.get("name") or entry["id"]).strip(),
                    })
            if cleaned:
                return cleaned
    except Exception:
        pass
    return [dict(item) for item in default]


@lru_cache()
def load_learning_tracks_config() -> Dict[str, Any]:
    allowed_domains = _parse_env_list("LEARNING_TRACK_ALLOWED_DOMAINS", DEFAULT_ALLOWED_DOMAINS)
    languages = _parse_env_list("LEARNING_TRACK_LANGUAGES", LEARNING_TRACK_DEFAULT_LANGUAGES)
    stacks = _parse_env_list("LEARNING_TRACK_STACKS", LEARNING_TRACK_DEFAULT_STACKS)
    goals = _parse_env_list("LEARNING_TRACK_GOALS", LEARNING_TRACK_DEFAULT_GOALS)
    companies = _parse_env_list("LEARNING_TRACK_COMPANIES", LEARNING_TRACK_DEFAULT_COMPANIES)
    compiler_languages = _parse_env_json_array("LEARNING_TRACK_COMPILER_LANGUAGES", LEARNING_TRACK_DEFAULT_COMPILER_LANGUAGES)
    planner_model = os.getenv("LEARNING_TRACK_PLANNER_MODEL", os.getenv("GEMINI_PLANNER_MODEL", "gemini-2.5-flash"))
    flashcard_model = os.getenv("LEARNING_TRACK_FLASHCARD_MODEL", GEMINI_NOTES_MODEL)
    code_explainer_model = os.getenv("LEARNING_TRACK_CODE_MODEL", GEMINI_NOTES_MODEL)
    mcq_model = os.getenv("LEARNING_TRACK_MCQ_MODEL", GEMINI_NOTES_MODEL)
    return {
        "languages": languages,
        "stacks": stacks,
        "goals": goals,
        "companies": companies,
        "allowed_domains": allowed_domains,
        "compiler_languages": compiler_languages,
        "planner_model": planner_model,
        "flashcard_model": flashcard_model,
        "code_explainer_model": code_explainer_model,
        "mcq_model": mcq_model,
        "notes_model": GEMINI_NOTES_MODEL,
    }

HEADERS = {
    "User-Agent": "Mozilla/5.0 (PaperX; +https://example.com) AppleWebKit/537.36 "
                  "(KHTML, like Gecko) Chrome/124.0 Safari/537.36"
}

REQ_TIMEOUT = 25
MAX_FETCH_BYTES = int(os.getenv("PAPERX_FETCH_MAX_BYTES", "5000000"))  # 5 MB safety cap per fetch


@dataclass
class SectionChunk:
    title: str
    text: str
    url: str
    quote: str = ""


@dataclass
class PageExtract:
    url: str
    title: str
    sections: List[SectionChunk] = field(default_factory=list)


def _normalize_degree_key(degree: Optional[str]) -> Optional[str]:
    if not degree:
        return None
    s = re.sub(r"[^a-zA-Z0-9]+", "", degree).upper()
    return s or None


def db_get_allowed_domains_for_degree(degree: Optional[str]) -> List[str]:
    """Fetch enabled allowed domains for the normalized degree key.

    Returns empty list if none configured (caller may fallback to DEFAULT).
    """
    key = _normalize_degree_key(degree)
    if not key:
        return []
    supabase = get_service_client()
    try:
        res = (
            supabase.table(DEGREE_ALLOWED_DOMAINS_TABLE)
            .select("domain,enabled")
            .eq("degree_key", key)
            .eq("enabled", True)
            .order("domain")
            .execute()
        )
        rows = getattr(res, "data", []) or []
        out = []
        for r in rows:
            d = (r.get("domain") or "").strip()
            if d:
                out.append(d)
        return out
    except Exception:
        return []


def is_allowed(url: str, allowed_domains: Optional[List[str]] = None) -> bool:
    """Check if URL's host matches any allowed domain.

    If allowed_domains is empty/None, allow all (no restriction).
    """
    try:
        host = urlparse(url).netloc.lower()
        domains = allowed_domains or []
        if not domains:
            return True
        return any(host.endswith(d) for d in domains)
    except Exception:
        return False


def serpapi_search(topic: str, num: int = 10, *, degree: Optional[str] = None, allowed_domains: Optional[List[str]] = None) -> List[str]:
    """Search Google via SerpAPI.

    Preference order for domain restriction:
      1) allowed_domains param if provided (non-empty)
      2) domains fetched from DB by degree
      3) DEFAULT_ALLOWED_DOMAINS

    Fallback: If no URLs matched the domain filter, retry WITHOUT any site filter (top web results).
    """
    if not SERPAPI_ENABLED:
        notes_logger.info("SerpAPI disabled; skipping search", extra={"topic": topic})
        return []
    if not SERPAPI_API_KEY:
        notes_logger.warning("SerpAPI key missing; skipping search", extra={"topic": topic})
        return []
    # Resolve domains according to priority
    domains: List[str] = []
    if allowed_domains:
        domains = [d for d in allowed_domains if d]
    elif degree:
        domains = db_get_allowed_domains_for_degree(degree)
    if not domains:
        domains = list(DEFAULT_ALLOWED_DOMAINS)

    site_filter = " OR ".join([f"site:{d}" for d in domains]) if domains else ""
    q = f"{topic} ({site_filter})" if site_filter else topic
    params = {
        "engine": "google",
        "q": q,
        "num": min(20, max(5, num)),
        "hl": "en",
        "safe": "active",
        "api_key": SERPAPI_API_KEY,
    }
    notes_logger.info("SerpAPI search start", extra={
        "topic": topic,
        "query": q,
        "allowed_domains": domains,
        "api_key_present": bool(SERPAPI_API_KEY),
    })
    def _fetch(p):
        return GoogleSearch(p).get_dict()

    results: Dict[str, Any] = {}
    try:
        with concurrent.futures.ThreadPoolExecutor(max_workers=1) as pool:
            fut = pool.submit(_fetch, params)
            results = fut.result(timeout=SERPAPI_TIMEOUT_SEC)
    except concurrent.futures.TimeoutError:
        notes_logger.error("SerpAPI search timed out", extra={"topic": topic, "timeout_sec": SERPAPI_TIMEOUT_SEC})
        return []
    except Exception as exc:
        notes_logger.error("SerpAPI search failed", exc_info=exc)
        return []
    urls: List[str] = []
    if not results:
        notes_logger.warning("SerpAPI returned empty response", extra={"topic": topic})
    else:
        err = results.get("error")
        if err:
            notes_logger.error("SerpAPI error for topic '%s': %s", topic, err)
        else:
            notes_logger.debug(
                "SerpAPI search metadata",
                extra={
                    "topic": topic,
                    "organic_count": len(results.get("organic_results") or []),
                    "related_questions": len(results.get("related_questions") or []),
                },
            )

            for item in (results.get("organic_results") or []):
                link = item.get("link")
                if link and is_allowed(link, domains):
                    urls.append(link)
            # Also parse related if available
            for item in (results.get("related_questions") or []):
                for src in (item.get("sources") or []):
                    link = src.get("link")
                    if link and is_allowed(link, domains):
                        urls.append(link)

    # Dedup preserve order
    seen: Set[str] = set()
    filtered: List[str] = []
    for u in urls:
        if u not in seen:
            seen.add(u)
            filtered.append(u)

    if filtered:
        notes_logger.info(
            "SerpAPI search success",
            extra={"topic": topic, "selected_urls": filtered[: min(3, len(filtered))], "total": len(filtered), "domains": domains},
        )
        return filtered[:num]

    # Fallback: re-run without site restriction to get top results
    notes_logger.warning("No URLs matched allowed domains; falling back to unrestricted search", extra={"topic": topic})
    try:
        with concurrent.futures.ThreadPoolExecutor(max_workers=1) as pool:
            fut2 = pool.submit(_fetch, {**params, "q": topic})
            results2 = fut2.result(timeout=SERPAPI_TIMEOUT_SEC)
    except concurrent.futures.TimeoutError:
        notes_logger.error("SerpAPI fallback timed out", extra={"topic": topic, "timeout_sec": SERPAPI_TIMEOUT_SEC})
        return []
    except Exception:
        return []
    urls2: List[str] = []
    for item in (results2.get("organic_results") or []):
        link = item.get("link")
        if link:
            urls2.append(link)
    # dedup preserve order
    seen2: Set[str] = set()
    out2: List[str] = []
    for u in urls2:
        if u not in seen2:
            seen2.add(u)
            out2.append(u)
    return out2[:num]


def fetch(url: str) -> str:
    """Fetch a URL with a strict size cap to avoid memory blow-ups from large assets.

    Uses streaming downloads and enforces a configurable max byte budget
    (PAPERX_FETCH_MAX_BYTES, default 5 MB). Raises RuntimeError on size
    overflow so callers can treat it as a fetch failure.
    """
    with requests.get(url, headers=HEADERS, timeout=REQ_TIMEOUT, stream=True) as r:
        r.raise_for_status()

        # Fast reject if Content-Length header is present and too large
        try:
            content_len = int(r.headers.get("Content-Length", "0"))
            if content_len and content_len > MAX_FETCH_BYTES:
                raise RuntimeError(f"Remote content too large ({content_len} bytes)")
        except ValueError:
            # Ignore malformed header; fall back to streamed limit below
            pass

        chunks: List[bytes] = []
        total = 0
        for chunk in r.iter_content(chunk_size=8192):
            if not chunk:
                continue
            total += len(chunk)
            if total > MAX_FETCH_BYTES:
                raise RuntimeError(f"Remote content exceeded max size ({MAX_FETCH_BYTES} bytes cap)")
            chunks.append(chunk)

    body = b"".join(chunks)
    encoding = r.encoding or "utf-8"
    try:
        return body.decode(encoding, errors="replace")
    except Exception:
        return body.decode("utf-8", errors="replace")


# -------------------- Image helpers --------------------
LOGO_HINTS = [
    "logo", "favicon", "icon", "sprite", "brandmark", "watermark",
    "placeholder", "default", "avatar", "badge", "mark"
]

def looks_like_logo(url: str) -> bool:
    u = url.lower()
    if any(h in u for h in LOGO_HINTS):
        return True
    # very small images by filename hints
    if re.search(r"[=_-](?:16|24|32|48|64)(?:x(?:16|24|32|48|64))?\.(?:png|jpg|jpeg|gif|webp)$", u):
        return True
    return False

def is_reasonable_image_url(url: str) -> bool:
    if not url:
        return False
    if looks_like_logo(url):
        return False
    return any(url.lower().endswith(ext) for ext in [".png", ".jpg", ".jpeg", ".webp", ".gif", ".svg"])

def serpapi_image_urls(topic: str, num: int = 10) -> List[str]:
    params = {
        "engine": "google",
        "q": topic,
        "tbm": "isch",
        "num": min(20, max(5, num)),
        "safe": "active",
        "api_key": SERPAPI_API_KEY,
        "hl": "en",
    }
    try:
        search = GoogleSearch(params)
        results = search.get_dict()
    except Exception:
        return []
    urls: List[str] = []
    for item in (results.get("images_results") or []):
        link = item.get("original") or item.get("thumbnail") or item.get("link")
        if link and is_reasonable_image_url(link):
            urls.append(link)
    # dedup
    seen = set()
    out: List[str] = []
    for u in urls:
        if u not in seen:
            seen.add(u)
            out.append(u)
    return out[:num]


def normalize_text(txt: str) -> str:
    txt = re.sub(r"\s+", " ", txt).strip()
    return txt


def extract_sections_from_html(url: str, html: str) -> PageExtract:
    """Extract title and sectioned text by H1/H2/H3."""
    soup = BeautifulSoup(html, "html.parser")
    title = soup.title.get_text(strip=True) if soup.title else url
    # remove nav/aside/footer/scripts
    for tag in soup(["script", "style", "noscript", "header", "footer", "nav", "form", "aside"]):
        tag.decompose()

    # identify content area heuristically
    main = soup.find(["article", "main"]) or soup.body or soup

    # Gather headings and their content
    sections: List[SectionChunk] = []
    headings = main.find_all(["h1", "h2", "h3"])
    if not headings:
        # fallback: big paragraphs
        text = normalize_text(main.get_text(" "))
        if text:
            sections.append(SectionChunk(title="Content", text=text, url=url))
        return PageExtract(url=url, title=title, sections=sections)

    for i, h in enumerate(headings):
        h_title = normalize_text(h.get_text(" "))
        content_parts = []
        for sib in h.next_siblings:
            if getattr(sib, "name", None) in ["h1", "h2", "h3"]:
                break
            if getattr(sib, "name", None) in ["p", "ul", "ol", "pre", "code", "table", "div"]:
                content_parts.append(sib.get_text(" ", strip=True))
            elif isinstance(sib, str):
                content_parts.append(str(sib).strip())
        content = normalize_text(" ".join([c for c in content_parts if c]))
        if h_title and content:
            # extract a short quote (first 200 chars) to attach as citation span
            quote = content[:200]
            sections.append(SectionChunk(title=h_title, text=content, url=url, quote=quote))
    return PageExtract(url=url, title=title, sections=sections)


def extract_image_urls_from_html(url: str, html: str, max_images: int = 10) -> List[str]:
    soup = BeautifulSoup(html, "html.parser")
    # remove non-content containers to reduce boilerplate images
    for tag in soup(["script", "style", "noscript", "header", "footer", "nav", "form", "aside"]):
        tag.decompose()
    images: List[str] = []
    for img in soup.find_all("img"):
        src = img.get("src") or img.get("data-src") or img.get("data-original")
        if not src:
            continue
        # resolve relative URLs
        if src.startswith("//"):
            src = f"https:{src}"
        elif src.startswith("/"):
            parsed = urlparse(url)
            src = f"{parsed.scheme}://{parsed.netloc}{src}"
        alt_text = (img.get("alt") or "").lower()
        if any(k in alt_text for k in ["logo", "icon", "favicon"]):
            continue
        if is_reasonable_image_url(src):
            images.append(src)
        if len(images) >= max_images:
            break
    # dedup preserve order
    seen = set()
    out: List[str] = []
    for u in images:
        if u not in seen:
            seen.add(u)
            out.append(u)
    return out[:max_images]


def unify_section_titles(pages: List[PageExtract]) -> List[str]:
    """
    Build a merged set of section titles observed across pages.
    Use fuzzy grouping to avoid duplicates (e.g., 'Advantages' vs 'Pros').
    """
    titles = []
    for p in pages:
        for s in p.sections:
            titles.append(s.title)

    # fuzzy dedup
    merged: List[str] = []
    for t in titles:
        if not t:
            continue
        if not merged:
            merged.append(t)
            continue
        scores = [fuzz.token_set_ratio(t.lower(), m.lower()) for m in merged]
        if max(scores) >= 85:
            # treat as same; skip adding
            continue
        merged.append(t)

    # Always ensure compulsory blocks exist as "virtual" targets
    compulsory = ["Introduction", "TL;DR", "Examples", "Conclusion", "Memory Aids", "Common Mistakes"]
    for c in compulsory:
        if all(fuzz.token_set_ratio(c.lower(), m.lower()) < 85 for m in merged):
            merged.append(c)
    return merged


def assemble_context_for_llm(pages: List[PageExtract], merged_titles: List[str], topic: str) -> str:
    """
    Build a compact, source-quoted context the model can use.
    """
    lines = [f"Topic: {topic}", "", "SOURCE EXCERPTS (keep factual grounding):"]
    for p in pages:
        lines.append(f"\n### {p.title}\nURL: {p.url}")
        for s in p.sections[:8]:  # keep compact
            lines.append(f"- [{s.title}] {s.quote}â€¦")
    lines.append("\nMERGED SECTION TITLES CANDIDATE ORDER:")
    for t in merged_titles:
        lines.append(f"- {t}")
    return "\n".join(lines)


SYSTEM_INSTRUCTIONS = """You are a senior educational writer building accurate, well-structured notes for college students in India.

CRITICAL RULES:
- Use ONLY the source excerpts provided; do not invent facts. If a fact is not supported, mark it as [needs review].
- Respect section headings actually observed on the referenced pages. You may merge similar headings (e.g., Advantages/Pros).
- You MUST also include these blocks even if not present: Introduction, TL;DR in short simple points, Examples, Conclusion, Memory Aids, Common Mistakes.
- Keep explanations concise but complete; use bullet points where helpful.
- Include at least one Mermaid diagram when process/relationships are relevant.
- Every non-obvious claim MUST carry an inline citation like [GFG], [TP], [Scaler], [Wiki], or [TPT] mapped in the CITATIONS section.
- Prefer plain text + Mermaid diagrams; do not embed external images.
 - Bold important keywords, symbols, and technical terms using Markdown **double asterisks**. Examples: **epsilon-greedy (Îµ-greedy)**, **Markov Decision Process (MDP)**, parameters like **Î¸**, **Î³**, **Î±**, algorithm names like **Q-learning**.

OUTPUT FORMAT (STRICT):
Return a single Markdown document with:
1) A title line: '# <Topic>'
2) For each merged section title (after light normalization), a '## <Section>' block
3) Use bullet points, short paragraphs, tables when appropriate (GitHub MD)
4) Mermaid diagram(s) in fenced code blocks: ```mermaid ... ```
5) A final '## CITATIONS' list mapping labels to URLs with short quoted spans

If sources contradict, mark the line with [conflict] and keep both with citations.

Keep it under ~1200â€“1500 words unless the topic is inherently longer.
"""


def build_agent() -> AssistantAgent:
    model_client = gemini_model_client
    assistant = AssistantAgent(
        "paperx_notes_agent",
        model_client=model_client,
        system_message=SYSTEM_INSTRUCTIONS,
    )
    return assistant


# --- Helper: safely run async assistant from sync code (works inside running event loop) ---

def _run_assistant_blocking(assistant: AssistantAgent, user_prompt: str):
    async def _coro():
        return await assistant.run(task=user_prompt)
    try:
        # If we're in a running loop (e.g., FastAPI), offload to a thread
        asyncio.get_running_loop()
        import concurrent.futures
        with concurrent.futures.ThreadPoolExecutor(max_workers=1) as ex:
            return ex.submit(lambda: asyncio.run(_coro())).result()
    except RuntimeError:
        # No running loop (CLI), safe to use asyncio.run directly
        return asyncio.run(_coro())


def generate_notes_markdown(topic: str, *, degree: Optional[str] = None) -> str:
    # 1) Search
    notes_logger.info("generate_notes_markdown:start", extra={"topic": topic})
    urls = serpapi_search(topic, num=10, degree=degree)
    if not urls:
        notes_logger.error("generate_notes_markdown:no_urls", extra={"topic": topic})
        raise RuntimeError("No results from allowed domains.")

    # 2) Fetch & extract
    pages: List[PageExtract] = []
    for u in urls[:6]:  # keep it tight
        try:
            html = fetch(u)
            pages.append(extract_sections_from_html(u, html))
            notes_logger.debug("Fetched page", extra={"topic": topic, "url": u})
        except Exception as e:
            # ignore broken pages
            notes_logger.warning(
                "Fetch failed", extra={"topic": topic, "url": u, "error": str(e)}
            )
            continue

    if not pages:
        notes_logger.error("generate_notes_markdown:no_pages", extra={"topic": topic})
        raise RuntimeError("Failed to extract any pages.")

    # 3) Merge section titles from sources
    merged_titles = unify_section_titles(pages)
    notes_logger.debug(
        "Merged titles",
        extra={"topic": topic, "titles": merged_titles[: min(5, len(merged_titles))]},
    )

    # 4) Build context for LLM
    context = assemble_context_for_llm(pages, merged_titles, topic)

    # 5) Call AutoGen Assistant
    assistant = build_agent()
    user_prompt = f"""
You will compose comprehensive, exam-ready Markdown notes for the topic "{topic}".

Context:
{context}

Instructions:
- Create DETAILED, THOROUGH notes - students need complete understanding for exams.
- Use the source context as foundation, but ADD your expert knowledge to fill gaps and provide complete coverage.
- Normalize section titles only lightly (e.g., "Applications" vs. "Use Cases" pick one).
- Include the compulsory sections even if they were not present in sources.
- Generate at least one mermaid diagram if suitable (e.g., flow of algorithm, hierarchy, pipeline).
- Build a final '## CITATIONS' mapping labels [GFG], [TPT], [Scaler], [Wiki], [TP] to URLs you used.
- Inline-cite like: "... property ... [GFG]" or "... step ... [Wiki]" after the sentence.
 - Bold important keywords/terms and symbols (e.g., Î¸, Î³, Î±, Îµ-greedy, key definitions) with **...** consistently; avoid over-bolding.

MANDATORY SECTIONS TO INCLUDE (if applicable to the topic):
- **TL;DR / Quick Summary**: Bullet points for quick revision
- **Introduction**: Comprehensive overview with context and importance
- **Need / Why It Is Required**: What problem does it solve? Why was it developed?
- **Definition / Core Concept**: Clear, precise technical definition of the "{topic}"

TARGET LENGTH: 1000-2000 words for comprehensive exam preparation.

Start with '# {topic}' and then the sections in a logical order.
"""
    # Use safe runner to support both CLI and FastAPI contexts
    result = _run_assistant_blocking(assistant, user_prompt)
    content = result.messages[-1].content
    notes_logger.info("generate_notes_markdown:success", extra={"topic": topic, "length": len(content)})
    return content


# ---------------- New: Streaming events generator for UI/API ----------------

def _build_variant_user_prompt(context: str, topic: str, variant: str) -> str:
    v = _normalize_variant(variant)
    if v == "cheatsheet":
        return f"""
You will compose an EXAM-READY CHEAT SHEET for the topic "{topic}" based ONLY on the source excerpts below.

Context:
{context}

Output rules (STRICT):
- Keep it ultra concise (â‰ˆ 250â€“400 words). Use bullets and tables.
- Start with a single H1: '# {topic} â€” Cheat Sheet'.
- Sections (H2):
  1) Core Concepts (5â€“10 bullets, crisp one-liners)
  2) Key Definitions & Formulas (bullets; inline math where relevant)
  3) Quick Steps / Algorithms (bulleted steps)
  4) Pitfalls / Gotchas (3â€“6 bullets)
  5) Keywords (comma-separated list)
- Bold key terms and symbols with **...**. Prefer compact phrasing over full sentences.
- If any fact is uncertain, mark [needs review].
 - Do NOT include sections titled 'TL;DR', 'Common Mistakes', or 'Memory Aids'.
 - Do NOT include a 'CITATIONS' section or any citation list.
""".strip()
    if v == "simple":
        return f"""
You will write a SIMPLE, EASY-TO-UNDERSTAND set of notes for "{topic}" using ONLY the source excerpts below.

Context:
{context}

Output rules (STRICT):
- Target length: 600â€“900 words, plain language, short sentences.
- Start with '# {topic} â€” Simple Notes'.
- Structure with logical H2 sections, including: Introduction, Concepts, Examples, TL;DR, Common Mistakes, Conclusion.
- Explain in everyday words without dumbing down definitions.
- Use bullets and small tables where helpful.
- Bold important terms with **...**.
- Include a final '## CITATIONS' section with labelâ†’URL list for the sources you used.
""".strip()
    # default detailed prompt - comprehensive and thorough
    return f"""
You will compose comprehensive, exam-ready Markdown notes for the topic "{topic}".

Context:
{context}

Instructions:
- Create DETAILED, THOROUGH notes - students need complete understanding for exams.
- Use the source context as foundation, but ADD your expert knowledge to fill gaps and provide complete coverage.
- Normalize section titles only lightly (e.g., "Applications" vs. "Use Cases" pick one).
- Include the compulsory sections even if they were not present in sources.
- Generate at least one mermaid diagram if suitable (e.g., flow of algorithm, hierarchy, pipeline).
- Build a final '## CITATIONS' mapping labels [GFG], [TPT], [Scaler], [Wiki], [TP] to URLs you used.
- Inline-cite like: "... property ... [GFG]" or "... step ... [Wiki]" after the sentence.
 - Bold important keywords/terms and symbols (e.g., Î¸, Î³, Î±, Îµ-greedy, key definitions) with **...** consistently; avoid over-bolding.

MANDATORY SECTIONS TO INCLUDE (if applicable to the topic):
- **TL;DR / Quick Summary**: Bullet points for quick revision
- **Introduction**: Comprehensive overview with context and importance
- **Need / Why It Is Required**: What problem does it solve? Why was it developed?
- **Definition / Core Concept**: Clear, precise technical definition of the "{topic}"

TARGET LENGTH: 1000-2000 words for comprehensive exam preparation.

Start with '# {topic}' and then the sections in a logical order.
""".strip()

def generate_notes_events(topic: str, *, stop_event: Optional[threading.Event] = None, variant: str = "detailed", degree: Optional[str] = None) -> Iterator[Tuple[str, Dict[str, Any]]]:
    """Yield (event_name, payload) tuples describing real-time progress and final output.

    Events emitted in order (names):
      - start
      - search_results
      - fetch_start / fetch_done / fetch_error
      - merged_titles
      - context_ready
      - llm_start / llm_done
      - images
      - final (includes markdown + image_urls)
      - error (early termination on fatal error)
    """
    try:
        if stop_event and stop_event.is_set():
            return
        dyn_domains = db_get_allowed_domains_for_degree(degree) if degree else []
        if not dyn_domains:
            dyn_domains = list(DEFAULT_ALLOWED_DOMAINS)
        yield ("start", {"topic": topic, "allowed_domains": dyn_domains, "degree": degree})
        if stop_event and stop_event.is_set():
            return
        urls = serpapi_search(topic, num=10, degree=degree)
        if stop_event and stop_event.is_set():
            return
        if not urls:
            yield ("error", {"message": "No results from allowed domains."})
            return
        yield ("search_results", {"urls": urls})

        # Fetch & extract
        pages: List[PageExtract] = []
        for u in urls[:6]:
            if stop_event and stop_event.is_set():
                return
            yield ("fetch_start", {"url": u})
            try:
                html = fetch(u)
                page = extract_sections_from_html(u, html)
                pages.append(page)
                yield ("fetch_done", {"url": u, "title": page.title, "sections": len(page.sections)})
            except Exception as e:
                yield ("fetch_error", {"url": u, "error": str(e)})

        if not pages:
            yield ("error", {"message": "Failed to extract any pages."})
            return

        if stop_event and stop_event.is_set():
            return
        merged_titles = unify_section_titles(pages)
        yield ("merged_titles", {"titles": merged_titles})

        if stop_event and stop_event.is_set():
            return
        context = assemble_context_for_llm(pages, merged_titles, topic)
        yield ("context_ready", {"chars": len(context)})

        assistant = build_agent()
        user_prompt = _build_variant_user_prompt(context, topic, variant)
        yield ("llm_start", {})
        try:
            # Use safe runner in case we're under FastAPI's loop
            result = _run_assistant_blocking(assistant, user_prompt)
            content = result.messages[-1].content
        except Exception as e:
            yield ("error", {"message": f"LLM error: {e}"})
            return
        yield ("llm_done", {"md_chars": len(content)})

        if stop_event and stop_event.is_set():
            return
        # Images from SERP + pages
        related_pages = urls[:8]
        image_urls = collect_image_urls(topic, related_pages, stop_event=stop_event)
        if stop_event and stop_event.is_set():
            return
        yield ("images", {"count": len(image_urls), "image_urls": image_urls})

        yield ("final", {"markdown": content, "image_urls": image_urls})
    except Exception as e:
        # last-resort catch to keep stream alive with an error
        yield ("error", {"message": str(e)})


def collect_image_urls(topic: str, page_urls: List[str], stop_event: Optional[threading.Event] = None) -> List[str]:
    if stop_event and stop_event.is_set():
        return []
    urls: List[str] = []
    # 1) From SerpAPI image search - REMOVED per user request
    # urls.extend(serpapi_image_urls(topic, num=12))
    if stop_event and stop_event.is_set():
        return urls
    # 2) From parsed webpages
    for u in page_urls[:6]:
        if stop_event and stop_event.is_set():
            break
        try:
            html = fetch(u)
        except Exception:
            continue
        urls.extend(extract_image_urls_from_html(u, html, max_images=6))
    # deduplicate while preserving order
    seen = set()
    out: List[str] = []
    for link in urls:
        if link not in seen:
            seen.add(link)
            out.append(link)
    return out[:20]


def save_md(topic: str, md_text: str) -> str:
    safe = re.sub(r"[^a-zA-Z0-9_.-]+", "_", topic.strip())[:80]
    fname = f"{safe}.md"
    with open(fname, "w", encoding="utf-8") as f:
        f.write(md_text)
    return os.path.abspath(fname)


def main():
    if len(sys.argv) < 2:
        print("Usage: python paperx_notes.py \"<topic>\"")
        sys.exit(1)

    topic = sys.argv[1].strip()
    degree = sys.argv[2].strip() if len(sys.argv) > 2 else None
    resolved_domains = db_get_allowed_domains_for_degree(degree) if degree else []
    if not resolved_domains:
        resolved_domains = list(DEFAULT_ALLOWED_DOMAINS)
    
    print(f"[Paper X] Generating notes for topic: {topic}\n"
          f"Degree: {degree or '-'}\n"
          f"Allowed domains: {', '.join(resolved_domains)}\n")

    md_text = generate_notes_markdown(topic, degree=degree)
    path = save_md(topic, md_text)
    print(md_text)
    # Collect related images without downloading (URLs only)
    related_pages = serpapi_search(topic, num=8, degree=degree)
    image_urls = collect_image_urls(topic, related_pages)
    if image_urls:
        print("\n---\nRelated image URLs (filtered, no logos):")
        for i, u in enumerate(image_urls, 1):
            print(f"{i}. {u}")
    print("\n---\nSaved:", path)


if __name__ == "__main__":
    main()

# --- Academics schemas ---



class BatchIn(BaseModel):
    from_year: int = Field(..., ge=1950, le=2100, alias="from")
    to_year: int = Field(..., ge=1950, le=2100, alias="to")

    @validator("to_year")
    def check_range(cls, v, values):
        f = values.get("from_year")
        if f is not None and v < f:
            raise ValueError("to_year must be >= from_year")
        return v


class DepartmentCreateIn(BaseModel):
    name: str
    batches: List[BatchIn]

    @validator("name")
    def normalize_name(cls, v: str):
        value = (v or "").strip()
        if not value:
            raise ValueError("Department name required.")
        return value.upper()

    @validator("batches")
    def ensure_batches(cls, v: List[BatchIn]):
        if not v:
            raise ValueError("Add at least one batch range.")
        seen: set[Tuple[int, int]] = set()
        unique: List[BatchIn] = []
        for batch in v:
            pair = (batch.from_year, batch.to_year)
            if pair in seen:
                continue
            seen.add(pair)
            unique.append(batch)
        return unique


class DegreeCreateIn(BaseModel):
    name: str = Field(..., min_length=2, max_length=256)
    level: Optional[str] = Field(None, max_length=64)
    duration_years: Optional[int] = Field(None, ge=1, le=10)
    departments: List[DepartmentCreateIn]

    @validator("name")
    def trim_degree_name(cls, v: str):
        value = (v or "").strip()
        if not value:
            raise ValueError("Degree name required.")
        return value

    @validator("departments")
    def ensure_departments(cls, v: List[DepartmentCreateIn]):
        if not v:
            raise ValueError("Add at least one department to the degree.")
        seen: set[str] = set()
        for dept in v:
            key = dept.name.upper()
            if key in seen:
                raise ValueError(f"Duplicate department '{dept.name}' in the same degree.")
            seen.add(key)
        return v


class CollegeCreateIn(BaseModel):
    college_name: str = Field(..., min_length=2, max_length=256)
    degrees: Optional[List[DegreeCreateIn]] = None
    departments: Optional[List[str]] = None  # legacy payload support
    batches: Optional[List[BatchIn]] = None  # legacy payload support

    @validator("college_name")
    def trim_college(cls, v: str):
        value = (v or "").strip()
        if not value:
            raise ValueError("College name required.")
        return value

    @root_validator(pre=True)
    def coerce_legacy_payload(cls, values):
        if not values:
            return values
        data = dict(values)
        if data.get("degrees"):
            return data
        legacy_departments = [
            (d or "").strip() for d in (data.get("departments") or []) if (d or "").strip()
        ]
        if legacy_departments:
            legacy_batches_raw = data.get("batches") or []
            if not legacy_batches_raw:
                raise ValueError("Provide batch ranges when using legacy departments payload.")
            coerced_batches = []
            for item in legacy_batches_raw:
                if isinstance(item, BatchIn):
                    coerced_batches.append(item.dict(by_alias=True))
                elif isinstance(item, dict):
                    if "from" in item and "to" in item:
                        coerced_batches.append({"from": item["from"], "to": item["to"]})
                    elif "from_year" in item and "to_year" in item:
                        coerced_batches.append({"from": item["from_year"], "to": item["to_year"]})
                    else:
                        raise ValueError("Invalid batch entry in legacy payload.")
                else:
                    raise ValueError("Invalid batch entry in legacy payload.")
            data["degrees"] = [
                {
                    "name": "B.Tech",
                    "level": None,
                    "duration_years": None,
                    "departments": [
                        {"name": dept, "batches": coerced_batches}
                        for dept in legacy_departments
                    ],
                }
            ]
            return data
        raise ValueError("Provide at least one degree with departments and batch ranges.")

    @validator("degrees")
    def ensure_degrees(cls, v: Optional[List[DegreeCreateIn]]):
        if not v:
            raise ValueError("At least one degree is required.")
        seen: set[str] = set()
        for degree in v:
            key = degree.name.strip().lower()
            if key in seen:
                raise ValueError(f"Duplicate degree '{degree.name}'.")
            seen.add(key)
        return v


class College(BaseModel):
    id: uuid.UUID
    name: str
    logo_url: Optional[str] = None


class CollegeNameOnlyIn(BaseModel):
    name: str = Field(..., min_length=2, max_length=256)

    @validator("name")
    def trim_name(cls, v: str):
        value = (v or "").strip()
        if not value:
            raise ValueError("College name required.")
        return value


class DepartmentWithBatchesOut(BaseModel):
    id: uuid.UUID
    name: str
    batches: List[BatchIn]


class DepartmentSimpleBase(BaseModel):
    name: str = Field(..., min_length=2, max_length=256)
    batches: Optional[List[BatchIn]] = None

    @validator("name")
    def normalize_name(cls, v: str):
        value = (v or "").strip()
        if not value:
            raise ValueError("Department name required.")
        return value.upper()

    @validator("batches")
    def dedupe_batches(cls, v: Optional[List[BatchIn]]):
        if v is None:
            return None
        seen: set[Tuple[int, int]] = set()
        unique: List[BatchIn] = []
        for batch in v:
            pair = (batch.from_year, batch.to_year)
            if pair in seen:
                continue
            seen.add(pair)
            unique.append(batch)
        return unique


class DepartmentSimpleCreateIn(DepartmentSimpleBase):
    pass


class DepartmentSimpleUpdateIn(DepartmentSimpleBase):
    pass


class DegreeOut(BaseModel):
    id: uuid.UUID
    name: str
    level: Optional[str] = None
    duration_years: Optional[int] = None
    departments: List[DepartmentWithBatchesOut]


class DegreeSimpleCreateIn(BaseModel):
    name: str = Field(..., min_length=2, max_length=256)
    level: Optional[str] = Field(None, max_length=64)
    duration_years: Optional[int] = Field(None, ge=1, le=10)

    @validator("name")
    def trim_name(cls, v: str):
        value = (v or "").strip()
        if not value:
            raise ValueError("Degree name required.")
        return value

    @validator("level")
    def normalize_level(cls, v: Optional[str]):
        if v is None:
            return None
        value = v.strip()
        return value or None


class CollegeFullOut(BaseModel):
    id: uuid.UUID
    name: str
    logo_url: Optional[str] = None
    degrees: List[DegreeOut]
    departments: List[str]
    batches: List[BatchIn]


class DepartmentOut(BaseModel):

    id: uuid.UUID
    name: str


class BatchWithIdOut(BaseModel):
    id: uuid.UUID
    from_year: int
    to_year: int


class UserAuth(BaseModel):
    email: str
    password: str


class SignupFullIn(BaseModel):
    name: str
    gender: Optional[str] = Field(None, pattern=r"^(female|male|other)$")
    phone: Optional[str] = Field(None, min_length=10, max_length=15)
    email: str
    password: str
    college: str
    department: str
    batch_from: int = Field(..., ge=1950, le=2100)
    batch_to: int = Field(..., ge=1950, le=2100)
    semester: int = Field(..., ge=1, le=12)
    regno: str

    @validator("department")
    def uppercase_dept(cls, v):
        return (v or "").upper()

    @validator("regno")
    def uppercase_regno(cls, v):
        return (v or "").upper()

    @validator("batch_to")
    def batch_years_valid(cls, v, values):
        f = values.get("batch_from")
        if f and v < f:
            raise ValueError("batch_to must be >= batch_from")
        return v


class TopicIn(BaseModel):
    topic: str = Field(..., min_length=1)

    @validator("topic", pre=True)
    def _strip_topic(cls, value: Any):  # noqa: N805
        if value is None:
            raise ValueError("Topic cannot be empty")
        if isinstance(value, str):
            trimmed = value.strip()
            if not trimmed:
                raise ValueError("Topic cannot be empty")
            return trimmed
        trimmed = str(value).strip()
        if not trimmed:
            raise ValueError("Topic cannot be empty")
        return trimmed


class TopicUpsertIn(TopicIn):
    """Topic payload for unit create/update.

    When `id` is provided, the server updates that existing topic row.
    When `id` is omitted, the server inserts a new topic row.
    """

    id: Optional[uuid.UUID] = None


class UnitIn(BaseModel):
    unit_title: str = Field(..., min_length=1)
    topics: List[TopicIn]

    @validator("topics")
    def non_empty_topics(cls, v):
        return v or []


class SyllabusCourseIn(BaseModel):
    batch_id: uuid.UUID
    semester: int = Field(..., ge=1, le=12)
    course_code: str = Field(..., min_length=1, max_length=64)
    title: str = Field(..., min_length=1, max_length=256)
    type: Optional[str] = Field(default="practical")
    units: List[UnitIn]

    @validator("type", pre=True, always=True)
    def _normalize_type(cls, value: Any):  # noqa: N805
        allowed = {"maths", "theorey", "practical"}
        if value is None:
            return "practical"
        if isinstance(value, str):
            v = value.strip().lower()
            return v if v in allowed else "practical"
        return "practical"


class TopicOut(BaseModel):
    id: uuid.UUID
    topic: str
    order_in_unit: int
    image_url: Optional[str] = None
    video_url: Optional[str] = None
    ppt_url: Optional[str] = None
    lab_url: Optional[str] = None
    unit_id: Optional[uuid.UUID] = None
    course_id: Optional[uuid.UUID] = None
    course_type: Optional[str] = None


class UnitOut(BaseModel):
    id: uuid.UUID
    unit_title: str
    order_in_course: int
    topics: List[TopicOut]


class SyllabusCourseOut(BaseModel):
    id: uuid.UUID
    batch_id: uuid.UUID
    semester: int
    course_code: str
    title: str
    type: Optional[str] = None
    units: List[UnitOut]


class SyllabusCourseSummaryOut(BaseModel):
    id: uuid.UUID
    batch_id: uuid.UUID
    semester: int
    course_code: str
    title: str
    type: Optional[str] = None


class SyllabusCourseSimpleBase(BaseModel):
    semester: int = Field(..., ge=1, le=12)
    course_code: str = Field(..., min_length=1, max_length=64)
    title: str = Field(..., min_length=1, max_length=256)
    type: Optional[str] = Field(default="practical")

    @validator("course_code", "title", pre=True)
    def _strip_text(cls, value: Any):  # noqa: N805
        if value is None:
            raise ValueError("Value is required")
        if isinstance(value, str):
            trimmed = value.strip()
            if not trimmed:
                raise ValueError("Value is required")
            return trimmed
        return str(value)

    @validator("course_code")
    def _uppercase_code(cls, value: str):  # noqa: N805
        return value.upper()

    @validator("type", pre=True, always=True)
    def _normalize_type(cls, value: Any):  # noqa: N805
        allowed = {"maths", "theorey", "practical"}
        if value is None:
            return "practical"
        if isinstance(value, str):
            v = value.strip().lower()
            return v if v in allowed else "practical"
        return "practical"


class SyllabusCourseSimpleCreateIn(SyllabusCourseSimpleBase):
    pass


class SyllabusCourseSimpleUpdateIn(SyllabusCourseSimpleBase):
    pass


class UnitTopicsIn(BaseModel):
    unit_title: str = Field(..., min_length=1, max_length=256)
    topics: List[TopicUpsertIn] = Field(default_factory=list)

    @validator("unit_title", pre=True)
    def _normalize_unit_title(cls, value: Any):  # noqa: N805
        if value is None:
            raise ValueError("Unit title cannot be empty")
        if isinstance(value, str):
            trimmed = value.strip()
            if not trimmed:
                raise ValueError("Unit title cannot be empty")
            return trimmed
        trimmed = str(value).strip()
        if not trimmed:
            raise ValueError("Unit title cannot be empty")
        return trimmed

    @validator("topics", pre=True)
    def _ensure_topics_list(cls, value: Any):  # noqa: N805
        if value is None:
            return []
        return value


class BatchResolveIn(BaseModel):
    college_id: uuid.UUID
    dept_name: str
    from_year: int = Field(..., ge=1950, le=2100)
    to_year: int = Field(..., ge=1950, le=2100)

    @validator("dept_name")
    def uppercase_dept_public(cls, v):
        return (v or "").upper()

    @validator("to_year")
    def check_years_public(cls, v, values):
        f = values.get("from_year")
        if f and v < f:
            raise ValueError("to_year must be >= from_year")
        return v


class ParseSyllabusIn(BaseModel):
    text: str = Field(..., min_length=10)
    course_code: Optional[str] = None
    title: Optional[str] = None


class ParsedSyllabusOut(BaseModel):
    course_code: str
    title: str
    units: List[UnitIn]

# --- Academics service ---


try:
    from supabase_auth.errors import AuthRetryableError, AuthApiError  # type: ignore
except Exception:  # pragma: no cover
    class AuthRetryableError(Exception):
        pass

    class AuthApiError(Exception):
        pass


def upsert_college(name: str) -> uuid.UUID:
    supabase = get_service_client()
    res = supabase.table("colleges").select("id").eq("name", name).limit(1).execute()
    if getattr(res, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (find college): {res.error}")
    if res.data:
        return uuid.UUID(res.data[0]["id"])

    ins = supabase.table("colleges").insert({"name": name}).execute()
    if getattr(ins, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (insert college): {ins.error}")

    res2 = supabase.table("colleges").select("id").eq("name", name).limit(1).execute()
    if getattr(res2, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (refetch college): {res2.error}")
    if not res2.data:
        raise HTTPException(status_code=500, detail="Failed to fetch inserted college.")
    return uuid.UUID(res2.data[0]["id"])


def sync_degree_hierarchy(college_id: uuid.UUID, degrees: List[DegreeCreateIn]) -> None:
    if not degrees:
        return
    supabase = get_service_client()

    degree_res = (
        supabase.table("degrees")
        .select("id,name,level,duration_years")
        .eq("college_id", str(college_id))
        .execute()
    )
    if getattr(degree_res, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (list degrees): {degree_res.error}")

    degree_map: Dict[str, Dict[str, Any]] = {}
    for row in degree_res.data or []:
        key = (row.get("name") or "").strip().lower()
        if key:
            degree_map[key] = row

    dept_res = (
        supabase.table("departments")
        .select("id,name,degree_id")
        .eq("college_id", str(college_id))
        .execute()
    )
    if getattr(dept_res, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (list departments): {dept_res.error}")
    dept_map: Dict[str, Dict[str, Any]] = {}
    for row in dept_res.data or []:
        name = (row.get("name") or "").upper()
        if name:
            dept_map[name] = row

    batch_res = (
        supabase.table("batches")
        .select("department_id,from_year,to_year")
        .eq("college_id", str(college_id))
        .execute()
    )
    if getattr(batch_res, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (list batches): {batch_res.error}")
    batches_by_department: Dict[str, set[Tuple[int, int]]] = {}
    for row in batch_res.data or []:
        dept_id = row.get("department_id")
        if not dept_id:
            continue
        from_year = row.get("from_year")
        to_year = row.get("to_year")
        if from_year is None or to_year is None:
            continue
        batches_by_department.setdefault(dept_id, set()).add((int(from_year), int(to_year)))

    for degree in degrees:
        degree_key = degree.name.strip().lower()
        degree_row = degree_map.get(degree_key)
        if degree_row:
            degree_id = uuid.UUID(degree_row["id"])
            updates: Dict[str, Any] = {}
            if degree_row.get("level") != degree.level:
                updates["level"] = degree.level
            if degree_row.get("duration_years") != degree.duration_years:
                updates["duration_years"] = degree.duration_years
            if updates:
                upd = supabase.table("degrees").update(updates).eq("id", str(degree_id)).execute()
                if getattr(upd, "error", None):
                    raise HTTPException(status_code=500, detail=f"Supabase error (update degree): {upd.error}")
                degree_row.update(updates)
        else:
            payload = {
                "college_id": str(college_id),
                "name": degree.name,
                "level": degree.level,
                "duration_years": degree.duration_years,
            }
            ins = supabase.table("degrees").insert(payload).execute()
            if getattr(ins, "error", None):
                raise HTTPException(status_code=500, detail=f"Supabase error (insert degree): {ins.error}")
            if ins.data:
                degree_row = ins.data[0]
            else:
                refetch = (
                    supabase.table("degrees")
                    .select("id,name,level,duration_years")
                    .eq("college_id", str(college_id))
                    .eq("name", degree.name)
                    .limit(1)
                    .execute()
                )
                if getattr(refetch, "error", None) or not refetch.data:
                    raise HTTPException(status_code=500, detail="Failed to insert degree.")
                degree_row = refetch.data[0]
            degree_id = uuid.UUID(degree_row["id"])
            degree_map[degree_key] = degree_row

        degree_id_str = str(degree_id)

        for department in degree.departments:
            dept_key = department.name  # already upper-case from validation
            dept_row = dept_map.get(dept_key)
            if dept_row:
                dept_id = uuid.UUID(dept_row["id"])
                if dept_row.get("degree_id") != degree_id_str:
                    upd = (
                        supabase.table("departments")
                        .update({"degree_id": degree_id_str})
                        .eq("id", str(dept_id))
                        .execute()
                    )
                    if getattr(upd, "error", None):
                        raise HTTPException(status_code=500, detail=f"Supabase error (update department): {upd.error}")
                    dept_row["degree_id"] = degree_id_str
            else:
                payload = {
                    "college_id": str(college_id),
                    "degree_id": degree_id_str,
                    "name": dept_key,
                }
                ins = supabase.table("departments").insert(payload).execute()
                if getattr(ins, "error", None):
                    raise HTTPException(status_code=500, detail=f"Supabase error (insert department): {ins.error}")
                if ins.data:
                    dept_row = ins.data[0]
                else:
                    refetch = (
                        supabase.table("departments")
                        .select("id,name,degree_id")
                        .eq("college_id", str(college_id))
                        .eq("name", dept_key)
                        .limit(1)
                        .execute()
                    )
                    if getattr(refetch, "error", None) or not refetch.data:
                        raise HTTPException(status_code=500, detail="Failed to insert department.")
                    dept_row = refetch.data[0]
                dept_map[dept_key] = dept_row
                dept_id = uuid.UUID(dept_row["id"])
                batches_by_department[str(dept_id)] = set()
            dept_id = uuid.UUID(dept_row["id"])
            dept_id_str = str(dept_id)
            existing_pairs = batches_by_department.setdefault(dept_id_str, set())
            to_insert = []
            for batch in department.batches:
                pair = (batch.from_year, batch.to_year)
                if pair in existing_pairs:
                    continue
                to_insert.append(
                    {
                        "college_id": str(college_id),
                        "department_id": dept_id_str,
                        "from_year": batch.from_year,
                        "to_year": batch.to_year,
                    }
                )
                existing_pairs.add(pair)
            if to_insert:
                ins_batches = supabase.table("batches").insert(to_insert).execute()
                if getattr(ins_batches, "error", None):
                    raise HTTPException(status_code=500, detail=f"Supabase error (insert batches): {ins_batches.error}")


def get_college_full(college_id: uuid.UUID):
    supabase = get_service_client()
    college = (
        supabase.table("colleges")
        .select("id,name,logo_url")
        .eq("id", str(college_id))
        .single()
        .execute()
    )
    if getattr(college, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (get college): {college.error}")
    if not college.data:
        raise HTTPException(status_code=404, detail="College not found.")

    degree_rows = (
        supabase.table("degrees")
        .select("id,name,level,duration_years")
        .eq("college_id", str(college_id))
        .order("name")
        .execute()
    )
    if getattr(degree_rows, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (get degrees): {degree_rows.error}")

    dept_rows = (
        supabase.table("departments")
        .select("id,name,degree_id")
        .eq("college_id", str(college_id))
        .order("name")
        .execute()
    )
    if getattr(dept_rows, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (get departments): {dept_rows.error}")

    dept_lookup: Dict[str, Dict[str, Any]] = {}
    dept_by_degree: Dict[str, List[Dict[str, Any]]] = {}
    for row in dept_rows.data or []:
        dept_id_str = row["id"]
        dept_info = {
            "id": uuid.UUID(dept_id_str),
            "name": row.get("name"),
            "batches": [],
        }
        dept_lookup[dept_id_str] = dept_info
        degree_id_str = row.get("degree_id")
        if degree_id_str:
            dept_by_degree.setdefault(degree_id_str, []).append(dept_info)

    for bucket in dept_by_degree.values():
        bucket.sort(key=lambda item: (item["name"] or "").upper())

    batch_rows = (
        supabase.table("batches")
        .select("department_id,from_year,to_year")
        .eq("college_id", str(college_id))
        .order("from_year")
        .execute()
    )
    if getattr(batch_rows, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (get batches): {batch_rows.error}")

    flat_seen: set[Tuple[int, int]] = set()
    dept_seen: Dict[str, set[Tuple[int, int]]] = {}
    flat_batches: List[BatchIn] = []
    for row in batch_rows.data or []:
        dept_id_str = row.get("department_id")
        from_year = row.get("from_year")
        to_year = row.get("to_year")
        if from_year is None or to_year is None:
            continue
        pair = (int(from_year), int(to_year))
        if pair not in flat_seen:
            flat_seen.add(pair)
            flat_batches.append(BatchIn(**{"from": pair[0], "to": pair[1]}))
        if not dept_id_str or dept_id_str not in dept_lookup:
            continue
        dept_pairs = dept_seen.setdefault(dept_id_str, set())
        if pair in dept_pairs:
            continue
        dept_pairs.add(pair)
        dept_lookup[dept_id_str]["batches"].append(BatchIn(**{"from": pair[0], "to": pair[1]}))

    for dept_info in dept_lookup.values():
        dept_info["batches"].sort(key=lambda b: (b.from_year, b.to_year))

    degree_data = sorted(degree_rows.data or [], key=lambda row: (row.get("name") or "").lower())
    degrees_out: List[DegreeOut] = []
    for row in degree_data:
        degree_id_str = row["id"]
        departments_out = [
            DepartmentWithBatchesOut(
                id=dept["id"],
                name=dept["name"],
                batches=dept["batches"],
            )
            for dept in dept_by_degree.get(degree_id_str, [])
        ]
        degrees_out.append(
            DegreeOut(
                id=uuid.UUID(degree_id_str),
                name=row.get("name"),
                level=row.get("level"),
                duration_years=row.get("duration_years"),
                departments=departments_out,
            )
        )

    department_names = sorted({(row.get("name") or "").upper() for row in (dept_rows.data or []) if row.get("name")})

    return {
        "id": uuid.UUID(college.data["id"]),
        "name": college.data["name"],
        "logo_url": college.data.get("logo_url"),
        "degrees": degrees_out,
        "departments": department_names,
        "batches": flat_batches,
    }


def _locate_department_from_snapshot(
    college_snapshot: Dict[str, Any],
    degree_id: uuid.UUID,
    department_id: uuid.UUID,
) -> DepartmentWithBatchesOut:
    degrees = college_snapshot.get("degrees") if isinstance(college_snapshot, dict) else []
    if degrees is None:
        degrees = []
    target_degree_id = uuid.UUID(str(degree_id))
    target_department_id = uuid.UUID(str(department_id))

    for degree in degrees:
        deg_id = getattr(degree, "id", None)
        if deg_id is None and isinstance(degree, dict):
            deg_id = degree.get("id")
        if deg_id is None:
            continue
        if str(deg_id) != str(target_degree_id):
            continue

        departments = getattr(degree, "departments", None)
        if departments is None and isinstance(degree, dict):
            departments = degree.get("departments")
        if not departments:
            break

        for dept in departments:
            dept_id = getattr(dept, "id", None)
            if dept_id is None and isinstance(dept, dict):
                dept_id = dept.get("id")
            if dept_id is None or str(dept_id) != str(target_department_id):
                continue

            name = getattr(dept, "name", None)
            if name is None and isinstance(dept, dict):
                name = dept.get("name")

            batches = getattr(dept, "batches", None)
            if batches is None and isinstance(dept, dict):
                batches = dept.get("batches")
            batch_models: List[BatchIn] = []
            for batch in batches or []:
                if isinstance(batch, BatchIn):
                    batch_models.append(batch)
                elif isinstance(batch, dict):
                    frm = batch.get("from") or batch.get("from_year")
                    to = batch.get("to") or batch.get("to_year")
                    if frm is not None and to is not None:
                        batch_models.append(BatchIn(**{"from": frm, "to": to}))
            return DepartmentWithBatchesOut(
                id=target_department_id,
                name=name,
                batches=batch_models,
            )

    raise HTTPException(status_code=404, detail="Department not found in college snapshot.")


def _extract_access_token(res) -> Optional[str]:
    try:
        session = getattr(res, "session", None) or (res.get("session") if isinstance(res, dict) else None)
        if session:
            token = getattr(session, "access_token", None) or (session.get("access_token") if isinstance(session, dict) else None)
            if token:
                return token
    except Exception:
        pass
    return None


def _get_user_id_from_auth_response(res) -> Optional[str]:
    try:
        user = getattr(res, "user", None) or (res.get("user") if isinstance(res, dict) else None)
        if user:
            uid = getattr(user, "id", None) or (user.get("id") if isinstance(user, dict) else None)
            return uid
    except Exception:
        pass
    return None


def _resolve_college_id_by_name(college_name: str) -> uuid.UUID:
    supabase = get_service_client()
    q = (
        supabase.table("colleges")
        .select("id")
        .eq("name", college_name)
        .limit(1)
        .execute()
    )
    if getattr(q, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (find college): {q.error}")
    if not q.data:
        return upsert_college(college_name)
    return uuid.UUID(q.data[0]["id"])


def _resolve_department_id(college_id: uuid.UUID, dept_name: str) -> uuid.UUID:
    supabase = get_service_client()
    dep_q = (
        supabase.table("departments")
        .select("id")
        .eq("college_id", str(college_id))
        .eq("name", dept_name.upper())
        .limit(1)
        .execute()
    )
    if getattr(dep_q, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (find department): {dep_q.error}")
    if not dep_q.data:
        raise HTTPException(status_code=404, detail="Department not found for college")
    return uuid.UUID(dep_q.data[0]["id"])


def _get_or_create_batch_id(
    college_id: uuid.UUID, department_id: uuid.UUID, from_year: int, to_year: int
) -> uuid.UUID:
    supabase = get_service_client()
    sel = (
        supabase.table("batches")
        .select("id")
        .eq("college_id", str(college_id))
        .eq("department_id", str(department_id))
        .eq("from_year", from_year)
        .eq("to_year", to_year)
        .limit(1)
        .execute()
    )
    if getattr(sel, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (find batch): {sel.error}")
    if sel.data:
        return uuid.UUID(sel.data[0]["id"])

    ins = (
        supabase.table("batches")
        .insert(
            {
                "college_id": str(college_id),
                "department_id": str(department_id),
                "from_year": from_year,
                "to_year": to_year,
            }
        )
        .execute()
    )
    if getattr(ins, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (insert batch): {ins.error}")

    ref = (
        supabase.table("batches")
        .select("id")
        .eq("college_id", str(college_id))
        .eq("department_id", str(department_id))
        .eq("from_year", from_year)
        .eq("to_year", to_year)
        .limit(1)
        .execute()
    )
    if getattr(ref, "error", None) or not ref.data:
        raise HTTPException(status_code=500, detail=f"Supabase error (refetch batch): {getattr(ref, 'error', None)}")
    return uuid.UUID(ref.data[0]["id"])


def _ensure_department_batches(
    college_id: uuid.UUID, department_id: uuid.UUID, batches: Optional[List[BatchIn]]
) -> None:
    if not batches:
        return

    supabase = get_service_client()
    existing = (
        supabase.table("batches")
        .select("from_year,to_year")
        .eq("college_id", str(college_id))
        .eq("department_id", str(department_id))
        .execute()
    )
    if getattr(existing, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (list department batches): {existing.error}")

    existing_pairs: set[Tuple[int, int]] = set()
    for row in getattr(existing, "data", []) or []:
        from_year = row.get("from_year")
        to_year = row.get("to_year")
        if from_year is None or to_year is None:
            continue
        existing_pairs.add((int(from_year), int(to_year)))

    to_insert: List[Dict[str, Any]] = []
    for batch in batches:
        pair = (batch.from_year, batch.to_year)
        if pair in existing_pairs:
            continue
        to_insert.append(
            {
                "college_id": str(college_id),
                "department_id": str(department_id),
                "from_year": batch.from_year,
                "to_year": batch.to_year,
            }
        )
        existing_pairs.add(pair)

    if to_insert:
        ins = supabase.table("batches").insert(to_insert).execute()
        if getattr(ins, "error", None):
            raise HTTPException(status_code=500, detail=f"Supabase error (insert department batches): {ins.error}")


def _clean_lines(text: str) -> List[str]:
    lines = [ln.strip() for ln in (text or "").splitlines()]
    return [ln for ln in lines if ln]


def _naive_extract_improved(text: str) -> dict:
    """Heuristic syllabus parser that:
    - Detects unit headers like "UNIT I â€“ Title:" and extracts unit title cleanly
    - Splits topics by commas and spaced dashes (" - ", " â€“ ", " â€” ")
    - Skips non-topic sections (TOTAL PERIODS, Text Books, Reference Books, Content Beyond Syllabus)
    - Attempts to guess course_code and title from a header line (e.g., "AI PE703 DEEP REINFORCEMENT LEARNING 3 - -")
    """
    lines = _clean_lines(text)

    # Merge hyphenated line breaks like "ar-" + "ray" => "array"
    def _dehyphenate(ls: List[str]) -> List[str]:
        out: List[str] = []
        i = 0
        while i < len(ls):
            cur = ls[i]
            if cur.endswith('-') and i + 1 < len(ls):
                nxt = ls[i + 1]
                # If next line starts with letters, merge tokens
                if re.match(r"^[A-Za-z]", nxt or ""):
                    merged = cur[:-1] + nxt.lstrip()
                    out.append(merged)
                    i += 2
                    continue
            out.append(cur)
            i += 1
        return out

    lines = _dehyphenate(lines)
    units: List[dict] = []
    current_unit: Optional[dict] = None
    seen_first_unit = False
    end_reached = False

    # Match UNIT headers even when there is no space before the numeral, e.g., "UNITV"
    unit_pat = re.compile(r"^(?:unit|module|chapter)(?=\s*[ivxlcdm\d])\s*([ivxlcdm]+|\d+)?", re.IGNORECASE)

    skip_topic_pat = re.compile(
        r"^(TOTAL\s+PERIODS|TEXT\s*BOOKS|REFERENCE\s*BOOKS|CONTENT\s+BEYOND\s+SYLLABUS|SUBJECT\s+CODE|SUBJECT\s+NAME|LECTURES|TUTORIALS|PRACTICALS?)\b",
        re.IGNORECASE,
    )
    skip_intro_head_pat = re.compile(
        r"^(COURSE\s+PRE[- ]?REQUISITE|COURSE\s+OBJECTIVES|COURSE\s+OUTCOMES)\b",
        re.IGNORECASE,
    )

    course_code: Optional[str] = None
    course_title: Optional[str] = None
    code_line_pat = re.compile(r"\b([A-Z]{2,4}\s*[A-Z]{0,3}\d{2,4}[A-Z]?)\b[\s,:-]+(.+)$")

    def split_topics(text_line: str) -> List[str]:
        cleaned = re.sub(r"^([\-*â€¢Â·ï¿½?ï¿½]+|\d+[.)])\s*", "", text_line).strip()
        if not cleaned or skip_topic_pat.search(cleaned):
            return []
        parts = re.split(r"\s*,\s*|\s+[–—-]\s+|-(?=[A-Z(])", cleaned)
        out: List[str] = []
        for p in parts:
            t = p.strip().strip(".;, ")
            if t and not skip_topic_pat.search(t):
                out.append(t)
        return out

    for ln in lines:
        if end_reached:
            break
        if course_code is None:
            mcode = code_line_pat.search(ln)
            if mcode:
                course_code = mcode.group(1).strip()
                tail = mcode.group(2)
                tail = re.sub(r"\b\d+\s*[â€“â€”-]\s*[â€“â€”-].*$", "", tail).strip()
                if tail:
                    course_title = tail.strip("-â€“â€”:; .") or None

        # Stop parsing after end-of-syllabus markers appear
        if re.match(r"^(TOTAL\s+PERIODS|TEXT\s*BOOKS|REFERENCE\s*BOOKS|CONTENT\s+BEYOND\s+SYLLABUS)\b", ln, flags=re.IGNORECASE):
            end_reached = True
            break

        # Before first unit header, ignore all lines from intro sections
        if not seen_first_unit:
            if skip_intro_head_pat.match(ln):
                # skip heading
                continue
            # Also skip bullet lines in intro (most have bullets like â€¢ or start with uppercase sentences)
            if unit_pat.match(ln):
                # fall through to create the first unit
                pass
            else:
                # Ignore until first real unit header
                continue

        um = unit_pat.match(ln)
        if um:
            # remove the leading keyword + numeral (with or without space), plus any immediate separators
            after = re.sub(r"^(?:unit|module|chapter)\s*([ivxlcdm]+|\d+)?\s*[:â€“â€”-]?\s*", "", ln, flags=re.IGNORECASE)
            parts = re.split(r"[:â€“â€”-]", after, maxsplit=1)
            title_part = (parts[1] if len(parts) > 1 else parts[0]).strip()
            title_main, title_rest = (title_part.split(":", 1) + [""])[:2]
            title_main = title_main.strip().strip("-â€“â€”:; ") or "Unit"

            current_unit = {"unit_title": title_main, "topics": []}
            units.append(current_unit)
            seen_first_unit = True
            if title_rest:
                for tp in split_topics(title_rest):
                    current_unit["topics"].append({"topic": tp})
            continue

        # Uppercase heading with colon indicates a new unit (e.g., BASIC PROBABILITY: ...)
        mhead = re.match(r"^([A-Z][A-Z0-9 ,\-/&().+]+?):\s*(.*)$", ln)
        if mhead and not skip_topic_pat.search(ln):
            title_main = (mhead.group(1) or "").strip().strip("-:; .")
            title_rest = (mhead.group(2) or "").strip()
            if title_main and len(title_main) >= 4:
                current_unit = {"unit_title": title_main, "topics": []}
                units.append(current_unit)
                seen_first_unit = True
                if title_rest:
                    for tp in split_topics(title_rest):
                        current_unit["topics"].append({"topic": tp})
                continue
        # Uppercase heading with no colon also indicates a unit (e.g., "STACK,QUEUE AND LINKED LISTS")
        mhead_nc = re.match(r"^([A-Z][A-Z0-9 ,\-/&().+]+)$", ln)
        if mhead_nc and not skip_topic_pat.search(ln):
            title_main = (mhead_nc.group(1) or "").strip().strip("-:; .")
            if title_main and len(title_main) >= 4:
                current_unit = {"unit_title": title_main, "topics": []}
                units.append(current_unit)
                seen_first_unit = True
                continue
        if not current_unit:
            # Should not happen now as we skip lines until the first unit header
            continue

        # Skip hour-only lines or stray numbers
        if re.match(r"^\(?\s*\d+\s*Hrs?\.?\s*\)?$", ln, flags=re.IGNORECASE) or ln.strip().isdigit():
            continue
        for tp in split_topics(ln):
            current_unit["topics"].append({"topic": tp})

    return {"course_code": course_code, "title": course_title, "units": units}


def _split_subject_sections(text: str) -> List[Dict[str, str]]:
    """Split a full-semester syllabus text into subject sections.

    Heuristic: a subject starts at a line that looks like a course code followed by a title, e.g.,
      "AI PE703 DEEP REINFORCEMENT LEARNING 3 - -"

    Returns list of dicts with keys: code, title, text.
    """
    lines = [ln.rstrip() for ln in (text or "").splitlines()]
    sections: List[Dict[str, str]] = []
    # Accept lines where code is followed by optional title on same line
    code_line_pat = re.compile(r"^\s*([A-Z]{2,4}\s*[A-Z]{0,3}\d{2,4}[A-Z]?)\b(?:[\s,:-]+(.+))?$")
    anchors: List[Tuple[int, str, Optional[str]]] = []
    for idx, raw in enumerate(lines):
        ln = raw.strip()
        m = code_line_pat.search(ln)
        if not m:
            continue
        code = m.group(1).strip()
        tail = (m.group(2) or "").strip()
        if tail:
            tail = re.sub(r"\b\d+\s*[â€“â€”-]\s*[â€“â€”-].*$", "", tail).strip()
        title = tail.strip("-â€“â€”:; .") if tail else None
        if code:
            anchors.append((idx, code, title))
    if not anchors:
        return []
    # Helper: find a reasonable title if missing on the code line
    def infer_title(start_idx: int) -> Optional[str]:
        # Look ahead a few lines for the subject name (skip meta headers)
        skip_pat = re.compile(r"^(subject\s+code|subject\s+name|lectures|tutorials|practical|course\s+pre|course\s+objectives|course\s+outcomes)\b", re.IGNORECASE)
        collected: List[str] = []
        for j in range(start_idx + 1, min(start_idx + 8, len(lines))):
            cand = lines[j].strip().strip("-â€“â€”:; .")
            if not cand or skip_pat.search(cand):
                continue
            # stop if we hit a unit header
            if re.match(r"^(?:unit|module|chapter)(?=\s*[ivxlcdm\d])", cand, flags=re.IGNORECASE):
                break
            collected.append(cand)
            # if the next piece starts with uppercase and the first chunk looks incomplete (endswith AND), join one more line
            if len(collected) == 1 and j + 1 < len(lines):
                nxt = lines[j + 1].strip().strip("-â€“â€”:; .")
                if nxt and re.match(r"^[A-Z]", nxt) and collected[0].upper().endswith(" AND"):
                    collected.append(nxt)
            break
        if collected:
            return " ".join(collected).strip()
        return None

    for i, (start, code, title) in enumerate(anchors):
        end = anchors[i + 1][0] if i + 1 < len(anchors) else len(lines)
        seg = "\n".join(lines[start:end]).strip()
        if not seg:
            continue
        ttl = title or infer_title(start)
        sections.append({"code": code, "title": (ttl or "").strip(), "text": seg})
    return sections

def _naive_extract(text: str) -> dict:
    lines = _clean_lines(text)
    units: List[dict] = []
    current_unit: Optional[dict] = None

    # Match UNIT headers even when there is no space before the numeral, e.g., "UNITV"
    unit_pat = re.compile(r"^(?:unit|module|chapter)(?=\s*[ivxlcdm\d])\s*([ivxlcdm]+|\d+)?", re.IGNORECASE)

    for ln in lines:
        if unit_pat.match(ln):
            title = ln
            current_unit = {"unit_title": title, "topics": []}
            units.append(current_unit)
            continue
        if not current_unit:
            current_unit = {"unit_title": "Unit 1", "topics": []}
            units.append(current_unit)
        topic = re.sub(r"^([\-*â€¢]+|\d+[.)])\s*", "", ln).strip()
        if topic:
            current_unit["topics"].append({"topic": topic})

    return {"course_code": None, "title": None, "units": units}


def _normalize_parsed_struct(parsed: dict, hints: dict) -> ParsedSyllabusOut:
    text_cc = hints.get("course_code") if hints else None
    text_title = hints.get("title") if hints else None

    units_in: List[UnitIn] = []
    for u in (parsed.get("units") or []):
        title = u.get("unit_title") or u.get("title") or "Unit"
        topics_src = u.get("topics") or []
        topics_in = []
        for t in topics_src:
            if isinstance(t, dict):
                tp = t.get("topic") or t.get("title") or t.get("text")
            else:
                tp = str(t)
            if tp:
                topics_in.append(TopicIn(topic=tp))
        units_in.append(UnitIn(unit_title=title, topics=topics_in))

    cc = text_cc or (parsed.get("course_code") or "") or "UNKNOWN"
    ttl = text_title or (parsed.get("title") or "") or "Untitled Course"
    return ParsedSyllabusOut(course_code=cc, title=ttl, units=units_in)


GEMINI_PARSE_MODEL = os.getenv("GEMINI_PARSE_MODEL", "gemini-2.5-flash").strip()


SYLLABUS_AI_PARSE_PROMPT = """Analyze this university syllabus/curriculum document and extract the structure as JSON.

This is a typical university syllabus with the following structure:
- Subject Code (e.g., 25UMAT21, CS101, AI PE703)
- Subject Title (e.g., DIFFERENTIAL EQUATIONS & TRANSFORMS)
- Course Prerequisites, Objectives, Outcomes (CO1-CO5) - skip these sections
- SYLLABUS section with UNIT I through UNIT V (or more)
- Each unit has: Unit number, Title, Topics, and Hours (e.g., 12)
- Text Books, Reference Books sections at the end - skip these

Extract all subjects/courses with their units and topics. Return ONLY valid JSON in this exact format:
{
  "subjects": [
    {
      "name": "Subject Title Here",
      "code": "SUBJECT_CODE or null if not found",
      "units": [
        {
          "name": "Unit Title (e.g., ORDINARY DIFFERENTIAL EQUATIONS)",
          "unit_number": 1,
          "topics": [
            {"name": "Topic 1 - should be a specific concept", "order_index": 0},
            {"name": "Topic 2", "order_index": 1}
          ]
        }
      ]
    }
  ]
}

RULES:
1. Extract ALL subjects mentioned in the document
2. For each subject, extract ALL units/modules (usually UNIT I, II, III, IV, V)
3. For each unit, extract ALL topics - split by commas, dashes, or line breaks
4. SKIP sections: Course Prerequisites, Course Objectives, Course Outcomes (CO1-CO5), Text Books, Reference Books, Total Periods
5. If unit numbers are not explicit, infer them from order (1, 2, 3...)
6. If subject codes are not present, set code to null
7. Clean up topic names - remove leading bullets, numbers, or special characters
8. Return ONLY the JSON, no markdown formatting or explanation

Document text:
"""


def _gemini_parse(text: str, hints: dict) -> Optional[dict]:
    if not GEMINI_API_KEY:
        return None
    try:
        import google.generativeai as genai
    except Exception:
        return None

    try:
        genai.configure(api_key=GEMINI_API_KEY)
        model = genai.GenerativeModel(GEMINI_PARSE_MODEL or "gemini-2.5-flash")
        
        # Limit the text length to keep latency low
        max_chars = int(os.getenv("GEMINI_PARSE_MAX_CHARS", "100000"))
        safe_text = (text or "")[:max_chars]
        
        # Use the improved syllabus parsing prompt
        full_prompt = SYLLABUS_AI_PARSE_PROMPT + safe_text
        
        resp = model.generate_content(full_prompt)
        raw = getattr(resp, "text", None)
        if not raw:
            try:
                raw = resp.candidates[0].content.parts[0].text
            except Exception:
                raw = None
        if not raw:
            return None

        # Extract JSON from response (might be wrapped in markdown code blocks)
        m = re.search(r"```(?:json)?\s*([\s\S]*?)```", raw)
        jtxt = m.group(1) if m else raw
        
        parsed = json.loads(jtxt)
        
        # Convert new format to expected format if it has 'subjects' key
        if "subjects" in parsed and parsed["subjects"]:
            # Take the first subject and convert to old format
            first_subject = parsed["subjects"][0]
            units = []
            for unit in first_subject.get("units", []):
                topics = []
                for topic in unit.get("topics", []):
                    topic_name = topic.get("name") if isinstance(topic, dict) else str(topic)
                    if topic_name:
                        topics.append({"topic": topic_name})
                units.append({
                    "unit_title": unit.get("name") or f"Unit {unit.get('unit_number', 1)}",
                    "topics": topics
                })
            return {
                "course_code": first_subject.get("code"),
                "title": first_subject.get("name"),
                "units": units
            }
        
        return parsed
    except Exception as e:
        print(f"[GEMINI_PARSE] Error: {e}")
        return None


async def _gemini_parse_with_timeout(text: str, hints: dict, timeout_s: float = 18.0) -> Optional[dict]:
    try:
        return await asyncio.wait_for(run_in_threadpool(_gemini_parse, text, hints), timeout=timeout_s)
    except Exception:
        return None


LAB_KEYWORD_PATTERN = re.compile(r"\b(lab|laboratory|practical|sessional|experiment|experiments)\b", re.IGNORECASE)


def _contains_lab(text: Optional[str]) -> bool:
    if not text:
        return False
    return bool(LAB_KEYWORD_PATTERN.search(text))


def _filter_lab_units(units: List[UnitIn]) -> List[UnitIn]:
    filtered: List[UnitIn] = []
    for unit in units or []:
        title = unit.unit_title or ""
        if _contains_lab(title):
            continue
        clean_topics: List[TopicIn] = []
        for topic in unit.topics or []:
            topic_text = getattr(topic, "topic", None)
            if not topic_text or _contains_lab(topic_text):
                continue
            clean_topics.append(TopicIn(topic=topic_text))
        if not clean_topics:
            continue
        filtered.append(UnitIn(unit_title=title, topics=clean_topics))
    return filtered


def parse_syllabus(payload: ParseSyllabusIn) -> ParsedSyllabusOut:
    text = (payload.text or "").strip()
    if len(text) < 10:
        raise HTTPException(status_code=400, detail="Text too short")
    hints = {"course_code": payload.course_code, "title": payload.title}

    # Always use the improved heuristic parser for stability
    parsed = _naive_extract_improved(text)

    norm = _normalize_parsed_struct(parsed, hints)
    if not norm.units:
        raise HTTPException(status_code=422, detail="Could not extract any units or topics")
    return norm


def _pdf_bytes_to_text(data: bytes) -> str:
    text = ""
    # Prefer PyMuPDF (fitz) for accuracy
    if data and fitz is not None:  # type: ignore[attr-defined]
        try:
            with fitz.open(stream=data, filetype="pdf") as doc:  # type: ignore[attr-defined]
                chunks: List[str] = []
                for page in doc:
                    try:
                        chunks.append(page.get_text("text"))
                    except Exception:
                        continue
                text = "\n".join(chunks).strip()
        except Exception:
            text = ""
    if not text and textract is not None:
        try:
            out = textract.process(io.BytesIO(data), extension='pdf')  # type: ignore
            try:
                text = out.decode('utf-8', errors='ignore').strip()
            except Exception:
                text = (out or b"").decode('latin-1', errors='ignore').strip()
        except Exception:
            text = ""
    if not text and PyPDF2 is not None:
        try:
            reader = PyPDF2.PdfReader(io.BytesIO(data))  # type: ignore[attr-defined]
            chunks: List[str] = []
            for page in reader.pages:
                try:
                    page_text = page.extract_text()  # type: ignore[attr-defined]
                except Exception:
                    page_text = None
                if page_text:
                    chunks.append(page_text)
            text = "\n".join(chunks).strip()
        except Exception:
            text = ""
    if not text:
        # Very last fallback: naive bytes decode
        try:
            text = data.decode('utf-8', errors='ignore')
        except Exception:
            text = ""
    return text


# NOTE: The /api/syllabus/upload route is registered later, after academics_router is created.


def signup_user(user):
    anon_client = get_anon_client()
    if not anon_client:
        raise HTTPException(status_code=500, detail="Server missing SUPABASE_ANON_KEY")
    try:
        res = anon_client.auth.sign_up({"email": user.email, "password": user.password})
        # Extract session data for persistent login
        session = getattr(res, "session", None)
        access_token = getattr(session, "access_token", None) if session else None
        refresh_token = getattr(session, "refresh_token", None) if session else None
        expires_in = getattr(session, "expires_in", 3600) if session else 3600
        return {
            "message": "Signup initiated",
            "access_token": access_token,
            "refresh_token": refresh_token,
            "expires_in": expires_in,
        }
    except Exception as e:
        supabase_logger.exception("Signup error")
        raise HTTPException(status_code=400, detail=str(e))


def login_user(user):
    anon_client = get_anon_client()
    if not anon_client:
        raise HTTPException(status_code=500, detail="Server missing SUPABASE_ANON_KEY")
    try:
        res = anon_client.auth.sign_in_with_password({"email": user.email, "password": user.password})
        # Extract session data for persistent login
        session = getattr(res, "session", None)
        access_token = getattr(session, "access_token", None) if session else None
        refresh_token = getattr(session, "refresh_token", None) if session else None
        expires_in = getattr(session, "expires_in", 3600) if session else 3600
        if not access_token:
            raise HTTPException(status_code=401, detail="Login failed: no session returned")
        return {
            "message": "Login successful",
            "access_token": access_token,
            "refresh_token": refresh_token,
            "expires_in": expires_in,
        }
    except HTTPException:
        raise
    except Exception:
        supabase_logger.exception("Login failed")
        raise HTTPException(status_code=401, detail="Invalid credentials or login failed")


def signup_full_user(payload):
    anon_client = get_anon_client()
    if not anon_client:
        raise HTTPException(status_code=500, detail="Server missing SUPABASE_ANON_KEY")
    try:
        auth_res = anon_client.auth.sign_up({"email": payload.email, "password": payload.password})
        user_id = _get_user_id_from_auth_response(auth_res)
        if not user_id:
            raise HTTPException(status_code=400, detail="Failed to create auth user")
        access_token = _extract_access_token(auth_res)

        college_id = _resolve_college_id_by_name(payload.college)
        department_id = _resolve_department_id(college_id, payload.department)
        batch_id = _get_or_create_batch_id(college_id, department_id, payload.batch_from, payload.batch_to)

        supabase = get_service_client()
        ins = (
            supabase.table("user_profiles")
            .insert(
                {
                    "auth_user_id": user_id,
                    "name": payload.name,
                    "gender": payload.gender,
                    "phone": payload.phone,
                    "email": payload.email,
                    "college_id": str(college_id),
                    "department_id": str(department_id),
                    "batch_id": str(batch_id),
                    "batch_from": payload.batch_from,
                    "batch_to": payload.batch_to,
                    "semester": payload.semester,
                    "regno": payload.regno,
                }
            )
            .execute()
        )
        if getattr(ins, "error", None):
            raise HTTPException(status_code=500, detail=f"Supabase error (insert profile): {ins.error}")

        prof_q = (
            supabase.table("user_profiles")
            .select("id")
            .eq("auth_user_id", user_id)
            .limit(1)
            .execute()
        )
        profile_id = prof_q.data[0]["id"] if prof_q.data else None
        return {
            "message": "Signup complete",
            "access_token": access_token,
            "user_id": user_id,
            "profile_id": profile_id,
        }
    except HTTPException:
        raise
    except Exception as e:
        supabase_logger.exception("Signup full error")
        raise HTTPException(status_code=500, detail=f"Unexpected error: {e}")


def get_current_user_profile(token: Optional[str]):
    anon_client = get_anon_client()
    if not anon_client:
        raise HTTPException(status_code=500, detail="Server missing SUPABASE_ANON_KEY")
    if not token:
        raise HTTPException(status_code=401, detail="Missing or invalid Authorization header")

    try:
        user_id = _get_user_id_with_retry(token)
        supabase = get_service_client()

        # 1. Fetch Profile WITH embedded College, Dept, Batch data in ONE request
        # Requires FKs: user_profiles.college_id -> colleges.id, etc.
        try:
            prof_q = _supabase_retry(
                lambda: (
                    supabase.table("user_profiles")
                    .select("*, colleges(id,name), departments(id,name), batches(id,from_year,to_year)")
                    .eq("auth_user_id", user_id)
                    .limit(1)
                    .execute()
                )
            )
        except HTTPXRemoteProtocolError as exc:
            supabase_logger.warning("/api/me profile fetch transient protocol error: %s", exc)
            raise HTTPException(status_code=503, detail="Upstream temporarily unavailable. Please retry.")
        
        if getattr(prof_q, "error", None):
            raise HTTPException(status_code=500, detail=f"Supabase error (get profile): {prof_q.error}")
        if not prof_q.data:
            raise HTTPException(status_code=404, detail="Profile not found")
        
        prof = prof_q.data[0]
        profile_id = prof.get("id")

        # Extract joined data or fallback to None
        college_data = prof.get("colleges")  # dict or list depending on cardinality (should be dict if FK is 1:1/M:1) -> actually Supabase returns dict for M:1
        department_data = prof.get("departments")
        batch_data = prof.get("batches")

        # Normalize potentially scalar or list return from join
        college = college_data if isinstance(college_data, dict) else (college_data[0] if isinstance(college_data, list) and college_data else None)
        department = department_data if isinstance(department_data, dict) else (department_data[0] if isinstance(department_data, list) and department_data else None)
        batch = None
        if batch_data:
            raw_batch = batch_data if isinstance(batch_data, dict) else (batch_data[0] if isinstance(batch_data, list) else None)
            if raw_batch:
                batch = {"id": raw_batch.get("id"), "from": raw_batch.get("from_year"), "to": raw_batch.get("to_year")}

        # 2. Parallel Fetch of Related Data (Experience, Education, etc.)
        # We use a ThreadPool to run these read-only fetches concurrently
        related_results = {}
        fetch_specs = [
            ("experiences", "user_experiences", [("order_index", False), ("start_date", True), ("created_at", False)]),
            ("education_entries", "user_education", [("order_index", False), ("created_at", False)]),
            ("certification_entries", "user_certifications", [("order_index", False), ("issue_date", True), ("created_at", False)]),
            ("portfolio_projects", "user_portfolio_projects", [("order_index", False), ("start_date", True), ("created_at", False)]),
            ("publication_entries", "user_publications", [("order_index", False), ("publication_date", True), ("created_at", False)]),
        ]

        if profile_id:
            import concurrent.futures
            with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:
                future_map = {
                    executor.submit(_fetch_profile_related, profile_id, table, sort_keys): key
                    for key, table, sort_keys in fetch_specs
                }
                for future in concurrent.futures.as_completed(future_map):
                    key = future_map[future]
                    try:
                        related_results[key] = future.result()
                    except Exception as e:
                        supabase_logger.error(f"Failed to fetch related {key}: {e}")
                        related_results[key] = []
        else:
             for key, _, _ in fetch_specs:
                 related_results[key] = []

        # Unpack related data
        related_education = related_results.get("education_entries", [])
        
        # Education Overrides (Semester, batch derived from text, etc.)
        final_semester = prof.get("semester")
        final_regno = prof.get("regno")
        derived_batch_years = None
        
        if related_education:
            # Sort by current_semester then order_index
            sem_sorted = [e for e in related_education if isinstance(e, dict)]
            if sem_sorted:
                sem_sorted.sort(key=lambda r: (r.get("current_semester") or 0, r.get("order_index") or 0), reverse=True)
                primary_edu = sem_sorted[0]
                if primary_edu.get("current_semester"):
                    final_semester = primary_edu.get("current_semester")
                if primary_edu.get("regno"):
                    final_regno = primary_edu.get("regno")
                
                # Text-based resolution fallbacks (only if direct IDs missing)
                # We must attempt to resolve IDs to support batch lookup
                if not college:
                    edu_school = primary_edu.get("school")
                    if edu_school:
                        try:
                            # Attempt resolution
                            cq2 = supabase.table("colleges").select("id,name").eq("name", edu_school).limit(1).execute()
                            if not getattr(cq2, "error", None) and cq2.data:
                                college = {"id": cq2.data[0]["id"], "name": cq2.data[0]["name"]}
                            else:
                                college = {"id": None, "name": edu_school}
                        except Exception:
                            college = {"id": None, "name": edu_school}
                
                if not department:
                    edu_dept = primary_edu.get("department")
                    if edu_dept:
                        try:
                            # Attempt resolution - requires college_id if we want to be strict, or just name? 
                            # DB schema: departments usually linked to college.
                            # Original logic matched name AND college_id if available. 
                            dept_q = supabase.table("departments").select("id,name").eq("name", (edu_dept or "").upper())
                            if college and college.get("id"):
                                dept_q = dept_q.eq("college_id", college.get("id"))
                            dq2 = dept_q.limit(1).execute()
                            
                            if not getattr(dq2, "error", None) and dq2.data:
                                department = {"id": dq2.data[0]["id"], "name": dq2.data[0]["name"]}
                            else:
                                department = {"id": None, "name": (edu_dept or "").upper()}
                        except Exception:
                            department = {"id": None, "name": (edu_dept or "").upper()}

                if not batch:
                    edu_batch_range = primary_edu.get("batch_range")
                    if edu_batch_range and isinstance(edu_batch_range, str):
                        import re as _re
                        years_full = _re.findall(r"\b(\d{4})\b", edu_batch_range)
                        if len(years_full) >= 2:
                            try:
                                from_year = int(years_full[0])
                                to_year = int(years_full[1])
                                batch = {"id": None, "from": from_year, "to": to_year}
                                derived_batch_years = (from_year, to_year)
                            except Exception:
                                pass

        # 3. Optimized Syllabus Fetch (N+1 -> 1 query)
        # Fetch Courses -> embedded Units -> embedded Topics
        syllabus = []
        effective_batch_id = prof.get("batch_id") or (batch.get("id") if batch else None)
        
        # Try to resolve batch ID from derived years if missing
        if not effective_batch_id and derived_batch_years and college and college.get("id") and department and department.get("id"):
             # This is a rare edge case, keeping the single lookup is fine, or arguably skip to save time.
             # We'll keep it but optimize slightly.
             try:
                 fy, ty = derived_batch_years
                 bq2 = supabase.table("batches").select("id").eq("college_id", college["id"]).eq("department_id", department["id"]).eq("from_year", fy).eq("to_year", ty).limit(1).execute()
                 if bq2.data:
                     effective_batch_id = bq2.data[0]["id"]
             except Exception:
                 pass

        if effective_batch_id and final_semester:
            try:
                # Deep query: courses -> units -> topics
                syllabus_query = (
                    supabase.table("syllabus_courses")
                    .select("id,course_code,title,semester, syllabus_units(id,unit_title,order_in_course, syllabus_topics(id,topic,order_in_unit,image_url,lab_url))")
                    .eq("batch_id", effective_batch_id)
                    .eq("semester", final_semester)
                    .execute()
                )
                
                if not getattr(syllabus_query, "error", None) and syllabus_query.data:
                    # Sort in memory
                    courses_data = syllabus_query.data
                    courses_data.sort(key=lambda c: c.get("course_code") or "")
                    
                    for cr in courses_data:
                        raw_units = cr.get("syllabus_units") or []
                        # Sort units
                        raw_units.sort(key=lambda u: u.get("order_in_course") or 0)
                        
                        clean_units = []
                        for u in raw_units:
                            raw_topics = u.get("syllabus_topics") or []
                            # Sort topics
                            raw_topics.sort(key=lambda t: t.get("order_in_unit") or 0)
                            
                            clean_topics = [
                                {
                                    "id": t.get("id"),
                                    "topic": t.get("topic"),
                                    "order_in_unit": t.get("order_in_unit"),
                                    "image_url": t.get("image_url"),
                                    "lab_url": t.get("lab_url"),
                                }
                                for t in raw_topics
                            ]
                            clean_units.append({
                                "id": u.get("id"),
                                "unit_title": u.get("unit_title"),
                                "order_in_course": u.get("order_in_course"),
                                "topics": clean_topics
                            })
                            
                        syllabus.append({
                            "id": cr.get("id"),
                            "course_code": cr.get("course_code"),
                            "title": cr.get("title"),
                            "semester": cr.get("semester"),
                            "units": clean_units
                        })
            except Exception as e:
                supabase_logger.warning(f"Syllabus fetch error: {e}")

        return {
            "profile": {
                "id": prof["id"],
                "auth_user_id": prof["auth_user_id"],
                "email": prof.get("email"),
                "name": prof.get("name"),
                "gender": prof.get("gender"),
                "phone": prof.get("phone"),
                "semester": final_semester,
                "regno": final_regno,
                "profile_image_url": prof.get("profile_image_url"),
                "resume_url": prof.get("resume_url"),
                "headline": prof.get("headline"),
                "location": prof.get("location"),
                "dob": prof.get("dob"),
                "linkedin": prof.get("linkedin"),
                "github": prof.get("github"),
                "leetcode": prof.get("leetcode"),
                "portfolio_url": prof.get("portfolio_url"),
                "website": prof.get("website"),
                "twitter": prof.get("twitter"),
                "instagram": prof.get("instagram"),
                "medium": prof.get("medium"),
                "verification_score": prof.get("verification_score"),
                "bio": prof.get("bio"),
                "technologies": prof.get("technologies"),
                "skills": prof.get("skills"),
                "certifications": prof.get("certifications"),
                "languages": prof.get("languages"),
                "interests": prof.get("interests"),
                "project_info": prof.get("project_info"),
                "publications": prof.get("publications"),
                "achievements": prof.get("achievements"),
                "experience": prof.get("experience"),
                "experiences": related_results.get("experiences", []),
                "education_entries": related_education,
                "certification_entries": related_results.get("certification_entries", []),
                "portfolio_projects": related_results.get("portfolio_projects", []),
                "publication_entries": related_results.get("publication_entries", []),
                "college": college,
                "department": department,
                "batch": batch,
            },
            "syllabus": syllabus,
        }

    except HTTPException:
        raise
    except Exception as e:
        msg = f"{e}".lower()
        if "invalid jwt" in msg or "token is malformed" in msg or "unable to parse" in msg:
            raise HTTPException(status_code=401, detail="Invalid or malformed token")
        supabase_logger.exception("/api/me error")
        raise HTTPException(status_code=500, detail=f"Unexpected error: {e}")


def upsert_syllabus_course(payload: SyllabusCourseIn) -> SyllabusCourseOut:
    supabase = get_service_client()
    existing = _supabase_retry(lambda: (
        supabase.table("syllabus_courses")
        .select("id,title,type")
        .eq("batch_id", str(payload.batch_id))
        .eq("semester", payload.semester)
        .eq("course_code", payload.course_code)
        .limit(1)
        .execute()
    ))
    if getattr(existing, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (find course): {existing.error}")
    if existing.data:
        course_id = uuid.UUID(existing.data[0]["id"])
        existing_type = existing.data[0].get("type")
        desired_type = payload.type or "practical"
        if existing.data[0].get("title") != payload.title or existing_type != desired_type:
            upd = _supabase_retry(lambda: (
                supabase.table("syllabus_courses")
                .update({"title": payload.title, "type": desired_type})
                .eq("id", str(course_id))
                .execute()
            ))
            if getattr(upd, "error", None):
                raise HTTPException(status_code=500, detail=f"Supabase error (update course): {upd.error}")
    else:
        ins = _supabase_retry(lambda: (
            supabase.table("syllabus_courses")
            .insert(
                {
                    "batch_id": str(payload.batch_id),
                    "semester": payload.semester,
                    "course_code": payload.course_code,
                    "title": payload.title,
                    "type": payload.type or "practical",
                }
            )
            .execute()
        ))
        if getattr(ins, "error", None):
            raise HTTPException(status_code=500, detail=f"Supabase error (insert course): {ins.error}")
        course_id = uuid.UUID(ins.data[0]["id"]) if ins.data else None

    # Fetch units/topics after ensuring course exists
    units = []  # placeholder; actual unit sync handled elsewhere
    return SyllabusCourseOut(
        id=course_id,
        batch_id=payload.batch_id,
        semester=payload.semester,
        course_code=payload.course_code,
        title=payload.title,
        type=payload.type,
        units=units,
    )


def sync_units_and_topics(course_id: uuid.UUID, units: List[UnitIn]) -> List[UnitOut]:
    supabase = get_service_client()
    ures = (
        supabase.table("syllabus_units")
        .select("id,unit_title")
        .eq("course_id", str(course_id))
        .execute()
    )
    if getattr(ures, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (list units): {ures.error}")
    unit_map: Dict[str, uuid.UUID] = {row["unit_title"]: uuid.UUID(row["id"]) for row in (ures.data or [])}

    for order_idx, u in enumerate(units):
        uid = unit_map.get(u.unit_title)
        if uid is None:
            ins = (
                supabase.table("syllabus_units")
                .insert(
                    {
                        "course_id": str(course_id),
                        "unit_title": u.unit_title,
                        "order_in_course": order_idx,
                    }
                )
                .execute()
            )
            if getattr(ins, "error", None):
                raise HTTPException(status_code=500, detail=f"Supabase error (insert unit): {ins.error}")
            ref = (
                supabase.table("syllabus_units")
                .select("id")
                .eq("course_id", str(course_id))
                .eq("unit_title", u.unit_title)
                .limit(1)
                .execute()
            )
            if getattr(ref, "error", None) or not ref.data:
                raise HTTPException(status_code=500, detail=f"Supabase error (refetch unit): {getattr(ref, 'error', None)}")
            uid = uuid.UUID(ref.data[0]["id"])
            unit_map[u.unit_title] = uid
        else:
            upd = (
                supabase.table("syllabus_units")
                .update({"order_in_course": order_idx})
                .eq("id", str(uid))
                .execute()
            )
            if getattr(upd, "error", None):
                raise HTTPException(status_code=500, detail=f"Supabase error (update unit order): {upd.error}")

        tres = (
            supabase.table("syllabus_topics")
            .select("id,topic")
            .eq("unit_id", str(uid))
            .execute()
        )
        if getattr(tres, "error", None):
            raise HTTPException(status_code=500, detail=f"Supabase error (list topics): {tres.error}")
        topic_map: Dict[str, uuid.UUID] = {row["topic"]: uuid.UUID(row["id"]) for row in (tres.data or [])}
        for t_order, t in enumerate(u.topics or []):
            tid = topic_map.get(t.topic)
            if tid is None:
                tins = (
                    supabase.table("syllabus_topics")
                    .insert(
                        {
                            "unit_id": str(uid),
                            "topic": t.topic,
                            "order_in_unit": t_order,
                        }
                    )
                    .execute()
                )
                if getattr(tins, "error", None):
                    raise HTTPException(status_code=500, detail=f"Supabase error (insert topic): {tins.error}")
                tref = (
                    supabase.table("syllabus_topics")
                    .select("id")
                    .eq("unit_id", str(uid))
                    .eq("topic", t.topic)
                    .limit(1)
                    .execute()
                )
                if getattr(tref, "error", None) or not tref.data:
                    raise HTTPException(status_code=500, detail=f"Supabase error (refetch topic): {getattr(tref, 'error', None)}")
                tid = uuid.UUID(tref.data[0]["id"])
                topic_map[t.topic] = tid
            else:
                tupd = (
                    supabase.table("syllabus_topics")
                    .update({"order_in_unit": t_order})
                    .eq("id", str(tid))
                    .execute()
                )
                if getattr(tupd, "error", None):
                    raise HTTPException(status_code=500, detail=f"Supabase error (update topic order): {tupd.error}")

    units_rows = (
        supabase.table("syllabus_units")
        .select("id,unit_title,order_in_course")
        .eq("course_id", str(course_id))
        .order("order_in_course")
        .execute()
    )
    if getattr(units_rows, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (get units): {units_rows.error}")
    out_units: List[UnitOut] = []
    for ur in (units_rows.data or []):
        uid = uuid.UUID(ur["id"])
        tops = (
            supabase.table("syllabus_topics")
            .select("id,topic,order_in_unit,image_url,video_url,ppt_url,lab_url")
            .eq("unit_id", str(uid))
            .order("order_in_unit")
            .execute()
        )
        if getattr(tops, "error", None):
            raise HTTPException(status_code=500, detail=f"Supabase error (get topics): {tops.error}")
        out_units.append(
            UnitOut(
                id=uid,
                unit_title=ur["unit_title"],
                order_in_course=ur["order_in_course"],
                topics=[
                    TopicOut(
                        id=uuid.UUID(tr["id"]),
                        topic=tr["topic"],
                        order_in_unit=tr["order_in_unit"],
                        image_url=tr.get("image_url"),
                        video_url=tr.get("video_url"),
                        ppt_url=tr.get("ppt_url"),
                        lab_url=tr.get("lab_url"),
                    )
                    for tr in (tops.data or [])
                ],
            )
        )
    return out_units


def load_course_with_units(course_id: uuid.UUID) -> SyllabusCourseOut:
    supabase = get_service_client()

    def _safe_exec(builder, label: str):
        # Lightweight retry loop so transient disconnects (RemoteProtocolError, etc.) don't crash the endpoint
        retries = 3
        delay = 0.15
        last_err = None
        for attempt in range(retries):
            try:
                res = builder.execute()
                break
            except Exception as e:  # httpx / network-level errors are fine to retry
                last_err = e
                if attempt == retries - 1:
                    raise HTTPException(status_code=503, detail=f"Supabase error ({label}): {e}")
                time.sleep(delay * (attempt + 1))
        if getattr(res, "error", None):
            raise HTTPException(status_code=503, detail=f"Supabase error ({label}): {res.error}")
        return res

    course_builder = (
        supabase.table("syllabus_courses")
        .select("id,batch_id,semester,course_code,title,type")
        .eq("id", str(course_id))
        .single()
    )
    course_q = _safe_exec(course_builder, "get course")
    if not course_q.data:
        raise HTTPException(status_code=404, detail="Course not found")

    units_builder = (
        supabase.table("syllabus_units")
        .select("id,unit_title,order_in_course")
        .eq("course_id", str(course_id))
        .order("order_in_course")
    )
    units_rows = _safe_exec(units_builder, "get units")

    units_out: List[UnitOut] = []
    for unit_row in units_rows.data or []:
        unit_id = uuid.UUID(unit_row["id"])
        topics_builder = (
            supabase.table("syllabus_topics")
            .select("id,topic,order_in_unit,image_url,video_url,ppt_url,lab_url")
            .eq("unit_id", str(unit_id))
            .order("order_in_unit")
        )
        topics_rows = _safe_exec(topics_builder, "get topics")
        units_out.append(
            UnitOut(
                id=unit_id,
                unit_title=unit_row["unit_title"],
                order_in_course=unit_row["order_in_course"],
                topics=[
                    TopicOut(
                        id=uuid.UUID(topic_row["id"]),
                        topic=topic_row["topic"],
                        order_in_unit=topic_row["order_in_unit"],
                        image_url=topic_row.get("image_url"),
                        video_url=topic_row.get("video_url"),
                        ppt_url=topic_row.get("ppt_url"),
                        lab_url=topic_row.get("lab_url"),
                    )
                    for topic_row in (topics_rows.data or [])
                ],
            )
        )

    data = course_q.data
    return SyllabusCourseOut(
        id=uuid.UUID(data["id"]),
        batch_id=uuid.UUID(data["batch_id"]),
        semester=int(data["semester"]),
        course_code=data.get("course_code"),
        title=data.get("title"),
        type=data.get("type"),
        units=units_out,
    )


def load_unit_with_topics(unit_id: uuid.UUID) -> UnitOut:
    supabase = get_service_client()
    unit_res = (
        supabase.table("syllabus_units")
        .select("id,unit_title,order_in_course")
        .eq("id", str(unit_id))
        .limit(1)
        .execute()
    )
    if getattr(unit_res, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (get unit): {unit_res.error}")
    if not unit_res.data:
        raise HTTPException(status_code=404, detail="Unit not found")

    unit_row = unit_res.data[0]
    topics_res = (
        supabase.table("syllabus_topics")
        .select("id,topic,order_in_unit,image_url,video_url,ppt_url,lab_url")
        .eq("unit_id", str(unit_id))
        .order("order_in_unit")
        .execute()
    )
    if getattr(topics_res, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (get unit topics): {topics_res.error}")

    return UnitOut(
        id=unit_id,
        unit_title=unit_row.get("unit_title"),
        order_in_course=int(unit_row.get("order_in_course", 0)),
        topics=[
            TopicOut(
                id=uuid.UUID(topic_row["id"]),
                topic=topic_row.get("topic"),
                order_in_unit=int(topic_row.get("order_in_unit", 0)),
                image_url=topic_row.get("image_url"),
                video_url=topic_row.get("video_url"),
                ppt_url=topic_row.get("ppt_url"),
                lab_url=topic_row.get("lab_url"),
            )
            for topic_row in (topics_res.data or [])
        ],
    )


def resolve_or_create_batch(
    college_id: uuid.UUID, dept_name: str, from_year: int, to_year: int
) -> BatchWithIdOut:
    department_id = _resolve_department_id(college_id, dept_name)
    batch_id = _get_or_create_batch_id(college_id, department_id, from_year, to_year)
    return BatchWithIdOut(id=batch_id, from_year=from_year, to_year=to_year)


# --------------------- Progress tracking services ---------------------------

def _get_user_id_with_retry(token: str, retries: int = 3, base_delay: float = 0.25) -> str:
    anon_client = get_anon_client()
    if not anon_client:
        raise HTTPException(status_code=500, detail="Server missing SUPABASE_ANON_KEY")
    last_exc: Optional[Exception] = None
    retryable_auth_errors: tuple[Any, ...] = (AuthRetryableError,)
    if httpx is not None:
        retryable_auth_errors = retryable_auth_errors + (httpx.RemoteProtocolError,)  # type: ignore
        if HTTPXRemoteProtocolError not in retryable_auth_errors:
            retryable_auth_errors = retryable_auth_errors + (HTTPXRemoteProtocolError,)  # type: ignore
    else:
        retryable_auth_errors = retryable_auth_errors + (HTTPXRemoteProtocolError,)

    for attempt in range(retries):
        try:
            auth_user = anon_client.auth.get_user(token)
            user = getattr(auth_user, "user", None) or (auth_user.get("user") if isinstance(auth_user, dict) else None)
            user_id = getattr(user, "id", None) or (user.get("id") if isinstance(user, dict) else None)
            if not user_id:
                raise HTTPException(status_code=401, detail="Invalid token or user not found")
            return user_id
        except AuthApiError as e:
            msg = getattr(e, "message", None) or str(e) or "Invalid or expired session"
            raise HTTPException(status_code=401, detail=msg)
        except retryable_auth_errors as e:  # type: ignore
            last_exc = e
            time.sleep(base_delay * (attempt + 1))
            continue
        except Exception:
            raise
    supabase_logger.warning("Auth get_user retry exhausted: %s", last_exc)
    raise HTTPException(status_code=503, detail="Authentication service temporarily unavailable; please retry")


def _require_user_and_profile(token: Optional[str]) -> tuple[str, str]:
    anon_client = get_anon_client()
    if not anon_client:
        raise HTTPException(status_code=500, detail="Server missing SUPABASE_ANON_KEY")
    if not token:
        raise HTTPException(status_code=401, detail="Missing or invalid Authorization header")
    user_id = _get_user_id_with_retry(token)
    supabase = get_service_client()
    prof_q = (
        supabase.table("user_profiles").select("id").eq("auth_user_id", user_id).limit(1).execute()
    )
    if getattr(prof_q, "error", None) or not prof_q.data:
        raise HTTPException(status_code=404, detail="Profile not found")
    profile_id = prof_q.data[0]["id"]
    return user_id, profile_id


def _ensure_user_and_profile(token: Optional[str]) -> tuple[str, str]:
    """Ensure there is a user_profiles row for the authenticated user.

    Returns (auth_user_id, profile_id). Creates a minimal row if missing.
    """
    anon_client = get_anon_client()
    if not anon_client:
        raise HTTPException(status_code=500, detail="Server missing SUPABASE_ANON_KEY")
    if not token:
        raise HTTPException(status_code=401, detail="Missing or invalid Authorization header")

    user_id = _get_user_id_with_retry(token)
    supabase = get_service_client()

    # Try existing profile first
    prof_q = (
        supabase.table("user_profiles").select("id").eq("auth_user_id", user_id).limit(1).execute()
    )
    if getattr(prof_q, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (get profile): {prof_q.error}")
    if prof_q.data:
        return user_id, prof_q.data[0]["id"]

    # Create a minimal profile using email/name from auth metadata when available
    email = None
    name = None
    try:
        auth_user = anon_client.auth.get_user(token)
        user_obj = getattr(auth_user, "user", None) or (auth_user.get("user") if isinstance(auth_user, dict) else None)
        email = (getattr(user_obj, "email", None) or (user_obj.get("email") if isinstance(user_obj, dict) else None))
        meta = (getattr(user_obj, "user_metadata", None) or (user_obj.get("user_metadata") if isinstance(user_obj, dict) else None)) or {}
        try:
            name = meta.get("full_name") or meta.get("name") or meta.get("preferred_username")
        except Exception:
            name = None
    except Exception:
        pass

    ins = (
        supabase.table("user_profiles")
        .insert({
            "auth_user_id": user_id,
            "email": email,
            "name": name,
        })
        .execute()
    )
    if getattr(ins, "error", None):
        # If a race created it already, proceed to refetch; otherwise fail
        err_txt = str(ins.error)
        if "duplicate" not in err_txt.lower():
            raise HTTPException(status_code=500, detail=f"Supabase error (create profile): {ins.error}")

    prof_q2 = (
        supabase.table("user_profiles").select("id").eq("auth_user_id", user_id).limit(1).execute()
    )
    if getattr(prof_q2, "error", None) or not prof_q2.data:
        raise HTTPException(status_code=500, detail=f"Supabase error (refetch profile): {getattr(prof_q2, 'error', None)}")
    return user_id, prof_q2.data[0]["id"]


def _get_profile_me(token: Optional[str]):
    # Ensure a profile exists (creates a minimal one for OAuth users)
    _, profile_id = _ensure_user_and_profile(token)
    supabase = get_service_client()
    prof_q = (
        supabase.table("user_profiles")
        .select(
            "id,auth_user_id,email,name,gender,phone,semester,regno,profile_image_url,resume_url,bio,linkedin,github,leetcode,specializations,projects,"\
            "headline,location,dob,portfolio_url,website,twitter,instagram,medium,verification_score,"
            "technologies,skills,certifications,languages,interests,project_info,publications,achievements,experience"
        )
        .eq("id", profile_id)
        .single()
        .execute()
    )
    if getattr(prof_q, "error", None) or not prof_q.data:
        raise HTTPException(status_code=404, detail="Profile not found")

    profile = dict(prof_q.data)
    profile_id_str = profile.get("id")
    if profile_id_str:
        profile["experiences"] = _fetch_profile_related(
            profile_id_str,
            "user_experiences",
            [("order_index", False), ("start_date", True), ("created_at", False)],
        )
        profile["education_entries"] = _fetch_profile_related(
            profile_id_str,
            "user_education",
            [("order_index", False), ("created_at", False)],
        )
        profile["certification_entries"] = _fetch_profile_related(
            profile_id_str,
            "user_certifications",
            [("order_index", False), ("issue_date", True), ("created_at", False)],
        )
        profile["portfolio_projects"] = _fetch_profile_related(
            profile_id_str,
            "user_portfolio_projects",
            [("order_index", False), ("start_date", True), ("created_at", False)],
        )
        profile["publication_entries"] = _fetch_profile_related(
            profile_id_str,
            "user_publications",
            [("order_index", False), ("publication_date", True), ("created_at", False)],
        )

    # Derive academic info strictly from education entries (FK columns if present)
    college = None
    department = None
    batch = None
    derived_semester = profile.get("semester")
    derived_regno = profile.get("regno")
    primary_edu = None
    edu_entries = profile.get("education_entries") or []
    if edu_entries:
        # choose highest current_semester, else first
        typed = [e for e in edu_entries if isinstance(e, dict)]
        if typed:
            typed.sort(key=lambda r: (r.get("current_semester") or 0, -(r.get("order_index") or 0)), reverse=True)
            primary_edu = typed[0]
    if primary_edu:
        if primary_edu.get("current_semester"):
            derived_semester = primary_edu.get("current_semester")
        if primary_edu.get("regno"):
            derived_regno = primary_edu.get("regno")
        supabase = get_service_client()
        # Prefer FK ids on education row (added by normalization) if present
        college_id = primary_edu.get("college_id")
        department_id = primary_edu.get("department_id")
        batch_id = primary_edu.get("batch_id")
        if college_id:
            cq = supabase.table("colleges").select("id,name").eq("id", college_id).limit(1).execute()
            if not getattr(cq, "error", None) and cq.data:
                college = {"id": cq.data[0]["id"], "name": cq.data[0]["name"]}
        if department_id:
            dq = supabase.table("departments").select("id,name").eq("id", department_id).limit(1).execute()
            if not getattr(dq, "error", None) and dq.data:
                department = {"id": dq.data[0]["id"], "name": dq.data[0]["name"]}
        if batch_id:
            bq = supabase.table("batches").select("id,from_year,to_year").eq("id", batch_id).limit(1).execute()
            if not getattr(bq, "error", None) and bq.data:
                batch = {"id": bq.data[0]["id"], "from": bq.data[0]["from_year"], "to": bq.data[0]["to_year"]}
        # Fallback on legacy text if FK not available
        if not college and primary_edu.get("school"):
            college = {"id": None, "name": primary_edu.get("school")}
        if not department and primary_edu.get("department"):
            department = {"id": None, "name": (primary_edu.get("department") or "").upper()}
        if not batch and primary_edu.get("batch_range"):
            import re as _re
            years = _re.findall(r"\b(\d{4})\b", primary_edu.get("batch_range") or "")
            if len(years) >= 2:
                try:
                    batch = {"id": None, "from": int(years[0]), "to": int(years[1])}
                except Exception:
                    pass
    profile["college"] = college
    profile["department"] = department
    profile["batch"] = batch
    profile["semester"] = derived_semester
    profile["regno"] = derived_regno
    return profile


def _clean_media_items(media_items: Optional[List[Dict[str, Any]]]) -> Optional[List[Dict[str, Any]]]:
    cleaned: List[Dict[str, Any]] = []
    for raw in media_items or []:
        if not isinstance(raw, dict):
            continue
        url = _strip_or_none(raw.get("url"))
        if not url:
            continue
        cleaned.append(
            {
                "kind": _strip_or_none(raw.get("kind")),
                "url": url,
                "title": _strip_or_none(raw.get("title")),
            }
        )
    return cleaned or None


def _prepare_experience_rows(rows: Optional[List[Dict[str, Any]]]) -> List[Dict[str, Any]]:
    prepared: List[Dict[str, Any]] = []
    now_iso = datetime.utcnow().isoformat()
    for idx, row in enumerate(rows or []):
        if not isinstance(row, dict):
            continue
        title = _strip_or_none(row.get("title"))
        start_date = _date_or_none(row.get("start_date"))
        if not title or not start_date:
            continue
        prepared_row: Dict[str, Any] = {
            "title": title,
            "employment_type": _strip_or_none(row.get("employment_type")),
            "company": _strip_or_none(row.get("company")),
            "company_logo_url": _strip_or_none(row.get("company_logo_url")),
            "location": _strip_or_none(row.get("location")),
            "location_type": _strip_or_none(row.get("location_type")),
            "start_date": start_date,
            "end_date": _date_or_none(row.get("end_date")),
            "is_current": bool(row.get("is_current")),
            "description": _strip_or_none(row.get("description")),
            "order_index": idx,
            "updated_at": now_iso,
        }
        media_items = row.get("media")
        cleaned_media = _clean_media_items(media_items if isinstance(media_items, list) else None)
        if cleaned_media is not None:
            prepared_row["media"] = cleaned_media
        row_id = _strip_or_none(row.get("id"))
        if not row_id:
            row_id = str(uuid.uuid4())
        prepared_row["id"] = row_id
        prepared.append(prepared_row)
    return prepared


def _prepare_education_rows(rows: Optional[List[Dict[str, Any]]]) -> List[Dict[str, Any]]:
    """Prepare revised education rows.

    Legacy keys (field_of_study, start_date, end_date) ignored after schema change.
    """
    prepared: List[Dict[str, Any]] = []
    now_iso = datetime.utcnow().isoformat()
    for idx, row in enumerate(rows or []):
        if not isinstance(row, dict):
            continue
        school = _strip_or_none(row.get("school"))
        if not school:
            continue
        # Attempt FK resolution when ids missing but names present.
        college_id_val = _strip_or_none(row.get("college_id"))
        degree_id_val = _strip_or_none(row.get("degree_id"))
        department_id_val = _strip_or_none(row.get("department_id"))
        batch_id_val = _strip_or_none(row.get("batch_id"))
        try:
            if not college_id_val and school:
                # Upsert college by name
                college_uuid = upsert_college(school)
                college_id_val = str(college_uuid)
            # Resolve or create degree if name provided and degree_id missing
            degree_name_candidate = _strip_or_none(row.get("degree"))
            if college_id_val and not degree_id_val and degree_name_candidate:
                supabase = get_service_client()
                deg_q = supabase.table("degrees").select("id").eq("college_id", college_id_val).eq("name", degree_name_candidate).limit(1).execute()
                if not getattr(deg_q, "error", None) and deg_q.data:
                    degree_id_val = deg_q.data[0]["id"]
                else:
                    ins_deg = supabase.table("degrees").insert({
                        "college_id": college_id_val,
                        "name": degree_name_candidate,
                    }).execute()
                    if not getattr(ins_deg, "error", None):
                        ref_deg = supabase.table("degrees").select("id").eq("college_id", college_id_val).eq("name", degree_name_candidate).limit(1).execute()
                        if not getattr(ref_deg, "error", None) and ref_deg.data:
                            degree_id_val = ref_deg.data[0]["id"]
            if college_id_val and not department_id_val:
                dept_name_candidate = _strip_or_none(row.get("department"))
                if dept_name_candidate:
                    try:
                        department_uuid = _resolve_department_id(uuid.UUID(college_id_val), dept_name_candidate)
                        department_id_val = str(department_uuid)
                    except HTTPException:
                        # Department not found; create minimal department without degree context
                        supabase = get_service_client()
                        dep_payload = {"college_id": college_id_val, "name": dept_name_candidate.upper()}
                        if degree_id_val:
                            dep_payload["degree_id"] = degree_id_val
                        ins_dep = supabase.table("departments").insert(dep_payload).execute()
                        if not getattr(ins_dep, "error", None):
                            ref_dep = supabase.table("departments").select("id").eq("college_id", college_id_val).eq("name", dept_name_candidate.upper()).limit(1).execute()
                            if not getattr(ref_dep, "error", None) and ref_dep.data:
                                department_id_val = ref_dep.data[0]["id"]
            if college_id_val and department_id_val and not batch_id_val:
                batch_range_raw = _strip_or_none(row.get("batch_range"))
                if batch_range_raw and "-" in batch_range_raw:
                    try:
                        yr_from = int(batch_range_raw.split("-",1)[0])
                        yr_to = int(batch_range_raw.split("-",1)[1])
                        batch_uuid = _get_or_create_batch_id(uuid.UUID(college_id_val), uuid.UUID(department_id_val), yr_from, yr_to)
                        batch_id_val = str(batch_uuid)
                    except Exception:
                        pass
        except Exception:
            # Fail soft; continue without FK resolution if anything goes wrong
            pass
        prepared_row: Dict[str, Any] = {
            "school": school,
            "degree": _strip_or_none(row.get("degree")),
            "department": _strip_or_none(row.get("department")),
            "batch_range": _strip_or_none(row.get("batch_range")),
            "section": _strip_or_none(row.get("section")),
            "regno": _strip_or_none(row.get("regno")),
            "current_semester": row.get("current_semester") if isinstance(row.get("current_semester"), int) else None,
            "grade": _strip_or_none(row.get("grade")),
            "activities": _strip_or_none(row.get("activities")),
            "description": _strip_or_none(row.get("description")),
            "order_index": idx,
            "updated_at": now_iso,
        }
        # Optional new FK id fields propagated from frontend (already resolved or chosen)
        for fk_key in ("college_id", "degree_id", "department_id", "batch_id"):
            val = _strip_or_none(row.get(fk_key))
            if val:
                prepared_row[fk_key] = val
        # Include resolved ones if not provided originally
        if college_id_val and "college_id" not in prepared_row:
            prepared_row["college_id"] = college_id_val
        if degree_id_val and "degree_id" not in prepared_row:
            prepared_row["degree_id"] = degree_id_val
        if department_id_val and "department_id" not in prepared_row:
            prepared_row["department_id"] = department_id_val
        if batch_id_val and "batch_id" not in prepared_row:
            prepared_row["batch_id"] = batch_id_val
        row_id = _strip_or_none(row.get("id"))
        if not row_id:
            row_id = str(uuid.uuid4())
        prepared_row["id"] = row_id
        prepared.append(prepared_row)
    return prepared


def _prepare_certification_rows(rows: Optional[List[Dict[str, Any]]]) -> List[Dict[str, Any]]:
    prepared: List[Dict[str, Any]] = []
    now_iso = datetime.utcnow().isoformat()
    for idx, row in enumerate(rows or []):
        if not isinstance(row, dict):
            continue
        name = _strip_or_none(row.get("name"))
        if not name:
            continue
        prepared_row: Dict[str, Any] = {
            "name": name,
            "issuing_org": _strip_or_none(row.get("issuing_org")),
            "issue_date": _date_or_none(row.get("issue_date")),
            "expiration_date": _date_or_none(row.get("expiration_date")),
            "does_not_expire": bool(row.get("does_not_expire")),
            "credential_id": _strip_or_none(row.get("credential_id")),
            "credential_url": _strip_or_none(row.get("credential_url")),
            "description": _strip_or_none(row.get("description")),
            "order_index": idx,
            "updated_at": now_iso,
        }
        row_id = _strip_or_none(row.get("id"))
        if not row_id:
            row_id = str(uuid.uuid4())
        prepared_row["id"] = row_id
        prepared.append(prepared_row)
    return prepared


def _prepare_project_rows(rows: Optional[List[Dict[str, Any]]]) -> List[Dict[str, Any]]:
    prepared: List[Dict[str, Any]] = []
    now_iso = datetime.utcnow().isoformat()
    for idx, row in enumerate(rows or []):
        if not isinstance(row, dict):
            continue
        name = _strip_or_none(row.get("name"))
        if not name:
            continue
        tech_stack_raw = row.get("tech_stack")
        tech_stack_list: Optional[List[str]] = None
        if isinstance(tech_stack_raw, list):
            tech_stack_list = [s for s in (_strip_or_none(item) for item in tech_stack_raw) if s]
        elif isinstance(tech_stack_raw, str):
            tech_stack_list = [s for s in (_strip_or_none(part) for part in tech_stack_raw.split(",")) if s]
        team_raw = row.get("team")
        team_list: Optional[List[Dict[str, Any]]] = None
        if isinstance(team_raw, list):
            normalized: List[Dict[str, Any]] = []
            for entry in team_raw:
                if isinstance(entry, dict):
                    name_val = _strip_or_none(entry.get("name"))
                    if name_val:
                        normalized.append(
                            {
                                "name": name_val,
                                "role": _strip_or_none(entry.get("role")),
                                "profile_url": _strip_or_none(entry.get("profile_url")),
                                "user_id": _strip_or_none(entry.get("user_id")),
                            }
                        )
                else:
                    item_name = _strip_or_none(entry)
                    if item_name:
                        normalized.append({"name": item_name})
            team_list = normalized or None
        elif isinstance(team_raw, str):
            members = [s for s in (_strip_or_none(part) for part in team_raw.split(",")) if s]
            if members:
                team_list = [{"name": member} for member in members]
        prepared_row: Dict[str, Any] = {
            "name": name,
            "associated_experience_id": _strip_or_none(row.get("associated_experience_id")),
            "associated_education_id": _strip_or_none(row.get("associated_education_id")),
            "start_date": _date_or_none(row.get("start_date")),
            "end_date": _date_or_none(row.get("end_date")),
            "url": _strip_or_none(row.get("url")),
            "description": _strip_or_none(row.get("description")),
            "order_index": idx,
            "updated_at": now_iso,
        }
        if tech_stack_list is not None:
            prepared_row["tech_stack"] = tech_stack_list
        if team_list is not None:
            prepared_row["team"] = team_list
        row_id = _strip_or_none(row.get("id"))
        if not row_id:
            row_id = str(uuid.uuid4())
        prepared_row["id"] = row_id
        prepared.append(prepared_row)
    return prepared


def _prepare_publication_rows(rows: Optional[List[Dict[str, Any]]]) -> List[Dict[str, Any]]:
    prepared: List[Dict[str, Any]] = []
    now_iso = datetime.utcnow().isoformat()
    for idx, row in enumerate(rows or []):
        if not isinstance(row, dict):
            continue
        title = _strip_or_none(row.get("title"))
        if not title:
            continue
        authors_raw = row.get("authors")
        authors_list: Optional[List[str]] = None
        if isinstance(authors_raw, list):
            authors_list = [s for s in (_strip_or_none(part) for part in authors_raw) if s]
        elif isinstance(authors_raw, str):
            authors_list = [s for s in (_strip_or_none(part) for part in authors_raw.split(",")) if s]
        prepared_row: Dict[str, Any] = {
            "title": title,
            "publisher": _strip_or_none(row.get("publisher")),
            "publication_date": _date_or_none(row.get("publication_date")),
            "url": _strip_or_none(row.get("url")),
            "abstract": _strip_or_none(row.get("abstract")),
            "order_index": idx,
            "updated_at": now_iso,
        }
        if authors_list is not None:
            prepared_row["authors"] = authors_list
        row_id = _strip_or_none(row.get("id"))
        if not row_id:
            row_id = str(uuid.uuid4())
        prepared_row["id"] = row_id
        prepared.append(prepared_row)
    return prepared


def _sync_profile_collection(profile_id: str, table: str, rows: List[Dict[str, Any]]):
    supabase = get_service_client()
    existing_q = (
        supabase.table(table)
        .select("id")
        .eq("user_profile_id", profile_id)
        .execute()
    )
    if getattr(existing_q, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (fetch {table}): {existing_q.error}")
    existing_ids = {row["id"] for row in (existing_q.data or []) if row.get("id")}
    incoming_ids = {row["id"] for row in rows if row.get("id")}
    to_delete = list(existing_ids - incoming_ids)

    delete_res = None
    existing_rows = [row for row in rows if row.get("id")]
    new_rows = [row for row in rows if not row.get("id")]

    if existing_rows:
        payload = [{**row, "user_profile_id": profile_id} for row in existing_rows]
        upsert = (
            supabase.table(table)
            .upsert(payload, on_conflict="id")
            .execute()
        )
        if getattr(upsert, "error", None):
            raise HTTPException(status_code=500, detail=f"Supabase error (upsert {table}): {upsert.error}")

    if new_rows:
        insert_payload = [{**row, "user_profile_id": profile_id} for row in new_rows]
        insert_res = supabase.table(table).insert(insert_payload).execute()
        if getattr(insert_res, "error", None):
            raise HTTPException(status_code=500, detail=f"Supabase error (insert {table}): {insert_res.error}")

    if to_delete:
        delete_res = (
            supabase.table(table)
            .delete()
            .in_("id", to_delete)
            .execute()
        )
        if getattr(delete_res, "error", None):
            raise HTTPException(status_code=500, detail=f"Supabase error (cleanup {table}): {delete_res.error}")

    if not rows and existing_ids and not to_delete:
        delete_res = (
            supabase.table(table)
            .delete()
            .eq("user_profile_id", profile_id)
            .execute()
        )
        if getattr(delete_res, "error", None):
            raise HTTPException(status_code=500, detail=f"Supabase error (clear {table}): {delete_res.error}")

    if delete_res is not None and getattr(delete_res, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (delete {table}): {delete_res.error}")


def _fetch_profile_related(profile_id: str, table: str, order_by: Optional[List[Tuple[str, bool]]] = None) -> List[Dict[str, Any]]:
    supabase = get_service_client()
    def _build_query():
        q = supabase.table(table).select("*").eq("user_profile_id", profile_id)
        if order_by:
            for column, desc in order_by:
                q = q.order(column, desc=bool(desc))
        return q
    try:
        res = _supabase_retry(lambda: _build_query().execute())
    except HTTPXRemoteProtocolError as exc:  # pragma: no cover - network dependent
        # Treat as empty on transient disconnects so /api/me still works
        supabase_logger.warning("profile related fetch '%s' transient protocol error: %s", table, exc)
        res = type("_R", (), {"data": [], "error": None})()
    if getattr(res, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (fetch {table}): {res.error}")
    items: List[Dict[str, Any]] = []
    for raw in res.data or []:
        if not isinstance(raw, dict):
            continue
        item = dict(raw)
        item.pop("user_profile_id", None)
        for key in ("created_at", "updated_at", "start_date", "end_date", "issue_date", "expiration_date", "publication_date"):
            if key in item and isinstance(item[key], (datetime, date)):
                item[key] = item[key].isoformat()
        if table == "user_experiences":
            item["media"] = item.get("media") or []
        if table == "user_portfolio_projects":
            item["tech_stack"] = item.get("tech_stack") or []
            item["team"] = item.get("team") or []
        if table == "user_publications":
            item["authors"] = item.get("authors") or []
        items.append(item)
    return items


def _update_profile_me(token: Optional[str], fields: dict):
    # Ensure a profile exists (creates a minimal one for OAuth users)
    _, profile_id = _ensure_user_and_profile(token)
    supabase = get_service_client()
    # allow base fields first
    allowed = {
        "name", "phone", "bio", "linkedin", "github", "leetcode", "specializations", "projects", "semester", "regno",
        # Tunex-like extra fields:
        "headline", "location", "dob", "portfolio_url", "website", "twitter", "instagram", "medium",
        "verification_score", "technologies", "skills", "certifications", "languages", "interests",
        "project_info", "publications", "achievements", "experience",
    }
    fields = fields or {}
    experiences_payload = fields.pop("experiences", None)
    education_payload = fields.pop("education_entries", None)
    certifications_payload = fields.pop("certification_entries", None)
    projects_payload = fields.pop("portfolio_projects", None)
    publications_payload = fields.pop("publication_entries", None)

    payload = {k: v for k, v in fields.items() if k in allowed}

    # Resolve academic relations if provided
    college_name = fields.get("college_name")
    department_name = fields.get("department_name")
    batch_from = fields.get("batch_from")
    batch_to = fields.get("batch_to")

    if college_name:
        college_id = _resolve_college_id_by_name(college_name)
        payload["college_id"] = str(college_id)
        if department_name:
            department_id = _resolve_department_id(college_id, department_name)
            payload["department_id"] = str(department_id)
            if batch_from is not None and batch_to is not None:
                batch_id = _get_or_create_batch_id(college_id, department_id, int(batch_from), int(batch_to))
                payload["batch_id"] = str(batch_id)
                payload["batch_from"] = int(batch_from)
                payload["batch_to"] = int(batch_to)

    update_payload = _supabase_payload(payload)
    if update_payload:
        retry_payload = dict(update_payload)
        fallback_keys = {"college_id", "department_id", "batch_id", "batch_from", "batch_to"}
        for attempt in range(3):
            if not retry_payload:
                break
            try:
                upd = supabase.table("user_profiles").update(retry_payload).eq("id", profile_id).execute()
            except APIError as exc:
                err_code = getattr(exc, "code", None)
                err_message = getattr(exc, "message", None) or getattr(exc, "details", None) or str(exc)
                lower_msg = (err_message or "").lower()
                # If undefined column error, remove potential FK fields and retry.
                if err_code == "42703":
                    removed = False
                    for key in list(retry_payload.keys()):
                        if key in fallback_keys:
                            retry_payload.pop(key, None)
                            removed = True
                    if removed:
                        continue
                # If a DB trigger references a non-existent NEW.department_id (stale trigger), skip base update gracefully.
                if ("record \"new\" has no field" in lower_msg) or ("record new has no field" in lower_msg):
                    supabase_logger.warning("Skipping user_profiles update due to trigger missing field: %s", err_message)
                    retry_payload.clear()
                    break
                raise HTTPException(status_code=500, detail=f"Supabase error (update profile): {err_message}")

            err_msg = getattr(upd, "error", None)
            if err_msg:
                msg = str(err_msg)
                lowered = msg.lower()
                if ("42703" in lowered) or any(field in lowered for field in ("college_id", "department_id", "batch_id")):
                    removed = False
                    for key in list(retry_payload.keys()):
                        if key in fallback_keys:
                            retry_payload.pop(key, None)
                            removed = True
                    if removed:
                        continue
                # Handle stale trigger error pattern on successful call that returned an error payload
                if ("record \"new\" has no field" in lowered) or ("record new has no field" in lowered):
                    supabase_logger.warning("Skipping user_profiles update due to trigger missing field: %s", msg)
                    break
                raise HTTPException(status_code=500, detail=f"Supabase error (update profile): {msg}")
            break
    if experiences_payload is not None:
        prepared = _prepare_experience_rows(experiences_payload if isinstance(experiences_payload, list) else None)
        _sync_profile_collection(profile_id, "user_experiences", prepared)
    if education_payload is not None:
        prepared = _prepare_education_rows(education_payload if isinstance(education_payload, list) else None)
        _sync_profile_collection(profile_id, "user_education", prepared)
    if certifications_payload is not None:
        prepared = _prepare_certification_rows(certifications_payload if isinstance(certifications_payload, list) else None)
        _sync_profile_collection(profile_id, "user_certifications", prepared)
    if projects_payload is not None:
        prepared = _prepare_project_rows(projects_payload if isinstance(projects_payload, list) else None)
        _sync_profile_collection(profile_id, "user_portfolio_projects", prepared)
    if publications_payload is not None:
        prepared = _prepare_publication_rows(publications_payload if isinstance(publications_payload, list) else None)
        _sync_profile_collection(profile_id, "user_publications", prepared)
    return {"updated": True}


def _storage_get_client(svc_client):
    storage = getattr(svc_client, "storage", None)
    if callable(storage):
        storage = storage()
    if storage is None:
        raise HTTPException(status_code=500, detail="Storage client unavailable")
    return storage


def _storage_upload_bytes(svc_client, bucket: str, dest: str, content: bytes, content_type: Optional[str] = None) -> str:
    storage = _storage_get_client(svc_client)
    last_err = None
    result = None
    attempts = []
    # Attempt 1: modern signature
    try:
        attempts.append("upload(dest, bytes, opts dict upsert)")
        result = storage.from_(bucket).upload(
            dest,
            content,
            {
                "content-type": content_type or "application/octet-stream",
                "upsert": "true",
                "cache-control": "86400",
            },
        )
    except Exception as exc:
        last_err = exc
        result = None
    # Attempt 2: keyword args path/file
    if result is None:
        try:
            attempts.append("upload(path=dest, file=bytes)")
            result = storage.from_(bucket).upload(path=dest, file=content)
        except Exception as exc:
            last_err = exc
            result = None
    # Attempt 3: wrap bytes in BytesIO
    if result is None:
        import io as _io
        try:
            attempts.append("upload(path=dest, file=BytesIO)")
            result = storage.from_(bucket).upload(path=dest, file=_io.BytesIO(content))
        except Exception as exc:
            last_err = exc
            result = None
    # Attempt 4: raw simple call (legacy)
    if result is None:
        try:
            attempts.append("upload(dest, bytes)")
            result = storage.from_(bucket).upload(dest, content)
        except Exception as exc:
            last_err = exc
            result = None
    if result is None:
        # Fallback: attempt a remove then a plain upload (handles 409 duplicate when upsert unsupported)
        try:
            attempts.append("remove+reupload fallback")
            try:
                storage.from_(bucket).remove([dest])
            except Exception:
                pass
            result = storage.from_(bucket).upload(dest, content, {"content-type": content_type or "application/octet-stream"})
        except Exception as exc:
            last_err = exc
            supabase_logger.error("All upload attempts failed", extra={"bucket": bucket, "dest": dest, "attempts": attempts, "error": str(last_err)})
            raise HTTPException(status_code=500, detail=f"Upload failed: {last_err}")
    if result is None:
        supabase_logger.exception("Upload failed: %s", last_err)
        raise HTTPException(status_code=500, detail=f"Upload failed: {last_err}")
    if isinstance(result, dict) and result.get("error"):
        raise HTTPException(status_code=500, detail=f"Upload error: {result.get('error')}")
    if getattr(result, "error", None):
        raise HTTPException(status_code=500, detail=f"Upload error: {result.error}")
    return dest


def _storage_public_url(svc_client, bucket: str, path: str) -> str:
    base_url = os.getenv("SUPABASE_URL", "").rstrip("/")
    storage = _storage_get_client(svc_client)
    try:
        res = storage.from_(bucket).get_public_url(path)
        if isinstance(res, dict):
            cand = res.get("publicUrl") or res.get("publicURL") or (res.get("data") or {}).get("publicUrl")
            if cand:
                return cand
        elif isinstance(res, str):
            return res
        else:
            for attr in ("public_url", "publicUrl", "publicURL"):
                if hasattr(res, attr):
                    val = getattr(res, attr)
                    if isinstance(val, str):
                        return val
    except Exception:
        pass
    encoded = quote(path, safe="")
    return f"{base_url}/storage/v1/object/public/{bucket}/{encoded}"


def _read_upload_bytes(file_obj) -> bytes:
    try:
        file_obj.file.seek(0)
        return file_obj.file.read()
    except Exception:
        try:
            return file_obj.read()
        except Exception:
            return b""



def _upload_profile_asset(token: Optional[str], kind: str, file):
    from uuid import uuid4

    # Ensure a profile exists (creates a minimal one for OAuth users)
    _, profile_id = _ensure_user_and_profile(token)
    supabase = get_service_client()
    bucket = os.getenv("SUPABASE_BUCKET", "").strip()
    if not bucket:
        raise HTTPException(status_code=500, detail="Missing SUPABASE_BUCKET in environment")

    filename = file.filename or ("upload-" + uuid4().hex)
    ext = os.path.splitext(filename)[1].lower()
    media_kind = (kind or "").strip().lower()
    if media_kind == "image" and ext not in {".png", ".jpg", ".jpeg", ".webp"}:
        raise HTTPException(status_code=400, detail="Invalid image type")
    if media_kind == "resume" and ext not in {".pdf", ".doc", ".docx"}:
        raise HTTPException(status_code=400, detail="Invalid resume type")

    blob = _read_upload_bytes(file)
    if not blob:
        raise HTTPException(status_code=400, detail="Empty upload")

    safe_ext = ext if ext else (".png" if media_kind == "image" else ".pdf")
    dest_folder = f"profiles/{profile_id}"
    dest = f"{dest_folder}/{uuid4().hex}{safe_ext}"
    _storage_upload_bytes(supabase, bucket, dest, blob, file.content_type)
    public_url = _storage_public_url(supabase, bucket, dest)

    column = "profile_image_url" if media_kind == "image" else "resume_url"
    upd = supabase.table("user_profiles").update({column: public_url}).eq("id", profile_id).execute()
    if getattr(upd, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (update profile asset): {upd.error}")

    return {"kind": media_kind, "url": public_url, "path": dest}




def get_completed_topic_ids(token: Optional[str]):
    # Ensure a profile exists to scope progress correctly for new OAuth users
    _, profile_id = _ensure_user_and_profile(token)
    supabase = get_service_client()
    try:
        q = _supabase_retry(
            lambda: (
                supabase.table("user_topic_progress")
                .select("topic_id")
                .eq("user_profile_id", profile_id)
                .execute()
            )
        )
    except HTTPXRemoteProtocolError as exc:  # pragma: no cover - network dependent
        supabase_logger.warning("Progress lookup transient protocol error: %s", exc)
        raise HTTPException(status_code=503, detail="Upstream temporarily unavailable. Please retry.")
    if getattr(q, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (get progress): {q.error}")
    return {"completed_topic_ids": [row["topic_id"] for row in (q.data or [])]}


def toggle_topic_completion(token: Optional[str], topic_id: uuid.UUID, completed: bool):
    # Ensure a profile exists for new OAuth users
    _, profile_id = _ensure_user_and_profile(token)
    supabase = get_service_client()
    if completed:
        # Use upsert (idempotent) or gracefully ignore duplicate key errors.
        try:
            # Prefer upsert if available for idempotency
            upsert_method = getattr(supabase.table("user_topic_progress"), "upsert", None)
            if callable(upsert_method):
                resp = upsert_method({"user_profile_id": profile_id, "topic_id": str(topic_id)}).execute()
                if getattr(resp, "error", None):
                    raise HTTPException(status_code=500, detail=f"Supabase error (mark done): {resp.error}")
            else:
                resp = (
                    supabase.table("user_topic_progress")
                    .insert({"user_profile_id": profile_id, "topic_id": str(topic_id)})
                    .execute()
                )
                if getattr(resp, "error", None):
                    txt = str(resp.error).lower()
                    if "duplicate" not in txt and "unique" not in txt:
                        raise HTTPException(status_code=500, detail=f"Supabase error (mark done): {resp.error}")
        except Exception as exc:
            # Allow duplicate insert races silently
            msg = str(getattr(exc, "detail", exc))
            if not ("duplicate" in msg.lower() or "unique" in msg.lower()):
                raise
        return {"completed": True}
    else:
        resp = (
            supabase.table("user_topic_progress")
            .delete()
            .eq("user_profile_id", profile_id)
            .eq("topic_id", str(topic_id))
            .execute()
        )
        if getattr(resp, "error", None):
            raise HTTPException(status_code=500, detail=f"Supabase error (unmark done): {resp.error}")
        return {"completed": False}


def get_progress_summary(token: Optional[str]):
    _, profile_id = _require_user_and_profile(token)
    supabase = get_service_client()

    # Fetch per-course -> per-unit totals and completed counts
    # Get current user's batch/semester first to scope courses
    prof_q = (
        supabase.table("user_profiles").select("batch_id,semester").eq("id", profile_id).limit(1).execute()
    )
    if getattr(prof_q, "error", None) or not prof_q.data:
        raise HTTPException(status_code=500, detail=f"Supabase error (profile scope): {getattr(prof_q, 'error', None)}")
    batch_id = prof_q.data[0].get("batch_id")
    semester = prof_q.data[0].get("semester")

    courses_q = (
        supabase.table("syllabus_courses")
        .select("id,course_code,title,semester")
        .eq("batch_id", batch_id)
        .eq("semester", semester)
        .execute()
    )
    if getattr(courses_q, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (courses for progress): {courses_q.error}")

    # Completed set for quick lookup
    comp_q = (
        supabase.table("user_topic_progress")
        .select("topic_id")
        .eq("user_profile_id", profile_id)
        .execute()
    )
    if getattr(comp_q, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (completed topics): {comp_q.error}")
    completed_set = {row["topic_id"] for row in (comp_q.data or [])}

    summary = []
    for c in (courses_q.data or []):
        course_id = c["id"]
        units_q = (
            supabase.table("syllabus_units")
            .select("id,unit_title,order_in_course")
            .eq("course_id", course_id)
            .order("order_in_course")
            .execute()
        )
        if getattr(units_q, "error", None):
            raise HTTPException(status_code=500, detail=f"Supabase error (units for progress): {units_q.error}")
        unit_summaries = []
        course_total = 0
        course_done = 0
        for u in (units_q.data or []):
            topics_q = (
                supabase.table("syllabus_topics")
                .select("id")
                .eq("unit_id", u["id"])
                .order("order_in_unit")
                .execute()
            )
            if getattr(topics_q, "error", None):
                raise HTTPException(status_code=500, detail=f"Supabase error (topics for progress): {topics_q.error}")
            topic_ids = [t["id"] for t in (topics_q.data or [])]
            done = sum(1 for tid in topic_ids if tid in completed_set)
            total = len(topic_ids)
            unit_summaries.append({
                "unit_id": u["id"],
                "unit_title": u["unit_title"],
                "done": done,
                "total": total,
            })
            course_done += done
            course_total += total
        summary.append({
            "course_id": course_id,
            "course_code": c["course_code"],
            "title": c["title"],
            "done": course_done,
            "total": course_total,
            "units": unit_summaries,
        })

    return {"courses": summary}


# --- Projects & collaboration API ------------------------------------------


def _listify(value: Any) -> List[Any]:
    if isinstance(value, list):
        return value
    if value in (None, ""):
        return []
    return [value]


class ProjectBasics(BaseModel):
    title: str
    tagline: str
    domains: List[str] = Field(default_factory=list)
    description: str
    tech_stack: List[str] = Field(default_factory=list)


class ProjectStatus(BaseModel):
    status: str = ""
    start_date: Optional[str] = None
    end_date: Optional[str] = None
    milestones: List[str] = Field(default_factory=list)


class ProjectLinks(BaseModel):
    github: Optional[str] = None
    demo: Optional[str] = None
    video: Optional[str] = None
    docs: Optional[str] = None


class ProjectFunding(BaseModel):
    stage: Optional[str] = None
    budget_inr: Optional[int] = None
    use: Optional[str] = None


class ProjectTeam(BaseModel):
    members: List[str] = Field(default_factory=list)
    roles_hiring: List[str] = Field(default_factory=list)
    compensation: Optional[str] = None
    hours: Optional[str] = None
    role_desc: Optional[str] = None


class ProjectIn(BaseModel):
    basics: ProjectBasics
    status: ProjectStatus
    links: ProjectLinks
    funding: ProjectFunding
    team: ProjectTeam


class ProjectOut(ProjectIn):
    id: str
    user_id: str
    cover_url: Optional[str] = None
    gallery_urls: List[str] = Field(default_factory=list)
    created_at: Optional[str] = None
    updated_at: Optional[str] = None


class ProjectApplicationIn(BaseModel):
    message: Optional[str] = None


class ProjectApplicationOut(BaseModel):
    id: str
    project_id: str
    applicant_user_id: str
    message: Optional[str] = None
    status: str
    created_at: Optional[str] = None
    updated_at: Optional[str] = None


class ProjectApplicationUpdateIn(BaseModel):
    status: str = Field(..., pattern=r"^(pending|accepted|rejected)$")

class CollabMessageIn(BaseModel):
    content: str


class CollabMessageOut(BaseModel):
    id: str
    application_id: str
    sender_user_id: str
    content: str
    created_at: Optional[str] = None


class SkillTestStartIn(BaseModel):
    skill: str


class SkillTestStartOut(BaseModel):
    session_id: str
    skill: str
    questions: List[Dict[str, Any]]


class SkillAnswer(BaseModel):
    question_id: str
    response: str


class SkillTestSubmitIn(BaseModel):
    answers: List[SkillAnswer]


class SkillVerificationOut(BaseModel):
    skill: str
    best_score: Optional[float] = None
    attempts: int = 0
    status: Optional[str] = None
    updated_at: Optional[str] = None



def _project_payload_from_body(body: ProjectIn, user_id: str) -> dict:
    basics = body.basics
    status = body.status
    links = body.links
    funding = body.funding
    team = body.team

    def clean_list(items: Optional[List[str]]) -> List[str]:
        return [item.strip() for item in (items or []) if isinstance(item, str) and item.strip()]

    payload = {
        "user_id": user_id,
        "title": (basics.title or "").strip(),
        "tagline": (basics.tagline or "").strip(),
        "domains": clean_list(basics.domains),
        "description": (basics.description or "").strip(),
        "tech_stack": clean_list(basics.tech_stack),
        "proj_status": (status.status or "").strip(),
        "start_date": status.start_date or None,
        "end_date": status.end_date or None,
        "milestones": clean_list(status.milestones),
        "github": (links.github or None),
        "demo": (links.demo or None),
        "video": (links.video or None),
        "docs": (links.docs or None),
        "fund_stage": (funding.stage or None),
        "fund_budget_inr": int(funding.budget_inr) if funding.budget_inr is not None else None,
        "fund_use": (funding.use or None),
        "team_members": clean_list(team.members),
        "roles_hiring": clean_list(team.roles_hiring),
        "compensation": (team.compensation or None),
        "hours": (team.hours or None),
        "role_desc": (team.role_desc or None),
    }
    return payload


def _project_row_to_out(row: dict) -> dict:
    if not row:
        raise HTTPException(status_code=500, detail="Project payload missing")
    basics = ProjectBasics(
        title=row.get("title") or "",
        tagline=row.get("tagline") or "",
        domains=_listify(row.get("domains")),
        description=row.get("description") or "",
        tech_stack=_listify(row.get("tech_stack")),
    )
    status = ProjectStatus(
        status=row.get("proj_status") or "",
        start_date=row.get("start_date"),
        end_date=row.get("end_date"),
        milestones=_listify(row.get("milestones")),
    )
    links = ProjectLinks(
        github=row.get("github"),
        demo=row.get("demo"),
        video=row.get("video"),
        docs=row.get("docs"),
    )
    funding = ProjectFunding(
        stage=row.get("fund_stage"),
        budget_inr=row.get("fund_budget_inr"),
        use=row.get("fund_use"),
    )
    team = ProjectTeam(
        members=_listify(row.get("team_members")),
        roles_hiring=_listify(row.get("roles_hiring")),
        compensation=row.get("compensation"),
        hours=row.get("hours"),
        role_desc=row.get("role_desc"),
    )
    project = ProjectOut(
        basics=basics,
        status=status,
        links=links,
        funding=funding,
        team=team,
        id=str(row.get("id")),
        user_id=str(row.get("user_id")),
        cover_url=row.get("cover_url"),
        gallery_urls=_listify(row.get("gallery_urls")),
        created_at=row.get("created_at"),
        updated_at=row.get("updated_at"),
    )
    return project.dict()


def _application_row_to_out(row: dict) -> dict:
    app = ProjectApplicationOut(
        id=str(row.get("id")),
        project_id=str(row.get("project_id")),
        applicant_user_id=str(row.get("applicant_user_id")),
        message=row.get("message"),
        status=str(row.get("status") or "pending"),
        created_at=row.get("created_at"),
        updated_at=row.get("updated_at"),
    )
    return app.dict()


def _ensure_project_exists(project_id: str) -> dict:
    supabase = get_service_client()
    res = supabase.table(PROJECTS_TABLE).select("*").eq("id", project_id).single().execute()
    if getattr(res, "error", None):
        raise HTTPException(status_code=400, detail=f"Supabase error: {res.error}")
    data = getattr(res, "data", None)
    if not data:
        raise HTTPException(status_code=404, detail="Project not found")
    return data


def _ensure_owner(project_id: str, user_id: str) -> dict:
    project = _ensure_project_exists(project_id)
    if str(project.get("user_id")) != str(user_id):
        raise HTTPException(status_code=403, detail="Not your project")
    return project


def _normalize_skill(skill: str) -> str:
    return (skill or "").strip()


def _fallback_language_for_skill(skill: str) -> str:
    low = (skill or "").lower()
    if any(key in low for key in ("python", "pandas", "django")):
        return "python"
    if any(key in low for key in ("javascript", "react", "node")):
        return "javascript"
    if "java" in low:
        return "java"
    if "c++" in low or "cpp" in low:
        return "cpp"
    if "sql" in low:
        return "sql"
    return "python"


def _fallback_questions(skill: str) -> List[Dict[str, Any]]:
    language = _fallback_language_for_skill(skill)
    base = skill or "the skill"
    skill_lower = (skill or "skill").lower()
    keywords_map = {
        "python": ["def", "list", "dict"],
        "javascript": ["function", "const", "array"],
        "java": ["class", "public", "method"],
        "cpp": ["vector", "std", "loop"],
        "sql": ["select", "where", "join"],
    }
    keywords = keywords_map.get(language, [skill_lower, "project", "team"])
    qid_prefix = uuid.uuid4().hex
    return [
        {
            "id": f"{qid_prefix}-mc",
            "kind": "ceq",
            "prompt": f"Which option best captures a practical use-case of {base}?",
            "options": [
                "A. Documenting theory without implementation",
                "B. Building or iterating on a real project with measurable outcomes",
                "C. Collecting inspirational quotes",
                "D. Focusing only on certifications",
            ],
            "answer_key": {"correct_option": "b"},
        },
        {
            "id": f"{qid_prefix}-code",
            "kind": "coding",
            "language": language,
            "prompt": f"Write a short {language} snippet that demonstrates how you would track progress while learning {base}. Include a test or printed output.",
            "answer_key": {"keywords": keywords, "min_hits": max(2, len(keywords) - 1)},
        },
        {
            "id": f"{qid_prefix}-reflect",
            "kind": "coding",
            "language": language,
            "prompt": f"Describe in {language} (code or structured comments) how you would onboard a collaborator to your {base} project, mentioning tools, communication, and deliverables.",
            "answer_key": {"keywords": ["plan", "deliverable", "feedback", base.lower()], "min_hits": 2},
        },
    ]


def _public_questions(questions: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    out: List[Dict[str, Any]] = []
    for q in questions:
        out.append({k: v for k, v in q.items() if k != "answer_key"})
    return out


def _grade_skill_answers(questions: List[Dict[str, Any]], answers: Dict[str, str]) -> Dict[str, Any]:
    details: List[Dict[str, Any]] = []
    total = 0.0
    counted = 0
    for question in questions:
        qid = question.get("id")
        if not qid:
            continue
        kind = question.get("kind") or ""
        response = (answers.get(qid) or "").strip()
        info: Dict[str, Any] = {"question_id": qid, "kind": kind, "score": 0.0}
        if not response:
            info["feedback"] = "No answer provided."
            details.append(info)
            continue
        score = 0.0
        if kind == "ceq":
            expected = (question.get("answer_key") or {}).get("correct_option", "").strip().lower()
            if expected and response.lower().startswith(expected):
                score = 100.0
                info["feedback"] = "Correct option."
            else:
                info["feedback"] = f"Expected option {expected.upper() or '?'}"
        else:
            key = (question.get("answer_key") or {})
            keywords = key.get("keywords") if isinstance(key, dict) else None
            if keywords:
                text_lower = response.lower()
                hits = sum(1 for kw in keywords if isinstance(kw, str) and kw.lower() in text_lower)
                min_hits = key.get("min_hits") or len(keywords)
                score = min(100.0, (hits / max(1, min_hits)) * 100.0)
                info["matched_keywords"] = hits
                info["keywords"] = keywords
                info["feedback"] = f"Matched {hits} of {len(keywords)} keywords."
            else:
                score = 50.0
                info["feedback"] = "Heuristic score (no rubric)."
        score = max(0.0, min(100.0, score))
        info["score"] = round(score, 2)
        details.append(info)
        total += score
        counted += 1
    final_score = round(total / counted, 2) if counted else 0.0
    status = "verified" if final_score >= 70 else "needs_review"
    return {"score": final_score, "status": status, "details": details}


def _insert_skill_test_session(user_id: str, skill: str, questions: List[Dict[str, Any]]) -> str:
    supabase = get_service_client()
    res = supabase.table(SKILL_TESTS_TABLE).insert({
        "user_id": user_id,
        "skill": skill,
        "questions": questions,
        "status": "active",
    }).execute()
    data = getattr(res, "data", None)
    if isinstance(data, list) and data:
        return data[0].get("id")
    if isinstance(data, dict) and data.get("id"):
        return data.get("id")
    raise HTTPException(status_code=500, detail="Unable to create test session")


def _get_skill_test_session(session_id: str) -> Optional[dict]:
    supabase = get_service_client()
    res = supabase.table(SKILL_TESTS_TABLE).select("*").eq("id", session_id).single().execute()
    if getattr(res, "error", None):
        return None
    return getattr(res, "data", None)


def _update_skill_test_submission(session_id: str, result: Dict[str, Any]):
    supabase = get_service_client()
    supabase.table(SKILL_TESTS_TABLE).update({
        "status": "completed",
        "score": result.get("score"),
        "result": result,
        "submitted_at": datetime.utcnow().isoformat() + "Z",
    }).eq("id", session_id).execute()


def _upsert_skill_verification(user_id: str, skill: str, score: float):
    supabase = get_service_client()
    payload = {
        "user_id": user_id,
        "skill": skill,
        "best_score": score,
        "attempts": 1,
        "status": "verified" if score >= 70 else "needs_review",
    }
    try:
        supabase.table(SKILL_VERIFICATIONS_TABLE).upsert(payload, on_conflict="user_id,skill").execute()
    except Exception:
        existing = supabase.table(SKILL_VERIFICATIONS_TABLE).select("best_score,attempts").eq("user_id", user_id).eq("skill", skill).single().execute()
        row = getattr(existing, "data", None)
        if row:
            best = max(float(row.get("best_score") or 0.0), score)
            attempts = int(row.get("attempts") or 0) + 1
            supabase.table(SKILL_VERIFICATIONS_TABLE).update({
                "best_score": best,
                "attempts": attempts,
                "status": "verified" if best >= 70 else "needs_review",
            }).eq("user_id", user_id).eq("skill", skill).execute()
        else:
            supabase.table(SKILL_VERIFICATIONS_TABLE).insert(payload).execute()


def _recompute_profile_verification_score(user_id: str):
    supabase = get_service_client()
    res = supabase.table(SKILL_VERIFICATIONS_TABLE).select("best_score").eq("user_id", user_id).execute()
    if getattr(res, "error", None):
        return
    scores = [float(row.get("best_score") or 0.0) for row in (res.data or []) if row]
    if not scores:
        agg = 0
    else:
        top = sorted(scores, reverse=True)[:3]
        agg = round(sum(top) / len(top))
    try:
        supabase.table("user_profiles").update({"verification_score": agg}).eq("auth_user_id", user_id).execute()
    except Exception:
        pass



projects_router = APIRouter()


@projects_router.post("/api/projects", response_model=ProjectOut)
def create_project(body: ProjectIn, authorization: Optional[str] = Header(default=None)):
    token = _parse_bearer_token(authorization)
    user_id, _ = _require_user_and_profile(token)
    supabase = get_service_client()
    payload = _project_payload_from_body(body, user_id)
    res = supabase.table(PROJECTS_TABLE).insert(payload).execute()
    if getattr(res, "error", None):
        raise HTTPException(status_code=400, detail=f"Supabase error: {res.error}")
    data = getattr(res, "data", None)
    if isinstance(data, list) and data:
        return _project_row_to_out(data[0])
    if isinstance(data, dict):
        return _project_row_to_out(data)
    raise HTTPException(status_code=500, detail="Unexpected insert response")


@projects_router.get("/api/projects", response_model=List[ProjectOut])
def list_projects(limit: int = 50):
    supabase = get_service_client()
    res = (
        supabase.table(PROJECTS_TABLE)
        .select("*")
        .order("created_at", desc=True)
        .limit(max(1, min(200, limit)))
        .execute()
    )
    if getattr(res, "error", None):
        raise HTTPException(status_code=400, detail=f"Supabase error: {res.error}")
    rows = getattr(res, "data", []) or []
    return [_project_row_to_out(row) for row in rows]


@projects_router.get("/api/projects/{project_id}", response_model=ProjectOut)
def get_project(project_id: str):
    row = _ensure_project_exists(project_id)
    return _project_row_to_out(row)


@projects_router.post("/api/projects/{project_id}/upload")
async def upload_project_media(
    project_id: str,
    kind: str = Form(...),
    file: UploadFile = File(...),
    authorization: Optional[str] = Header(default=None),
):
    token = _parse_bearer_token(authorization)
    user_id, _ = _require_user_and_profile(token)
    if not PROJECTS_BUCKET:
        raise HTTPException(status_code=500, detail="Missing SUPABASE_PROJECTS_BUCKET or SUPABASE_BUCKET")

    project = _ensure_owner(project_id, user_id)
    media_kind = (kind or "").strip().lower()
    if media_kind not in PROJECT_MEDIA_EXTENSIONS:
        raise HTTPException(status_code=400, detail="Unsupported media kind")

    extension = os.path.splitext(file.filename or "")[1].lower()
    allowed = PROJECT_MEDIA_EXTENSIONS[media_kind]
    if extension and allowed and extension not in allowed:
        raise HTTPException(status_code=400, detail="File type not allowed")

    blob = await file.read()
    if not blob:
        raise HTTPException(status_code=400, detail="Empty upload")

    safe_ext = extension or (".png" if media_kind == "cover" else ".bin")
    storage_path = f"project_assets/{user_id}/{project_id}/{media_kind}_{int(time.time())}{safe_ext}"
    supabase = get_service_client()
    _storage_upload_bytes(supabase, PROJECTS_BUCKET, storage_path, blob, file.content_type)
    public_url = _storage_public_url(supabase, PROJECTS_BUCKET, storage_path)

    update: Dict[str, Any]
    if (media_kind == "cover"):
        update = {"cover_url": public_url}
    else:
        gallery = _listify(project.get("gallery_urls"))
        gallery.append(public_url)
        update = {"gallery_urls": gallery}

    supabase.table(PROJECTS_TABLE).update(update).eq("id", project_id).execute()
    refreshed = _ensure_project_exists(project_id)
    return {
        "project": _project_row_to_out(refreshed),
        "uploaded": {"kind": media_kind, "url": public_url, "path": storage_path},
    }


@projects_router.post("/api/projects/{project_id}/apply")
def apply_to_project(project_id: str, body: ProjectApplicationIn = None, authorization: Optional[str] = Header(default=None)):
    token = _parse_bearer_token(authorization)
    user_id, _ = _require_user_and_profile(token)
    project = _ensure_project_exists(project_id)
    if str(project.get("user_id")) == str(user_id):
        raise HTTPException(status_code=400, detail="You cannot apply to your own project")

    supabase = get_service_client()
    existing = (
        supabase.table(PROJECT_APPLICATIONS_TABLE)
        .select("*")
        .eq("project_id", project_id)
        .eq("applicant_user_id", user_id)
        .limit(1)
        .execute()
    )
    rows = getattr(existing, "data", []) or []
    if rows:
        return {"ok": True, "status": rows[0].get("status") or "pending"}

    payload = {
        "project_id": project_id,
        "applicant_user_id": user_id,
        "message": ((body.message if body else None) or None),
        "status": "pending",
    }
    res = supabase.table(PROJECT_APPLICATIONS_TABLE).insert(payload).execute()
    if getattr(res, "error", None):
        msg = str(res.error).lower()
        if "duplicate" in msg or "unique" in msg:
            return {"ok": True}
        raise HTTPException(status_code=400, detail=f"Supabase error: {res.error}")
    data = getattr(res, "data", None)
    if isinstance(data, list) and data:
        return {"ok": True, "status": data[0].get("status") or "pending"}
    return {"ok": True}


@projects_router.get("/api/projects/{project_id}/applications", response_model=List[ProjectApplicationOut])
def list_project_applications(project_id: str, authorization: Optional[str] = Header(default=None)):
    token = _parse_bearer_token(authorization)
    user_id, _ = _require_user_and_profile(token)
    _ensure_owner(project_id, user_id)
    supabase = get_service_client()
    res = (
        supabase.table(PROJECT_APPLICATIONS_TABLE)
        .select("*")
        .eq("project_id", project_id)
        .order("created_at", desc=True)
        .limit(1000)
        .execute()
    )
    if getattr(res, "error", None):
        raise HTTPException(status_code=400, detail=f"Supabase error: {res.error}")
    rows = getattr(res, "data", []) or []
    return [_application_row_to_out(row) for row in rows]


@projects_router.get("/api/projects/{project_id}/applications/me")
def get_my_project_application(project_id: str, authorization: Optional[str] = Header(default=None)):
    token = _parse_bearer_token(authorization)
    user_id, _ = _require_user_and_profile(token)
    supabase = get_service_client()
    res = (
        supabase.table(PROJECT_APPLICATIONS_TABLE)
        .select("*")
        .eq("project_id", project_id)
        .eq("applicant_user_id", user_id)
        .limit(1)
        .execute()
    )
    rows = getattr(res, "data", []) or []
    if not rows:
        return {"applied": False}
    return {"applied": True, "application": _application_row_to_out(rows[0])}


@projects_router.get("/api/projects/{project_id}/applications/{application_id}", response_model=ProjectApplicationOut)
def get_project_application(project_id: str, application_id: str, authorization: Optional[str] = Header(default=None)):
    token = _parse_bearer_token(authorization)
    user_id, _ = _require_user_and_profile(token)
    _ensure_owner(project_id, user_id)
    supabase = get_service_client()
    res = (
        supabase.table(PROJECT_APPLICATIONS_TABLE)
        .select("*")
        .eq("id", application_id)
        .eq("project_id", project_id)
        .single()
        .execute()
    )
    if getattr(res, "error", None):
        raise HTTPException(status_code=400, detail=f"Supabase error: {res.error}")
    data = getattr(res, "data", None)
    if not data:
        raise HTTPException(status_code=404, detail="Application not found")
    return _application_row_to_out(data)


@projects_router.patch("/api/projects/{project_id}/applications/{application_id}")
def update_project_application(project_id: str, application_id: str, body: ProjectApplicationUpdateIn, authorization: Optional[str] = Header(default=None)):
    token = _parse_bearer_token(authorization)
    user_id, _ = _require_user_and_profile(token)
    _ensure_owner(project_id, user_id)
    supabase = get_service_client()
    supabase.table(PROJECT_APPLICATIONS_TABLE).update({"status": body.status}).eq("id", application_id).eq("project_id", project_id).execute()
    res = (
        supabase.table(PROJECT_APPLICATIONS_TABLE)
        .select("*")
        .eq("id", application_id)
        .eq("project_id", project_id)
        .single()
        .execute()
    )
    if getattr(res, "error", None):
        raise HTTPException(status_code=400, detail=f"Supabase error: {res.error}")
    row = getattr(res, "data", None)
    if not row:
        raise HTTPException(status_code=404, detail="Application not found")
    return _application_row_to_out(row)


@projects_router.get("/api/applications/incoming")
def list_incoming_applications(authorization: Optional[str] = Header(default=None)):
    token = _parse_bearer_token(authorization)
    user_id, _ = _require_user_and_profile(token)
    supabase = get_service_client()
    projects_res = supabase.table(PROJECTS_TABLE).select("id,title,cover_url").eq("user_id", user_id).limit(1000).execute()
    if getattr(projects_res, "error", None):
        raise HTTPException(status_code=400, detail=f"Supabase error: {projects_res.error}")
    projects = getattr(projects_res, "data", []) or []
    project_ids = [p.get("id") for p in projects if p and p.get("id")]
    if not project_ids:
        return {"applications": [], "projects": []}
    apps_res = (
        supabase.table(PROJECT_APPLICATIONS_TABLE)
        .select("*")
        .in_("project_id", project_ids)
        .order("created_at", desc=True)
        .limit(2000)
        .execute()
    )
    if getattr(apps_res, "error", None):
        raise HTTPException(status_code=400, detail=f"Supabase error: {apps_res.error}")
    apps = getattr(apps_res, "data", []) or []
    return {
        "applications": [_application_row_to_out(row) for row in apps],
        "projects": [{"id": p.get("id"), "title": p.get("title"), "cover_url": p.get("cover_url")} for p in projects if p],
    }


@projects_router.get("/api/applications/mine")
def list_my_applications(authorization: Optional[str] = Header(default=None)):
    token = _parse_bearer_token(authorization)
    user_id, _ = _require_user_and_profile(token)
    supabase = get_service_client()
    apps_res = (
        supabase.table(PROJECT_APPLICATIONS_TABLE)
        .select("*")
        .eq("applicant_user_id", user_id)
        .order("created_at", desc=True)
        .limit(2000)
        .execute()
    )
    if getattr(apps_res, "error", None):
        raise HTTPException(status_code=400, detail=f"Supabase error: {apps_res.error}")
    apps = getattr(apps_res, "data", []) or []
    if not apps:
        return {"applications": [], "projects": []}
    project_ids = list({row.get("project_id") for row in apps if row and row.get("project_id")})
    projects = []
    if project_ids:
        proj_res = (
            supabase.table(PROJECTS_TABLE)
            .select("id,title,cover_url")
            .in_("id", project_ids)
            .limit(2000)
            .execute()
        )
        if getattr(proj_res, "data", None):
            projects = [{"id": r.get("id"), "title": r.get("title"), "cover_url": r.get("cover_url")} for r in proj_res.data if r]
    return {
        "applications": [_application_row_to_out(row) for row in apps],
        "projects": projects,
    }


@projects_router.get("/api/applications/{application_id}")
def get_application_by_id(application_id: str, authorization: Optional[str] = Header(default=None)):
    token = _parse_bearer_token(authorization)
    user_id, _ = _require_user_and_profile(token)
    supabase = get_service_client()
    res = supabase.table(PROJECT_APPLICATIONS_TABLE).select("*").eq("id", application_id).single().execute()
    if getattr(res, "error", None):
        raise HTTPException(status_code=400, detail=f"Supabase error: {res.error}")
    row = getattr(res, "data", None)
    if not row:
        raise HTTPException(status_code=404, detail="Application not found")
    project = _ensure_project_exists(str(row.get("project_id")))
    if str(row.get("applicant_user_id")) != str(user_id) and str(project.get("user_id")) != str(user_id):
        raise HTTPException(status_code=403, detail="Not permitted")
    return _application_row_to_out(row)


@projects_router.get("/api/collab/{application_id}/messages")
def list_collab_messages(application_id: str, authorization: Optional[str] = Header(default=None)):
    token = _parse_bearer_token(authorization)
    user_id, _ = _require_user_and_profile(token)
    supabase = get_service_client()
    app_res = supabase.table(PROJECT_APPLICATIONS_TABLE).select("*").eq("id", application_id).single().execute()
    if getattr(app_res, "error", None):
        raise HTTPException(status_code=400, detail=f"Supabase error: {app_res.error}")
    app_row = getattr(app_res, "data", None)
    if not app_row:
        raise HTTPException(status_code=404, detail="Application not found")
    project = _ensure_project_exists(str(app_row.get("project_id")))
    if str(app_row.get("status", "")).lower() != "accepted":
        raise HTTPException(status_code=400, detail="Collaboration opens after acceptance")
    if str(app_row.get("applicant_user_id")) != str(user_id) and str(project.get("user_id")) != str(user_id):
        raise HTTPException(status_code=403, detail="Not permitted")
    msgs_res = (
        supabase.table(PROJECT_COLLAB_TABLE)
        .select("*")
        .eq("application_id", application_id)
        .order("created_at")
        .limit(500)
        .execute()
    )
    if getattr(msgs_res, "error", None):
        raise HTTPException(status_code=400, detail=f"Supabase error: {msgs_res.error}")
    msgs = getattr(msgs_res, "data", []) or []
    return {"messages": msgs}


@projects_router.post("/api/collab/{application_id}/messages")
def send_collab_message(application_id: str, body: CollabMessageIn, authorization: Optional[str] = Header(default=None)):
    token = _parse_bearer_token(authorization)
    user_id, _ = _require_user_and_profile(token)
    content = (body.content or "").strip()
    if not content:
        raise HTTPException(status_code=400, detail="Message content required")
    supabase = get_service_client()
    app_res = supabase.table(PROJECT_APPLICATIONS_TABLE).select("*").eq("id", application_id).single().execute()
    if getattr(app_res, "error", None):
        raise HTTPException(status_code=400, detail=f"Supabase error: {app_res.error}")
    app_row = getattr(app_res, "data", None)
    if not app_row:
        raise HTTPException(status_code=404, detail="Application not found")
    project = _ensure_project_exists(str(app_row.get("project_id")))
    if str(app_row.get("status", "")).lower() != "accepted":
        raise HTTPException(status_code=400, detail="Collaboration opens after acceptance")
    if str(app_row.get("applicant_user_id")) != str(user_id) and str(project.get("user_id")) != str(user_id):
        raise HTTPException(status_code=403, detail="Not permitted")
    res = supabase.table(PROJECT_COLLAB_TABLE).insert({
        "application_id": application_id,
        "sender_user_id": user_id,
        "content": content,
    }).execute()
    if getattr(res, "error", None):
        raise HTTPException(status_code=400, detail=f"Supabase error: {res.error}")
    data = getattr(res, "data", None)
    row = data[0] if isinstance(data, list) and data else data
    return {"ok": True, "message": row}


@projects_router.get("/api/public/profiles/{user_id}")
def get_public_profile(user_id: str):
    supabase = get_service_client()
    res = (
        supabase.table("user_profiles")
        .select(
            "auth_user_id,name,profile_image_url,bio,headline,location,linkedin,github,leetcode,technologies,skills,certifications,languages,interests,project_info,publications,achievements,experience,verification_score"
        )
        .eq("auth_user_id", user_id)
        .limit(1)
        .execute()
    )
    if getattr(res, "error", None):
        raise HTTPException(status_code=400, detail=f"Supabase error: {res.error}")
    if not res.data:
        raise HTTPException(status_code=404, detail="Profile not found")
    row = dict(res.data[0])
    profile_id = row.get("id")
    if profile_id:
        row["experiences"] = _fetch_profile_related(
            profile_id,
            "user_experiences",
            [("order_index", False), ("start_date", True), ("created_at", False)],
        )
        row["education_entries"] = _fetch_profile_related(
            profile_id,
            "user_education",
            [("order_index", False), ("created_at", False)],
        )
        row["certification_entries"] = _fetch_profile_related(
            profile_id,
            "user_certifications",
            [("order_index", False), ("issue_date", True), ("created_at", False)],
        )
        row["portfolio_projects"] = _fetch_profile_related(
            profile_id,
            "user_portfolio_projects",
            [("order_index", False), ("start_date", True), ("created_at", False)],
        )
        row["publication_entries"] = _fetch_profile_related(
            profile_id,
            "user_publications",
            [("order_index", False), ("publication_date", True), ("created_at", False)],
        )
    row["user_id"] = row.pop("auth_user_id")
    return row


@projects_router.get("/api/public/skills/verifications/{user_id}", response_model=List[SkillVerificationOut])
def list_public_skill_verifications(user_id: str):
    supabase = get_service_client()
    res = (
        supabase.table(SKILL_VERIFICATIONS_TABLE)
        .select("skill,best_score,attempts,status,updated_at")
        .eq("user_id", user_id)
        .order("best_score", desc=True)
        .order("updated_at", desc=True)
        .execute()
    )
    if getattr(res, "error", None):
        raise HTTPException(status_code=400, detail=f"Supabase error: {res.error}")
    rows = getattr(res, "data", []) or []
    return [SkillVerificationOut(**{**row, "best_score": float(row.get("best_score") or 0.0)}).dict() for row in rows]


@projects_router.get("/api/skills/verifications", response_model=List[SkillVerificationOut])
def list_my_skill_verifications(authorization: Optional[str] = Header(default=None)):
    token = _parse_bearer_token(authorization)
    user_id, _ = _require_user_and_profile(token)
    supabase = get_service_client()
    res = (
        supabase.table(SKILL_VERIFICATIONS_TABLE)
        .select("skill,best_score,attempts,status,updated_at")
        .eq("user_id", user_id)
        .order("best_score", desc=True)
        .order("updated_at", desc=True)
        .execute()
    )
    if getattr(res, "error", None):
        raise HTTPException(status_code=400, detail=f"Supabase error: {res.error}")
    rows = getattr(res, "data", []) or []
    return [SkillVerificationOut(**{**row, "best_score": float(row.get("best_score") or 0.0)}).dict() for row in rows]


@projects_router.post("/api/skills/tests/start", response_model=SkillTestStartOut)
def start_skill_test(body: SkillTestStartIn, authorization: Optional[str] = Header(default=None)):
    token = _parse_bearer_token(authorization)
    user_id, _ = _require_user_and_profile(token)
    skill = _normalize_skill(body.skill)
    if not skill:
        raise HTTPException(status_code=400, detail="Skill is required")
    questions = _fallback_questions(skill)
    session_id = _insert_skill_test_session(user_id, skill, questions)
    public = _public_questions(questions)
    return {"session_id": session_id, "skill": skill, "questions": public}


@projects_router.post("/api/skills/tests/{session_id}/submit")
def submit_skill_test(session_id: str, body: SkillTestSubmitIn, authorization: Optional[str] = Header(default=None)):
    token = _parse_bearer_token(authorization)
    user_id, _ = _require_user_and_profile(token)
    session = _get_skill_test_session(session_id)
    if not session:
        raise HTTPException(status_code=404, detail="Session not found")
    if str(session.get("user_id")) != str(user_id):
        raise HTTPException(status_code=403, detail="Not your session")
    if (session.get("status") or "").lower() == "completed" and session.get("result"):
        return session.get("result")
    questions = session.get("questions") or []
    answers = {ans.question_id: (ans.response or "").strip() for ans in (body.answers or []) if ans.question_id}
    result = _grade_skill_answers(questions, answers)
    _update_skill_test_submission(session_id, result)
    score = float(result.get("score") or 0.0)
    skill_name = session.get("skill") or ""
    if skill_name:
        _upsert_skill_verification(user_id, skill_name, score)
        _recompute_profile_verification_score(user_id)
    return result


@projects_router.get("/api/profile")
def get_profile_compat(authorization: Optional[str] = Header(default=None)):
    token = _parse_bearer_token(authorization)
    return _get_profile_me(token)


@projects_router.post("/api/profile")
def update_profile_compat(payload: dict, authorization: Optional[str] = Header(default=None)):
    token = _parse_bearer_token(authorization)
    _update_profile_me(token, payload or {})
    return _get_profile_me(token)

# --- Academics API ---



academics_router = APIRouter()


@academics_router.post("/api/parse/syllabus-text", response_model=ParsedSyllabusOut, summary="Parse raw syllabus text into structured units/topics")
def parse_syllabus_text(payload: ParseSyllabusIn):
    return parse_syllabus(payload)

@academics_router.post("/api/syllabus/upload", response_model=SyllabusCourseOut, summary="Upload a syllabus PDF and parse + store units/topics")
async def upload_syllabus_pdf(
    request: Request,
    batch_id: uuid.UUID = Form(...),
    file: UploadFile = File(...),
    course_code: Optional[str] = Form(None),
    title: Optional[str] = Form(None),
    semester: Optional[int] = Form(None),
    prefer_naive: Optional[str] = Form(None),
):
    try:
        print(f"[UPLOAD] {request.method} {request.url.path} origin={request.headers.get('origin')} content-type={request.headers.get('content-type')}")
    except Exception:
        pass
    if not file or (file.content_type not in ("application/pdf", None) and not file.filename.lower().endswith(".pdf")):
        raise HTTPException(status_code=400, detail="Only PDF files are supported")

    blob = await file.read()
    if not blob:
        raise HTTPException(status_code=400, detail="Empty file")

    # Extract text off the event loop to avoid blocking concurrent requests
    raw_text = await run_in_threadpool(_pdf_bytes_to_text, blob)
    if not raw_text or len(raw_text) < 16:
        raise HTTPException(status_code=422, detail="Could not extract text from PDF")

    hints = {"course_code": course_code, "title": title}
    # Determine if client asked to skip AI parsing
    def _to_bool(val: Optional[str]) -> bool:
        if val is None:
            return False
        v = str(val).strip().lower()
        return v in {"1", "true", "yes", "y", "on"}

    # Always use the improved heuristic parser for stability
    parsed = _naive_extract_improved(raw_text)

    norm = _normalize_parsed_struct(parsed or {}, hints)
    filtered_units = _filter_lab_units(norm.units)
    if not filtered_units:
        fallback = _naive_extract_improved(raw_text)
        fallback_norm = _normalize_parsed_struct(fallback or {}, hints)
        filtered_units = _filter_lab_units(fallback_norm.units) or fallback_norm.units
        if filtered_units:
            norm = ParsedSyllabusOut(
                course_code=fallback_norm.course_code or norm.course_code,
                title=fallback_norm.title or norm.title,
                units=filtered_units,
            )
    else:
        norm = ParsedSyllabusOut(
            course_code=norm.course_code,
            title=norm.title,
            units=filtered_units,
        )

    if not norm.units:
        fallback_topics: List[TopicIn] = []
        for line in raw_text.splitlines():
            line = line.strip()
            if len(line) < 4:
                continue
            if _contains_lab(line):
                continue
            fallback_topics.append(TopicIn(topic=line))
            if len(fallback_topics) >= 12:
                break
        if fallback_topics:
            norm = ParsedSyllabusOut(
                course_code=norm.course_code,
                title=norm.title,
                units=[UnitIn(unit_title="Unit 1", topics=fallback_topics)],
            )

    sem_val: Optional[int] = None
    if semester is not None:
        sem_val = int(semester)
    else:
        try:
            sem_from_json = parsed.get("semester") if isinstance(parsed, dict) else None
        except Exception:
            sem_from_json = None
        if isinstance(sem_from_json, (int, float)):
            sem_val = int(sem_from_json)
        elif isinstance(sem_from_json, str) and sem_from_json.strip().isdigit():
            sem_val = int(sem_from_json.strip())
        if sem_val is None:
            m = re.search(r"sem(?:ester)?\s*[:\-]?\s*(\d{1,2})", raw_text, flags=re.IGNORECASE)
            if m:
                try:
                    sem_val = int(m.group(1))
                except Exception:
                    sem_val = None
    if not sem_val or sem_val < 1 or sem_val > 12:
        sem_val = 1
        try:
            print("[UPLOAD] semester fallback -> 1 (provide explicit semester to override)")
        except Exception:
            pass

    course_in = SyllabusCourseIn(
        batch_id=batch_id,
        semester=sem_val,
        course_code=(norm.course_code or "UNKNOWN").upper(),
        title=norm.title or "Untitled Course",
        units=norm.units or [],
    )
    course = await run_in_threadpool(upsert_syllabus_course, course_in)
    if not course.id:
        raise HTTPException(status_code=500, detail="Failed to create or resolve course id")
    units = await run_in_threadpool(sync_units_and_topics, course.id, norm.units or [])
    try:
        print(f"[UPLOAD] saved course id={course.id} code={course.course_code} title={course.title} units={len(units)}")
    except Exception:
        pass
    return SyllabusCourseOut(
        id=course.id,
        batch_id=course.batch_id,
        semester=course.semester,
        course_code=course.course_code,
        title=course.title,
        units=units,
    )

@academics_router.post("/api/syllabus/upload-bulk", summary="Upload a semester syllabus PDF containing multiple subjects; parse and store all")
async def upload_syllabus_pdf_bulk(
    request: Request,
    batch_id: uuid.UUID = Form(...),
    file: UploadFile = File(...),
    semester: Optional[int] = Form(None),
    prefer_naive: Optional[str] = Form(None),
):
    try:
        print(f"[UPLOAD-BULK] {request.method} {request.url.path} origin={request.headers.get('origin')} content-type={request.headers.get('content-type')}")
    except Exception:
        pass
    if not file or (file.content_type not in ("application/pdf", None) and not file.filename.lower().endswith(".pdf")):
        raise HTTPException(status_code=400, detail="Only PDF files are supported")

    blob = await file.read()
    if not blob:
        raise HTTPException(status_code=400, detail="Empty file")

    raw_text = await run_in_threadpool(_pdf_bytes_to_text, blob)
    if not raw_text or len(raw_text) < 16:
        raise HTTPException(status_code=422, detail="Could not extract text from PDF")

    def _to_bool(val: Optional[str]) -> bool:
        if val is None:
            return False
        v = str(val).strip().lower()
        return v in {"1", "true", "yes", "y", "on"}

    sections = _split_subject_sections(raw_text)
    results: List[SyllabusCourseOut] = []
    use_ai = not _to_bool(prefer_naive)  # Use AI by default unless prefer_naive is set
    sem_val = int(semester) if semester else 1
    
    # Helper function to process a subject from AI-parsed data
    async def save_subject_from_ai(subject_data: dict, sem: int) -> Optional[SyllabusCourseOut]:
        try:
            units_in: List[UnitIn] = []
            for unit in subject_data.get("units", []):
                topics_in = []
                for topic in unit.get("topics", []):
                    topic_name = topic.get("name") if isinstance(topic, dict) else str(topic)
                    if topic_name and topic_name.strip():
                        topics_in.append(TopicIn(topic=topic_name.strip()))
                if topics_in:
                    units_in.append(UnitIn(
                        unit_title=unit.get("name") or f"Unit {unit.get('unit_number', 1)}",
                        topics=topics_in
                    ))
            
            if not units_in:
                return None
            
            # Filter out lab units
            filtered_units = _filter_lab_units(units_in)
            if not filtered_units:
                filtered_units = units_in
            
            course_code = (subject_data.get("code") or "UNKNOWN").upper().strip()
            title = (subject_data.get("name") or "Untitled Course").strip()
            
            course_in = SyllabusCourseIn(
                batch_id=batch_id,
                semester=sem,
                course_code=course_code,
                title=title,
                units=filtered_units,
            )
            course = await run_in_threadpool(upsert_syllabus_course, course_in)
            units_saved = await run_in_threadpool(sync_units_and_topics, course.id, filtered_units)
            return SyllabusCourseOut(
                id=course.id,
                batch_id=course.batch_id,
                semester=course.semester,
                course_code=course.course_code,
                title=course.title,
                units=units_saved,
            )
        except Exception as e:
            print(f"[UPLOAD-BULK] Error saving subject: {e}")
            return None
    
    # Try AI parsing first for better accuracy with university syllabi
    if use_ai and GEMINI_API_KEY:
        try:
            print(f"[UPLOAD-BULK] Using AI (Gemini) for syllabus parsing...")
            ai_parsed = await run_in_threadpool(_gemini_parse, raw_text, {})
            
            if ai_parsed:
                # Check if AI returned multiple subjects
                if "subjects" in ai_parsed and ai_parsed["subjects"]:
                    print(f"[UPLOAD-BULK] AI found {len(ai_parsed['subjects'])} subjects")
                    for subj in ai_parsed["subjects"]:
                        result = await save_subject_from_ai(subj, sem_val)
                        if result:
                            results.append(result)
                elif ai_parsed.get("units"):
                    # Single subject format
                    subj_data = {
                        "code": ai_parsed.get("course_code"),
                        "name": ai_parsed.get("title"),
                        "units": ai_parsed.get("units", [])
                    }
                    # Convert units to new format if needed
                    converted_units = []
                    for u in ai_parsed.get("units", []):
                        topics = []
                        for t in u.get("topics", []):
                            topic_name = t.get("topic") if isinstance(t, dict) else str(t)
                            if topic_name:
                                topics.append({"name": topic_name})
                        converted_units.append({
                            "name": u.get("unit_title", "Unit"),
                            "unit_number": len(converted_units) + 1,
                            "topics": topics
                        })
                    subj_data["units"] = converted_units
                    result = await save_subject_from_ai(subj_data, sem_val)
                    if result:
                        results.append(result)
                
                if results:
                    print(f"[UPLOAD-BULK] AI parsing successful: saved {len(results)} courses")
                    return results
        except Exception as e:
            print(f"[UPLOAD-BULK] AI parsing failed, falling back to heuristic: {e}")
    
    # Fallback to heuristic parsing
    print(f"[UPLOAD-BULK] Using heuristic parsing...")
    sections = _split_subject_sections(raw_text)
    
    if not sections:
        # Fallback: treat as a single subject using the single-subject pipeline
        hints = {"course_code": None, "title": None}
        parsed = _naive_extract_improved(raw_text)
        norm = _normalize_parsed_struct(parsed or {}, hints)
        filtered_units = _filter_lab_units(norm.units)
        if not filtered_units:
            fallback_norm = _normalize_parsed_struct(_naive_extract_improved(raw_text), hints)
            filtered_units = _filter_lab_units(fallback_norm.units) or fallback_norm.units
            if filtered_units:
                norm = ParsedSyllabusOut(
                    course_code=fallback_norm.course_code or norm.course_code,
                    title=fallback_norm.title or norm.title,
                    units=filtered_units,
                )
        course_in = SyllabusCourseIn(
            batch_id=batch_id,
            semester=sem_val,
            course_code=(norm.course_code or "UNKNOWN").upper(),
            title=norm.title or "Untitled Course",
            units=norm.units or [],
        )
        course = await run_in_threadpool(upsert_syllabus_course, course_in)
        units_saved = await run_in_threadpool(sync_units_and_topics, course.id, norm.units or [])
        results.append(
            SyllabusCourseOut(
                id=course.id,
                batch_id=course.batch_id,
                semester=course.semester,
                course_code=course.course_code,
                title=course.title,
                units=units_saved,
            )
        )
        return results

    # Multi-section path
    for sec in sections:
        sec_text = sec.get("text") or ""
        hints = {"course_code": sec.get("code"), "title": sec.get("title")}
        parsed = _naive_extract_improved(sec_text)
        norm = _normalize_parsed_struct(parsed or {}, hints)
        filtered_units = _filter_lab_units(norm.units)
        if not filtered_units:
            fallback_norm = _normalize_parsed_struct(_naive_extract_improved(sec_text), hints)
            filtered_units = _filter_lab_units(fallback_norm.units) or fallback_norm.units
            if filtered_units:
                norm = ParsedSyllabusOut(
                    course_code=fallback_norm.course_code or norm.course_code,
                    title=fallback_norm.title or norm.title,
                    units=filtered_units,
                )
        course_in = SyllabusCourseIn(
            batch_id=batch_id,
            semester=sem_val,
            course_code=(norm.course_code or "UNKNOWN").upper(),
            title=norm.title or "Untitled Course",
            units=norm.units or [],
        )
        course = await run_in_threadpool(upsert_syllabus_course, course_in)
        units_saved = await run_in_threadpool(sync_units_and_topics, course.id, norm.units or [])
        results.append(
            SyllabusCourseOut(
                id=course.id,
                batch_id=course.batch_id,
                semester=course.semester,
                course_code=course.course_code,
                title=course.title,
                units=units_saved,
            )
        )
    try:
        print(f"[UPLOAD-BULK] saved {len(results)} courses for batch={batch_id} semester={sem_val}")
    except Exception:
        pass
    return results

@academics_router.get("/api/syllabus/upload", summary="Info: how to use the syllabus upload endpoint")
def upload_syllabus_info():
    return {
        "ok": True,
        "message": "Use POST multipart/form-data to /api/syllabus/upload with fields: file (PDF), batch_id (UUID), optional course_code, title, semester (1-12)",
    }

@academics_router.options("/api/syllabus/upload")
def upload_syllabus_options():
    # Explicit OPTIONS handler to help with certain proxies while debugging
    return Response(status_code=200)


# ---------------- Admin: List Users -----------------

def _is_admin_user(user_id: Optional[str], email: Optional[str]) -> bool:
    """Basic admin gating: match against comma-separated ADMIN_USER_IDS or ADMIN_EMAILS env vars.

    Falls back to allowing emails ending with domains in ADMIN_EMAIL_DOMAINS (comma-separated) if provided.
    """
    if not user_id and not email:
        return False
    ids = {s.strip() for s in os.getenv("ADMIN_USER_IDS", "").split(",") if s.strip()}
    if user_id and user_id in ids:
        return True
    emails = {s.strip().lower() for s in os.getenv("ADMIN_EMAILS", "").split(",") if s.strip()}
    if email and email.lower() in emails:
        return True
    domains = {s.strip().lower() for s in os.getenv("ADMIN_EMAIL_DOMAINS", "").split(",") if s.strip()}
    if email and domains:
        try:
            domain = email.split("@",1)[1].lower()
            if domain in domains:
                return True
        except Exception:
            pass
    # Database role check (admin_roles table) if we have a service client and user id
    try:
        if user_id:
            supabase = get_service_client()
            if supabase:
                resp = supabase.table("admin_roles").select("role").eq("auth_user_id", user_id).limit(1).execute()
                data = getattr(resp, "data", []) or []
                if data and (data[0].get("role") == "admin"):
                    return True
    except Exception:
        # Silent fail: do not block auth if table missing or permission issue
        pass
    return False


@academics_router.get("/api/admin/users", summary="Admin: list user profiles")
def list_admin_users(
    authorization: Optional[str] = Header(default=None),
    limit: int = Query(default=500, ge=1, le=2000),
    q: Optional[str] = Query(default=None, max_length=120),
    role: Optional[str] = Query(default=None, max_length=32),
    department: Optional[str] = Query(default=None, max_length=120),
    section: Optional[str] = Query(default=None, max_length=32),
    batch_range: Optional[str] = Query(default=None, max_length=32),
    semester: Optional[int] = Query(default=None, ge=1, le=12),
    min_streak: Optional[int] = Query(default=None, ge=0, le=3650),
    last_login_days: Optional[int] = Query(default=None, ge=1, le=3650),
):
    token = _parse_bearer_token(authorization)
    if not token:
        raise HTTPException(status_code=401, detail="Missing bearer token")
    anon_client = get_anon_client()
    if not anon_client:
        raise HTTPException(status_code=500, detail="Auth disabled (no anon client)")
    try:
        auth_user = anon_client.auth.get_user(token)
    except Exception:
        raise HTTPException(status_code=401, detail="Invalid token")
    user_obj = getattr(auth_user, "user", None) or (auth_user.get("user") if isinstance(auth_user, dict) else None)
    user_id = None
    email = None
    if user_obj:
        user_id = getattr(user_obj, "id", None) or (user_obj.get("id") if isinstance(user_obj, dict) else None)
        email = getattr(user_obj, "email", None) or (user_obj.get("email") if isinstance(user_obj, dict) else None)
    if not _is_admin_user(user_id, email):
        raise HTTPException(status_code=403, detail="Not an admin user")

    supabase = get_service_client()
    # Core fields from user_profiles. Department/batch are sourced from user_education.
    base_cols = [
        "id","auth_user_id","email","name","gender","phone","semester","regno",
        "profile_image_url","verification_score","updated_at","created_at","linkedin","github",
        "leetcode","skills","technologies","specializations"
    ]
    try:
        query = supabase.table("user_profiles").select(",".join(base_cols))
        if q:
            qv = (q or "").strip()
            if qv:
                # Search by name/email/regno (case-insensitive)
                pattern = f"%{qv}%"
                query = query.or_(
                    f"name.ilike.{pattern},email.ilike.{pattern},regno.ilike.{pattern}"
                )
        res = query.order("updated_at", desc=True).limit(limit).execute()
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Supabase error (list users) exec: {str(e)}")
    err = getattr(res, "error", None)
    if err:
        raise HTTPException(status_code=500, detail=f"Supabase error (list users): {err}")
    rows: List[dict] = getattr(res, "data", []) or []

    # Load roles for all involved auth_user_ids in one query
    auth_ids = [r.get("auth_user_id") for r in rows if r.get("auth_user_id")]
    uniq_ids = list({i for i in auth_ids if i})

    # Fetch primary education per profile from user_education.
    # NOTE: user_education links to user_profiles via user_profile_id (not auth_user_id).
    edu_map: Dict[str, dict] = {}
    profile_ids = [r.get("id") for r in rows if r.get("id")]
    uniq_profile_ids = list({pid for pid in profile_ids if pid})
    if uniq_profile_ids:
        # Try with order_index if present, else fallback.
        edu_select_try = [
            "user_profile_id,department_id,batch_id,college_id,degree_id,section,current_semester,regno,order_index,created_at",
            "user_profile_id,department_id,batch_id,college_id,degree_id,section,current_semester,regno,created_at",
            "user_profile_id,department_id,batch_id,college_id,degree_id,current_semester,regno,order_index,created_at",
            "user_profile_id,department_id,batch_id,college_id,degree_id,current_semester,regno,created_at",
            "user_profile_id,department_id,batch_id,college_id,section,current_semester,regno,order_index,created_at",
            "user_profile_id,department_id,batch_id,college_id,section,current_semester,regno,created_at",
            "user_profile_id,department_id,batch_id,college_id,current_semester,regno,order_index,created_at",
            "user_profile_id,department_id,batch_id,college_id,current_semester,regno,created_at",
            # Fallback if user_education has no college_id
            "user_profile_id,department_id,batch_id,section,current_semester,regno,order_index,created_at",
            "user_profile_id,department_id,batch_id,section,current_semester,regno,created_at",
            "user_profile_id,department_id,batch_id,current_semester,regno,order_index,created_at",
            "user_profile_id,department_id,batch_id,current_semester,regno,created_at",
        ]
        edu_rows: List[dict] = []
        for sel in edu_select_try:
            try:
                eres = supabase.table("user_education").select(sel).in_("user_profile_id", uniq_profile_ids).execute()
            except Exception as e:
                msg = str(e)
                if "order_index" in sel and "order_index" in msg:
                    continue
                if "degree_id" in sel and "degree_id" in msg:
                    continue
                # If user_education doesn't exist or query fails, just skip education enrichment.
                edu_rows = []
                break
            if getattr(eres, "error", None):
                edu_rows = []
                break
            edu_rows = getattr(eres, "data", []) or []
            break

        def _edu_rank(ed: dict) -> Tuple[int, float]:
            oi_raw = ed.get("order_index")
            try:
                oi = int(oi_raw) if oi_raw is not None else 9999
            except Exception:
                oi = 9999
            ts = 0.0
            try:
                if ed.get("created_at"):
                    ts = datetime.fromisoformat(str(ed.get("created_at")).replace("Z", "+00:00")).timestamp()
            except Exception:
                ts = 0.0
            # Prefer lower order_index; for ties prefer latest created_at.
            return (oi, -ts)

        for ed in edu_rows:
            pid = ed.get("user_profile_id")
            if not pid:
                continue
            prev = edu_map.get(pid)
            if not prev or _edu_rank(ed) < _edu_rank(prev):
                edu_map[pid] = ed

    # Preload department + batch + college + degree info to enrich output
    dept_ids = {ed.get("department_id") for ed in edu_map.values() if ed.get("department_id")}
    batch_ids = {ed.get("batch_id") for ed in edu_map.values() if ed.get("batch_id")}
    college_ids = {ed.get("college_id") for ed in edu_map.values() if ed.get("college_id")}
    degree_ids = {ed.get("degree_id") for ed in edu_map.values() if ed.get("degree_id")}
    dept_map: Dict[str, dict] = {}
    batch_map: Dict[str, dict] = {}
    college_map: Dict[str, dict] = {}
    degree_map: Dict[str, dict] = {}
    role_map: Dict[str, str] = {}
    if dept_ids:
        dres = supabase.table("departments").select("id,name").in_("id", list(dept_ids)).execute()
        if not getattr(dres, "error", None):
            for d in dres.data or []:
                dept_map[d.get("id")] = d
    if batch_ids:
        bres = supabase.table("batches").select("id,from_year,to_year").in_("id", list(batch_ids)).execute()
        if not getattr(bres, "error", None):
            for b in bres.data or []:
                batch_map[b.get("id")] = b
    if college_ids:
        cres = supabase.table("colleges").select("id,name").in_("id", list(college_ids)).execute()
        if not getattr(cres, "error", None):
            for c in cres.data or []:
                college_map[c.get("id")] = c
    if degree_ids:
        gres = supabase.table("degrees").select("id,name").in_("id", list(degree_ids)).execute()
        if not getattr(gres, "error", None):
            for g in gres.data or []:
                degree_map[g.get("id")] = g
    try:
        if uniq_ids:
            rres = supabase.table("admin_roles").select("auth_user_id,role").in_("auth_user_id", uniq_ids).execute()
            if not getattr(rres, "error", None):
                for rr in (rres.data or []):
                    rid = rr.get("auth_user_id")
                    if rid:
                        role_map[rid] = rr.get("role") or "student"
    except Exception:
        pass

    # Load streaks for all profiles in one query (table name may vary across deployments).
    # NOTE: academicas.html uses /api/streak which reads from notex_streak.
    streak_current_map: Dict[str, int] = {}  # profile_id -> current_streak
    streak_longest_map: Dict[str, int] = {}  # profile_id -> longest_streak
    streak_last_activity_map: Dict[str, Any] = {}  # profile_id -> last_activity_date
    if uniq_profile_ids:
        for streak_table in ("notex_streak", "user_streaks"):
            try:
                sres = supabase.table(streak_table).select(
                    "user_profile_id,current_streak,longest_streak,last_activity_date"
                ).in_(
                    "user_profile_id", uniq_profile_ids
                ).execute()
            except Exception:
                continue
            if getattr(sres, "error", None):
                continue
            for s in (getattr(sres, "data", None) or []):
                pid = s.get("user_profile_id")
                if pid:
                    try:
                        streak_current_map[pid] = int(s.get("current_streak") or 0)
                    except Exception:
                        streak_current_map[pid] = 0
                    try:
                        streak_longest_map[pid] = int(s.get("longest_streak") or 0)
                    except Exception:
                        streak_longest_map[pid] = 0
                    if s.get("last_activity_date") is not None:
                        streak_last_activity_map[pid] = s.get("last_activity_date")
            # If we got any rows, stop trying fallbacks.
            if streak_current_map:
                break

    # Load last_seen_at per auth user from user_sessions (aggregate in Python).
    last_seen_map: Dict[str, str] = {}  # auth_user_id -> ISO timestamp
    if uniq_ids:
        try:
            # Cap rows to avoid unbounded reads on very large session tables.
            sres = (
                supabase.table("user_sessions")
                .select("user_id,last_seen_at")
                .in_("user_id", uniq_ids)
                .order("last_seen_at", desc=True)
                .limit(50000)
                .execute()
            )
            if not getattr(sres, "error", None):
                for row in (getattr(sres, "data", None) or []):
                    uid = row.get("user_id")
                    ts = row.get("last_seen_at")
                    if uid and ts and uid not in last_seen_map:
                        # Because results are ordered desc, first seen is the latest.
                        last_seen_map[uid] = ts
        except Exception:
            pass

    out: List[dict] = []
    for r in rows:
        pid = r.get("id")
        edu = edu_map.get(pid, {}) if pid else {}
        dept = dept_map.get(edu.get("department_id")) if edu.get("department_id") else {}
        batch = batch_map.get(edu.get("batch_id")) if edu.get("batch_id") else {}
        college = college_map.get(edu.get("college_id")) if edu.get("college_id") else {}
        degree = degree_map.get(edu.get("degree_id")) if edu.get("degree_id") else {}
        semester_val = r.get("semester") if r.get("semester") is not None else edu.get("current_semester")
        regno_val = r.get("regno") if r.get("regno") else edu.get("regno")
        user_id_val = r.get("auth_user_id")
        current_streak_val = streak_current_map.get(pid, 0) if pid else 0
        longest_streak_val = streak_longest_map.get(pid, 0) if pid else 0
        last_seen_val = last_seen_map.get(user_id_val) if user_id_val else None
        batch_range_val = (f"{batch.get('from_year')}-{batch.get('to_year')}" if batch and batch.get('from_year') and batch.get('to_year') else None)

        out.append({
            "profile_id": r.get("id"),
            "user_id": user_id_val,
            "name": r.get("name"),
            "email": r.get("email"),
            "role": role_map.get(r.get("auth_user_id"), "student"),
            "semester": semester_val,
            "regno": regno_val,
            "college": college.get("name") if college else None,
            "degree": degree.get("name") if degree else None,
            "department": dept.get("name") if dept else None,
            "section": edu.get("section") if edu else None,
            "batch_from": batch.get("from_year") if batch else None,
            "batch_to": batch.get("to_year") if batch else None,
            "batch_range": batch_range_val,
            # Back-compat (users.html initially used streak_current)
            "streak_current": current_streak_val,
            # Align naming with /api/streak response (academicas.html)
            "current_streak": current_streak_val,
            "longest_streak": longest_streak_val,
            "last_activity_date": streak_last_activity_map.get(pid) if pid else None,
            "last_seen_at": last_seen_val,
            "profile_image_url": r.get("profile_image_url"),
            "verification_score": r.get("verification_score"),
            "linkedin": r.get("linkedin"),
            "github": r.get("github"),
            "leetcode": r.get("leetcode"),
            "skills": r.get("skills") or [],
            "technologies": r.get("technologies") or [],
            "specializations": r.get("specializations") or [],
            "updated_at": r.get("updated_at"),
            "created_at": r.get("created_at"),
        })

    # Server-side filters (for real-time filter UI)
    def _norm(s: Any) -> str:
        return (str(s).strip().lower() if s is not None else "")

    filtered = out
    if role:
        want = _norm(role)
        filtered = [u for u in filtered if _norm(u.get("role")) == want]
    if department:
        want = _norm(department)
        filtered = [u for u in filtered if _norm(u.get("department")) == want]
    if section:
        want = _norm(section)
        filtered = [u for u in filtered if _norm(u.get("section")) == want]
    if batch_range:
        want = _norm(batch_range)
        filtered = [u for u in filtered if _norm(u.get("batch_range")) == want]
    if semester is not None:
        filtered = [u for u in filtered if str(u.get("semester") or "") == str(semester)]
    if min_streak is not None:
        filtered = [u for u in filtered if int(u.get("current_streak") or 0) >= int(min_streak)]
    if last_login_days is not None:
        cutoff = datetime.utcnow() - timedelta(days=int(last_login_days))

        def _is_recent(u: dict) -> bool:
            ts = u.get("last_seen_at")
            if not ts:
                return False
            try:
                dt = datetime.fromisoformat(str(ts).replace("Z", "+00:00"))
                # Compare as naive UTC-ish (safe even if dt is tz-aware)
                if dt.tzinfo is not None:
                    dt = dt.replace(tzinfo=None)
                return dt >= cutoff
            except Exception:
                return False

        filtered = [u for u in filtered if _is_recent(u)]

    return {"users": filtered, "count": len(filtered)}


@academics_router.get("/api/admin/self-check", summary="Admin: verify current token admin status")
def admin_self_check(authorization: Optional[str] = Header(default=None)):
    token = _parse_bearer_token(authorization)
    if not token:
        raise HTTPException(status_code=401, detail="Missing bearer token")
    anon_client = get_anon_client()
    if not anon_client:
        raise HTTPException(status_code=500, detail="Auth disabled (no anon client)")
    try:
        auth_user = anon_client.auth.get_user(token)
    except Exception:
        raise HTTPException(status_code=401, detail="Invalid token")
    user_obj = getattr(auth_user, "user", None) or (auth_user.get("user") if isinstance(auth_user, dict) else None)
    user_id = getattr(user_obj, "id", None) if user_obj else None
    email = getattr(user_obj, "email", None) if user_obj else None
    is_admin = _is_admin_user(user_id, email)
    if not is_admin:
        raise HTTPException(status_code=403, detail="Not an admin user")
    return {"ok": True, "user_id": user_id, "email": email, "admin": True}


class RoleUpdateIn(BaseModel):
    role: str


class AdminAcademicUpdateIn(BaseModel):
    # Optional FK ids (preferred when known)
    college_id: Optional[str] = None
    degree_id: Optional[str] = None
    department_id: Optional[str] = None
    batch_id: Optional[str] = None

    college_name: Optional[str] = None
    degree_name: Optional[str] = None
    department_name: Optional[str] = None
    batch_from: Optional[int] = Field(default=None, ge=1900, le=2100)
    batch_to: Optional[int] = Field(default=None, ge=1900, le=2100)
    batch_range: Optional[str] = None  # e.g. "2022-2026"
    section: Optional[str] = None
    semester: Optional[int] = Field(default=None, ge=1, le=12)
    regno: Optional[str] = None

    @validator(
        "college_id",
        "degree_id",
        "department_id",
        "batch_id",
        "college_name",
        "degree_name",
        "department_name",
        "batch_range",
        "section",
        "regno",
        pre=True,
    )
    def _trim_admin_academic(cls, v: Any):  # noqa: N805
        return _strip_or_none(v)


def _get_auth_user(authorization: Optional[str]) -> Tuple[Optional[str], Optional[str]]:
    token = _parse_bearer_token(authorization)
    if not token:
        raise HTTPException(status_code=401, detail="Missing bearer token")
    anon_client = get_anon_client()
    if not anon_client:
        raise HTTPException(status_code=500, detail="Auth disabled (no anon client)")
    try:
        auth_user = anon_client.auth.get_user(token)
    except Exception:
        raise HTTPException(status_code=401, detail="Invalid token")
    user_obj = getattr(auth_user, "user", None) or (auth_user.get("user") if isinstance(auth_user, dict) else None)
    if not user_obj:
        raise HTTPException(status_code=401, detail="Invalid auth context")
    user_id = getattr(user_obj, "id", None) or (user_obj.get("id") if isinstance(user_obj, dict) else None)
    email = getattr(user_obj, "email", None) or (user_obj.get("email") if isinstance(user_obj, dict) else None)
    return user_id, email


def _require_admin(authorization: Optional[str]):
    uid, em = _get_auth_user(authorization)
    if not _is_admin_user(uid, em):
        raise HTTPException(status_code=403, detail="Not an admin user")
    return uid, em


def _count_admins(supabase) -> int:
    try:
        resp = supabase.table("admin_roles").select("role", count='exact').eq("role", "admin").execute()
        # Some supabase libs embed count differently; attempt both
        if hasattr(resp, 'count') and resp.count is not None:
            return resp.count
        data = getattr(resp, 'data', []) or []
        return len([r for r in data if r.get('role') == 'admin'])
    except Exception:
        return 0


VALID_ROLES = {"admin", "teacher", "student", "moderator", "employee"}


def _admin_pick_primary_education_row(supabase, profile_id: str) -> Optional[dict]:
    try:
        res = (
            supabase.table("user_education")
            .select("id,order_index,created_at")
            .eq("user_profile_id", profile_id)
            .limit(50)
            .execute()
        )
    except Exception:
        return None
    if getattr(res, "error", None) or not getattr(res, "data", None):
        return None
    rows = [r for r in (res.data or []) if isinstance(r, dict) and r.get("id")]
    if not rows:
        return None

    def _rank(ed: dict) -> Tuple[int, float]:
        oi_raw = ed.get("order_index")
        try:
            oi = int(oi_raw) if oi_raw is not None else 9999
        except Exception:
            oi = 9999
        ts = 0.0
        try:
            if ed.get("created_at"):
                ts = datetime.fromisoformat(str(ed.get("created_at")).replace("Z", "+00:00")).timestamp()
        except Exception:
            ts = 0.0
        return (oi, -ts)

    rows.sort(key=_rank)
    return rows[0]


def _admin_safe_update_user_education(supabase, edu_id: str, updates: Dict[str, Any]):
    """Best-effort update: retries by dropping unknown columns if schema differs."""
    if not updates:
        return
    retry_payload = dict(updates)
    for _attempt in range(3):
        try:
            upd = supabase.table("user_education").update(retry_payload).eq("id", edu_id).execute()
        except APIError as exc:
            err_message = getattr(exc, "message", None) or getattr(exc, "details", None) or str(exc)
            lower_msg = (err_message or "").lower()
            # Undefined column (Postgres 42703)
            if getattr(exc, "code", None) == "42703" or "42703" in lower_msg or "column" in lower_msg and "does not exist" in lower_msg:
                # Drop likely FK columns first
                removed = False
                for k in ("college_id", "degree_id", "department_id", "batch_id"):
                    if k in retry_payload:
                        retry_payload.pop(k, None)
                        removed = True
                if removed:
                    continue
            raise HTTPException(status_code=500, detail=f"Supabase error (update education): {err_message}")
        if getattr(upd, "error", None):
            msg = str(upd.error)
            lowered = msg.lower()
            if ("42703" in lowered) or ("does not exist" in lowered and "column" in lowered):
                removed = False
                for k in ("college_id", "degree_id", "department_id", "batch_id"):
                    if k in retry_payload:
                        retry_payload.pop(k, None)
                        removed = True
                if removed:
                    continue
            raise HTTPException(status_code=500, detail=f"Supabase error (update education): {msg}")
        break


@academics_router.post("/api/admin/users/{auth_user_id}/academic", summary="Admin: update user's academic/education details")
def admin_update_user_academic(
    auth_user_id: str,
    payload: AdminAcademicUpdateIn,
    authorization: Optional[str] = Header(default=None),
):
    _require_admin(authorization)
    supabase = get_service_client()

    # Resolve profile id
    prof_q = (
        supabase.table("user_profiles")
        .select("id")
        .eq("auth_user_id", auth_user_id)
        .limit(1)
        .execute()
    )
    if getattr(prof_q, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (get profile): {prof_q.error}")
    if not prof_q.data:
        raise HTTPException(status_code=404, detail="User profile not found")
    profile_id = prof_q.data[0].get("id")
    if not profile_id:
        raise HTTPException(status_code=404, detail="User profile not found")

    data = payload.dict(exclude_unset=True)
    # Update basic user_profiles fields when present
    prof_updates: Dict[str, Any] = {}
    if data.get("semester") is not None:
        prof_updates["semester"] = data.get("semester")
    if data.get("regno") is not None:
        prof_updates["regno"] = data.get("regno")
    if prof_updates:
        upd = supabase.table("user_profiles").update(_supabase_payload(prof_updates)).eq("id", profile_id).execute()
        if getattr(upd, "error", None):
            raise HTTPException(status_code=500, detail=f"Supabase error (update profile academic): {upd.error}")

    # Prepare a single education row (for FK resolution) without wiping other rows
    school = data.get("college_name")
    college_id = data.get("college_id")
    degree_name = data.get("degree_name")
    degree_id = data.get("degree_id")
    dept = data.get("department_name")
    department_id = data.get("department_id")
    batch_id = data.get("batch_id")
    section = data.get("section")
    regno = data.get("regno")
    current_semester = data.get("semester")

    if not school and college_id:
        try:
            cres = supabase.table("colleges").select("name").eq("id", college_id).limit(1).execute()
            if not getattr(cres, "error", None) and (cres.data or []):
                school = cres.data[0].get("name")
        except Exception:
            pass

    batch_range = data.get("batch_range")
    if not batch_range:
        bf = data.get("batch_from")
        bt = data.get("batch_to")
        if bf is not None and bt is not None:
            batch_range = f"{int(bf)}-{int(bt)}"

    edu_in = {
        "school": school or "",
        "degree": degree_name,
        "department": dept,
        "batch_range": batch_range,
        "section": section,
        "regno": regno,
        "current_semester": current_semester,
        "grade": None,
        "activities": None,
        "description": None,
        "college_id": college_id,
        "degree_id": degree_id,
        "department_id": department_id,
        "batch_id": batch_id,
    }

    prepared_list = _prepare_education_rows([edu_in])
    if not prepared_list:
        # If admin didn't provide college_name, we can't build education entry; still allow profile update.
        return {"ok": True, "updated": True, "auth_user_id": auth_user_id, "profile_id": profile_id, "education_updated": False}

    prepared = dict(prepared_list[0])
    # Remove fields not in user_education table payload
    prepared.pop("id", None)
    prepared.pop("updated_at", None)
    # Ensure we don't set null-like empty strings
    edu_updates = {k: v for k, v in prepared.items() if v is not None}
    edu_updates["updated_at"] = datetime.utcnow().isoformat()

    # Update primary education row, or insert if missing
    primary = _admin_pick_primary_education_row(supabase, profile_id)
    if primary and primary.get("id"):
        _admin_safe_update_user_education(supabase, primary["id"], edu_updates)
        edu_row_id = primary["id"]
    else:
        insert_payload = {**edu_updates, "user_profile_id": profile_id}
        if "order_index" not in insert_payload:
            insert_payload["order_index"] = 0
        ins = supabase.table("user_education").insert(insert_payload).execute()
        if getattr(ins, "error", None):
            raise HTTPException(status_code=500, detail=f"Supabase error (insert education): {ins.error}")
        # Try refetch
        ref = _admin_pick_primary_education_row(supabase, profile_id)
        edu_row_id = (ref.get("id") if ref else None)

    return {
        "ok": True,
        "updated": True,
        "auth_user_id": auth_user_id,
        "profile_id": profile_id,
        "education_updated": True,
        "user_education_id": edu_row_id,
    }


@academics_router.post("/api/admin/users/{auth_user_id}/role", summary="Admin: update a user's role")
def update_user_role(auth_user_id: str, payload: RoleUpdateIn, authorization: Optional[str] = Header(default=None)):
    _require_admin(authorization)
    desired = payload.role.lower().strip()
    if desired not in VALID_ROLES:
        raise HTTPException(status_code=400, detail=f"Invalid role '{desired}'")
    supabase = get_service_client()
    if desired == 'admin':
        # Upsert row
        res = supabase.table('admin_roles').upsert({"auth_user_id": auth_user_id, "role": "admin"}).execute()
        if getattr(res, 'error', None):
            raise HTTPException(status_code=500, detail=f"Role upsert failed: {res.error}")
    else:
        # If demoting from admin ensure not last admin
        # Check if currently admin
        existing = supabase.table('admin_roles').select('role').eq('auth_user_id', auth_user_id).limit(1).execute()
        is_admin_now = False
        if not getattr(existing, 'error', None):
            rows = getattr(existing, 'data', []) or []
            is_admin_now = bool(rows and rows[0].get('role') == 'admin')
        if is_admin_now:
            admin_count = _count_admins(supabase)
            if admin_count <= 1:
                raise HTTPException(status_code=400, detail="Cannot demote the last admin")
            del_res = supabase.table('admin_roles').delete().eq('auth_user_id', auth_user_id).execute()
            if getattr(del_res, 'error', None):
                raise HTTPException(status_code=500, detail=f"Role demote failed: {del_res.error}")
        # For non-admin roles we can store or remove row (choose store for non-student roles)
        if desired in {"teacher", "moderator", "employee"}:
            up_res = supabase.table('admin_roles').upsert({"auth_user_id": auth_user_id, "role": desired}).execute()
            if getattr(up_res, 'error', None):
                raise HTTPException(status_code=500, detail=f"Role update failed: {up_res.error}")
        elif desired == 'student':
            # Remove row if not needed
            supabase.table('admin_roles').delete().eq('auth_user_id', auth_user_id).execute()
    return {"ok": True, "auth_user_id": auth_user_id, "role": desired}


@academics_router.delete("/api/admin/users/{auth_user_id}", summary="Admin: delete a user (profile + role)")
def delete_user(auth_user_id: str, authorization: Optional[str] = Header(default=None)):
    _require_admin(authorization)
    supabase = get_service_client()
    # Prevent deleting last admin if target is sole admin
    existing_role = supabase.table('admin_roles').select('role').eq('auth_user_id', auth_user_id).limit(1).execute()
    target_is_admin = False
    if not getattr(existing_role, 'error', None):
        rows = getattr(existing_role, 'data', []) or []
        target_is_admin = bool(rows and rows[0].get('role') == 'admin')
    if target_is_admin:
        admin_count = _count_admins(supabase)
        if admin_count <= 1:
            raise HTTPException(status_code=400, detail="Cannot delete the last admin")
    # Delete profile first (soft cascade pattern) then role row
    prof_del = supabase.table('user_profiles').delete().eq('auth_user_id', auth_user_id).execute()
    if getattr(prof_del, 'error', None):
        raise HTTPException(status_code=500, detail=f"Profile delete failed: {prof_del.error}")
    supabase.table('admin_roles').delete().eq('auth_user_id', auth_user_id).execute()
    # NOTE: We are NOT deleting from auth.users here (would require service role elevated call). Document manual removal if needed.
    return {"ok": True, "deleted_auth_user_id": auth_user_id}

# ================= Teacher Feature Backend =====================

teacher_router = APIRouter()


class TeacherSignupIn(BaseModel):
    name: str
    email: str
    password: str
    college: Optional[str] = None
    department: Optional[str] = None
    subjects: Optional[List[str]] = None

    @validator("subjects", pre=True, always=True)
    def _clean_subjects(cls, v):
        if not v:
            return []
        out = []
        for s in v:
            if not s:
                continue
            s2 = str(s).strip()
            if s2 and s2 not in out:
                out.append(s2[:64])
        return out


class TeacherApproveIn(BaseModel):
    status: str = Field(..., pattern=r"^(approved|rejected)$")
    notes: Optional[str] = None


class TeacherMessageIn(BaseModel):
    content: str = Field(..., min_length=1, max_length=4000)


# ---------------- WebSocket Chat Manager (Teacher) -----------------
class TeacherChatManager:
    """In-memory tracking of active teacher chat WebSocket connections.

    Structure: {connection_id: {user_id: websocket}}
    For multi-process / multi-instance deployments, replace with shared pub/sub.
    """
    def __init__(self):
        self.active: dict[str, dict[str, WebSocket]] = {}

    async def connect(self, connection_id: str, user_id: str, websocket: WebSocket):
        await websocket.accept()
        self.active.setdefault(connection_id, {})[user_id] = websocket

    def disconnect(self, connection_id: str, user_id: str):
        try:
            if connection_id in self.active and user_id in self.active[connection_id]:
                del self.active[connection_id][user_id]
                if not self.active[connection_id]:
                    del self.active[connection_id]
        except Exception:
            pass

    async def broadcast(self, connection_id: str, payload: dict):
        conns = self.active.get(connection_id, {})
        stale = []
        for uid, ws in conns.items():
            try:
                await ws.send_json(payload)
            except Exception:
                stale.append(uid)
        for uid in stale:
            self.disconnect(connection_id, uid)


chat_manager = TeacherChatManager()


def _ensure_teacher_role(auth_user_id: str):
    supabase = get_service_client()
    # Upsert teacher role if not exists
    row = supabase.table("admin_roles").select("role").eq("auth_user_id", auth_user_id).limit(1).execute()
    if getattr(row, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (get role): {row.error}")
    data = row.data or []
    if data:
        role = data[0].get("role")
        if role != "teacher":
            # do not override admin; if admin keep dual capability
            if role in {"admin","moderator"}:
                return
            upd = supabase.table("admin_roles").update({"role": "teacher"}).eq("auth_user_id", auth_user_id).execute()
            if getattr(upd, "error", None):
                raise HTTPException(status_code=500, detail=f"Supabase error (promote teacher): {upd.error}")
    else:
        ins = supabase.table("admin_roles").insert({"auth_user_id": auth_user_id, "role": "teacher"}).execute()
        if getattr(ins, "error", None):
            raise HTTPException(status_code=500, detail=f"Supabase error (insert teacher role): {ins.error}")


def _require_teacher(authorization: Optional[str]):
    uid, _ = _get_auth_user(authorization)
    supabase = get_service_client()
    role_q = supabase.table("admin_roles").select("role").eq("auth_user_id", uid).limit(1).execute()
    if getattr(role_q, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (role check): {role_q.error}")
    data = role_q.data or []
    role = data[0].get("role") if data else "student"
    if role not in {"teacher","admin"}:  # admins also allowed
        raise HTTPException(status_code=403, detail="Teacher role required")
    return uid


@teacher_router.post("/api/teacher/signup", summary="Teacher signup with ID card images (multipart)")
async def teacher_signup(
    email: str = Form(...),
    password: str = Form(...),
    confirm_password: str = Form(...),
    name: str = Form(...),
    college: Optional[str] = Form(None),
    department: Optional[str] = Form(None),
    subjects: Optional[str] = Form(None),  # JSON array or comma list
    id_card_front: UploadFile = File(...),
    id_card_back: UploadFile = File(...),
):
    if password != confirm_password:
        raise HTTPException(status_code=400, detail="Passwords do not match")
    # basic file validation
    allowed = {"image/png","image/jpeg","image/jpg","image/webp"}
    if id_card_front.content_type not in allowed or id_card_back.content_type not in allowed:
        raise HTTPException(status_code=400, detail="ID card images must be png/jpg/webp")
    anon_client = get_anon_client()
    if not anon_client:
        raise HTTPException(status_code=500, detail="Auth disabled")
    try:
        auth_res = anon_client.auth.sign_up({"email": email, "password": password})
        auth_user_id = _get_user_id_from_auth_response(auth_res)
        if not auth_user_id:
            raise HTTPException(status_code=400, detail="Failed to create auth user")
        access_token = _extract_access_token(auth_res)
        supabase = get_service_client()
        college_id = None
        department_id = None
        if college:
            try:
                college_id = str(_resolve_college_id_by_name(college))
            except Exception:
                college_id = None
        if department and college_id:
            try:
                department_id = str(_resolve_department_id(uuid.UUID(college_id), department.upper()))
            except Exception:
                department_id = None
        # subjects parse
        subj_list: List[str] = []
        if subjects:
            try:
                if subjects.strip().startswith("["):
                    subj_list = [s[:64] for s in json.loads(subjects) if isinstance(s, str)]
                else:
                    subj_list = [s.strip()[:64] for s in subjects.split(',') if s.strip()]
            except Exception:
                subj_list = []
        # store images locally (assets/teacher_ids/)
        base_dir = Path(__file__).parent / "assets" / "teacher_ids"
        base_dir.mkdir(parents=True, exist_ok=True)
        front_ext = Path(id_card_front.filename or "front").suffix.lower() or ".jpg"
        back_ext = Path(id_card_back.filename or "back").suffix.lower() or ".jpg"
        front_name = f"{auth_user_id}_front{front_ext}"
        back_name = f"{auth_user_id}_back{back_ext}"
        front_path = base_dir / front_name
        back_path = base_dir / back_name
        # write files
        front_bytes = await id_card_front.read()
        back_bytes = await id_card_back.read()
        if len(front_bytes) > 5*1024*1024 or len(back_bytes) > 5*1024*1024:
            raise HTTPException(status_code=400, detail="Image too large (max 5MB)")
        front_path.write_bytes(front_bytes)
        back_path.write_bytes(back_bytes)
        rel_front = f"assets/teacher_ids/{front_name}"
        rel_back = f"assets/teacher_ids/{back_name}"
        ins = supabase.table("teacher_applications").insert({
            "auth_user_id": auth_user_id,
            "email": email,
            "name": name,
            "college_id": college_id,
            "department_id": department_id,
            "subjects": subj_list,
            "id_card_front_path": rel_front,
            "id_card_back_path": rel_back,
            "status": "pending"
        }).execute()
        if getattr(ins, "error", None):
            raise HTTPException(status_code=500, detail=f"Supabase error (teacher application insert): {ins.error}")
        return {"message": "Teacher application submitted", "access_token": access_token, "user_id": auth_user_id}
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Unexpected signup error: {e}")


@teacher_router.get("/api/teacher/applications", summary="Admin: list teacher applications")
def list_teacher_applications(status: Optional[str] = Query(default=None), authorization: Optional[str] = Header(default=None)):
    _require_admin(authorization)
    supabase = get_service_client()
    query = supabase.table("teacher_applications").select("*").order("created_at", desc=True)
    if status:
        query = query.eq("status", status)
    res = query.execute()
    if getattr(res, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (list teacher apps): {res.error}")
    return {"applications": res.data or []}


@teacher_router.post("/api/teacher/applications/{application_id}/review", summary="Admin: approve or reject a teacher application")
def review_teacher_application(application_id: str, payload: TeacherApproveIn, authorization: Optional[str] = Header(default=None)):
    admin_uid, _ = _require_admin(authorization)
    supabase = get_service_client()
    debug = bool(os.getenv("TEACHER_PROFILE_DEBUG"))
    if debug:
        try:
            print(f"[TPROF_DEBUG] Review start application_id={application_id} target_status={payload.status} admin={admin_uid}")
        except Exception:
            pass
    app_q = supabase.table("teacher_applications").select("auth_user_id,status").eq("id", application_id).limit(1).execute()
    if getattr(app_q, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (fetch app): {app_q.error}")
    if not app_q.data:
        raise HTTPException(status_code=404, detail="Application not found")
    row = app_q.data[0]
    if debug:
        try:
            print(f"[TPROF_DEBUG] Existing application status={row.get('status')} auth_user_id={row.get('auth_user_id')}")
        except Exception:
            pass
    if row.get("status") != "pending":
        # allow re-review? Only if moving from rejected to approved maybe
        pass
    upd = supabase.table("teacher_applications").update({
        "status": payload.status,
        "notes": payload.notes,
        "reviewed_by": admin_uid,
        "reviewed_at": datetime.utcnow().isoformat()
    }).eq("id", application_id).execute()
    if getattr(upd, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (update app): {upd.error}")
    sync_result: Dict[str, Any] = {"profile_sync": "skipped"}
    if payload.status == "approved":
        _ensure_teacher_role(row.get("auth_user_id"))
        try:
            sync_result = _sync_teacher_profile_from_application(supabase, row.get("auth_user_id")) or {"profile_sync": "no-op"}
            if debug:
                try:
                    print(f"[TPROF_DEBUG] Sync result: {sync_result}")
                except Exception:
                    pass
        except Exception as e:
            # Non-fatal: surface minimal info
            sync_result = {"profile_sync": "error", "error": str(e)}
            try:
                supabase_logger.exception(f"Teacher profile sync failed on approval: {e}")
            except Exception:
                pass
    return {"ok": True, "application_id": application_id, "status": payload.status, **sync_result}


@teacher_router.get("/api/teacher/me/status", summary="Teacher applicant status (self)")
def teacher_me_status(authorization: Optional[str] = Header(default=None)):
    uid, _ = _get_auth_user(authorization)
    supabase = get_service_client()
    app_q = supabase.table("teacher_applications").select("status").eq("auth_user_id", uid).limit(1).execute()
    if getattr(app_q, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (get teacher status): {app_q.error}")
    status = app_q.data[0]["status"] if app_q.data else None
    role_q = supabase.table("admin_roles").select("role").eq("auth_user_id", uid).limit(1).execute()
    current_role = None
    if not getattr(role_q, "error", None) and role_q.data:
        current_role = role_q.data[0].get("role")
    return {"status": status, "role": current_role}


@teacher_router.get("/api/teachers", summary="List approved teachers")
def list_teachers(limit: int = Query(default=100, ge=1, le=500)):
    supabase = get_service_client()
    # join applications + roles + profile (if exists)
    apps = supabase.table("teacher_applications").select("auth_user_id,name,email,college_id,department_id").eq("status", "approved").order("created_at", desc=True).limit(limit).execute()
    if getattr(apps, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (list teachers): {apps.error}")
    rows = apps.data or []
    # Attach role sanity & optional college/department names
    college_ids = {r.get("college_id") for r in rows if r.get("college_id")}
    dept_ids = {r.get("department_id") for r in rows if r.get("department_id")}
    college_map = {}
    dept_map = {}
    if college_ids:
        def _fetch_colleges():
            return supabase.table("colleges").select("id,name").in_("id", list(college_ids)).execute()
        try:
            csel = _supabase_retry(_fetch_colleges)
            if not getattr(csel, "error", None):
                for c in csel.data or []:
                    college_map[c.get("id")] = c
        except Exception as e:  # fallback: proceed without college names
            supabase_logger.warning("College lookup failed after retries: %s", e)
    if dept_ids:
        def _fetch_departments():
            return supabase.table("departments").select("id,name").in_("id", list(dept_ids)).execute()
        try:
            dsel = _supabase_retry(_fetch_departments)
            if not getattr(dsel, "error", None):
                for d in dsel.data or []:
                    dept_map[d.get("id")] = d
        except Exception as e:  # fallback: proceed without department names
            supabase_logger.warning("Department lookup failed after retries: %s", e)
    out = []
    for r in rows:
        out.append({
            "auth_user_id": r.get("auth_user_id"),
            "name": r.get("name"),
            "email": r.get("email"),
            "college": college_map.get(r.get("college_id"), {}).get("name"),
            "department": dept_map.get(r.get("department_id"), {}).get("name"),
        })
    return {"teachers": out, "count": len(out)}


def _canonical_pair(a: str, b: str) -> Tuple[str, str]:
    return (a, b) if a < b else (b, a)


@teacher_router.post("/api/teacher/connect/{other_user_id}", summary="Create or fetch teacher connection")
def teacher_connect(other_user_id: str, authorization: Optional[str] = Header(default=None)):
    uid = _require_teacher(authorization)
    if other_user_id == uid:
        raise HTTPException(status_code=400, detail="Cannot connect to self")
    supabase = get_service_client()
    a, b = _canonical_pair(uid, other_user_id)
    q = supabase.table("teacher_connections").select("id").eq("teacher_a", a).eq("teacher_b", b).limit(1).execute()
    if getattr(q, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (fetch connection): {q.error}")
    if q.data:
        return {"connection_id": q.data[0]["id"], "existing": True}
    ins = supabase.table("teacher_connections").insert({"teacher_a": a, "teacher_b": b}).execute()
    if getattr(ins, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (create connection): {ins.error}")
    cid = ins.data[0]["id"] if ins.data else None
    return {"connection_id": cid, "existing": False}


@teacher_router.get("/api/teacher/connections", summary="List my teacher connections")
def list_my_connections(authorization: Optional[str] = Header(default=None)):
    uid = _require_teacher(authorization)
    supabase = get_service_client()
    # union pattern via OR filter not supported; fetch both sides
    # Apply lightweight retry for transient httpx/httpcore protocol disconnects
    rows_a: List[Dict[str, Any]] = []
    rows_b: List[Dict[str, Any]] = []
    try:
        a_res = _supabase_retry(
            lambda: (
                supabase
                .table("teacher_connections")
                .select("id,teacher_a,teacher_b,created_at")
                .eq("teacher_a", uid)
                .execute()
            )
        )
        if getattr(a_res, "error", None):
            raise HTTPException(status_code=500, detail=f"Supabase error (conn A): {a_res.error}")
        rows_a = a_res.data or []
    except HTTPXRemoteProtocolError as exc:  # pragma: no cover - network timing dependent
        logging.getLogger("teachers").warning(
            "list_my_connections transient protocol error on A-side: %s", exc
        )
        rows_a = []
    try:
        b_res = _supabase_retry(
            lambda: (
                supabase
                .table("teacher_connections")
                .select("id,teacher_a,teacher_b,created_at")
                .eq("teacher_b", uid)
                .execute()
            )
        )
        if getattr(b_res, "error", None):
            raise HTTPException(status_code=500, detail=f"Supabase error (conn B): {b_res.error}")
        rows_b = b_res.data or []
    except HTTPXRemoteProtocolError as exc:  # pragma: no cover - network timing dependent
        logging.getLogger("teachers").warning(
            "list_my_connections transient protocol error on B-side: %s", exc
        )
        rows_b = []
    rows = rows_a + rows_b
    partner_ids = []
    for r in rows:
        partner_ids.append(r.get("teacher_b") if r.get("teacher_a") == uid else r.get("teacher_a"))
    # enrich partner basic info from teacher_applications (fallback to user_profiles)
    partner_ids = [p for p in partner_ids if p]
    uniq = list({p for p in partner_ids})
    partner_map = {}
    if uniq:
        try:
            tapp = _supabase_retry(
                lambda: (
                    supabase
                    .table("teacher_applications")
                    .select("auth_user_id,name")
                    .in_("auth_user_id", uniq)
                    .execute()
                )
            )
            if not getattr(tapp, "error", None):
                for t in tapp.data or []:
                    partner_map[t.get("auth_user_id")] = t
        except HTTPXRemoteProtocolError as exc:  # pragma: no cover - network timing dependent
            logging.getLogger("teachers").warning(
                "list_my_connections transient protocol error on partner lookup: %s", exc
            )
    out = []
    for r in rows:
        partner = r.get("teacher_b") if r.get("teacher_a") == uid else r.get("teacher_a")
        out.append({
            "connection_id": r.get("id"),
            "partner_user_id": partner,
            "partner_name": partner_map.get(partner, {}).get("name"),
            "created_at": r.get("created_at"),
        })
    return {"connections": out, "count": len(out)}


@teacher_router.post("/api/teacher/connections/{connection_id}/messages", summary="Send message on a connection")
def send_message(connection_id: str, payload: TeacherMessageIn, authorization: Optional[str] = Header(default=None)):
    uid = _require_teacher(authorization)
    supabase = get_service_client()
    # verify membership
    c = supabase.table("teacher_connections").select("teacher_a,teacher_b").eq("id", connection_id).limit(1).execute()
    if getattr(c, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (get conn): {c.error}")
    if not c.data:
        raise HTTPException(status_code=404, detail="Connection not found")
    row = c.data[0]
    if uid not in {row.get("teacher_a"), row.get("teacher_b")}:
        raise HTTPException(status_code=403, detail="Not a participant")
    ins = supabase.table("teacher_messages").insert({
        "connection_id": connection_id,
        "sender_user_id": uid,
        "content": payload.content
    }).execute()
    if getattr(ins, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (insert msg): {ins.error}")
    return {"ok": True, "message_id": ins.data[0]["id"] if ins.data else None}


@teacher_router.get("/api/teacher/connections/{connection_id}/messages", summary="List messages in a connection")
def list_messages(connection_id: str, since: Optional[str] = Query(default=None), limit: int = Query(default=200, ge=1, le=500), authorization: Optional[str] = Header(default=None)):
    uid = _require_teacher(authorization)
    supabase = get_service_client()
    c = supabase.table("teacher_connections").select("teacher_a,teacher_b").eq("id", connection_id).limit(1).execute()
    if getattr(c, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (get conn): {c.error}")
    if not c.data:
        raise HTTPException(status_code=404, detail="Connection not found")
    row = c.data[0]
    if uid not in {row.get("teacher_a"), row.get("teacher_b")}:
        raise HTTPException(status_code=403, detail="Not a participant")
    q = supabase.table("teacher_messages").select("id,sender_user_id,content,created_at").eq("connection_id", connection_id).order("created_at")
    if since:
        q = q.gt("created_at", since)
    res = q.limit(limit).execute()
    if getattr(res, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (list msgs): {res.error}")
    return {"messages": res.data or [], "count": len(res.data or [])}


@teacher_router.get("/api/teacher/notes/upload-meta", summary="Dynamic academic dropdown metadata for teacher notes upload")
def teacher_notes_meta(authorization: Optional[str] = Header(default=None)):
    _require_teacher(authorization)  # role gating only
    supabase = get_service_client()

    def _safe_select(table: str, columns: str, order: Optional[str] = None):
        try:
            q = supabase.table(table).select(columns)
            if order:
                q = q.order(order)
            res = q.execute()
            if getattr(res, "error", None):
                # Best-effort: return empty list on transient failures
                try:
                    supabase_logger.warning(f"Teacher meta query error: {table}: {res.error}")
                except Exception:
                    pass
                return []
            return res.data or []
        except Exception as e:  # network/protocol errors
            try:
                supabase_logger.exception(f"Teacher meta exception on {table}")
            except Exception:
                pass
            return []

    colleges = _safe_select("colleges", "id,name", order="name")
    degrees = _safe_select("degrees", "id,name,college_id")
    departments = _safe_select("departments", "id,name,college_id,degree_id")
    batches = _safe_select("batches", "id,department_id,from_year,to_year")

    if not (colleges or degrees or departments or batches):
        raise HTTPException(status_code=500, detail="Supabase unavailable for teacher meta")

    return {
        "colleges": colleges,
        "degrees": degrees,
        "departments": departments,
        "batches": batches,
    }


# Integrate simple reuse of existing marketplace notes for teacher uploads: teacher uses existing /api/marketplace/notes routes.
# Extra filtering endpoint for teacher's own notes.
@teacher_router.get("/api/teacher/notes/mine", summary="List notes uploaded by the current teacher (marketplace integration)")
def teacher_my_notes(authorization: Optional[str] = Header(default=None)):
    uid = _require_teacher(authorization)
    supabase = get_service_client()
    res = (
        supabase.table("marketplace_notes")
        .select("id,title,description,subject,subject_id,semester,created_at,updated_at,price_cents,unit,exam_type,categories,original_filename")
        .eq("owner_user_id", uid)
        .order("created_at", desc=True)
        .limit(200)
        .execute()
    )
    if getattr(res, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (my teacher notes): {res.error}")
    return {"notes": res.data or []}

# ---------------- Teacher Profile & Classes Aggregation ----------------

def _fetch_teacher_core(supabase: Client, teacher_user_id: str) -> Dict[str, Any]:
    """Fetch teacher core info from teacher_applications + admin_roles + user_profiles + teacher_profiles."""
    core: Dict[str, Any] = {"auth_user_id": teacher_user_id}
    # Extended teacher profile (now primary source for identity & academic linkage)
    tprof = supabase.table("teacher_profiles").select("name,email,college_id,department_id,headline,bio,specialization,years_experience,qualification,availability,social,profile_image_url").eq("auth_user_id", teacher_user_id).limit(1).execute()
    if not getattr(tprof, "error", None) and tprof.data:
        rowp = tprof.data[0]
        # Identity / academic fields preferred from teacher_profiles first
        for k in ["name","email","college_id","department_id"]:
            if rowp.get(k):
                core[k] = rowp.get(k)
        core.update({k: rowp.get(k) for k in ["headline","bio","specialization","years_experience","qualification","availability","social"] if k in rowp})
        if rowp.get("profile_image_url"):
            core["avatar_url"] = rowp.get("profile_image_url")
    # Application (fallback / supplemental for subjects & status & missing identity)
    app = supabase.table("teacher_applications").select("name,email,college_id,department_id,subjects,status").eq("auth_user_id", teacher_user_id).limit(1).execute()
    if getattr(app, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (teacher application): {app.error}")
    if app.data:
        arow = app.data[0]
        # subjects and status always sourced from application
        for k in ["subjects","status"]:
            if arow.get(k) is not None:
                core[k] = arow.get(k)
        # Only fill identity if still missing
        for k in ["name","email","college_id","department_id"]:
            if not core.get(k) and arow.get(k):
                core[k] = arow.get(k)
    # Role
    role_res = supabase.table("admin_roles").select("role").eq("auth_user_id", teacher_user_id).limit(1).execute()
    if not getattr(role_res, "error", None) and role_res.data:
        core["role"] = role_res.data[0].get("role")
    # User profile (legacy avatar/headline/bio fallback)
    prof = supabase.table("user_profiles").select("name,profile_image_url,headline,bio,semester,batch_from,batch_to").eq("auth_user_id", teacher_user_id).limit(1).execute()
    if not getattr(prof, "error", None) and prof.data:
        row = prof.data[0]
        if not core.get("name") and row.get("name"):
            core["name"] = row.get("name")
        if not core.get("avatar_url") and row.get("profile_image_url"):
            core["avatar_url"] = row.get("profile_image_url")
        if not core.get("headline") and row.get("headline"):
            core["headline"] = row.get("headline")
        if not core.get("bio") and row.get("bio"):
            core["bio"] = row.get("bio")
    return core

def _sync_teacher_profile_from_application(supabase: Client, auth_user_id: Optional[str]) -> Optional[Dict[str, Any]]:
    """Idempotently upsert identity & academic linkage fields from approved teacher_application into teacher_profiles.

    Returns a dict with keys: profile_sync (insert|update|no-op|error), and maybe details.
    Silent no-op if application missing or not approved.
    """
    debug = bool(os.getenv("TEACHER_PROFILE_DEBUG"))
    if not auth_user_id:
        if debug:
            try: print("[TPROF_DEBUG] No auth_user_id passed to sync")
            except Exception: pass
        return {"profile_sync": "no-op", "reason": "missing auth_user_id"}
    app = supabase.table("teacher_applications").select("name,email,college_id,department_id,status").eq("auth_user_id", auth_user_id).limit(1).execute()
    if getattr(app, "error", None) or not app.data:
        if debug:
            try: print(f"[TPROF_DEBUG] Application missing or error error={getattr(app,'error',None)}")
            except Exception: pass
        return {"profile_sync": "no-op", "reason": "application missing"}
    row = app.data[0]
    if row.get("status") != "approved":
        if debug:
            try: print(f"[TPROF_DEBUG] Application not approved status={row.get('status')}")
            except Exception: pass
        return {"profile_sync": "no-op", "reason": "not approved"}
    payload = _supabase_payload({
        "auth_user_id": str(auth_user_id) if auth_user_id else None,
        "name": row.get("name"),
        "email": row.get("email"),
        "college_id": row.get("college_id"),
        "department_id": row.get("department_id"),
    })
    existing = supabase.table("teacher_profiles").select("auth_user_id").eq("auth_user_id", auth_user_id).limit(1).execute()
    if getattr(existing, "error", None):
        if debug:
            try: print(f"[TPROF_DEBUG] Existing profile lookup error={existing.error}")
            except Exception: pass
        return {"profile_sync": "error", "error": str(existing.error)}
    if existing.data:
        upd_payload = {k: v for k, v in payload.items() if k != "auth_user_id"}
        upd = supabase.table("teacher_profiles").update(upd_payload).eq("auth_user_id", auth_user_id).execute()
        if getattr(upd, "error", None):
            err_txt = str(upd.error)
            if debug:
                try: print(f"[TPROF_DEBUG] Update error err={err_txt}")
                except Exception: pass
            return {"profile_sync": "error", "error": err_txt}
        if debug:
            try: print("[TPROF_DEBUG] Profile updated")
            except Exception: pass
        return {"profile_sync": "update"}
    ins = supabase.table("teacher_profiles").insert(payload).execute()
    if getattr(ins, "error", None):
        err_txt = str(ins.error)
        if debug:
            try: print(f"[TPROF_DEBUG] Insert error err={err_txt}")
            except Exception: pass
        # Common cause: migration not applied (new columns absent)
        return {"profile_sync": "error", "error": err_txt, "hint": "Run ALTER TABLE statements from db.sql migration note"}
    if debug:
        try: print("[TPROF_DEBUG] Profile inserted")
        except Exception: pass
    return {"profile_sync": "insert"}

@teacher_router.post("/api/admin/teacher/profile/resync/{user_id}", summary="Admin: force resync teacher profile from application")
def admin_resync_teacher_profile(user_id: str, authorization: Optional[str] = Header(default=None)):
    _require_admin(authorization)
    supabase = get_service_client()
    result = _sync_teacher_profile_from_application(supabase, user_id)
    return {"ok": True, **(result or {"profile_sync": "no-op"})}

@teacher_router.get("/api/admin/teacher-profiles/{user_id}", summary="Admin: fetch raw teacher_profiles row")
def admin_get_teacher_profile_row(user_id: str, authorization: Optional[str] = Header(default=None)):
    _require_admin(authorization)
    supabase = get_service_client()
    res = supabase.table("teacher_profiles").select("*").eq("auth_user_id", user_id).limit(1).execute()
    if getattr(res, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (get teacher profile raw): {res.error}")
    return {"row": (res.data or [None])[0]}

@teacher_router.post("/api/admin/teacher-profiles/backfill", summary="Admin: backfill all approved teacher applications into teacher_profiles")
def admin_backfill_teacher_profiles(authorization: Optional[str] = Header(default=None)):
    _require_admin(authorization)
    supabase = get_service_client()
    # Fetch all approved applications
    apps = supabase.table("teacher_applications").select("auth_user_id,name,email,college_id,department_id,status").eq("status","approved").execute()
    if getattr(apps, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (fetch approved apps): {apps.error}")
    rows = apps.data or []
    inserted = 0; updated = 0; errors = []
    for r in rows:
        uid = r.get("auth_user_id")
        if not uid:
            continue
        payload = _supabase_payload({
            "auth_user_id": str(uid) if uid else None,
            "name": r.get("name"),
            "email": r.get("email"),
            "college_id": r.get("college_id"),
            "department_id": r.get("department_id"),
        })
        existing = supabase.table("teacher_profiles").select("auth_user_id").eq("auth_user_id", uid).limit(1).execute()
        if getattr(existing, "error", None):
            errors.append({"user": uid, "error": str(existing.error)})
            continue
        if existing.data:
            upd_payload = {k: v for k, v in payload.items() if k != "auth_user_id"}
            upd = supabase.table("teacher_profiles").update(upd_payload).eq("auth_user_id", uid).execute()
            if getattr(upd, "error", None):
                errors.append({"user": uid, "error": str(upd.error)})
            else:
                updated += 1
        else:
            ins = supabase.table("teacher_profiles").insert(payload).execute()
            if getattr(ins, "error", None):
                errors.append({"user": uid, "error": str(ins.error)})
            else:
                inserted += 1
    return {"ok": True, "inserted": inserted, "updated": updated, "errors": errors, "total_approved": len(rows)}

@teacher_router.get("/api/admin/teacher-profiles/diagnostics", summary="Admin: diagnostics counts for teacher profiles vs applications")
def admin_teacher_profiles_diagnostics(authorization: Optional[str] = Header(default=None)):
    _require_admin(authorization)
    supabase = get_service_client()
    apps = supabase.table("teacher_applications").select("auth_user_id,status").execute()
    if getattr(apps, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (apps diag): {apps.error}")
    profs = supabase.table("teacher_profiles").select("auth_user_id").execute()
    if getattr(profs, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (profiles diag): {profs.error}")
    app_rows = apps.data or []
    prof_rows = profs.data or []
    approved_set = {r.get("auth_user_id") for r in app_rows if r.get("status") == "approved" and r.get("auth_user_id")}
    profile_set = {r.get("auth_user_id") for r in prof_rows if r.get("auth_user_id")}
    missing = sorted(list(approved_set - profile_set))
    extra = sorted(list(profile_set - approved_set))
    return {"approved_count": len(approved_set), "profile_count": len(profile_set), "missing_profiles_for_approved": missing, "profiles_without_approved_app": extra}

def _enrich_academics(supabase: Client, core: Dict[str, Any]):
    college_id = core.get("college_id")
    dept_id = core.get("department_id")
    if college_id:
        c = supabase.table("colleges").select("name").eq("id", college_id).limit(1).execute()
        if not getattr(c, "error", None) and c.data:
            core["college_name"] = c.data[0].get("name")
    if dept_id:
        d = supabase.table("departments").select("name,degree_id").eq("id", dept_id).limit(1).execute()
        degree_id = None
        if not getattr(d, "error", None) and d.data:
            core["department_name"] = d.data[0].get("name")
            degree_id = d.data[0].get("degree_id")
        if degree_id:
            deg = supabase.table("degrees").select("name").eq("id", degree_id).limit(1).execute()
            if not getattr(deg, "error", None) and deg.data:
                core["degree_name"] = deg.data[0].get("name")

def _fetch_teacher_classes(supabase: Client, teacher_user_id: str) -> List[Dict[str, Any]]:
    cls = supabase.table("teacher_classes").select("id,batch_id,semester,subject,subject_id,section,degree_id,department_id,college_id,created_at").eq("teacher_user_id", teacher_user_id).order("created_at", desc=True).limit(500).execute()
    if getattr(cls, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (teacher classes): {cls.error}")
    classes = cls.data or []
    # Collect IDs for enrichment
    batch_ids = {c.get("batch_id") for c in classes if c.get("batch_id")}
    degree_ids = {c.get("degree_id") for c in classes if c.get("degree_id")}
    dept_ids = {c.get("department_id") for c in classes if c.get("department_id")}
    college_ids = {c.get("college_id") for c in classes if c.get("college_id")}
    batch_map: Dict[str, Any] = {}
    def _sel(table, ids, cols):
        if not ids:
            return {}
        res = supabase.table(table).select(cols).in_("id", list(ids)).execute()
        out = {}
        if not getattr(res, "error", None):
            for r in res.data or []:
                out[r.get("id")] = r
        return out
    batch_map = _sel("batches", batch_ids, "id,from_year,to_year")
    degree_map = _sel("degrees", degree_ids, "id,name")
    dept_map = _sel("departments", dept_ids, "id,name")
    college_map = _sel("colleges", college_ids, "id,name")
    for c in classes:
        bid = c.get("batch_id"); b = batch_map.get(bid)
        if b:
            c["batch_range"] = f"{b.get('from_year')}-{b.get('to_year')}" if b.get('from_year') and b.get('to_year') else None
        if c.get("degree_id"):
            c["degree_name"] = degree_map.get(c.get("degree_id"), {}).get("name")
        if c.get("department_id"):
            c["department_name"] = dept_map.get(c.get("department_id"), {}).get("name")
        if c.get("college_id"):
            c["college_name"] = college_map.get(c.get("college_id"), {}).get("name")
        # Label for UI
        sem = c.get("semester")
        subj = c.get("subject")
        c["label"] = f"Sem {sem}: {subj}" if sem and subj else (subj or "Class")
    return classes

def _group_classes(classes: List[Dict[str, Any]]) -> Dict[str, List[Dict[str, Any]]]:
    groups: Dict[str, List[Dict[str, Any]]] = {}
    for c in classes:
        sem = c.get("semester") or 0
        key = f"Semester {sem}" if sem else "Unassigned"
        groups.setdefault(key, []).append(c)
    # Sort within groups by subject
    for g in groups.values():
        g.sort(key=lambda x: (x.get("subject") or "").lower())
    # Order groups numerically
    ordered = dict(sorted(groups.items(), key=lambda kv: int(re.sub(r"[^0-9]", "", kv[0]) or 0)))
    return ordered

@teacher_router.get("/api/teacher/profile/{user_id}", summary="Get teacher profile (public)")
async def get_teacher_profile(
    user_id: str,
    authorization: Optional[str] = Header(default=None),
    strict_q: Optional[str] = Query(default=None, alias="strict"),
    x_strict: Optional[str] = Header(default=None, alias="X-TeacherProfile-Strict"),
):
    """Optimized variant that:
    - Resolves 'me' and then performs concurrent Supabase reads
    - Honors `strict` flag (query or header) to avoid legacy fallbacks for faster response
    - Uses estimated count for notes to reduce DB cost
    """
    supabase = get_service_client()

    # Resolve 'me'
    if user_id == "me":
        try:
            uid, _ = _get_auth_user(authorization)
            user_id = uid
        except HTTPException:
            raise HTTPException(status_code=401, detail="Authentication required for 'me'")

    def _is_truthy(val: Optional[str]) -> bool:
        if val is None:
            return False
        v = str(val).strip().lower()
        return v in {"1", "true", "yes", "on"}

    strict = _is_truthy(strict_q) or _is_truthy(x_strict)

    # --- Concurrent fetches: core, classes, and notes count ---
    def _retry_blocking(fn, *a, retries=3, base_delay=0.15, **kw):
        last_exc = None
        for attempt in range(retries):
            try:
                return fn(*a, **kw)
            except RETRYABLE_EXCEPTIONS as e:  # type: ignore
                last_exc = e
                time.sleep(base_delay * (attempt + 1))
                continue
        if last_exc:
            supabase_logger.warning("teacher_profile core fetch retry exhausted: %s", last_exc)
        return fn(*a, **kw)  # final attempt, let exception propagate

    async def fetch_core():
        # Parallelize core sources: teacher_profiles, teacher_applications, admin_roles, user_profiles (if not strict)
        loop = asyncio.get_running_loop()
        tprof_fut = loop.run_in_executor(None, lambda: _retry_blocking(lambda: supabase.table("teacher_profiles").select(
            "name,email,college_id,department_id,headline,bio,specialization,years_experience,qualification,availability,social,profile_image_url"
        ).eq("auth_user_id", user_id).limit(1).execute()))
        tapp_fut = loop.run_in_executor(None, lambda: _retry_blocking(lambda: supabase.table("teacher_applications").select(
            "name,email,college_id,department_id,subjects,status"
        ).eq("auth_user_id", user_id).limit(1).execute()))
        role_fut = loop.run_in_executor(None, lambda: _retry_blocking(lambda: supabase.table("admin_roles").select("role").eq("auth_user_id", user_id).limit(1).execute()))
        prof_fut = None
        if not strict:
            prof_fut = loop.run_in_executor(None, lambda: _retry_blocking(lambda: supabase.table("user_profiles").select(
                "name,profile_image_url,headline,bio,semester,batch_from,batch_to"
            ).eq("auth_user_id", user_id).limit(1).execute()))

        tprof, tapp, role_res, prof = await asyncio.gather(
            tprof_fut, tapp_fut, role_fut, prof_fut if prof_fut is not None else asyncio.sleep(0, result=None)
        )

        if getattr(tapp, "error", None):
            raise HTTPException(status_code=500, detail=f"Supabase error (teacher application): {tapp.error}")

        core: Dict[str, Any] = {"auth_user_id": user_id}
        if not getattr(tprof, "error", None) and getattr(tprof, "data", None):
            rowp = tprof.data[0]
            for k in ["name", "email", "college_id", "department_id"]:
                if rowp.get(k):
                    core[k] = rowp.get(k)
            core.update({k: rowp.get(k) for k in ["headline", "bio", "specialization", "years_experience", "qualification", "availability", "social"] if k in rowp})
            if rowp.get("profile_image_url"):
                core["avatar_url"] = rowp.get("profile_image_url")

        if getattr(tapp, "data", None):
            arow = tapp.data[0]
            for k in ["subjects", "status"]:
                if arow.get(k) is not None:
                    core[k] = arow.get(k)
            for k in ["name", "email", "college_id", "department_id"]:
                if not core.get(k) and arow.get(k):
                    core[k] = arow.get(k)

        if not getattr(role_res, "error", None) and getattr(role_res, "data", None):
            core["role"] = role_res.data[0].get("role")

        if (not strict) and prof is not None and (not getattr(prof, "error", None)) and getattr(prof, "data", None):
            row = prof.data[0]
            if not core.get("name") and row.get("name"):
                core["name"] = row.get("name")
            if not core.get("avatar_url") and row.get("profile_image_url"):
                core["avatar_url"] = row.get("profile_image_url")
            if not core.get("headline") and row.get("headline"):
                core["headline"] = row.get("headline")
            if not core.get("bio") and row.get("bio"):
                core["bio"] = row.get("bio")

        return core

    async def fetch_classes():
        # Fetch classes, then enrich lookups concurrently
        loop = asyncio.get_running_loop()
        cls = await loop.run_in_executor(None, lambda: _retry_blocking(lambda: supabase.table("teacher_classes").select(
            "id,batch_id,semester,subject,subject_id,section,degree_id,department_id,college_id,created_at"
        ).eq("teacher_user_id", user_id).order("created_at", desc=True).limit(500).execute()))
        if getattr(cls, "error", None):
            raise HTTPException(status_code=500, detail=f"Supabase error (teacher classes): {cls.error}")
        classes = cls.data or []
        batch_ids = {c.get("batch_id") for c in classes if c.get("batch_id")}
        degree_ids = {c.get("degree_id") for c in classes if c.get("degree_id")}
        dept_ids = {c.get("department_id") for c in classes if c.get("department_id")}
        college_ids = {c.get("college_id") for c in classes if c.get("college_id")}

        def _sel(table, ids, cols):
            if not ids:
                return {}
            def _do():
                return supabase.table(table).select(cols).in_("id", list(ids)).execute()
            res = _retry_blocking(_do)
            out = {}
            if not getattr(res, "error", None):
                for r in res.data or []:
                    out[r.get("id")] = r
            return out

        # Run the 4 lookups concurrently
        batch_fut = asyncio.to_thread(_sel, "batches", batch_ids, "id,from_year,to_year")
        degree_fut = asyncio.to_thread(_sel, "degrees", degree_ids, "id,name")
        dept_fut = asyncio.to_thread(_sel, "departments", dept_ids, "id,name")
        college_fut = asyncio.to_thread(_sel, "colleges", college_ids, "id,name")
        batch_map, degree_map, dept_map, college_map = await asyncio.gather(batch_fut, degree_fut, dept_fut, college_fut)

        for c in classes:
            bid = c.get("batch_id"); b = batch_map.get(bid)
            if b:
                c["batch_range"] = f"{b.get('from_year')}-{b.get('to_year')}" if b.get('from_year') and b.get('to_year') else None
            if c.get("degree_id"):
                c["degree_name"] = degree_map.get(c.get("degree_id"), {}).get("name")
            if c.get("department_id"):
                c["department_name"] = dept_map.get(c.get("department_id"), {}).get("name")
            if c.get("college_id"):
                c["college_name"] = college_map.get(c.get("college_id"), {}).get("name")
            sem = c.get("semester"); subj = c.get("subject")
            c["label"] = f"Sem {sem}: {subj}" if sem and subj else (subj or "Class")
        return classes

    async def enrich_academics(core: Dict[str, Any]):
        # Resolve college/department names; do department & college in parallel, then maybe degree
        loop = asyncio.get_running_loop()
        college_id = core.get("college_id")
        dept_id = core.get("department_id")
        if not college_id and not dept_id:
            return
        college_fut = loop.run_in_executor(None, lambda: _retry_blocking(lambda: supabase.table("colleges").select("name").eq("id", college_id).limit(1).execute())) if college_id else asyncio.sleep(0, result=None)
        dept_fut = loop.run_in_executor(None, lambda: _retry_blocking(lambda: supabase.table("departments").select("name,degree_id").eq("id", dept_id).limit(1).execute())) if dept_id else asyncio.sleep(0, result=None)
        college_res, dept_res = await asyncio.gather(college_fut, dept_fut)
        degree_id = None
        if college_res and not getattr(college_res, "error", None) and getattr(college_res, "data", None):
            core["college_name"] = college_res.data[0].get("name")
        if dept_res and not getattr(dept_res, "error", None) and getattr(dept_res, "data", None):
            core["department_name"] = dept_res.data[0].get("name")
            degree_id = dept_res.data[0].get("degree_id")
        if degree_id:
            deg = await loop.run_in_executor(None, lambda: _retry_blocking(lambda: supabase.table("degrees").select("name").eq("id", degree_id).limit(1).execute()))
            if not getattr(deg, "error", None) and getattr(deg, "data", None):
                core["degree_name"] = deg.data[0].get("name")

    async def fetch_notes_count():
        loop = asyncio.get_running_loop()
        try:
            # estimated is much cheaper than exact for counts
            nres = await loop.run_in_executor(None, lambda: supabase.table("marketplace_notes").select("id", count="estimated").eq("owner_user_id", user_id).execute())
            if not getattr(nres, "error", None):
                return getattr(nres, "count", 0) or 0
        except Exception:
            return 0
        return 0

    core, classes, notes_count = await asyncio.gather(fetch_core(), fetch_classes(), fetch_notes_count())

    if core.get("role") not in {"teacher", "admin"} and core.get("status") != "approved":
        raise HTTPException(status_code=404, detail="Teacher not found")

    # Enrich academics after core is fetched
    await enrich_academics(core)

    subjects = sorted({c.get("subject") for c in classes if c.get("subject")})
    grouped = _group_classes(classes)
    stats = {
        "notes_count": notes_count,
        "classes_count": len(classes),
        "subjects_count": len(subjects),
    }
    return {"teacher": core, "classes": classes, "grouped_classes": grouped, "subjects": subjects, "stats": stats}

class TeacherProfileUpsertIn(BaseModel):
    headline: Optional[str] = None
    bio: Optional[str] = None
    specialization: Optional[List[str]] = None
    years_experience: Optional[int] = Field(None, ge=0, le=80)
    qualification: Optional[str] = None
    availability: Optional[Dict[str, Any]] = None
    social: Optional[Dict[str, Any]] = None
    # Allow editing identity/academic linkage if needed (optional; admin may validate externally)
    name: Optional[str] = None
    email: Optional[str] = None
    college_id: Optional[uuid.UUID] = None
    department_id: Optional[uuid.UUID] = None
    profile_image_url: Optional[str] = None  # normally set via upload endpoint

# ================= Teacher Classes CRUD Models ==================
class TeacherClassIn(BaseModel):
    subject: str = Field(..., min_length=1, max_length=255)
    semester: Optional[int] = Field(None, ge=1, le=12)
    batch_id: Optional[uuid.UUID] = None
    section: Optional[str] = Field(None, max_length=32)
    college_id: Optional[uuid.UUID] = None
    degree_id: Optional[uuid.UUID] = None
    department_id: Optional[uuid.UUID] = None
    subject_id: Optional[uuid.UUID] = None  # link to syllabus_courses.id
    notes: Optional[str] = None

class TeacherClassUpdate(BaseModel):
    subject: Optional[str] = Field(None, min_length=1, max_length=255)
    semester: Optional[int] = Field(None, ge=1, le=12)
    batch_id: Optional[uuid.UUID] = None
    section: Optional[str] = Field(None, max_length=32)
    college_id: Optional[uuid.UUID] = None
    degree_id: Optional[uuid.UUID] = None
    department_id: Optional[uuid.UUID] = None
    subject_id: Optional[uuid.UUID] = None
    notes: Optional[str] = None

class TeacherClassOut(BaseModel):
    id: uuid.UUID
    teacher_user_id: uuid.UUID
    subject: str
    semester: Optional[int] = None
    batch_id: Optional[uuid.UUID] = None
    section: Optional[str] = None
    college_id: Optional[uuid.UUID] = None
    degree_id: Optional[uuid.UUID] = None
    department_id: Optional[uuid.UUID] = None
    subject_id: Optional[uuid.UUID] = None
    notes: Optional[str] = None
    created_at: Optional[datetime] = None
    updated_at: Optional[datetime] = None


class TeacherClassStudent(BaseModel):
    profile_id: uuid.UUID
    user_education_id: uuid.UUID
    user_auth_id: Optional[uuid.UUID] = None
    name: Optional[str] = None
    email: Optional[str] = None
    regno: Optional[str] = None
    phone: Optional[str] = None
    section: Optional[str] = None
    batch_id: Optional[uuid.UUID] = None
    batch_label: Optional[str] = None
    current_semester: Optional[int] = None
    avatar_url: Optional[str] = None
    completed_topics: int = 0
    total_topics: int = 0
    progress_pct: Optional[float] = None


class TeacherClassStudentsResponse(BaseModel):
    class_info: TeacherClassOut
    students: List[TeacherClassStudent]
    total: int
    applied_filters: Dict[str, Any] = Field(default_factory=dict)

def _map_teacher_class_row(row: Dict[str, Any]) -> TeacherClassOut:
    return TeacherClassOut(
        id=uuid.UUID(row["id"]),
        teacher_user_id=uuid.UUID(row["teacher_user_id"]),
        subject=row.get("subject") or "",
        semester=row.get("semester"),
        batch_id=uuid.UUID(row["batch_id"]) if row.get("batch_id") else None,
        section=row.get("section"),
        college_id=uuid.UUID(row["college_id"]) if row.get("college_id") else None,
        degree_id=uuid.UUID(row["degree_id"]) if row.get("degree_id") else None,
        department_id=uuid.UUID(row["department_id"]) if row.get("department_id") else None,
        subject_id=uuid.UUID(row["subject_id"]) if row.get("subject_id") else None,
        notes=row.get("notes"),
        created_at=row.get("created_at"),
        updated_at=row.get("updated_at"),
    )

@teacher_router.put("/api/teacher/profile/me", summary="Upsert my extended teacher profile")
def upsert_teacher_profile(payload: TeacherProfileUpsertIn, authorization: Optional[str] = Header(default=None)):
    uid = _require_teacher(authorization)
    supabase = get_service_client()
    row = _supabase_payload(payload.dict(exclude_unset=True))
    if not row:
        return {"ok": True, "updated": False}
    row["auth_user_id"] = str(uid)
    row = _supabase_payload(row)
    # Try update first
    existing = supabase.table("teacher_profiles").select("auth_user_id").eq("auth_user_id", uid).limit(1).execute()
    if getattr(existing, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (get teacher profile): {existing.error}")
    if existing.data:
        upd = supabase.table("teacher_profiles").update(row).eq("auth_user_id", uid).execute()
        if getattr(upd, "error", None):
            raise HTTPException(status_code=500, detail=f"Supabase error (update teacher profile): {upd.error}")
    else:
        ins = supabase.table("teacher_profiles").insert(row).execute()
        if getattr(ins, "error", None):
            raise HTTPException(status_code=500, detail=f"Supabase error (insert teacher profile): {ins.error}")
    return {"ok": True, "updated": True}

@teacher_router.post("/api/teacher/profile/avatar", summary="Upload/replace teacher profile avatar (stores public URL in teacher_profiles)")
async def upload_teacher_avatar(file: UploadFile = File(...), authorization: Optional[str] = Header(default=None)):
    """Store teacher avatar in a Supabase Storage bucket and persist the public URL.

    Env vars consulted (first found wins):
      SUPABASE_TEACHER_AVATARS_BUCKET | SUPABASE_AVATARS_BUCKET | SUPABASE_PUBLIC_BUCKET | (fallback) 'teacher-avatars'
    Object key pattern: teacher_avatars/<auth_user_id><ext>
    """
    uid = _require_teacher(authorization)
    supabase = get_service_client()
    debug = bool(os.getenv("TEACHER_PROFILE_DEBUG"))
    filename = file.filename or "avatar.jpg"
    ext = (Path(filename).suffix or ".jpg").lower()
    allowed = {".jpg", ".jpeg", ".png", ".webp", ".gif"}
    if ext not in allowed:
        raise HTTPException(status_code=400, detail="Unsupported image type")
    data = await file.read()
    if len(data) > 3 * 1024 * 1024:
        raise HTTPException(status_code=400, detail="Image too large (max 3MB)")
    mime_map = {".jpg": "image/jpeg", ".jpeg": "image/jpeg", ".png": "image/png", ".webp": "image/webp", ".gif": "image/gif"}
    content_type = mime_map.get(ext, "application/octet-stream")
    bucket = (
        os.getenv("SUPABASE_TEACHER_AVATARS_BUCKET")
        or os.getenv("SUPABASE_AVATARS_BUCKET")
        or os.getenv("SUPABASE_BUCKET")  # generic project bucket key you provided
        or os.getenv("SUPABASE_PUBLIC_BUCKET")
        or "teacher-avatars"
    )
    object_path = f"teacher_avatars/{uid}{ext}"
    try:
        # Ensure bucket exists (service role key has permission)
        try:
            buckets = supabase.storage.list_buckets()
            names = {b.get('name') for b in (buckets or []) if isinstance(b, dict)}
            if bucket not in names:
                if debug:
                    try: print(f"[TPROF_DEBUG] Creating missing bucket {bucket}")
                    except Exception: pass
                supabase.storage.create_bucket(bucket, public=True)
        except Exception as be:
            if debug:
                try: print(f"[TPROF_DEBUG] Bucket check/create error={be}")
                except Exception: pass
            # Continue; upload may still work if race condition
        storage = supabase.storage.from_(bucket)
        # Supabase python client expects header values as str; use 'true' not True
        file_opts = {
            "content-type": content_type,
            "upsert": "true",  # critical: must be string
            "cache-control": "86400",
        }
        upload_res = storage.upload(object_path, data, file_opts)
        if debug:
            try: print(f"[TPROF_DEBUG] Avatar upload result bucket={bucket} path={object_path} res={upload_res}")
            except Exception: pass
    except Exception as e:
        msg = str(e)
        if debug:
            try: print(f"[TPROF_DEBUG] Avatar upload exception msg={msg}")
            except Exception: pass
        if "Header value" in msg and "bool" in msg:
            msg += " (probable cause: boolean value in file options; fixed to string but please retry)"
        raise HTTPException(status_code=500, detail=f"Failed to upload avatar: {msg}")
    # Obtain public URL
    try:
        pub = storage.get_public_url(object_path)
        # supabase-py returns dict with publicUrl key
        if isinstance(pub, dict):
            public_url = pub.get("publicUrl") or pub.get("public_url") or pub.get("data") or ""
        else:
            public_url = str(pub)
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to get public URL: {e}")
    if not public_url:
        raise HTTPException(status_code=500, detail="Public URL empty after upload")
    # Update / upsert teacher_profiles
    existing = supabase.table("teacher_profiles").select("auth_user_id").eq("auth_user_id", uid).limit(1).execute()
    if getattr(existing, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (get teacher profile for avatar): {existing.error}")
    payload = {"profile_image_url": public_url}
    if existing.data:
        upd = supabase.table("teacher_profiles").update(payload).eq("auth_user_id", uid).execute()
        if getattr(upd, "error", None):
            raise HTTPException(status_code=500, detail=f"Supabase error (update avatar): {upd.error}")
    else:
        payload["auth_user_id"] = uid
        ins = supabase.table("teacher_profiles").insert(payload).execute()
        if getattr(ins, "error", None):
            raise HTTPException(status_code=500, detail=f"Supabase error (insert avatar profile): {ins.error}")
    return {"ok": True, "profile_image_url": public_url, "avatar_url": public_url, "bucket": bucket, "path": object_path}

@teacher_router.post("/api/teacher/profile/me/avatar", summary="Upload/replace my teacher profile avatar (alt path)")
async def upload_teacher_avatar_alt(file: UploadFile = File(...), authorization: Optional[str] = Header(default=None)):
    # Reuse logic by calling original function
    return await upload_teacher_avatar(file=file, authorization=authorization)

# ================= Teacher Academics Options (college/departments/batches) ==================
@teacher_router.get("/api/teacher/academics/mine", summary="Return teacher's academic linkage and available departments + batches")
def get_teacher_academics_mine(authorization: Optional[str] = Header(default=None)):
    uid = _require_teacher(authorization)
    supabase = get_service_client()
    # Resolve college / department primarily from teacher_profiles then fallback application
    prof = supabase.table("teacher_profiles").select("college_id,department_id").eq("auth_user_id", uid).limit(1).execute()
    college_id = department_id = None
    if not getattr(prof, "error", None) and prof.data:
        row = prof.data[0]
        college_id = row.get("college_id") or None
        department_id = row.get("department_id") or None
    if not college_id or not department_id:
        app = supabase.table("teacher_applications").select("college_id,department_id").eq("auth_user_id", uid).limit(1).execute()
        if not getattr(app, "error", None) and app.data:
            arow = app.data[0]
            college_id = college_id or arow.get("college_id") or None
            department_id = department_id or arow.get("department_id") or None
    departments: List[Dict[str, Any]] = []
    batches: List[Dict[str, Any]] = []
    degrees: List[Dict[str, Any]] = []
    degree_id: Optional[str] = None
    if college_id:
        # Departments for college
        dres = supabase.table("departments").select("id,name,degree_id").eq("college_id", str(college_id)).order("name").execute()
        if getattr(dres, "error", None):
            raise HTTPException(status_code=500, detail=f"Supabase error (departments): {dres.error}")
        departments = dres.data or []
        # Degrees for college (only those referenced by departments for efficiency)
        deg_ids = sorted({d.get("degree_id") for d in departments if d.get("degree_id")})
        if deg_ids:
            deg_res = supabase.table("degrees").select("id,name").in_("id", deg_ids).execute()
            if not getattr(deg_res, "error", None):
                degrees = deg_res.data or []
        # Determine teacher's degree id via their department row
        if department_id:
            for d in departments:
                if d.get("id") == department_id:
                    degree_id = d.get("degree_id")
                    break
        # Batches (filtered later by department client-side)
        bres = supabase.table("batches").select("id,department_id,from_year,to_year").eq("college_id", str(college_id)).order("from_year").execute()
        if getattr(bres, "error", None):
            raise HTTPException(status_code=500, detail=f"Supabase error (batches): {bres.error}")
        raw_batches = bres.data or []
        for b in raw_batches:
            fy = b.get("from_year"); ty = b.get("to_year")
            b["label"] = f"{fy}-{ty}" if fy and ty else "Batch"
            batches.append(b)
    return {
        "college_id": college_id,
        "department_id": department_id,
        "degree_id": degree_id,
        "departments": departments,
        "batches": batches,
        "degrees": degrees,
    }
# ================= Debug Helpers (can be removed in production) ==================
@teacher_router.get("/api/debug/teacher-routes", summary="Debug: list registered teacher routes")
def debug_list_teacher_routes():
    from fastapi.routing import APIRoute
    routes = []
    for r in teacher_router.routes:  # only teacher_router scope
        if isinstance(r, APIRoute):
            routes.append({
                "path": r.path,
                "methods": sorted(list(r.methods - {"HEAD"})),
                "name": r.name,
            })
    return {"routes": routes}

@teacher_router.get("/api/debug/teacher-avatar-route", summary="Debug: confirm avatar route present")
def debug_avatar_route_presence():
    from fastapi.routing import APIRoute
    present = False
    methods: List[str] = []
    for r in teacher_router.routes:
        if isinstance(r, APIRoute) and r.path == "/api/teacher/profile/avatar":
            present = True
            methods = sorted(list(r.methods - {"HEAD"}))
            break
    return {"avatar_route_present": present, "methods": methods}

# ================= Teacher Classes CRUD Endpoints ==================
@teacher_router.get("/api/teacher/classes/mine", response_model=List[TeacherClassOut], summary="List my teacher classes")
def list_my_teacher_classes(authorization: Optional[str] = Header(default=None)):
    uid = _require_teacher(authorization)
    supabase = get_service_client()
    # Apply lightweight retry for transient protocol disconnections from httpx/httpcore
    try:
        res = _supabase_retry(
            lambda: (
                supabase
                .table("teacher_classes")
                .select("*")
                .eq("teacher_user_id", uid)
                .order("updated_at", desc=True)
                .execute()
            )
        )
    except HTTPXRemoteProtocolError as exc:  # pragma: no cover - network timing dependent
        logging.getLogger("teachers").warning(
            "list_my_teacher_classes transient protocol error, returning empty list: %s", exc
        )
        return []
    if getattr(res, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (list classes): {res.error}")
    rows = res.data or []
    return [_map_teacher_class_row(r) for r in rows]


@teacher_router.get("/api/teacher/classes/{class_id}", response_model=TeacherClassOut, summary="Get a teacher class")
def get_teacher_class(class_id: uuid.UUID, authorization: Optional[str] = Header(default=None)):
    uid = _require_teacher(authorization)
    supabase = get_service_client()
    res = supabase.table("teacher_classes").select("*").eq("id", str(class_id)).limit(1).execute()
    if getattr(res, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (get class): {res.error}")
    if not res.data:
        raise HTTPException(status_code=404, detail="Class not found")
    row = res.data[0]
    if row.get("teacher_user_id") != uid:
        raise HTTPException(status_code=403, detail="Cannot view another teacher's class")
    return _map_teacher_class_row(row)

@teacher_router.post("/api/teacher/classes", response_model=TeacherClassOut, summary="Create a teacher class")
def create_teacher_class(payload: TeacherClassIn, authorization: Optional[str] = Header(default=None)):
    uid = _require_teacher(authorization)
    supabase = get_service_client()
    row = _supabase_payload(payload.dict(exclude_unset=True))
    row["teacher_user_id"] = str(uid)
    row = _supabase_payload(row)
    ins = supabase.table("teacher_classes").insert(row).execute()
    if getattr(ins, "error", None):
        err_txt = str(ins.error)
        if "duplicate key value" in err_txt or "unique" in err_txt.lower():
            raise HTTPException(status_code=409, detail="Class already exists for subject + batch + semester + section")
        raise HTTPException(status_code=500, detail=f"Supabase error (insert class): {ins.error}")
    created = (ins.data or [])[0]
    return _map_teacher_class_row(created)

@teacher_router.put("/api/teacher/classes/{class_id}", response_model=TeacherClassOut, summary="Update a teacher class")
def update_teacher_class(class_id: uuid.UUID, payload: TeacherClassUpdate, authorization: Optional[str] = Header(default=None)):
    uid = _require_teacher(authorization)
    supabase = get_service_client()
    # Ensure ownership
    existing = supabase.table("teacher_classes").select("id,teacher_user_id").eq("id", str(class_id)).limit(1).execute()
    if getattr(existing, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (get class): {existing.error}")
    if not existing.data:
        raise HTTPException(status_code=404, detail="Class not found")
    if existing.data[0].get("teacher_user_id") != uid:
        raise HTTPException(status_code=403, detail="Cannot modify another teacher's class")
    updates = _supabase_payload(payload.dict(exclude_unset=True))
    if not updates:
        row_res = supabase.table("teacher_classes").select("*").eq("id", str(class_id)).limit(1).execute()
        if getattr(row_res, "error", None):
            raise HTTPException(status_code=500, detail=f"Supabase error (reload class): {row_res.error}")
        return _map_teacher_class_row(row_res.data[0])
    upd = supabase.table("teacher_classes").update(updates).eq("id", str(class_id)).execute()
    if getattr(upd, "error", None):
        err_txt = str(upd.error)
        if "duplicate key value" in err_txt or "unique" in err_txt.lower():
            raise HTTPException(status_code=409, detail="Another class with same keys exists")
        raise HTTPException(status_code=500, detail=f"Supabase error (update class): {upd.error}")
    row_res = supabase.table("teacher_classes").select("*").eq("id", str(class_id)).limit(1).execute()
    if getattr(row_res, "error", None) or not row_res.data:
        raise HTTPException(status_code=500, detail="Failed to reload updated class")
    return _map_teacher_class_row(row_res.data[0])

@teacher_router.delete("/api/teacher/classes/{class_id}", summary="Delete a teacher class")
def delete_teacher_class(class_id: uuid.UUID, authorization: Optional[str] = Header(default=None)):
    uid = _require_teacher(authorization)
    supabase = get_service_client()
    existing = supabase.table("teacher_classes").select("id,teacher_user_id").eq("id", str(class_id)).limit(1).execute()
    if getattr(existing, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (get class): {existing.error}")
    if not existing.data:
        raise HTTPException(status_code=404, detail="Class not found")
    if existing.data[0].get("teacher_user_id") != uid:
        raise HTTPException(status_code=403, detail="Cannot delete another teacher's class")
    del_res = supabase.table("teacher_classes").delete().eq("id", str(class_id)).execute()
    if getattr(del_res, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (delete class): {del_res.error}")
    return {"ok": True, "deleted": True, "id": str(class_id)}


@teacher_router.get(
    "/api/teacher/classes/{class_id}/students",
    response_model=TeacherClassStudentsResponse,
    summary="List students for a teacher class",
)
def list_teacher_class_students(class_id: uuid.UUID, authorization: Optional[str] = Header(default=None)):
    uid = _require_teacher(authorization)
    supabase = get_service_client()
    class_res = _supabase_retry(
        lambda: supabase.table("teacher_classes").select("*").eq("id", str(class_id)).limit(1).execute()
    )
    if getattr(class_res, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (get class): {class_res.error}")
    if not class_res.data:
        raise HTTPException(status_code=404, detail="Class not found")
    class_row = class_res.data[0]
    if class_row.get("teacher_user_id") != uid:
        raise HTTPException(status_code=403, detail="Cannot view another teacher's class")

    course_topic_ids: List[str] = []
    total_course_topics = 0
    subject_id_value = class_row.get("subject_id")
    if subject_id_value:
        units_res = _supabase_retry(
            lambda: supabase
            .table("syllabus_units")
            .select("id")
            .eq("course_id", str(subject_id_value))
            .execute()
        )
        if getattr(units_res, "error", None):
            raise HTTPException(status_code=500, detail=f"Supabase error (units for course): {units_res.error}")
        unit_ids = [row.get("id") for row in (units_res.data or []) if row.get("id")]
        if unit_ids:
            topics_res = _supabase_retry(
                lambda: supabase
                .table("syllabus_topics")
                .select("id")
                .in_("unit_id", unit_ids)
                .execute()
            )
            if getattr(topics_res, "error", None):
                raise HTTPException(status_code=500, detail=f"Supabase error (topics for course): {topics_res.error}")
            course_topic_ids = [row.get("id") for row in (topics_res.data or []) if row.get("id")]
            total_course_topics = len(course_topic_ids)

    applied_filters: Dict[str, Any] = {}
    def _track_filter(key: str, value: Any):
        if value is None:
            return
        if isinstance(value, str) and value == "":
            return
        if isinstance(value, (list, tuple, set, dict)) and not value:
            return
        applied_filters[key] = value

    edu_query = supabase.table("user_education").select(
        "id,user_profile_id,batch_id,section,current_semester,regno,degree_id,department_id,college_id"
    )
    filter_count = 0
    for column, value in (
        ("batch_id", class_row.get("batch_id")),
        ("section", class_row.get("section")),
        ("current_semester", class_row.get("semester")),
        ("degree_id", class_row.get("degree_id")),
        ("department_id", class_row.get("department_id")),
        ("college_id", class_row.get("college_id")),
    ):
        if value not in {None, ""}:
            eq_value = str(value) if column.endswith("_id") or column == "batch_id" else value
            edu_query = edu_query.eq(column, eq_value)
            filter_count += 1
            _track_filter(column, value)

    if filter_count == 0:
        return TeacherClassStudentsResponse(
            class_info=_map_teacher_class_row(class_row),
            students=[],
            total=0,
            applied_filters={"missing_filters": True},
        )

    edu_query = edu_query.order("updated_at", desc=True).limit(500)
    edu_res = _supabase_retry(lambda: edu_query.execute())
    if getattr(edu_res, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (student list): {edu_res.error}")
    edu_rows = edu_res.data or []
    profile_ids = {row.get("user_profile_id") for row in edu_rows if row.get("user_profile_id")}

    profiles: Dict[str, Dict[str, Any]] = {}
    if profile_ids:
        prof_res = _supabase_retry(
            lambda: supabase
            .table("user_profiles")
            .select("id,auth_user_id,name,email,regno,phone,semester,profile_image_url")
            .in_("id", list(profile_ids))
            .execute()
        )
        if getattr(prof_res, "error", None):
            raise HTTPException(status_code=500, detail=f"Supabase error (profiles): {prof_res.error}")
        for row in prof_res.data or []:
            profiles[row.get("id")] = row

    batch_ids = {row.get("batch_id") for row in edu_rows if row.get("batch_id")}
    if class_row.get("batch_id"):
        batch_ids.add(class_row.get("batch_id"))
    batch_labels: Dict[str, str] = {}
    if batch_ids:
        batch_res = _supabase_retry(
            lambda: supabase
            .table("batches")
            .select("id,from_year,to_year")
            .in_("id", list(batch_ids))
            .execute()
        )
        if getattr(batch_res, "error", None):
            raise HTTPException(status_code=500, detail=f"Supabase error (batches): {batch_res.error}")
        for row in batch_res.data or []:
            fid = row.get("id")
            if not fid:
                continue
            fy = row.get("from_year")
            ty = row.get("to_year")
            if fy and ty:
                batch_labels[fid] = f"{fy}-{ty}"

    progress_counts: Dict[str, int] = {}
    if profile_ids and course_topic_ids:
        prog_res = _supabase_retry(
            lambda: supabase
            .table("user_topic_progress")
            .select("user_profile_id,topic_id")
            .in_("user_profile_id", list(profile_ids))
            .in_("topic_id", course_topic_ids)
            .execute()
        )
        if getattr(prog_res, "error", None):
            raise HTTPException(status_code=500, detail=f"Supabase error (topic progress): {prog_res.error}")
        for row in prog_res.data or []:
            pid = row.get("user_profile_id")
            if pid:
                progress_counts[pid] = progress_counts.get(pid, 0) + 1

    students: List[TeacherClassStudent] = []
    for edu in edu_rows:
        prof_id = edu.get("user_profile_id")
        prof = profiles.get(prof_id)
        if not prof_id or not prof:
            continue
        completed = progress_counts.get(prof_id, 0)
        total_topics = total_course_topics
        progress_pct = None
        if total_topics > 0:
            progress_pct = round((completed / total_topics) * 100, 1)
        try:
            entry = TeacherClassStudent(
                profile_id=uuid.UUID(prof_id),
                user_education_id=uuid.UUID(edu["id"]),
                user_auth_id=uuid.UUID(prof["auth_user_id"]) if prof.get("auth_user_id") else None,
                name=prof.get("name"),
                email=prof.get("email"),
                regno=(prof.get("regno") or edu.get("regno")),
                phone=prof.get("phone"),
                section=edu.get("section"),
                batch_id=uuid.UUID(edu["batch_id"]) if edu.get("batch_id") else None,
                batch_label=batch_labels.get(edu.get("batch_id")),
                current_semester=edu.get("current_semester") or prof.get("semester"),
                avatar_url=prof.get("profile_image_url"),
                completed_topics=completed,
                total_topics=total_topics,
                progress_pct=progress_pct,
            )
        except (KeyError, ValueError):
            continue
        students.append(entry)

    students.sort(key=lambda s: ((s.name or "").lower(), s.regno or ""))
    if class_row.get("batch_id") and class_row.get("batch_id") in batch_labels:
        applied_filters["batch_label"] = batch_labels[class_row.get("batch_id")]

    return TeacherClassStudentsResponse(
        class_info=_map_teacher_class_row(class_row),
        students=students,
        total=len(students),
        applied_filters=applied_filters,
    )


def _asset_debug(msg: str, **extra):  # lightweight conditional debug
    if os.getenv("ASSET_DEBUG"):
        try:
            print(f"[ASSET_DEBUG] {msg} " + (" ".join(f"{k}={v}" for k,v in extra.items())))
        except Exception:
            pass


def _upload_college_logo(college_id: uuid.UUID, file: UploadFile):
    supabase = get_service_client()
    # Resolve primary bucket with fallbacks
    bucket = (
        os.getenv("SUPABASE_BUCKET", "").strip()
        or os.getenv("SUPABASE_ASSETS_BUCKET", "").strip()
        or os.getenv("SUPABASE_PROJECTS_BUCKET", "").strip()
    )
    if not bucket:
        raise HTTPException(status_code=500, detail="Missing SUPABASE_BUCKET (and fallbacks) in environment")
    filename = file.filename or "logo.png"
    ext = os.path.splitext(filename)[1].lower()
    if ext not in {".png", ".jpg", ".jpeg", ".webp", ".gif", ".svg"}:
        raise HTTPException(status_code=400, detail="Unsupported logo image type")
    blob = _read_upload_bytes(file)
    if not blob:
        raise HTTPException(status_code=400, detail="Empty upload")
    # Path pattern: colleges/{college_id}/logo{rand}.ext (keep history by random hash)
    from uuid import uuid4
    dest = f"colleges/{college_id}/logo-{uuid4().hex}{ext}"
    try:
        supabase_logger.info(
            "Uploading college logo", extra={
                "college_id": str(college_id),
                "bucket": bucket,
                "dest": dest,
                "size": len(blob),
                "content_type": file.content_type,
            }
        )
        _asset_debug("start_logo_upload", college_id=college_id, bucket=bucket, dest=dest, size=len(blob), ct=file.content_type)
        _storage_upload_bytes(supabase, bucket, dest, blob, file.content_type or "image/png")
    except HTTPException:
        _asset_debug("logo_upload_http_exception", college_id=college_id)
        raise
    except Exception as exc:
        supabase_logger.exception("College logo upload unexpected failure")
        _asset_debug("logo_upload_unexpected_failure", college_id=college_id, error=exc)
        raise HTTPException(status_code=500, detail=f"Unexpected upload failure: {exc}")
    public_url = _storage_public_url(supabase, bucket, dest)
    _asset_debug("logo_public_url", college_id=college_id, url=public_url)
    upd = supabase.table("colleges").update({"logo_url": public_url}).eq("id", str(college_id)).execute()
    if getattr(upd, "error", None):
        _asset_debug("logo_db_update_failed", college_id=college_id, error=upd.error)
        raise HTTPException(status_code=500, detail=f"Supabase error (update college logo): {upd.error}")
    _asset_debug("logo_db_update_success", college_id=college_id)
    return {"college_id": str(college_id), "logo_url": public_url, "path": dest}


@academics_router.post("/api/colleges/{college_id}/logo", summary="Upload/replace college logo")
def upload_college_logo(college_id: uuid.UUID, file: UploadFile = File(...)):
    # Existence check (fast fail)
    supabase = get_service_client()
    _asset_debug("endpoint_invoked", college_id=college_id, filename=file.filename, ct=file.content_type)
    exists = supabase.table("colleges").select("id").eq("id", str(college_id)).limit(1).execute()
    if getattr(exists, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (find college): {exists.error}")
    if not exists.data:
        raise HTTPException(status_code=404, detail="College not found")
    return _upload_college_logo(college_id, file)


@academics_router.get("/api/colleges/{college_id}/logo/debug", summary="Debug: list stored logo objects for a college")
def debug_list_college_logos(college_id: uuid.UUID):  # pragma: no cover - debug utility
    supabase = get_service_client()
    bucket = (
        os.getenv("SUPABASE_BUCKET", "").strip()
        or os.getenv("SUPABASE_ASSETS_BUCKET", "").strip()
        or os.getenv("SUPABASE_PROJECTS_BUCKET", "").strip()
    )
    if not bucket:
        raise HTTPException(status_code=500, detail="Missing SUPABASE_BUCKET (and fallbacks) in environment")
    storage = _storage_get_client(supabase)
    prefix = f"colleges/{college_id}".rstrip("/")
    try:
        # Some SDK versions: list(path=prefix, ...) ; others: from_(bucket).list(path=prefix)
        try:
            objs = storage.from_(bucket).list(prefix)
        except TypeError:
            objs = storage.from_(bucket).list(path=prefix)
    except Exception as exc:
        supabase_logger.exception("List objects failed")
        raise HTTPException(status_code=500, detail=f"List failed: {exc}")
    out = []
    if isinstance(objs, list):
        for o in objs:
            if not isinstance(o, dict):
                continue
            name = o.get("name") or o.get("Key")
            if not name:
                continue
            full_path = f"{prefix}/{name}" if not name.startswith(prefix) else name
            out.append({
                "name": name,
                "path": full_path,
                "size": o.get("metadata", {}).get("size") if isinstance(o.get("metadata"), dict) else o.get("size"),
                "last_modified": o.get("updated_at") or o.get("LastModified") or o.get("last_modified"),
                "public_url": _storage_public_url(supabase, bucket, full_path),
            })
    return {"bucket": bucket, "prefix": prefix, "objects": out}


@academics_router.post("/api/colleges", response_model=CollegeFullOut, summary="Create or update a college with departments & batches")
def create_college(payload: CollegeCreateIn):
    college_id = upsert_college(payload.college_name)
    sync_degree_hierarchy(college_id, payload.degrees or [])
    data = get_college_full(college_id)
    return CollegeFullOut(**data)


@academics_router.post(
    "/api/colleges/simple",
    response_model=CollegeFullOut,
    summary="Create a college by name with no academic structure",
)
def create_college_simple(payload: CollegeNameOnlyIn):
    college_id = upsert_college(payload.name)
    data = get_college_full(college_id)
    return CollegeFullOut(**data)


@academics_router.get("/api/colleges", response_model=List[College], summary="List all colleges (id & name)")
def list_colleges():
    supabase = get_service_client()
    # Apply lightweight retry for transient RemoteProtocolError or connection resets
    import time, logging
    from httpx import RemoteProtocolError
    attempts = 0
    last_exc = None
    backoffs = [0.0, 0.15, 0.35, 0.75]
    while attempts < len(backoffs):
        try:
            res = supabase.table("colleges").select("id,name,logo_url").order("name").execute()
            # Some SDK versions expose .error
            if getattr(res, "error", None):
                raise RuntimeError(f"Supabase error: {res.error}")
            return [
                {"id": row["id"], "name": row["name"], "logo_url": row.get("logo_url")}
                for row in (res.data or [])
                if isinstance(row, dict) and row.get("id") and row.get("name")
            ]
        except (RemoteProtocolError, ConnectionError) as exc:  # transient network layer
            last_exc = exc
            wait = backoffs[attempts]
            logging.getLogger("academics").warning(
                "list_colleges transient error attempt %s/%s: %s (backing off %.2fs)",
                attempts + 1,
                len(backoffs),
                exc,
                wait,
            )
            time.sleep(wait)
            attempts += 1
            continue
        except Exception as exc:  # non-transient
            logging.getLogger("academics").exception("list_colleges failed unrecoverably")
            raise HTTPException(status_code=500, detail=f"Failed to list colleges: {exc}")
    # Fallback after retries exhausted
    logging.getLogger("academics").error("list_colleges exhausted retries: %s", last_exc)
    # Graceful empty list so UI can still render (client can retry separately)
    return []

@academics_router.get(
    "/api/colleges/{college_id}/degrees",
    response_model=List[DegreeOut],
    summary="List degrees for a college (with departments & batches)",
)
def list_degrees_for_college(college_id: uuid.UUID):
    """Return the degrees for a college.

    Reuses the existing get_college_full aggregation logic so each DegreeOut
    includes its departments (with batches) consistent with other responses.
    This complements the existing POST /api/colleges/{college_id}/degrees which creates a degree.
    """
    data = get_college_full(college_id)
    degrees: List[DegreeOut] = data["degrees"]
    return degrees


@academics_router.post(
    "/api/colleges/{college_id}/degrees",
    response_model=DegreeOut,
    summary="Create a degree for a college",
)
def create_degree_simple(college_id: uuid.UUID, payload: DegreeSimpleCreateIn):
    supabase = get_service_client()

    existing = (
        supabase.table("degrees")
        .select("id,name,level,duration_years")
        .eq("college_id", str(college_id))
        .eq("name", payload.name)
        .limit(1)
        .execute()
    )
    if getattr(existing, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (find degree): {existing.error}")

    if existing.data:
        degree_row = existing.data[0]
    else:
        insert_payload: Dict[str, Any] = {
            "college_id": str(college_id),
            "name": payload.name,
        }
        if payload.level is not None:
            insert_payload["level"] = payload.level
        if payload.duration_years is not None:
            insert_payload["duration_years"] = payload.duration_years

        ins = supabase.table("degrees").insert(insert_payload).execute()
        if getattr(ins, "error", None):
            raise HTTPException(status_code=500, detail=f"Supabase error (insert degree): {ins.error}")
        if ins.data:
            degree_row = ins.data[0]
        else:
            refetch = (
                supabase.table("degrees")
                .select("id,name,level,duration_years")
                .eq("college_id", str(college_id))
                .eq("name", payload.name)
                .limit(1)
                .execute()
            )
            if getattr(refetch, "error", None) or not refetch.data:
                raise HTTPException(status_code=500, detail="Failed to retrieve created degree.")
            degree_row = refetch.data[0]

    college_snapshot = get_college_full(college_id)
    degree_id = uuid.UUID(degree_row["id"])
    for item in college_snapshot["degrees"]:
        if item.id == degree_id:
            return item

    raise HTTPException(status_code=404, detail="Created degree not found in college snapshot.")


@academics_router.put(
    "/api/degrees/{degree_id}",
    response_model=DegreeOut,
    summary="Update degree metadata",
)
def update_degree(degree_id: uuid.UUID, payload: DegreeSimpleCreateIn):
    supabase = get_service_client()

    existing = (
        supabase.table("degrees")
        .select("id,college_id")
        .eq("id", str(degree_id))
        .limit(1)
        .execute()
    )
    if getattr(existing, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (find degree): {existing.error}")
    if not existing.data:
        raise HTTPException(status_code=404, detail="Degree not found")

    degree_row = existing.data[0]
    college_id_str = degree_row.get("college_id")
    if not college_id_str:
        raise HTTPException(status_code=400, detail="Degree missing college reference")

    updates: Dict[str, Any] = {
        "name": payload.name,
        "level": payload.level,
        "duration_years": payload.duration_years,
    }

    upd = (
        supabase.table("degrees")
        .update(updates)
        .eq("id", str(degree_id))
        .execute()
    )
    if getattr(upd, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (update degree): {upd.error}")

    college_snapshot = get_college_full(uuid.UUID(college_id_str))
    for item in college_snapshot["degrees"]:
        if item.id == degree_id:
            return item

    raise HTTPException(status_code=404, detail="Updated degree not found in college snapshot.")


def _check_department_delete_blockers(supabase: Client, department_id: str) -> List[str]:
    """Return list of human-readable blockers for deleting a department id."""
    blocking_sources: List[str] = []
    dependency_checks = [
        ("marketplace_notes", "marketplace notes", "department_id"),
        ("teacher_applications", "teacher applications", "department_id"),
        ("teacher_classes", "teacher classes", "department_id"),
        ("teacher_profiles", "teacher profiles", "department_id"),
        ("user_education", "user education records", "department_id"),
    ]
    for table_name, label, column in dependency_checks:
        check = (
            supabase.table(table_name)
            .select(column)
            .eq(column, str(department_id))
            .limit(1)
            .execute()
        )
        if getattr(check, "error", None):
            raise HTTPException(
                status_code=500,
                detail=f"Supabase error (check {label} for department): {check.error}",
            )
        if check.data:
            blocking_sources.append(label)
    return blocking_sources


@academics_router.delete(
    "/api/degrees/{degree_id}",
    summary="Delete a degree, its departments, batches, and related syllabus data",
)
def delete_degree(degree_id: uuid.UUID):
    supabase = get_service_client()

    deg_res = (
        supabase.table("degrees")
        .select("id,college_id,name")
        .eq("id", str(degree_id))
        .limit(1)
        .execute()
    )
    if getattr(deg_res, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (find degree): {deg_res.error}")
    if not deg_res.data:
        raise HTTPException(status_code=404, detail="Degree not found")

    deg_row = deg_res.data[0]
    college_id_str = deg_row.get("college_id")
    if not college_id_str:
        raise HTTPException(status_code=400, detail="Degree missing college reference")

    # Block deletion if other products depend on the degree directly
    direct_blockers: List[str] = []
    for table_name, label, column in (
        ("teacher_classes", "teacher classes", "degree_id"),
        ("user_education", "user education records", "degree_id"),
    ):
        check = supabase.table(table_name).select(column).eq(column, str(degree_id)).limit(1).execute()
        if getattr(check, "error", None):
            raise HTTPException(status_code=500, detail=f"Supabase error (check {label} for degree): {check.error}")
        if check.data:
            direct_blockers.append(label)

    if direct_blockers:
        formatted = ", ".join(direct_blockers)
        raise HTTPException(
            status_code=409,
            detail=f"Cannot delete degree because related data exists: {formatted}. Remove or reassign those records first.",
        )

    dept_res = (
        supabase.table("departments")
        .select("id,name")
        .eq("degree_id", str(degree_id))
        .execute()
    )
    if getattr(dept_res, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (list departments for degree): {dept_res.error}")

    # Check blockers per department before any deletion
    per_dept_blockers: Dict[str, List[str]] = {}
    for dept_row in dept_res.data or []:
        dept_id = dept_row.get("id")
        if not dept_id:
            continue
        blockers = _check_department_delete_blockers(supabase, dept_id)
        if blockers:
            per_dept_blockers[dept_row.get("name") or dept_id] = blockers

    if per_dept_blockers:
        # Keep message short but actionable
        names = ", ".join(list(per_dept_blockers.keys())[:5])
        suffix = "" if len(per_dept_blockers) <= 5 else f" (+{len(per_dept_blockers) - 5} more)"
        raise HTTPException(
            status_code=409,
            detail=f"Cannot delete degree because related data exists under departments: {names}{suffix}. Remove or reassign those records first.",
        )

    stats = {"departments": 0, "batches": 0, "courses": 0}
    for dept_row in dept_res.data or []:
        dept_id_str = dept_row.get("id")
        if not dept_id_str:
            continue
        dept_uuid = uuid.UUID(dept_id_str)
        batch_stats = _cascade_delete_department_batches(supabase, dept_uuid)
        stats["batches"] += int(batch_stats.get("batches", 0))
        stats["courses"] += int(batch_stats.get("courses", 0))

        del_dept = supabase.table("departments").delete().eq("id", dept_id_str).execute()
        if getattr(del_dept, "error", None):
            raise HTTPException(status_code=500, detail=f"Supabase error (delete department): {del_dept.error}")
        stats["departments"] += 1

    del_deg = supabase.table("degrees").delete().eq("id", str(degree_id)).execute()
    if getattr(del_deg, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (delete degree): {del_deg.error}")

    return {
        "ok": True,
        "deleted_degree_id": str(degree_id),
        "deleted_departments": stats["departments"],
        "deleted_batches": stats["batches"],
        "deleted_courses": stats["courses"],
    }


@academics_router.post(
    "/api/degrees/{degree_id}/departments",
    response_model=DepartmentWithBatchesOut,
    summary="Create a department for a degree",
)
def create_department_simple(degree_id: uuid.UUID, payload: DepartmentSimpleCreateIn):
    supabase = get_service_client()

    degree_res = (
        supabase.table("degrees")
        .select("id,college_id")
        .eq("id", str(degree_id))
        .limit(1)
        .execute()
    )
    if getattr(degree_res, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (find degree): {degree_res.error}")
    if not degree_res.data:
        raise HTTPException(status_code=404, detail="Degree not found")

    degree_row = degree_res.data[0]
    college_id_str = degree_row.get("college_id")
    if not college_id_str:
        raise HTTPException(status_code=400, detail="Degree missing college reference")
    college_id = uuid.UUID(college_id_str)

    department_id: Optional[uuid.UUID] = None
    existing = (
        supabase.table("departments")
        .select("id")
        .eq("degree_id", str(degree_id))
        .eq("name", payload.name)
        .limit(1)
        .execute()
    )
    if getattr(existing, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (find department): {existing.error}")

    if existing.data:
        department_id = uuid.UUID(existing.data[0]["id"])
    else:
        insert_payload: Dict[str, Any] = {
            "college_id": str(college_id),
            "degree_id": str(degree_id),
            "name": payload.name,
        }
        ins = supabase.table("departments").insert(insert_payload).execute()
        if getattr(ins, "error", None):
            error_text = str(ins.error)
            if "duplicate key value" in error_text:
                raise HTTPException(
                    status_code=409,
                    detail="A department with this name already exists for this degree.",
                )
            raise HTTPException(status_code=500, detail=f"Supabase error (insert department): {ins.error}")
        if ins.data:
            department_id = uuid.UUID(ins.data[0]["id"])
        else:
            refetch = (
                supabase.table("departments")
                .select("id")
                .eq("degree_id", str(degree_id))
                .eq("name", payload.name)
                .limit(1)
                .execute()
            )
            if getattr(refetch, "error", None) or not refetch.data:
                raise HTTPException(status_code=500, detail="Failed to retrieve created department.")
            department_id = uuid.UUID(refetch.data[0]["id"])

    if department_id is None:
        raise HTTPException(status_code=500, detail="Unable to determine department identifier after upsert.")

    _ensure_department_batches(college_id, department_id, payload.batches)

    college_snapshot = get_college_full(college_id)
    return _locate_department_from_snapshot(college_snapshot, degree_id, department_id)


@academics_router.put(
    "/api/departments/{department_id}",
    response_model=DepartmentWithBatchesOut,
    summary="Update department metadata",
)
def update_department_simple(department_id: uuid.UUID, payload: DepartmentSimpleUpdateIn):
    supabase = get_service_client()

    dept_res = (
        supabase.table("departments")
        .select("id,degree_id,college_id")
        .eq("id", str(department_id))
        .limit(1)
        .execute()
    )
    if getattr(dept_res, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (find department): {dept_res.error}")
    if not dept_res.data:
        raise HTTPException(status_code=404, detail="Department not found")

    dept_row = dept_res.data[0]
    college_id_str = dept_row.get("college_id")
    degree_id_str = dept_row.get("degree_id")
    if not college_id_str or not degree_id_str:
        raise HTTPException(status_code=400, detail="Department missing degree or college reference")

    dup_res = (
        supabase.table("departments")
        .select("id")
        .eq("degree_id", degree_id_str)
        .eq("name", payload.name)
        .neq("id", str(department_id))
        .limit(1)
        .execute()
    )
    if getattr(dup_res, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (check duplicate department): {dup_res.error}")
    if dup_res.data:
        raise HTTPException(status_code=409, detail="Another department with this name already exists for the degree.")

    upd = (
        supabase.table("departments")
        .update({"name": payload.name})
        .eq("id", str(department_id))
        .execute()
    )
    if getattr(upd, "error", None):
        error_text = str(upd.error)
        if "duplicate key value" in error_text:
            raise HTTPException(
                status_code=409,
                detail="A department with this name already exists for the college. Please choose a different name.",
            )
        raise HTTPException(status_code=500, detail=f"Supabase error (update department): {upd.error}")

    college_id = uuid.UUID(college_id_str)
    degree_uuid = uuid.UUID(degree_id_str)
    _ensure_department_batches(college_id, department_id, payload.batches)

    college_snapshot = get_college_full(college_id)
    return _locate_department_from_snapshot(college_snapshot, degree_uuid, department_id)


def _cascade_delete_department_batches(supabase: Client, department_id: uuid.UUID) -> Dict[str, int]:
    """Delete all batches (and their subjects) owned by a department."""
    stats = {"batches": 0, "courses": 0}
    batch_res = (
        supabase.table("batches").select("id").eq("department_id", str(department_id)).execute()
    )
    if getattr(batch_res, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (list batches for department): {batch_res.error}")

    for batch_row in batch_res.data or []:
        batch_id_str = batch_row.get("id")
        if not batch_id_str:
            continue

        course_res = (
            supabase.table("syllabus_courses").select("id").eq("batch_id", batch_id_str).execute()
        )
        if getattr(course_res, "error", None):
            raise HTTPException(status_code=500, detail=f"Supabase error (list courses for batch): {course_res.error}")

        for course_row in course_res.data or []:
            course_id_str = course_row.get("id")
            if not course_id_str:
                continue
            delete_course_cascade(uuid.UUID(course_id_str))
            stats["courses"] += 1

        batch_del = supabase.table("batches").delete().eq("id", batch_id_str).execute()
        if getattr(batch_del, "error", None):
            raise HTTPException(status_code=500, detail=f"Supabase error (delete batch): {batch_del.error}")
        stats["batches"] += 1

    return stats


@academics_router.delete(
    "/api/departments/{department_id}",
    summary="Delete a department, its batches, and related syllabus data",
)
def delete_department(department_id: uuid.UUID):
    supabase = get_service_client()

    dept_res = (
        supabase.table("departments")
        .select("id,college_id,degree_id,name")
        .eq("id", str(department_id))
        .limit(1)
        .execute()
    )
    if getattr(dept_res, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (find department): {dept_res.error}")
    if not dept_res.data:
        raise HTTPException(status_code=404, detail="Department not found")

    blocking_sources: List[str] = []
    dependency_checks = [
        ("marketplace_notes", "marketplace notes"),
        ("teacher_applications", "teacher applications"),
        ("teacher_classes", "teacher classes"),
        ("teacher_profiles", "teacher profiles"),
        ("user_education", "user education records"),
    ]
    for table_name, label in dependency_checks:
        check = (
            supabase.table(table_name)
            .select("department_id")
            .eq("department_id", str(department_id))
            .limit(1)
            .execute()
        )
        if getattr(check, "error", None):
            raise HTTPException(
                status_code=500,
                detail=f"Supabase error (check {label} for department): {check.error}",
            )
        if check.data:
            blocking_sources.append(label)

    if blocking_sources:
        formatted = ", ".join(blocking_sources)
        raise HTTPException(
            status_code=409,
            detail=f"Cannot delete department because related data exists: {formatted}. Remove or reassign those records first.",
        )

    batch_stats = _cascade_delete_department_batches(supabase, department_id)

    del_res = supabase.table("departments").delete().eq("id", str(department_id)).execute()
    if getattr(del_res, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (delete department): {del_res.error}")

    return {
        "ok": True,
        "deleted_department_id": str(department_id),
        "deleted_batches": batch_stats["batches"],
        "deleted_courses": batch_stats["courses"],
    }


@academics_router.put(
    "/api/colleges/{college_id}",
    response_model=College,
    summary="Rename a college",
)
def rename_college(college_id: uuid.UUID, payload: CollegeNameOnlyIn):
    supabase = get_service_client()
    upd = (
        supabase.table("colleges")
        .update({"name": payload.name})
        .eq("id", str(college_id))
        .execute()
    )
    if getattr(upd, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (rename college): {upd.error}")

    ref = (
        supabase.table("colleges")
        .select("id,name")
        .eq("id", str(college_id))
        .limit(1)
        .execute()
    )
    if getattr(ref, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (fetch college): {ref.error}")
    if not ref.data:
        raise HTTPException(status_code=404, detail="College not found")
    row = ref.data[0]
    return College(id=uuid.UUID(row["id"]), name=row["name"])


@academics_router.delete(
    "/api/colleges/{college_id}",
    summary="Delete a college, its degrees/departments/batches, and related syllabus data",
)
def delete_college(college_id: uuid.UUID):
    supabase = get_service_client()

    col_res = (
        supabase.table("colleges")
        .select("id,name")
        .eq("id", str(college_id))
        .limit(1)
        .execute()
    )
    if getattr(col_res, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (find college): {col_res.error}")
    if not col_res.data:
        raise HTTPException(status_code=404, detail="College not found")

    # Block deletion if other products depend on the college directly
    direct_blockers: List[str] = []
    for table_name, label, column in (
        ("teacher_applications", "teacher applications", "college_id"),
        ("teacher_profiles", "teacher profiles", "college_id"),
        ("teacher_classes", "teacher classes", "college_id"),
        ("user_education", "user education records", "college_id"),
    ):
        check = supabase.table(table_name).select(column).eq(column, str(college_id)).limit(1).execute()
        if getattr(check, "error", None):
            raise HTTPException(status_code=500, detail=f"Supabase error (check {label} for college): {check.error}")
        if check.data:
            direct_blockers.append(label)

    if direct_blockers:
        formatted = ", ".join(direct_blockers)
        raise HTTPException(
            status_code=409,
            detail=f"Cannot delete college because related data exists: {formatted}. Remove or reassign those records first.",
        )

    deg_res = (
        supabase.table("degrees")
        .select("id")
        .eq("college_id", str(college_id))
        .execute()
    )
    if getattr(deg_res, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (list degrees for college): {deg_res.error}")

    stats = {"degrees": 0, "departments": 0, "batches": 0, "courses": 0}
    for deg_row in deg_res.data or []:
        deg_id_str = deg_row.get("id")
        if not deg_id_str:
            continue

        dept_res = (
            supabase.table("departments")
            .select("id")
            .eq("degree_id", deg_id_str)
            .execute()
        )
        if getattr(dept_res, "error", None):
            raise HTTPException(status_code=500, detail=f"Supabase error (list departments for degree): {dept_res.error}")

        # Ensure no department blockers exist (defensive; should already be implied by direct blockers)
        for drow in dept_res.data or []:
            did = drow.get("id")
            if not did:
                continue
            blockers = _check_department_delete_blockers(supabase, did)
            if blockers:
                formatted = ", ".join(blockers)
                raise HTTPException(
                    status_code=409,
                    detail=f"Cannot delete college because related data exists under a department: {formatted}. Remove or reassign those records first.",
                )

        for drow in dept_res.data or []:
            did = drow.get("id")
            if not did:
                continue
            dept_uuid = uuid.UUID(did)
            batch_stats = _cascade_delete_department_batches(supabase, dept_uuid)
            stats["batches"] += int(batch_stats.get("batches", 0))
            stats["courses"] += int(batch_stats.get("courses", 0))
            del_dept = supabase.table("departments").delete().eq("id", did).execute()
            if getattr(del_dept, "error", None):
                raise HTTPException(status_code=500, detail=f"Supabase error (delete department): {del_dept.error}")
            stats["departments"] += 1

        del_deg = supabase.table("degrees").delete().eq("id", deg_id_str).execute()
        if getattr(del_deg, "error", None):
            raise HTTPException(status_code=500, detail=f"Supabase error (delete degree): {del_deg.error}")
        stats["degrees"] += 1

    del_col = supabase.table("colleges").delete().eq("id", str(college_id)).execute()
    if getattr(del_col, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (delete college): {del_col.error}")

    return {
        "ok": True,
        "deleted_college_id": str(college_id),
        "deleted_degrees": stats["degrees"],
        "deleted_departments": stats["departments"],
        "deleted_batches": stats["batches"],
        "deleted_courses": stats["courses"],
    }


@academics_router.get("/api/colleges/{college_id}", response_model=CollegeFullOut, summary="Get a college with departments & batches")
def get_college(college_id: uuid.UUID):
    data = get_college_full(college_id)
    return CollegeFullOut(**data)


@academics_router.get("/api/colleges/{college_id}/departments", response_model=List[str], summary="List departments for a college")
def list_departments_for_college(college_id: uuid.UUID):
    supabase = get_service_client()
    res = (
        supabase.table("departments").select("name").eq("college_id", str(college_id)).order("name").execute()
    )
    if getattr(res, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (list departments): {res.error}")
    return [row["name"] for row in (res.data or [])]


@academics_router.get(
    "/api/colleges/{college_id}/departments/full",
    response_model=List[DepartmentOut],
    summary="List departments (id & name) for a college",
)
def list_departments_full(college_id: uuid.UUID):
    supabase = get_service_client()
    res = (
        supabase.table("departments").select("id,name").eq("college_id", str(college_id)).order("name").execute()
    )
    if getattr(res, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (list departments full): {res.error}")
    return [{"id": row["id"], "name": row["name"]} for row in (res.data or [])]


@academics_router.get(
    "/api/colleges/{college_id}/departments/{dept_name}/batches",
    response_model=List[BatchIn],
    summary="List batches for a department within a college",
)
def list_batches_for_department(college_id: uuid.UUID, dept_name: str):
    supabase = get_service_client()
    dept_q = (
        supabase.table("departments")
        .select("id")
        .eq("college_id", str(college_id))
        .eq("name", dept_name.upper())
        .limit(1)
        .execute()
    )
    if getattr(dept_q, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (find department): {dept_q.error}")
    if not dept_q.data:
        raise HTTPException(status_code=404, detail="Department not found for college")
    dept_id = dept_q.data[0]["id"]

    b = (
        supabase.table("batches")
        .select("from_year,to_year")
        .eq("college_id", str(college_id))
        .eq("department_id", dept_id)
        .order("from_year")
        .execute()
    )
    if getattr(b, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (get dept batches): {b.error}")
    return [BatchIn(**{"from": row["from_year"], "to": row["to_year"]}) for row in (b.data or [])]


@academics_router.get(
    "/api/colleges/{college_id}/departments/{dept_name}/batches/full",
    response_model=List[BatchWithIdOut],
    summary="List batches (with id) for a department within a college",
)
def list_batches_for_department_with_ids(college_id: uuid.UUID, dept_name: str):
    supabase = get_service_client()
    dept_q = (
        supabase.table("departments")
        .select("id")
        .eq("college_id", str(college_id))
        .eq("name", dept_name.upper())
        .limit(1)
        .execute()
    )
    if getattr(dept_q, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (find department): {dept_q.error}")
    if not dept_q.data:
        raise HTTPException(status_code=404, detail="Department not found for college")
    dept_id = dept_q.data[0]["id"]

    b = (
        supabase.table("batches")
        .select("id,from_year,to_year")
        .eq("college_id", str(college_id))
        .eq("department_id", dept_id)
        .order("from_year")
        .execute()
    )
    if getattr(b, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (get dept batches full): {b.error}")
    return [
        {"id": row["id"], "from_year": row["from_year"], "to_year": row["to_year"]}
        for row in (b.data or [])
    ]


@academics_router.get(
    "/api/departments/{department_id}/batches/full",
    response_model=List[BatchWithIdOut],
    summary="List batches (with id) for a department by id",
)
def list_batches_for_department_with_ids_by_id(department_id: uuid.UUID):
    supabase = get_service_client()
    dept_res = (
        supabase.table("departments")
        .select("id,college_id")
        .eq("id", str(department_id))
        .limit(1)
        .execute()
    )
    if getattr(dept_res, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (find department by id): {dept_res.error}")
    if not dept_res.data:
        raise HTTPException(status_code=404, detail="Department not found")

    college_id_value = dept_res.data[0].get("college_id")

    query = (
        supabase.table("batches")
        .select("id,from_year,to_year")
        .eq("department_id", str(department_id))
        .order("from_year")
    )
    if college_id_value:
        query = query.eq("college_id", str(college_id_value))

    b = query.execute()
    if getattr(b, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (get dept batches full by id): {b.error}")
    return [
        {"id": row["id"], "from_year": row["from_year"], "to_year": row["to_year"]}
        for row in (b.data or [])
    ]


@academics_router.post(
    "/api/departments/{department_id}/batches",
    response_model=BatchWithIdOut,
    summary="Create a batch for a department",
)
def create_batch_for_department(department_id: uuid.UUID, payload: BatchIn):
    supabase = get_service_client()

    dept_res = (
        supabase.table("departments")
        .select("id,college_id")
        .eq("id", str(department_id))
        .limit(1)
        .execute()
    )
    if getattr(dept_res, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (find department): {dept_res.error}")
    if not dept_res.data:
        raise HTTPException(status_code=404, detail="Department not found")

    dept_row = dept_res.data[0]
    college_id_str = dept_row.get("college_id")
    if not college_id_str:
        raise HTTPException(status_code=400, detail="Department missing college reference")

    dup_res = (
        supabase.table("batches")
        .select("id")
        .eq("department_id", str(department_id))
        .eq("from_year", payload.from_year)
        .eq("to_year", payload.to_year)
        .limit(1)
        .execute()
    )
    if getattr(dup_res, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (check duplicate batch): {dup_res.error}")
    if dup_res.data:
        raise HTTPException(status_code=409, detail="This batch already exists for the department.")

    insert_payload = {
        "college_id": college_id_str,
        "department_id": str(department_id),
        "from_year": payload.from_year,
        "to_year": payload.to_year,
    }
    ins = supabase.table("batches").insert(insert_payload).execute()
    if getattr(ins, "error", None):
        error_text = str(ins.error)
        if "duplicate key value" in error_text:
            raise HTTPException(status_code=409, detail="This batch already exists for the department.")
        raise HTTPException(status_code=500, detail=f"Supabase error (insert batch): {ins.error}")

    if not ins.data:
        refetch = (
            supabase.table("batches")
            .select("id,from_year,to_year")
            .eq("department_id", str(department_id))
            .eq("from_year", payload.from_year)
            .eq("to_year", payload.to_year)
            .limit(1)
            .execute()
        )
        if getattr(refetch, "error", None) or not refetch.data:
            raise HTTPException(status_code=500, detail="Failed to retrieve created batch.")
        batch_row = refetch.data[0]
    else:
        batch_row = ins.data[0]

    return BatchWithIdOut(
        id=uuid.UUID(batch_row["id"]),
        from_year=int(batch_row.get("from_year", payload.from_year)),
        to_year=int(batch_row.get("to_year", payload.to_year)),
    )


@academics_router.put(
    "/api/batches/{batch_id}",
    response_model=BatchWithIdOut,
    summary="Update batch years",
)
def update_batch(batch_id: uuid.UUID, payload: BatchIn):
    supabase = get_service_client()

    batch_res = (
        supabase.table("batches")
        .select("id,college_id,department_id")
        .eq("id", str(batch_id))
        .limit(1)
        .execute()
    )
    if getattr(batch_res, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (find batch): {batch_res.error}")
    if not batch_res.data:
        raise HTTPException(status_code=404, detail="Batch not found")

    batch_row = batch_res.data[0]
    department_id_str = batch_row.get("department_id")
    if not department_id_str:
        raise HTTPException(status_code=400, detail="Batch missing department reference")

    dup_res = (
        supabase.table("batches")
        .select("id")
        .eq("department_id", department_id_str)
        .eq("from_year", payload.from_year)
        .eq("to_year", payload.to_year)
        .neq("id", str(batch_id))
        .limit(1)
        .execute()
    )
    if getattr(dup_res, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (check duplicate batch): {dup_res.error}")
    if dup_res.data:
        raise HTTPException(status_code=409, detail="Another batch with this year range already exists for the department.")

    upd = (
        supabase.table("batches")
        .update({"from_year": payload.from_year, "to_year": payload.to_year})
        .eq("id", str(batch_id))
        .execute()
    )
    if getattr(upd, "error", None):
        error_text = str(upd.error)
        if "duplicate key value" in error_text:
            raise HTTPException(status_code=409, detail="Another batch with this year range already exists for the department.")
        raise HTTPException(status_code=500, detail=f"Supabase error (update batch): {upd.error}")

    return BatchWithIdOut(id=batch_id, from_year=payload.from_year, to_year=payload.to_year)


@academics_router.delete(
    "/api/batches/{batch_id}",
    summary="Delete a batch and related syllabus data",
)
def delete_batch(batch_id: uuid.UUID):
    supabase = get_service_client()

    batch_res = (
        supabase.table("batches")
        .select("id")
        .eq("id", str(batch_id))
        .limit(1)
        .execute()
    )
    if getattr(batch_res, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (find batch): {batch_res.error}")
    if not batch_res.data:
        raise HTTPException(status_code=404, detail="Batch not found")

    deleted_courses = 0
    course_res = (
        supabase.table("syllabus_courses")
        .select("id")
        .eq("batch_id", str(batch_id))
        .execute()
    )
    if getattr(course_res, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (list courses for batch): {course_res.error}")

    for course_row in course_res.data or []:
        course_id_str = course_row.get("id")
        if not course_id_str:
            continue
        delete_course_cascade(uuid.UUID(course_id_str))
        deleted_courses += 1

    del_res = supabase.table("batches").delete().eq("id", str(batch_id)).execute()
    if getattr(del_res, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (delete batch): {del_res.error}")

    return {
        "ok": True,
        "deleted_batch_id": str(batch_id),
        "deleted_courses": deleted_courses,
    }


@academics_router.post(
    "/api/batches/{batch_id}/courses",
    response_model=SyllabusCourseSummaryOut,
    summary="Create a subject for a batch",
)
def create_course_for_batch(batch_id: uuid.UUID, payload: SyllabusCourseSimpleCreateIn):
    supabase = get_service_client()

    batch_res = (
        supabase.table("batches")
        .select("id")
        .eq("id", str(batch_id))
        .limit(1)
        .execute()
    )
    if getattr(batch_res, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (find batch): {batch_res.error}")
    if not batch_res.data:
        raise HTTPException(status_code=404, detail="Batch not found")

    dup_res = (
        supabase.table("syllabus_courses")
        .select("id")
        .eq("batch_id", str(batch_id))
        .eq("semester", payload.semester)
        .eq("course_code", payload.course_code)
        .limit(1)
        .execute()
    )
    if getattr(dup_res, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (check duplicate course): {dup_res.error}")
    if dup_res.data:
        raise HTTPException(
            status_code=409,
            detail="A subject with this course code already exists for the semester in this batch.",
        )

    insert_payload = {
        "batch_id": str(batch_id),
        "semester": payload.semester,
        "course_code": payload.course_code,
        "title": payload.title,
        "type": getattr(payload, "type", None) or "practical",
    }
    ins = supabase.table("syllabus_courses").insert(insert_payload).execute()
    if getattr(ins, "error", None):
        error_text = str(ins.error)
        if "duplicate key value" in error_text:
            raise HTTPException(
                status_code=409,
                detail="A subject with this course code already exists for the semester in this batch.",
            )
        raise HTTPException(status_code=500, detail=f"Supabase error (insert course): {ins.error}")

    if ins.data:
        row = ins.data[0]
    else:
        refetch = (
            supabase.table("syllabus_courses")
            .select("id,semester,course_code,title,type")
            .eq("batch_id", str(batch_id))
            .eq("semester", payload.semester)
            .eq("course_code", payload.course_code)
            .limit(1)
            .execute()
        )
        if getattr(refetch, "error", None) or not refetch.data:
            raise HTTPException(status_code=500, detail="Failed to retrieve created subject.")
        row = refetch.data[0]

    return SyllabusCourseSummaryOut(
        id=uuid.UUID(row["id"]),
        batch_id=batch_id,
        semester=int(row.get("semester", payload.semester)),
        course_code=row.get("course_code", payload.course_code),
        title=row.get("title", payload.title),
        type=row.get("type", getattr(payload, "type", None)),
    )


@academics_router.post("/api/batches/resolve", response_model=BatchWithIdOut, summary="Resolve or create a batch id for a college + dept + year range")
def resolve_or_create_batch(payload: BatchResolveIn):
    return resolve_or_create_batch(payload.college_id, payload.dept_name, payload.from_year, payload.to_year)


@academics_router.post("/signup")
def signup(user: UserAuth):
    return signup_user(user)


@academics_router.post("/login")
def login(user: UserAuth):
    return login_user(user)


class RefreshTokenRequest(BaseModel):
    refresh_token: str


@academics_router.post("/refresh", summary="Refresh access token using refresh token")
def refresh_token(payload: RefreshTokenRequest):
    """Refresh the access token using a valid refresh token.
    
    This endpoint allows clients to obtain a new access token without
    requiring the user to re-enter credentials. Used for persistent sessions.
    """
    anon_client = get_anon_client()
    if not anon_client:
        raise HTTPException(status_code=500, detail="Server missing SUPABASE_ANON_KEY")
    if not payload.refresh_token:
        raise HTTPException(status_code=400, detail="refresh_token is required")
    try:
        # Use Supabase's refresh_session method
        res = anon_client.auth.refresh_session(payload.refresh_token)
        session = getattr(res, "session", None)
        access_token = getattr(session, "access_token", None) if session else None
        refresh_token = getattr(session, "refresh_token", None) if session else None
        expires_in = getattr(session, "expires_in", 3600) if session else 3600
        if not access_token:
            raise HTTPException(status_code=401, detail="Token refresh failed: invalid or expired refresh token")
        return {
            "message": "Token refreshed successfully",
            "access_token": access_token,
            "refresh_token": refresh_token,
            "expires_in": expires_in,
        }
    except HTTPException:
        raise
    except Exception as e:
        supabase_logger.exception("Token refresh failed")
        raise HTTPException(status_code=401, detail="Token refresh failed: invalid or expired refresh token")


@academics_router.get("/api/public/supabase", summary="Public Supabase client config")
def public_supabase_config():
    base_url = os.getenv("SUPABASE_URL", "").rstrip("/")
    anon = os.getenv("SUPABASE_ANON_KEY", "")
    if not base_url or not anon:
        raise HTTPException(status_code=500, detail="Missing SUPABASE_URL or SUPABASE_ANON_KEY")
    return {"url": base_url, "anonKey": anon}


@academics_router.get("/api/public/academic-meta", summary="Public academic hierarchy for signup")
def public_academic_meta():
    """Return colleges, degrees, departments (minimal fields) without requiring auth.

    Used by teacher signup (pre-auth). Batches omitted for brevity.
    """
    supabase = get_service_client()
    colleges = supabase.table("colleges").select("id,name").order("name").execute()
    if getattr(colleges, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (colleges): {colleges.error}")
    degrees = supabase.table("degrees").select("id,name,college_id").execute()
    departments = supabase.table("departments").select("id,name,college_id,degree_id").execute()
    return {
        "colleges": getattr(colleges, "data", []) or [],
        "degrees": getattr(degrees, "data", []) or [],
        "departments": getattr(departments, "data", []) or [],
    }


@academics_router.post("/api/signup/full")
def signup_full(payload: SignupFullIn):
    return signup_full_user(payload)


def _parse_bearer_token(authorization: Optional[str]) -> Optional[str]:
    if not authorization:
        return None
    parts = authorization.split()
    if len(parts) == 2 and parts[0].lower() == "bearer":
        return parts[1]
    return None


@academics_router.get("/api/me")
def get_me(authorization: Optional[str] = Header(default=None)):
    token = _parse_bearer_token(authorization)
    return get_current_user_profile(token)


@academics_router.post("/api/syllabus/courses", response_model=SyllabusCourseOut, summary="Upsert syllabus course with units & topics")
def api_upsert_syllabus_course(payload: SyllabusCourseIn):
    return upsert_syllabus_course(payload)


@academics_router.put(
    "/api/syllabus/courses/{course_id}",
    response_model=SyllabusCourseSummaryOut,
    summary="Update subject metadata",
)
def update_course_metadata(course_id: uuid.UUID, payload: SyllabusCourseSimpleUpdateIn):
    supabase = get_service_client()

    existing = (
        supabase.table("syllabus_courses")
        .select("id,batch_id")
        .eq("id", str(course_id))
        .limit(1)
        .execute()
    )
    if getattr(existing, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (find course): {existing.error}")
    if not existing.data:
        raise HTTPException(status_code=404, detail="Subject not found")

    row = existing.data[0]
    batch_id_str = row.get("batch_id")
    if not batch_id_str:
        raise HTTPException(status_code=400, detail="Subject missing batch reference")

    dup_res = (
        supabase.table("syllabus_courses")
        .select("id")
        .eq("batch_id", batch_id_str)
        .eq("semester", payload.semester)
        .eq("course_code", payload.course_code)
        .neq("id", str(course_id))
        .limit(1)
        .execute()
    )
    if getattr(dup_res, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (check duplicate course): {dup_res.error}")
    if dup_res.data:
        raise HTTPException(
            status_code=409,
            detail="Another subject with this course code already exists for the semester in this batch.",
        )

    updates = {
        "semester": payload.semester,
        "course_code": payload.course_code,
        "title": payload.title,
        "type": getattr(payload, "type", None) or "practical",
    }
    upd = (
        supabase.table("syllabus_courses")
        .update(updates)
        .eq("id", str(course_id))
        .execute()
    )
    if getattr(upd, "error", None):
        error_text = str(upd.error)
        if "duplicate key value" in error_text:
            raise HTTPException(
                status_code=409,
                detail="Another subject with this course code already exists for the semester in this batch.",
            )
        raise HTTPException(status_code=500, detail=f"Supabase error (update course): {upd.error}")

    return SyllabusCourseSummaryOut(
        id=course_id,
        batch_id=uuid.UUID(batch_id_str),
        semester=payload.semester,
        course_code=payload.course_code,
        title=payload.title,
        type=updates.get("type"),
    )

# Alias route to support existing UI paths
@academics_router.put(
    "/api/courses/{course_id}",
    response_model=SyllabusCourseSummaryOut,
    summary="Update subject metadata (alias)",
)
def update_course_metadata_alias(course_id: uuid.UUID, payload: SyllabusCourseSimpleUpdateIn):
    return update_course_metadata(course_id, payload)


@academics_router.delete(
    "/api/courses/{course_id}",
    summary="Delete a subject and all units & topics",
)
def delete_course_cascade(course_id: uuid.UUID):
    supabase = get_service_client()

    # Verify course exists
    course_q = (
        supabase.table("syllabus_courses")
        .select("id")
        .eq("id", str(course_id))
        .limit(1)
        .execute()
    )
    if getattr(course_q, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (find course): {course_q.error}")
    if not course_q.data:
        raise HTTPException(status_code=404, detail="Subject not found")

    # Collect unit ids
    units_q = (
        supabase.table("syllabus_units")
        .select("id")
        .eq("course_id", str(course_id))
        .execute()
    )
    if getattr(units_q, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (list units): {units_q.error}")
    unit_ids = [u.get("id") for u in (units_q.data or []) if u.get("id")]

    # For each unit: delete progress for its topics, then delete topics
    total_deleted_topics = 0
    for uid in unit_ids:
        # topics under this unit
        t_q = (
            supabase.table("syllabus_topics")
            .select("id")
            .eq("unit_id", uid)
            .execute()
        )
        if getattr(t_q, "error", None):
            raise HTTPException(status_code=500, detail=f"Supabase error (list topics): {t_q.error}")
        t_ids = [t.get("id") for t in (t_q.data or []) if t.get("id")]

        # delete progress in chunks to avoid IN payload issues
        if t_ids:
            for i in range(0, len(t_ids), 100):
                chunk = t_ids[i:i+100]
                prog_del = (
                    supabase.table("user_topic_progress")
                    .delete()
                    .in_("topic_id", chunk)
                    .execute()
                )
                if getattr(prog_del, "error", None):
                    raise HTTPException(status_code=500, detail=f"Supabase error (delete progress): {prog_del.error}")

            # delete topics for this unit
            topics_del = (
                supabase.table("syllabus_topics")
                .delete()
                .eq("unit_id", uid)
                .execute()
            )
            if getattr(topics_del, "error", None):
                raise HTTPException(status_code=500, detail=f"Supabase error (delete topics): {topics_del.error}")
            total_deleted_topics += len(t_ids)

    # Delete units by course_id
    if unit_ids:
        units_del = (
            supabase.table("syllabus_units")
            .delete()
            .eq("course_id", str(course_id))
            .execute()
        )
        if getattr(units_del, "error", None):
            raise HTTPException(status_code=500, detail=f"Supabase error (delete units): {units_del.error}")

    # Delete course
    course_del = (
        supabase.table("syllabus_courses")
        .delete()
        .eq("id", str(course_id))
        .execute()
    )
    if getattr(course_del, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (delete course): {course_del.error}")

    return {"ok": True, "deleted_units": len(unit_ids), "deleted_topics": total_deleted_topics}


@academics_router.get(
    "/api/syllabus/units/{unit_id}/topics",
    response_model=List[TopicOut],
    summary="List topics for a unit (direct from syllabus_topics)",
)
def api_list_topics_for_unit(unit_id: uuid.UUID):
    supabase = get_service_client()
    res = (
        supabase.table("syllabus_topics")
        .select("id,topic,order_in_unit,image_url,video_url,ppt_url,lab_url")
        .eq("unit_id", str(unit_id))
        .order("order_in_unit")
        .execute()
    )
    if getattr(res, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (get topics for unit): {res.error}")

    return [
        TopicOut(
            id=uuid.UUID(row["id"]),
            topic=row.get("topic"),
            order_in_unit=int(row.get("order_in_unit", 0)),
            image_url=row.get("image_url"),
            video_url=row.get("video_url"),
            ppt_url=row.get("ppt_url"),
            lab_url=row.get("lab_url"),
        )
        for row in (res.data or [])
    ]


@academics_router.get(
    "/api/syllabus/topics/by-title",
    response_model=List[TopicOut],
    summary="Find syllabus topics by exact title (with URLs)",
)
def api_find_topics_by_title(topic: str = Query(..., min_length=1, max_length=512)):
    """Lookup topics in syllabus_topics by exact topic title.

    This is used by the notes generator to attach a recommended video
    from the structured syllabus (video_url) ahead of generic search
    results.
    """
    clean = (topic or "").strip()
    if not clean:
        return []

    supabase = get_service_client()
    res = (
        supabase.table("syllabus_topics")
        .select("id,topic,order_in_unit,image_url,video_url,ppt_url,lab_url,unit_id")
        .eq("topic", clean)
        .order("order_in_unit")
        .limit(5)
        .execute()
    )
    if getattr(res, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (find topic by title): {res.error}")

    unit_ids = [row.get("unit_id") for row in (res.data or []) if row.get("unit_id")]
    course_by_unit: Dict[str, str] = {}
    course_types: Dict[str, str] = {}
    if unit_ids:
        units_res = (
            supabase.table("syllabus_units")
            .select("id,course_id")
            .in_("id", list({uid for uid in unit_ids}))
            .execute()
        )
        if getattr(units_res, "error", None):
            raise HTTPException(status_code=500, detail=f"Supabase error (lookup units for topics): {units_res.error}")
        course_ids = [row.get("course_id") for row in (units_res.data or []) if row.get("course_id")]
        for row in (units_res.data or []):
            uid = row.get("id")
            cid = row.get("course_id")
            if uid and cid:
                course_by_unit[str(uid)] = str(cid)
        if course_ids:
            courses_res = (
                supabase.table("syllabus_courses")
                .select("id,type")
                .in_("id", list({cid for cid in course_ids}))
                .execute()
            )
            if getattr(courses_res, "error", None):
                raise HTTPException(status_code=500, detail=f"Supabase error (lookup course types): {courses_res.error}")
            for crow in (courses_res.data or []):
                if crow.get("id"):
                    course_types[str(crow["id"])] = crow.get("type")

    out: List[TopicOut] = []
    for row in (res.data or []):
        uid_raw = row.get("unit_id")
        uid_key = str(uid_raw) if uid_raw else None
        cid_raw = course_by_unit.get(uid_key) if uid_key else None
        out.append(
            TopicOut(
                id=uuid.UUID(row["id"]),
                topic=row.get("topic"),
                order_in_unit=int(row.get("order_in_unit", 0)),
                image_url=row.get("image_url"),
                video_url=row.get("video_url"),
                ppt_url=row.get("ppt_url"),
                lab_url=row.get("lab_url"),
                unit_id=uuid.UUID(uid_key) if uid_key else None,
                course_id=uuid.UUID(cid_raw) if cid_raw else None,
                course_type=course_types.get(str(cid_raw)) if cid_raw else None,
            )
        )
    return out


@academics_router.get(
    "/api/batches/{batch_id}/courses",
    response_model=List[SyllabusCourseSummaryOut],
    summary="List syllabus courses (subjects) for a batch",
)
def api_list_courses_for_batch(
    batch_id: uuid.UUID,
    semester: Optional[int] = Query(default=None, ge=1, le=12),
):
    supabase = get_service_client()
    query = (
        supabase.table("syllabus_courses")
        .select("id,batch_id,semester,course_code,title,type")
        .eq("batch_id", str(batch_id))
    )
    if semester is not None:
        query = query.eq("semester", semester)
    res = query.order("course_code").execute()
    if getattr(res, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (list courses): {res.error}")
    return [
        SyllabusCourseSummaryOut(
            id=uuid.UUID(row["id"]),
            batch_id=uuid.UUID(row["batch_id"]),
            semester=int(row["semester"]),
            course_code=row.get("course_code"),
            title=row.get("title"),
            type=row.get("type"),
        )
        for row in (res.data or [])
    ]


@academics_router.get(
    "/api/syllabus/courses/{course_id}",
    response_model=SyllabusCourseOut,
    summary="Get syllabus course with units & topics",
)
def api_get_syllabus_course(course_id: uuid.UUID):
    return load_course_with_units(course_id)


@academics_router.post(
    "/api/syllabus/courses/{course_id}/units",
    response_model=UnitOut,
    summary="Create a unit (with topics) for a syllabus course",
)
def create_unit_for_course(course_id: uuid.UUID, payload: UnitTopicsIn):
    supabase = get_service_client()

    course_res = (
        supabase.table("syllabus_courses")
        .select("id")
        .eq("id", str(course_id))
        .limit(1)
        .execute()
    )
    if getattr(course_res, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (find course): {course_res.error}")
    if not course_res.data:
        raise HTTPException(status_code=404, detail="Course not found")

    dup_res = (
        supabase.table("syllabus_units")
        .select("id")
        .eq("course_id", str(course_id))
        .eq("unit_title", payload.unit_title)
        .limit(1)
        .execute()
    )
    if getattr(dup_res, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (check duplicate unit): {dup_res.error}")
    if dup_res.data:
        raise HTTPException(status_code=409, detail="A unit with this title already exists for this course.")

    units_res = (
        supabase.table("syllabus_units")
        .select("id")
        .eq("course_id", str(course_id))
        .execute()
    )
    if getattr(units_res, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (list units): {units_res.error}")
    next_order = len(units_res.data or [])

    ins = (
        supabase.table("syllabus_units")
        .insert(
            {
                "course_id": str(course_id),
                "unit_title": payload.unit_title,
                "order_in_course": next_order,
            }
        )
        .execute()
    )
    if getattr(ins, "error", None):
        error_text = str(ins.error)
        if "duplicate key value" in error_text:
            raise HTTPException(status_code=409, detail="A unit with this title already exists for this course.")
        raise HTTPException(status_code=500, detail=f"Supabase error (insert unit): {ins.error}")

    if ins.data:
        unit_id_str = ins.data[0].get("id")
    else:
        refetch = (
            supabase.table("syllabus_units")
            .select("id")
            .eq("course_id", str(course_id))
            .eq("unit_title", payload.unit_title)
            .limit(1)
            .execute()
        )
        if getattr(refetch, "error", None) or not refetch.data:
            raise HTTPException(status_code=500, detail="Failed to retrieve created unit.")
        unit_id_str = refetch.data[0].get("id")

    if not unit_id_str:
        raise HTTPException(status_code=500, detail="Unit identifier missing after creation.")
    unit_id = uuid.UUID(unit_id_str)

    topic_rows = [
        {
            "unit_id": str(unit_id),
            "topic": topic.topic,
            "order_in_unit": index,
        }
        for index, topic in enumerate(payload.topics or [])
    ]
    if topic_rows:
        tins = supabase.table("syllabus_topics").insert(topic_rows).execute()
        if getattr(tins, "error", None):
            raise HTTPException(status_code=500, detail=f"Supabase error (insert topics): {tins.error}")

    return load_unit_with_topics(unit_id)


@academics_router.put(
    "/api/syllabus/units/{unit_id}",
    response_model=UnitOut,
    summary="Update a syllabus unit title and topics",
)
def update_unit_topics(unit_id: uuid.UUID, payload: UnitTopicsIn):
    supabase = get_service_client()

    unit_res = (
        supabase.table("syllabus_units")
        .select("id,course_id")
        .eq("id", str(unit_id))
        .limit(1)
        .execute()
    )
    if getattr(unit_res, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (find unit): {unit_res.error}")
    if not unit_res.data:
        raise HTTPException(status_code=404, detail="Unit not found")

    course_id_str = unit_res.data[0].get("course_id")
    if not course_id_str:
        raise HTTPException(status_code=400, detail="Unit missing course reference")

    dup_res = (
        supabase.table("syllabus_units")
        .select("id")
        .eq("course_id", course_id_str)
        .eq("unit_title", payload.unit_title)
        .neq("id", str(unit_id))
        .limit(1)
        .execute()
    )
    if getattr(dup_res, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (check duplicate unit): {dup_res.error}")
    if dup_res.data:
        raise HTTPException(status_code=409, detail="Another unit with this title already exists for this course.")

    upd = (
        supabase.table("syllabus_units")
        .update({"unit_title": payload.unit_title})
        .eq("id", str(unit_id))
        .execute()
    )
    if getattr(upd, "error", None):
        error_text = str(upd.error)
        if "duplicate key value" in error_text:
            raise HTTPException(status_code=409, detail="Another unit with this title already exists for this course.")
        raise HTTPException(status_code=500, detail=f"Supabase error (update unit): {upd.error}")

    # --- Update topics without wiping per-topic URL fields ---
    # Strategy:
    # 1) If client sends topic IDs, upsert by ID and delete removed IDs.
    # 2) If no IDs are sent (legacy clients), fall back to positional update to
    #    preserve URLs as much as possible.

    existing_q = (
        supabase.table("syllabus_topics")
        .select("id,order_in_unit")
        .eq("unit_id", str(unit_id))
        .order("order_in_unit")
        .execute()
    )
    if getattr(existing_q, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (list existing topics): {existing_q.error}")
    existing_rows = existing_q.data or []
    existing_ids: List[str] = [r.get("id") for r in existing_rows if r.get("id")]
    existing_id_set: Set[str] = set(existing_ids)

    incoming_topics = payload.topics or []
    incoming_ids: List[str] = [str(t.id) for t in incoming_topics if getattr(t, "id", None)]
    incoming_id_set: Set[str] = set(incoming_ids)
    any_ids_provided = bool(incoming_id_set)

    def _delete_topics_and_progress(topic_ids: List[str]) -> None:
        if not topic_ids:
            return
        for i in range(0, len(topic_ids), 100):
            chunk = topic_ids[i : i + 100]
            prog_del = (
                supabase.table("user_topic_progress")
                .delete()
                .in_("topic_id", chunk)
                .execute()
            )
            if getattr(prog_del, "error", None):
                raise HTTPException(status_code=500, detail=f"Supabase error (delete progress): {prog_del.error}")

        t_del = supabase.table("syllabus_topics").delete().in_("id", topic_ids).execute()
        if getattr(t_del, "error", None):
            raise HTTPException(status_code=500, detail=f"Supabase error (delete topics): {t_del.error}")

    if any_ids_provided:
        # Update or insert each topic row
        for index, topic in enumerate(incoming_topics):
            tid = getattr(topic, "id", None)
            if tid is not None and str(tid) in existing_id_set:
                upd_t = (
                    supabase.table("syllabus_topics")
                    .update({"topic": topic.topic, "order_in_unit": index})
                    .eq("id", str(tid))
                    .eq("unit_id", str(unit_id))
                    .execute()
                )
                if getattr(upd_t, "error", None):
                    raise HTTPException(status_code=500, detail=f"Supabase error (update topic): {upd_t.error}")
            else:
                ins_t = (
                    supabase.table("syllabus_topics")
                    .insert({"unit_id": str(unit_id), "topic": topic.topic, "order_in_unit": index})
                    .execute()
                )
                if getattr(ins_t, "error", None):
                    raise HTTPException(status_code=500, detail=f"Supabase error (insert topic): {ins_t.error}")

        # Delete topics that were removed from the unit
        removed = sorted(existing_id_set - incoming_id_set)
        _delete_topics_and_progress(removed)
    else:
        # Legacy client path: positional update to avoid wiping URL fields.
        existing_pos_ids = [r.get("id") for r in existing_rows if r.get("id")]
        keep_count = min(len(existing_pos_ids), len(incoming_topics))
        for index in range(keep_count):
            tid = existing_pos_ids[index]
            topic = incoming_topics[index]
            upd_t = (
                supabase.table("syllabus_topics")
                .update({"topic": topic.topic, "order_in_unit": index})
                .eq("id", str(tid))
                .eq("unit_id", str(unit_id))
                .execute()
            )
            if getattr(upd_t, "error", None):
                raise HTTPException(status_code=500, detail=f"Supabase error (update topic): {upd_t.error}")

        for index in range(keep_count, len(incoming_topics)):
            topic = incoming_topics[index]
            ins_t = (
                supabase.table("syllabus_topics")
                .insert({"unit_id": str(unit_id), "topic": topic.topic, "order_in_unit": index})
                .execute()
            )
            if getattr(ins_t, "error", None):
                raise HTTPException(status_code=500, detail=f"Supabase error (insert topic): {ins_t.error}")

        removed = existing_pos_ids[keep_count:]
        _delete_topics_and_progress([str(tid) for tid in removed if tid])

    return load_unit_with_topics(unit_id)


@academics_router.delete(
    "/api/syllabus/units/{unit_id}",
    summary="Delete a syllabus unit and all its topics",
)
def delete_unit_cascade(unit_id: uuid.UUID):
    """Delete a single unit and cascade-delete its topics and related user progress."""
    supabase = get_service_client()

    # Verify unit exists
    unit_q = (
        supabase.table("syllabus_units")
        .select("id")
        .eq("id", str(unit_id))
        .limit(1)
        .execute()
    )
    if getattr(unit_q, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (find unit): {unit_q.error}")
    if not unit_q.data:
        raise HTTPException(status_code=404, detail="Unit not found")

    # Gather topic ids under this unit
    topics_q = (
        supabase.table("syllabus_topics")
        .select("id")
        .eq("unit_id", str(unit_id))
        .execute()
    )
    if getattr(topics_q, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (list topics): {topics_q.error}")
    topic_ids = [t.get("id") for t in (topics_q.data or []) if t.get("id")]

    # Delete progress rows referencing these topics, in chunks
    if topic_ids:
        for i in range(0, len(topic_ids), 100):
            chunk = topic_ids[i:i+100]
            prog_del = (
                supabase.table("user_topic_progress")
                .delete()
                .in_("topic_id", chunk)
                .execute()
            )
            if getattr(prog_del, "error", None):
                raise HTTPException(status_code=500, detail=f"Supabase error (delete progress): {prog_del.error}")

        # Delete the topics
        t_del = (
            supabase.table("syllabus_topics")
            .delete()
            .eq("unit_id", str(unit_id))
            .execute()
        )
        if getattr(t_del, "error", None):
            raise HTTPException(status_code=500, detail=f"Supabase error (delete topics): {t_del.error}")

    # Finally delete the unit
    u_del = (
        supabase.table("syllabus_units")
        .delete()
        .eq("id", str(unit_id))
        .execute()
    )
    if getattr(u_del, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (delete unit): {u_del.error}")


@academics_router.put(
    "/api/syllabus/topics/{topic_id}/image",
    summary="Set or update image URL for a syllabus topic",
)
def set_topic_image_url(topic_id: uuid.UUID, payload: Dict[str, Any]):
    """Update the image_url for a single topic.

    Expects JSON body: {"image_url": "https://..."}.
    Pass null/empty string to clear via the dedicated DELETE endpoint instead.
    """

    supabase = get_service_client()

    if not isinstance(payload, dict):
        raise HTTPException(status_code=400, detail="Invalid payload")

    raw_url = payload.get("image_url")
    image_url: Optional[str]
    if raw_url is None:
        image_url = None
    else:
        if not isinstance(raw_url, str):
            raise HTTPException(status_code=400, detail="image_url must be a string or null")
        trimmed = raw_url.strip()
        image_url = trimmed or None

    # Ensure topic exists first
    topic_q = (
        supabase.table("syllabus_topics")
        .select("id")
        .eq("id", str(topic_id))
        .limit(1)
        .execute()
    )
    if getattr(topic_q, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (find topic): {topic_q.error}")
    if not topic_q.data:
        raise HTTPException(status_code=404, detail="Topic not found")

    upd = (
        supabase.table("syllabus_topics")
        .update({"image_url": image_url})
        .eq("id", str(topic_id))
        .execute()
    )
    if getattr(upd, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (update topic image): {upd.error}")

    return {"id": str(topic_id), "image_url": image_url}


@academics_router.delete(
    "/api/syllabus/topics/{topic_id}/image",
    summary="Clear image URL for a syllabus topic",
)
def clear_topic_image_url(topic_id: uuid.UUID):
    """Clear (set to null) the image_url for a single topic."""

    supabase = get_service_client()

    topic_q = (
        supabase.table("syllabus_topics")
        .select("id")
        .eq("id", str(topic_id))
        .limit(1)
        .execute()
    )
    if getattr(topic_q, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (find topic): {topic_q.error}")
    if not topic_q.data:
        raise HTTPException(status_code=404, detail="Topic not found")

    upd = (
        supabase.table("syllabus_topics")
        .update({"image_url": None})
        .eq("id", str(topic_id))
        .execute()
    )
    if getattr(upd, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (clear topic image): {upd.error}")

    return {"id": str(topic_id), "image_url": None}


def _update_topic_url_field(topic_id: uuid.UUID, field: str, value: Optional[str]) -> Dict[str, Any]:
    """Internal helper to validate topic existence and update a single URL field."""

    supabase = get_service_client()

    topic_q = (
        supabase.table("syllabus_topics")
        .select("id")
        .eq("id", str(topic_id))
        .limit(1)
        .execute()
    )
    if getattr(topic_q, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (find topic): {topic_q.error}")
    if not topic_q.data:
        raise HTTPException(status_code=404, detail="Topic not found")

    upd = (
        supabase.table("syllabus_topics")
        .update({field: value})
        .eq("id", str(topic_id))
        .execute()
    )
    if getattr(upd, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (update topic {field}): {upd.error}")

    return {"id": str(topic_id), field: value}


@academics_router.put(
    "/api/syllabus/topics/{topic_id}/video",
    summary="Set or update video URL for a syllabus topic",
)
def set_topic_video_url(topic_id: uuid.UUID, payload: Dict[str, Any]):
    """Update the video_url for a single topic.

    Expects JSON body: {"video_url": "https://..."}.
    Use the DELETE endpoint to clear.
    """

    if not isinstance(payload, dict):
        raise HTTPException(status_code=400, detail="Invalid payload")

    raw_url = payload.get("video_url")
    if raw_url is None:
        video_url = None
    else:
        if not isinstance(raw_url, str):
            raise HTTPException(status_code=400, detail="video_url must be a string or null")
        video_url = raw_url.strip() or None

    return _update_topic_url_field(topic_id, "video_url", video_url)


@academics_router.delete(
    "/api/syllabus/topics/{topic_id}/video",
    summary="Clear video URL for a syllabus topic",
)
def clear_topic_video_url(topic_id: uuid.UUID):
    """Clear (set to null) the video_url for a single topic."""

    return _update_topic_url_field(topic_id, "video_url", None)


@academics_router.put(
    "/api/syllabus/topics/{topic_id}/ppt",
    summary="Set or update PPT URL for a syllabus topic",
)
def set_topic_ppt_url(topic_id: uuid.UUID, payload: Dict[str, Any]):
    """Update the ppt_url for a single topic.

    Expects JSON body: {"ppt_url": "https://..."}.
    Use the DELETE endpoint to clear.
    """

    if not isinstance(payload, dict):
        raise HTTPException(status_code=400, detail="Invalid payload")

    raw_url = payload.get("ppt_url")
    if raw_url is None:
        ppt_url = None
    else:
        if not isinstance(raw_url, str):
            raise HTTPException(status_code=400, detail="ppt_url must be a string or null")
        ppt_url = raw_url.strip() or None

    return _update_topic_url_field(topic_id, "ppt_url", ppt_url)


@academics_router.delete(
    "/api/syllabus/topics/{topic_id}/ppt",
    summary="Clear PPT URL for a syllabus topic",
)
def clear_topic_ppt_url(topic_id: uuid.UUID):
    """Clear (set to null) the ppt_url for a single topic."""

    return _update_topic_url_field(topic_id, "ppt_url", None)


@academics_router.put(
    "/api/syllabus/topics/{topic_id}/lab",
    summary="Set or update Lab URL for a syllabus topic",
)
def set_topic_lab_url(topic_id: uuid.UUID, payload: Dict[str, Any]):
    """Update the lab_url for a single topic.

    Expects JSON body: {"lab_url": "https://..."}.
    Use the DELETE endpoint to clear.
    """

    if not isinstance(payload, dict):
        raise HTTPException(status_code=400, detail="Invalid payload")

    raw_url = payload.get("lab_url")
    if raw_url is None:
        lab_url = None
    else:
        if not isinstance(raw_url, str):
            raise HTTPException(status_code=400, detail="lab_url must be a string or null")
        lab_url = raw_url.strip() or None

    return _update_topic_url_field(topic_id, "lab_url", lab_url)


@academics_router.delete(
    "/api/syllabus/topics/{topic_id}/lab",
    summary="Clear Lab URL for a syllabus topic",
)
def clear_topic_lab_url(topic_id: uuid.UUID):
    """Clear (set to null) the lab_url for a single topic."""

    return _update_topic_url_field(topic_id, "lab_url", None)


# ---------- Topic Ratings ----------

class TopicRatingIn(BaseModel):
    rating: int = Field(..., ge=1, le=3, description="Rating value from 1 to 3 stars")


class TopicRatingOut(BaseModel):
    topic_id: uuid.UUID
    rating: int
    updated_at: Optional[datetime] = None


@academics_router.put(
    "/api/syllabus/topics/{topic_id}/rating",
    response_model=TopicRatingOut,
    summary="Set or update rating for a syllabus topic (1-3 stars)",
)
def set_topic_rating(
    topic_id: uuid.UUID,
    payload: TopicRatingIn,
    authorization: Optional[str] = Header(default=None),
):
    """Upsert a rating for a single topic by the current teacher.

    Rating must be between 1 and 3 (inclusive).
    Requires authentication via Bearer token.
    """
    token = _parse_bearer_token(authorization)
    if not token:
        raise HTTPException(status_code=401, detail="Authentication required")

    # Get user ID from token
    supabase = get_service_client()
    try:
        user_res = supabase.auth.get_user(token)
        user = getattr(user_res, "user", None)
        if not user or not getattr(user, "id", None):
            raise HTTPException(status_code=401, detail="Invalid or expired token")
        teacher_user_id = str(user.id)
    except Exception as e:
        raise HTTPException(status_code=401, detail="Invalid or expired token")

    # Verify topic exists
    topic_q = (
        supabase.table("syllabus_topics")
        .select("id")
        .eq("id", str(topic_id))
        .limit(1)
        .execute()
    )
    if getattr(topic_q, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (find topic): {topic_q.error}")
    if not topic_q.data:
        raise HTTPException(status_code=404, detail="Topic not found")

    # Upsert rating (insert or update based on unique constraint)
    now_iso = datetime.now(timezone.utc).isoformat()
    
    # Check if rating exists
    existing = (
        supabase.table("topic_ratings")
        .select("id")
        .eq("topic_id", str(topic_id))
        .eq("teacher_user_id", teacher_user_id)
        .limit(1)
        .execute()
    )
    
    if existing.data:
        # Update existing rating
        upd = (
            supabase.table("topic_ratings")
            .update({"rating": payload.rating, "updated_at": now_iso})
            .eq("topic_id", str(topic_id))
            .eq("teacher_user_id", teacher_user_id)
            .execute()
        )
        if getattr(upd, "error", None):
            raise HTTPException(status_code=500, detail=f"Supabase error (update rating): {upd.error}")
    else:
        # Insert new rating
        ins = (
            supabase.table("topic_ratings")
            .insert({
                "topic_id": str(topic_id),
                "teacher_user_id": teacher_user_id,
                "rating": payload.rating,
                "created_at": now_iso,
                "updated_at": now_iso,
            })
            .execute()
        )
        if getattr(ins, "error", None):
            raise HTTPException(status_code=500, detail=f"Supabase error (insert rating): {ins.error}")

    return TopicRatingOut(
        topic_id=topic_id,
        rating=payload.rating,
        updated_at=datetime.now(timezone.utc),
    )


@academics_router.get(
    "/api/syllabus/topics/{topic_id}/rating",
    response_model=Optional[TopicRatingOut],
    summary="Get current user's rating for a syllabus topic",
)
def get_topic_rating(
    topic_id: uuid.UUID,
    authorization: Optional[str] = Header(default=None),
):
    """Get the current teacher's rating for a single topic.

    Returns null if no rating exists.
    Requires authentication via Bearer token.
    """
    token = _parse_bearer_token(authorization)
    if not token:
        raise HTTPException(status_code=401, detail="Authentication required")

    # Get user ID from token
    supabase = get_service_client()
    try:
        user_res = supabase.auth.get_user(token)
        user = getattr(user_res, "user", None)
        if not user or not getattr(user, "id", None):
            raise HTTPException(status_code=401, detail="Invalid or expired token")
        teacher_user_id = str(user.id)
    except Exception as e:
        raise HTTPException(status_code=401, detail="Invalid or expired token")

    # Get rating
    rating_q = (
        supabase.table("topic_ratings")
        .select("topic_id,rating,updated_at")
        .eq("topic_id", str(topic_id))
        .eq("teacher_user_id", teacher_user_id)
        .limit(1)
        .execute()
    )
    if getattr(rating_q, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (get rating): {rating_q.error}")

    if not rating_q.data:
        return None

    row = rating_q.data[0]
    return TopicRatingOut(
        topic_id=uuid.UUID(row["topic_id"]),
        rating=int(row["rating"]),
        updated_at=row.get("updated_at"),
    )


@academics_router.get(
    "/api/syllabus/courses/{course_id}/ratings",
    summary="Get all topic ratings for a course by current user",
)
def get_course_topic_ratings(
    course_id: uuid.UUID,
    authorization: Optional[str] = Header(default=None),
):
    """Get all topic ratings for a course by the current teacher.

    Returns a dict mapping topic_id to rating value.
    Requires authentication via Bearer token.
    """
    token = _parse_bearer_token(authorization)
    if not token:
        return {"ratings": {}}

    # Get user ID from token
    supabase = get_service_client()
    try:
        user_res = supabase.auth.get_user(token)
        user = getattr(user_res, "user", None)
        if not user or not getattr(user, "id", None):
            return {"ratings": {}}
        teacher_user_id = str(user.id)
    except Exception:
        return {"ratings": {}}

    # Get all unit IDs for this course
    units_q = (
        supabase.table("syllabus_units")
        .select("id")
        .eq("course_id", str(course_id))
        .execute()
    )
    if getattr(units_q, "error", None) or not units_q.data:
        return {"ratings": {}}

    unit_ids = [u["id"] for u in units_q.data]

    # Get all topic IDs for these units
    topics_q = (
        supabase.table("syllabus_topics")
        .select("id")
        .in_("unit_id", unit_ids)
        .execute()
    )
    if getattr(topics_q, "error", None) or not topics_q.data:
        return {"ratings": {}}

    topic_ids = [t["id"] for t in topics_q.data]

    # Get all ratings for these topics by this user
    ratings_q = (
        supabase.table("topic_ratings")
        .select("topic_id,rating")
        .eq("teacher_user_id", teacher_user_id)
        .in_("topic_id", topic_ids)
        .execute()
    )
    if getattr(ratings_q, "error", None):
        return {"ratings": {}}

    ratings = {r["topic_id"]: r["rating"] for r in (ratings_q.data or [])}
    return {"ratings": ratings}


@academics_router.post(
    "/api/syllabus/topics/ratings/batch",
    summary="Get ratings for multiple topics (public, for student view)",
)
def get_topic_ratings_batch(
    topic_ids: List[uuid.UUID] = Body(..., embed=True),
):
    """Get ratings for a batch of topic IDs.

    Returns a dict mapping topic_id to rating value.
    Public endpoint - no authentication required.
    Returns the first/only rating for each topic (since one staff per topic typically).
    """
    if not topic_ids:
        return {"ratings": {}}

    supabase = get_service_client()
    
    # Convert UUIDs to strings for query
    topic_id_strs = [str(tid) for tid in topic_ids]
    
    # Get all ratings for these topics
    ratings_q = (
        supabase.table("topic_ratings")
        .select("topic_id,rating")
        .in_("topic_id", topic_id_strs)
        .execute()
    )
    if getattr(ratings_q, "error", None):
        return {"ratings": {}}

    # Return first rating found for each topic
    ratings = {}
    for r in (ratings_q.data or []):
        tid = r["topic_id"]
        if tid not in ratings:
            ratings[tid] = r["rating"]
    
    return {"ratings": ratings}


# ---------- Progress tracking ----------

class TopicToggleIn(BaseModel):
    topic_id: uuid.UUID
    completed: bool


@academics_router.get("/api/progress/topics", summary="Get completed topic ids for current user")
def get_completed_topics(authorization: Optional[str] = Header(default=None)):
    token = _parse_bearer_token(authorization)
    return get_completed_topic_ids(token)


@academics_router.post("/api/progress/toggle", summary="Mark/unmark a topic as completed for current user")
def toggle_topic(payload: TopicToggleIn, authorization: Optional[str] = Header(default=None)):
    token = _parse_bearer_token(authorization)
    return toggle_topic_completion(token, payload.topic_id, payload.completed)


@academics_router.get("/api/progress/summary", summary="Progress summary per course and unit for current user")
def progress_summary(authorization: Optional[str] = Header(default=None)):
    token = _parse_bearer_token(authorization)
    return get_progress_summary(token)


# ---------- Wishlist API ----------


class WishlistToggleIn(BaseModel):
    topic_id: uuid.UUID


@academics_router.get("/api/wishlist", summary="Get user's wishlisted topics")
def get_wishlist(authorization: Optional[str] = Header(default=None)):
    token = _parse_bearer_token(authorization)
    _, profile_id = _ensure_user_and_profile(token)
    supabase = get_service_client()
    try:
        q = supabase.table("user_topic_wishlist").select(
            "id,topic_id,created_at,syllabus_topics(id,topic,unit_id,syllabus_units(id,unit_title,course_id,syllabus_courses(id,course_code,title)))"
        ).eq("user_profile_id", profile_id).order("created_at", desc=True).execute()
    except Exception as exc:
        raise HTTPException(status_code=500, detail=f"Error fetching wishlist: {exc}")
    if getattr(q, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (wishlist): {q.error}")
    items = []
    for row in (q.data or []):
        topic_data = row.get("syllabus_topics") or {}
        unit_data = topic_data.get("syllabus_units") or {}
        course_data = unit_data.get("syllabus_courses") or {}
        items.append({
            "id": row.get("id"),
            "topic_id": row.get("topic_id"),
            "topic_name": topic_data.get("topic"),
            "unit_title": unit_data.get("unit_title"),
            "course_code": course_data.get("course_code"),
            "course_title": course_data.get("title"),
            "created_at": row.get("created_at"),
        })
    return {"wishlist": items}


@academics_router.post("/api/wishlist/toggle", summary="Add/remove topic from wishlist")
def toggle_wishlist(payload: WishlistToggleIn, authorization: Optional[str] = Header(default=None)):
    token = _parse_bearer_token(authorization)
    _, profile_id = _ensure_user_and_profile(token)
    supabase = get_service_client()
    topic_id_str = str(payload.topic_id)
    # Check if already in wishlist
    existing = supabase.table("user_topic_wishlist").select("id").eq(
        "user_profile_id", profile_id
    ).eq("topic_id", topic_id_str).limit(1).execute()
    if existing.data:
        # Remove from wishlist
        supabase.table("user_topic_wishlist").delete().eq("id", existing.data[0]["id"]).execute()
        return {"wishlisted": False, "message": "Removed from wishlist"}
    else:
        # Add to wishlist
        try:
            supabase.table("user_topic_wishlist").insert({
                "user_profile_id": profile_id,
                "topic_id": topic_id_str
            }).execute()
            return {"wishlisted": True, "message": "Added to wishlist"}
        except Exception as exc:
            msg = str(exc).lower()
            if "duplicate" in msg or "unique" in msg:
                return {"wishlisted": True, "message": "Already in wishlist"}
            raise HTTPException(status_code=500, detail=f"Error adding to wishlist: {exc}")


@academics_router.delete("/api/wishlist/{topic_id}", summary="Remove topic from wishlist")
def remove_from_wishlist(topic_id: uuid.UUID, authorization: Optional[str] = Header(default=None)):
    token = _parse_bearer_token(authorization)
    _, profile_id = _ensure_user_and_profile(token)
    supabase = get_service_client()
    supabase.table("user_topic_wishlist").delete().eq(
        "user_profile_id", profile_id
    ).eq("topic_id", str(topic_id)).execute()
    return {"message": "Removed from wishlist"}


@academics_router.get("/api/wishlist/check/{topic_id}", summary="Check if topic is wishlisted")
def check_wishlist(topic_id: uuid.UUID, authorization: Optional[str] = Header(default=None)):
    token = _parse_bearer_token(authorization)
    _, profile_id = _ensure_user_and_profile(token)
    supabase = get_service_client()
    existing = supabase.table("user_topic_wishlist").select("id").eq(
        "user_profile_id", profile_id
    ).eq("topic_id", str(topic_id)).limit(1).execute()
    return {"wishlisted": bool(existing.data)}


# ---------- History API ----------


class HistoryRecordIn(BaseModel):
    topic_id: uuid.UUID
    topic_name: str


@academics_router.get("/api/history", summary="Get user's recently viewed topics")
def get_history(
    limit: int = Query(default=50, le=200),
    authorization: Optional[str] = Header(default=None)
):
    token = _parse_bearer_token(authorization)
    _, profile_id = _ensure_user_and_profile(token)
    supabase = get_service_client()
    try:
        q = supabase.table("user_topic_history").select(
            "id,topic_id,topic_name,viewed_at"
        ).eq("user_profile_id", profile_id).order("viewed_at", desc=True).limit(limit).execute()
    except Exception as exc:
        raise HTTPException(status_code=500, detail=f"Error fetching history: {exc}")
    if getattr(q, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (history): {q.error}")
    return {"history": q.data or []}


@academics_router.post("/api/history/record", summary="Record a topic view")
def record_history(payload: HistoryRecordIn, authorization: Optional[str] = Header(default=None)):
    token = _parse_bearer_token(authorization)
    _, profile_id = _ensure_user_and_profile(token)
    supabase = get_service_client()
    topic_id_str = str(payload.topic_id)
    topic_name = (payload.topic_name or "").strip()[:500]  # Limit topic name length
    try:
        supabase.table("user_topic_history").insert({
            "user_profile_id": profile_id,
            "topic_id": topic_id_str,
            "topic_name": topic_name
        }).execute()
        return {"recorded": True}
    except Exception as exc:
        # Log but don't fail - history is non-critical
        supabase_logger.warning("Failed to record history: %s", exc)
        return {"recorded": False}


@academics_router.delete("/api/history/clear", summary="Clear user's history")
def clear_history(authorization: Optional[str] = Header(default=None)):
    token = _parse_bearer_token(authorization)
    _, profile_id = _ensure_user_and_profile(token)
    supabase = get_service_client()
    supabase.table("user_topic_history").delete().eq("user_profile_id", profile_id).execute()
    return {"message": "History cleared"}


# ---------- Profile update & uploads ----------


def _strip_or_none(value: Any) -> Optional[str]:
    if value is None:
        return None
    if isinstance(value, str):
        trimmed = value.strip()
        return trimmed or None
    return str(value)


def _date_or_none(value: Any) -> Optional[str]:
    if value is None or value == "":
        return None
    if isinstance(value, datetime):
        return value.date().isoformat()
    if isinstance(value, date):
        return value.isoformat()
    if isinstance(value, str):
        try:
            return date.fromisoformat(value.strip()).isoformat()
        except ValueError:
            return None
    return None


class ProfileMediaItemIn(BaseModel):
    kind: Optional[str] = None
    url: Optional[str] = None
    title: Optional[str] = None

    @validator("kind", "url", "title", pre=True)
    def _normalize(cls, v: Any):  # noqa: N805
        return _strip_or_none(v)


class ExperienceIn(BaseModel):
    id: Optional[str] = None
    title: str
    employment_type: Optional[str] = None
    company: Optional[str] = None
    company_logo_url: Optional[str] = None
    location: Optional[str] = None
    location_type: Optional[str] = None
    start_date: date
    end_date: Optional[date] = None
    is_current: Optional[bool] = False
    description: Optional[str] = None
    media: Optional[List[ProfileMediaItemIn]] = None

    @validator("title", "employment_type", "company", "company_logo_url", "location", "location_type", "description", pre=True)
    def _trim_str(cls, v: Any):  # noqa: N805
        return _strip_or_none(v)


class EducationIn(BaseModel):
    """Revised education model: replaces field_of_study + start/end dates with department, batch_range, regno, current_semester."""

    id: Optional[str] = None
    school: str
    degree: Optional[str] = None
    department: Optional[str] = None
    batch_range: Optional[str] = None  # e.g. "2022-2026"
    section: Optional[str] = None
    regno: Optional[str] = None
    current_semester: Optional[int] = None
    grade: Optional[str] = None
    activities: Optional[str] = None
    description: Optional[str] = None

    @validator(
        "school",
        "degree",
        "department",
        "batch_range",
        "section",
        "regno",
        "grade",
        "activities",
        "description",
        pre=True,
    )
    def _trim(cls, v: Any):  # noqa: N805
        return _strip_or_none(v)


class CertificationIn(BaseModel):
    id: Optional[str] = None
    name: str
    issuing_org: Optional[str] = None
    issue_date: Optional[date] = None
    expiration_date: Optional[date] = None
    does_not_expire: Optional[bool] = False
    credential_id: Optional[str] = None
    credential_url: Optional[str] = None
    description: Optional[str] = None

    @validator("name", "issuing_org", "credential_id", "credential_url", "description", pre=True)
    def _trim(cls, v: Any):  # noqa: N805
        return _strip_or_none(v)


class PortfolioProjectIn(BaseModel):
    id: Optional[str] = None
    name: str
    associated_experience_id: Optional[str] = None
    associated_education_id: Optional[str] = None
    start_date: Optional[date] = None
    end_date: Optional[date] = None
    url: Optional[str] = None
    description: Optional[str] = None
    tech_stack: Optional[List[str]] = None
    team: Optional[List[Dict[str, Any]]] = None

    @validator("name", "associated_experience_id", "associated_education_id", "url", "description", pre=True)
    def _trim(cls, v: Any):  # noqa: N805
        return _strip_or_none(v)


class PublicationIn(BaseModel):
    id: Optional[str] = None
    title: str
    publisher: Optional[str] = None
    publication_date: Optional[date] = None
    authors: Optional[List[str]] = None
    url: Optional[str] = None
    abstract: Optional[str] = None

    @validator("title", "publisher", "url", "abstract", pre=True)
    def _trim(cls, v: Any):  # noqa: N805
        return _strip_or_none(v)

    @validator("authors", pre=True)
    def _normalize_authors(cls, v: Any):  # noqa: N805
        if v is None:
            return None
        if isinstance(v, str):
            return [part.strip() for part in v.split(",") if part.strip()]
        if isinstance(v, list):
            cleaned = []
            for item in v:
                cleaned_item = _strip_or_none(item)
                if cleaned_item:
                    cleaned.append(cleaned_item)
            return cleaned
        return None


class ProfileUpdateIn(BaseModel):
    name: Optional[str] = None
    phone: Optional[str] = None
    bio: Optional[str] = None
    linkedin: Optional[str] = None
    github: Optional[str] = None
    leetcode: Optional[str] = None
    specializations: Optional[list[str]] = None
    projects: Optional[list[dict]] = None
    experiences: Optional[List[ExperienceIn]] = None
    education_entries: Optional[List[EducationIn]] = None
    certification_entries: Optional[List[CertificationIn]] = None
    portfolio_projects: Optional[List[PortfolioProjectIn]] = None
    publication_entries: Optional[List[PublicationIn]] = None
    # Extended profile fields (UI sends these too)
    headline: Optional[str] = None
    location: Optional[str] = None
    dob: Optional[str] = None  # YYYY-MM-DD
    portfolio_url: Optional[str] = None
    website: Optional[str] = None
    twitter: Optional[str] = None
    instagram: Optional[str] = None
    medium: Optional[str] = None
    technologies: Optional[str] = None
    skills: Optional[str] = None
    certifications: Optional[str] = None
    languages: Optional[str] = None
    interests: Optional[str] = None
    achievements: Optional[str] = None
    experience: Optional[str] = None
    publications: Optional[str] = None
    project_info: Optional[str] = None
    # Academic/identity fields
    semester: Optional[int] = None
    regno: Optional[str] = None
    college_name: Optional[str] = None
    department_name: Optional[str] = None
    batch_from: Optional[int] = None
    batch_to: Optional[int] = None


@academics_router.get("/api/profile/me", summary="Get current user's extended profile")
def get_profile_me(authorization: Optional[str] = Header(default=None)):
    token = _parse_bearer_token(authorization)
    return _get_profile_me(token)


@academics_router.put("/api/profile/me", summary="Update current user's profile fields")
def update_profile_me(payload: ProfileUpdateIn, authorization: Optional[str] = Header(default=None)):
    token = _parse_bearer_token(authorization)
    return _update_profile_me(token, payload.dict(exclude_unset=True))


@academics_router.post("/api/profile/upload", summary="Upload profile image or resume and save URL")
def upload_profile_asset(
    kind: str = Form(..., pattern=r"^(image|resume)$"),  # use pattern instead of regex
    file: UploadFile = File(...),
    authorization: Optional[str] = Header(default=None),
):
    token = _parse_bearer_token(authorization)
    return _upload_profile_asset(token, kind, file)

# --- Notes API ---




notes_router = APIRouter()

# ---- Degree-specific Allowed Domains API ----

class DegreeDomainsIn(BaseModel):
    degree_label: str = Field(..., min_length=1, max_length=128)
    domains: List[str] = Field(default_factory=list, description="List of hostnames like 'geeksforgeeks.org'")


class DegreeDomainsOut(BaseModel):
    degree_key: str
    degree_label: str
    domains: List[str]


def db_replace_allowed_domains_for_degree(degree_label: str, domains: List[str]) -> DegreeDomainsOut:
    key = _normalize_degree_key(degree_label)
    if not key:
        raise HTTPException(status_code=400, detail="Invalid degree label")
    # normalize domains (host only, lowercase)
    cleaned: List[str] = []
    for d in domains:
        d = (d or "").strip().lower()
        if not d:
            continue
        # If a URL was pasted, extract host
        try:
            if "://" in d:
                d = urlparse(d).netloc or d
        except Exception:
            pass
        d = d.strip()
        d = d.lstrip("*")  # don't allow wildcards
        if d and d not in cleaned:
            cleaned.append(d)

    supabase = get_service_client()
    now = datetime.utcnow().isoformat()
    # Upsert rows
    rows = [{
        "degree_key": key,
        "degree_label": degree_label,
        "domain": dom,
        "enabled": True,
        "updated_at": now,
    } for dom in cleaned]
    try:
        if rows:
            supabase.table(DEGREE_ALLOWED_DOMAINS_TABLE).upsert(rows, on_conflict="degree_key,domain").execute()
        # Remove extras not in the new list
        if cleaned:
            supabase.table(DEGREE_ALLOWED_DOMAINS_TABLE).delete().eq("degree_key", key).notin_("domain", cleaned).execute()
        else:
            # If empty list provided, delete all for degree
            supabase.table(DEGREE_ALLOWED_DOMAINS_TABLE).delete().eq("degree_key", key).execute()
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"DB upsert failed: {e}")
    return DegreeDomainsOut(degree_key=key, degree_label=degree_label, domains=cleaned)


def db_delete_degree_domain(degree: str, domain: str) -> dict:
    key = _normalize_degree_key(degree)
    if not key or not domain:
        raise HTTPException(status_code=400, detail="Invalid parameters")
    supabase = get_service_client()
    try:
        supabase.table(DEGREE_ALLOWED_DOMAINS_TABLE).delete().eq("degree_key", key).eq("domain", domain.lower()).execute()
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Delete failed: {e}")
    return {"ok": True}


def db_list_degree_configs() -> List[DegreeDomainsOut]:
    supabase = get_service_client()
    try:
        res = supabase.table(DEGREE_ALLOWED_DOMAINS_TABLE).select("degree_key,degree_label,domain,enabled").eq("enabled", True).execute()
        rows = getattr(res, 'data', []) or []
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"DB fetch failed: {e}")
    agg: Dict[str, Dict[str, Any]] = {}
    for r in rows:
        key = (r.get("degree_key") or "").strip()
        lbl = (r.get("degree_label") or "").strip() or key
        dom = (r.get("domain") or "").strip().lower()
        if not key or not dom:
            continue
        if key not in agg:
            agg[key] = {"degree_key": key, "degree_label": lbl, "domains": []}
        if dom not in agg[key]["domains"]:
            agg[key]["domains"].append(dom)
    out: List[DegreeDomainsOut] = []
    for v in agg.values():
        v["domains"].sort()
        out.append(DegreeDomainsOut(**v))
    # sort by label
    out.sort(key=lambda x: x.degree_label.lower())
    return out


@notes_router.get("/api/notes/allowed-domains", summary="Get allowed domains for a degree", response_model=DegreeDomainsOut)
def api_get_allowed_domains(degree: str = Query(..., min_length=1)):
    key = _normalize_degree_key(degree)
    domains = db_get_allowed_domains_for_degree(degree)
    return DegreeDomainsOut(degree_key=key or "", degree_label=degree, domains=domains or [])


@notes_router.post("/api/notes/allowed-domains", summary="Replace allowed domains for a degree", response_model=DegreeDomainsOut)
def api_set_allowed_domains(payload: DegreeDomainsIn):
    return db_replace_allowed_domains_for_degree(payload.degree_label, payload.domains or [])


@notes_router.delete("/api/notes/allowed-domains", summary="Delete a single domain from a degree")
def api_delete_domain(degree: str = Query(..., min_length=1), domain: str = Query(..., min_length=1)):
    return db_delete_degree_domain(degree, domain)


@notes_router.get("/api/notes/degrees", summary="List degrees with configured domains", response_model=List[DegreeDomainsOut])
def api_list_degrees():
    return db_list_degree_configs()

# --- Print API ---
print_router = APIRouter()

# --------- Print/Orders domain models ---------

class ShopCapabilities(BaseModel):
    color: bool = True
    duplex: bool = True
    sizes: List[str] = Field(default_factory=lambda: ["A4"])  # e.g., ["A4","A3","Letter"]
    bindings: List[str] = Field(default_factory=lambda: ["none", "staple", "spiral"])
    gsm: List[int] = Field(default_factory=lambda: [70, 80, 100])


class PrintShopIn(BaseModel):
    name: str
    phone: Optional[str] = None
    email: Optional[str] = None
    address: Optional[str] = None
    lat: Optional[float] = None
    lng: Optional[float] = None
    hours: Optional[dict] = None
    capabilities: Optional[ShopCapabilities] = None


class PriceTier(BaseModel):
    min: Optional[int] = Field(default=None, ge=0)
    upto: Optional[int] = Field(default=None, ge=1)
    per_page: float = Field(description="Price per page for this tier")


class ShopPricing(BaseModel):
    bw_single: List[PriceTier] = Field(default_factory=list)
    bw_duplex: List[PriceTier] = Field(default_factory=list)
    color_single: List[PriceTier] = Field(default_factory=list)
    color_duplex: List[PriceTier] = Field(default_factory=list)


class UpdateShopIn(BaseModel):
    name: Optional[str] = None
    phone: Optional[str] = None
    email: Optional[str] = None
    address: Optional[str] = None
    lat: Optional[float] = None
    lng: Optional[float] = None
    hours: Optional[dict] = None
    capabilities: Optional[ShopCapabilities] = None
    is_open: Optional[bool] = None
    paused: Optional[bool] = None
    price_hint: Optional[str] = None
    pricing: Optional[ShopPricing] = None


@print_router.post("/api/shop/logo", summary="Upload or replace shop logo (public URL stored in print_shops.logo_url)")
async def upload_shop_logo(file: UploadFile = File(...), authorization: Optional[str] = Header(default=None)):
    uid, _ = _get_auth_user(authorization)
    supabase = get_service_client()
    # Resolve shop owned by user
    shop_q = supabase.table(PRINT_SHOPS_TABLE).select("id").eq("owner_user_id", uid).limit(1).execute()
    shop = (getattr(shop_q, 'data', []) or [{}])[0]
    if not shop:
        raise HTTPException(status_code=404, detail="No shop found for user")
    shop_id = shop.get("id")

    # Validate file
    filename = file.filename or "logo.png"
    ext = (Path(filename).suffix or ".png").lower()
    allowed = {".jpg", ".jpeg", ".png", ".webp", ".gif", ".svg"}
    if ext not in allowed:
        raise HTTPException(status_code=400, detail="Unsupported image type")
    blob = await file.read()
    if len(blob) > 4 * 1024 * 1024:
        raise HTTPException(status_code=400, detail="Image too large (max 4MB)")
    mime_map = {".jpg": "image/jpeg", ".jpeg": "image/jpeg", ".png": "image/png", ".webp": "image/webp", ".gif": "image/gif", ".svg": "image/svg+xml"}
    content_type = mime_map.get(ext, "application/octet-stream")

    # Resolve bucket
    bucket = (
        os.getenv("SUPABASE_SHOP_ASSETS_BUCKET", "").strip()
        or os.getenv("SUPABASE_ASSETS_BUCKET", "").strip()
        or os.getenv("SUPABASE_BUCKET", "").strip()
        or os.getenv("SUPABASE_PUBLIC_BUCKET", "").strip()
        or "paperx-assets"
    )
    # Key path (use upsert to replace same path for caching simplicity)
    dest = f"shops/{shop_id}/logo{ext}"
    try:
        _storage_upload_bytes(supabase, bucket, dest, blob, content_type)
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Upload failed: {e}")
    public_url = _storage_public_url(supabase, bucket, dest)
    if not public_url:
        raise HTTPException(status_code=500, detail="Failed to resolve public URL")
    # Persist to shop row (logo_url text column expected)
    try:
        upd = supabase.table(PRINT_SHOPS_TABLE).update({"logo_url": public_url, "updated_at": _now_iso()}).eq("id", shop_id).execute()
        if getattr(upd, 'error', None):
            # If column missing, expose clear message
            msg = str(upd.error)
            raise HTTPException(status_code=500, detail=f"DB update failed (did you add logo_url column?): {msg}")
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"DB update error: {e}")
    return {"ok": True, "url": public_url, "bucket": bucket, "path": dest}


class PrintSettings(BaseModel):
    copies: int = Field(default=1, ge=1, le=50)
    color_mode: str = Field(default="auto", description="auto|bw|color")
    duplex: str = Field(default="off", description="off|long|short")
    n_up: int = Field(default=1, description="1|2|4|6|9")
    paper_size: str = Field(default="A4")
    paper_gsm: Optional[int] = Field(default=None)
    finishing: str = Field(default="none")
    page_range: str = Field(default="all")
    scale: str = Field(default="fit")
    collate: bool = Field(default=True)
    orientation: Literal["vertical", "horizontal"] = Field(default="vertical")
    notes_to_shop: Optional[str] = None


class CreateJobIn(BaseModel):
    shop_id: str
    settings: PrintSettings
    marketplace_note_id: Optional[str] = None
    estimated_pages: Optional[int] = None
    estimated_price: Optional[float] = None
    file_size: Optional[int] = None
    contact_name: Optional[str] = None
    contact_phone: Optional[str] = None
    pickup_window: Optional[str] = None


def _random_otp() -> str:
    return f"{random.randint(0, 999999):06d}"


def _now_iso() -> str:
    return datetime.utcnow().isoformat()


# --------- Print/Orders endpoints ---------

@print_router.get("/api/print/shops", summary="List print shops with simple filters")
def list_print_shops(
    q: Optional[str] = Query(default=None),
    open_now: Optional[bool] = Query(default=None),
    color: Optional[bool] = Query(default=None),
    size: Optional[str] = Query(default=None, description="A4|A3|Letter"),
    binding: Optional[str] = Query(default=None),
    sort: Optional[str] = Query(default="nearest"),
    limit: int = Query(default=50, ge=1, le=200),
):
    supabase = get_service_client()
    query = supabase.table(PRINT_SHOPS_TABLE).select("*")
    try:
        res = query.execute()
        data = getattr(res, 'data', []) or []
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to fetch shops: {e}")

    def match_filters(shop: dict) -> bool:
        caps = (shop.get("capabilities") or {})
        if isinstance(caps, str):
            try:
                caps = json.loads(caps)
            except Exception:
                caps = {}
        if q:
            s = (shop.get("name") or '') + ' ' + (shop.get("address") or '')
            if q.lower() not in s.lower():
                return False
        if open_now is True and not shop.get("is_open", False):
            return False
        if color is True and not caps.get("color", False):
            return False
        if size and size not in (caps.get("sizes") or []):
            return False
        if binding and binding not in (caps.get("bindings") or []):
            return False
        return True

    filtered = [s for s in data if match_filters(s)]
    # very rough sort: rating desc for "fastest" else by name
    if sort == "fastest":
        filtered.sort(key=lambda s: (-(s.get("rating") or 0), s.get("name") or ""))
    else:
        filtered.sort(key=lambda s: s.get("name") or "")
    return {"shops": filtered[:limit]}


@print_router.post("/api/print/jobs", summary="Create a print job (no payment)")
def create_print_job(payload: CreateJobIn, authorization: Optional[str] = Header(default=None)):
    try:
        user_id, _ = _get_auth_user(authorization)
    except HTTPException:
        # allow unauth for demo; attribute to null user
        user_id = None
    supabase = get_service_client()
    job_id = str(uuid.uuid4())
    otp = _random_otp()
    now = _now_iso()
    row = {
        "id": job_id,
        "user_id": user_id,
        "shop_id": payload.shop_id,
        "status": "submitted",
        "otp": otp,
        "settings": _supabase_payload(payload.settings.dict()),
        "estimated_pages": payload.estimated_pages,
        "estimated_price": payload.estimated_price,
        "file_size": payload.file_size,
        "marketplace_note_id": payload.marketplace_note_id,
        "pickup_window": payload.pickup_window,
        "contact_name": payload.contact_name,
        "contact_phone": payload.contact_phone,
        "created_at": now,
        "updated_at": now,
    }
    try:
        ins = supabase.table(PRINT_JOBS_TABLE).insert(row).execute()
        if getattr(ins, 'error', None):
            raise Exception(ins.error)
        supabase.table(PRINT_JOB_EVENTS_TABLE).insert({
            "job_id": job_id, "status": "submitted", "note": "Job submitted",
            "created_at": now
        }).execute()
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to create job: {e}")
    return {"ok": True, "job_id": job_id, "otp": otp}


@print_router.get("/api/orders", summary="List my print jobs")
def list_my_orders(status: Optional[str] = Query(default=None), authorization: Optional[str] = Header(default=None)):
    try:
        user_id, _ = _get_auth_user(authorization)
    except HTTPException:
        user_id = None
    supabase = get_service_client()
    try:
        q = supabase.table(PRINT_JOBS_TABLE).select("*")
        if user_id:
            q = q.eq("user_id", user_id)
        if status:
            q = q.eq("status", status)
        res = q.order("created_at", desc=True).execute()
        data = getattr(res, 'data', []) or []
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to list orders: {e}")
    return {"jobs": data}


@print_router.get("/api/orders/{job_id}", summary="Get job details")
def get_order(job_id: str, authorization: Optional[str] = Header(default=None)):
    supabase = get_service_client()
    try:
        job = supabase.table(PRINT_JOBS_TABLE).select("*").eq("id", job_id).limit(1).execute()
        job_row = (getattr(job, 'data', []) or [{}])[0]
        ev = supabase.table(PRINT_JOB_EVENTS_TABLE).select("*").eq("job_id", job_id).order("created_at").execute()
        events = getattr(ev, 'data', []) or []
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed: {e}")
    if not job_row:
        raise HTTPException(status_code=404, detail="Not found")
    return {"job": job_row, "events": events}


@print_router.post("/api/orders/{job_id}/cancel", summary="Cancel a job if not accepted")
def cancel_order(job_id: str, authorization: Optional[str] = Header(default=None)):
    user_id, _ = _get_auth_user(authorization)
    supabase = get_service_client()
    try:
        job = supabase.table(PRINT_JOBS_TABLE).select("id,user_id,status").eq("id", job_id).limit(1).execute()
        row = (getattr(job, 'data', []) or [{}])[0]
    except Exception:
        row = {}
    if not row:
        raise HTTPException(status_code=404, detail="Not found")
    if row.get("user_id") and user_id and row.get("user_id") != user_id:
        raise HTTPException(status_code=403, detail="Forbidden")
    if row.get("status") not in {"submitted"}:
        raise HTTPException(status_code=400, detail="Cannot cancel now")
    now = _now_iso()
    supabase.table(PRINT_JOBS_TABLE).update({"status": "cancelled", "updated_at": now}).eq("id", job_id).execute()
    supabase.table(PRINT_JOB_EVENTS_TABLE).insert({"job_id": job_id, "status": "cancelled", "note": "Cancelled by student", "created_at": now}).execute()
    return {"ok": True}


@print_router.post("/api/orders/{job_id}/resend-otp", summary="Regenerate OTP")
def resend_otp(job_id: str, authorization: Optional[str] = Header(default=None)):
    user_id, _ = _get_auth_user(authorization)
    supabase = get_service_client()
    otp = _random_otp()
    now = _now_iso()
    supabase.table(PRINT_JOBS_TABLE).update({"otp": otp, "updated_at": now}).eq("id", job_id).execute()
    supabase.table(PRINT_JOB_EVENTS_TABLE).insert({"job_id": job_id, "status": "otp", "note": "OTP regenerated", "created_at": now}).execute()
    return {"ok": True, "otp": otp}


# ---- Shop owner endpoints (require token matching owner_user_id) ----

def _require_shop_owner(shop_id: str, authorization: Optional[str]) -> str:
    uid, _ = _get_auth_user(authorization)
    supabase = get_service_client()
    res = supabase.table(PRINT_SHOPS_TABLE).select("id,owner_user_id").eq("id", shop_id).limit(1).execute()
    row = (getattr(res, 'data', []) or [{}])[0]
    if not row:
        raise HTTPException(status_code=404, detail="Shop not found")
    if row.get("owner_user_id") != uid:
        raise HTTPException(status_code=403, detail="Not your shop")
    return uid


@print_router.post("/api/shop/signup", summary="Create a shop owned by current user")
def shop_signup(payload: PrintShopIn, authorization: Optional[str] = Header(default=None)):
    uid, _ = _get_auth_user(authorization)
    supabase = get_service_client()
    shop_id = str(uuid.uuid4())
    row = _supabase_payload({
        "id": shop_id,
        "owner_user_id": uid,
        "name": payload.name,
        "phone": payload.phone,
        "email": payload.email,
        "address": payload.address,
        "lat": payload.lat,
        "lng": payload.lng,
        "hours": payload.hours or {},
        "capabilities": payload.capabilities.dict() if payload.capabilities else {},
        "is_open": True,
        "paused": False,
        "rating": 0,
        "created_at": _now_iso(),
        "updated_at": _now_iso(),
    })
    ins = supabase.table(PRINT_SHOPS_TABLE).insert(row).execute()
    if getattr(ins, 'error', None):
        raise HTTPException(status_code=500, detail=f"Failed: {ins.error}")
    return {"ok": True, "shop_id": shop_id}


@print_router.get("/api/shop/me", summary="Get my shop profile")
def shop_me(authorization: Optional[str] = Header(default=None)):
    uid, _ = _get_auth_user(authorization)
    supabase = get_service_client()
    res = supabase.table(PRINT_SHOPS_TABLE).select("*").eq("owner_user_id", uid).limit(1).execute()
    row = (getattr(res, 'data', []) or [{}])[0]
    if not row:
        raise HTTPException(status_code=404, detail="No shop found for user")
    return row


@print_router.patch("/api/shop/me", summary="Update my shop profile")
def shop_me_update(payload: UpdateShopIn, authorization: Optional[str] = Header(default=None)):
    uid, _ = _get_auth_user(authorization)
    supabase = get_service_client()
    # Resolve my shop id
    res = supabase.table(PRINT_SHOPS_TABLE).select("id").eq("owner_user_id", uid).limit(1).execute()
    row = (getattr(res, 'data', []) or [{}])[0]
    if not row:
        raise HTTPException(status_code=404, detail="No shop found for user")
    shop_id = row.get("id")
    # Build update dict
    to_update: Dict[str, Any] = {}
    if payload.name is not None:
        to_update["name"] = payload.name
    if payload.phone is not None:
        to_update["phone"] = payload.phone
    if payload.email is not None:
        to_update["email"] = payload.email
    if payload.address is not None:
        to_update["address"] = payload.address
    if payload.lat is not None:
        to_update["lat"] = payload.lat
    if payload.lng is not None:
        to_update["lng"] = payload.lng
    if payload.hours is not None:
        to_update["hours"] = payload.hours
    if payload.capabilities is not None:
        to_update["capabilities"] = payload.capabilities.dict()
    if payload.is_open is not None:
        to_update["is_open"] = payload.is_open
    if payload.paused is not None:
        to_update["paused"] = payload.paused
    if payload.price_hint is not None:
        to_update["price_hint"] = payload.price_hint
    if payload.pricing is not None:
        to_update["pricing"] = payload.pricing.dict()
    to_update["updated_at"] = _now_iso()
    if not to_update:
        return {"ok": True, "updated": 0}
    upd = supabase.table(PRINT_SHOPS_TABLE).update(_supabase_payload(to_update)).eq("id", shop_id).execute()
    if getattr(upd, 'error', None):
        raise HTTPException(status_code=500, detail=f"Failed to update: {upd.error}")
    return {"ok": True}


# ---- Admin endpoints: roles + shops listing ----

@print_router.get("/api/admin/roles/me", summary="Return current user's admin role")
def admin_role_me(authorization: Optional[str] = Header(default=None)):
    token = _bearer_token_from_header(authorization)
    uid = _require_auth_user_id(token)
    supabase = get_service_client()
    try:
        r = supabase.table("admin_roles").select("role,permissions").eq("auth_user_id", uid).limit(1).execute()
        row = (getattr(r, 'data', []) or [{}])[0]
        role = (row.get("role") or "student")
        perms = row.get("permissions") or {}
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to fetch role: {e}")
    return {"role": role, "permissions": perms}


@print_router.get("/api/admin/print/shops", summary="Admin: list print shops")
def admin_list_shops(q: Optional[str] = Query(default=None), limit: int = Query(default=1000, ge=1, le=5000), authorization: Optional[str] = Header(default=None)):
    _require_admin(authorization)
    supabase = get_service_client()
    cols = "id,name,phone,email,address,is_open,paused,rating,logo_url,owner_user_id,updated_at,created_at"
    try:
        builder = supabase.table(PRINT_SHOPS_TABLE).select(cols).order("updated_at", desc=True).limit(limit)
        # Conservative: apply search client-side if ilike is unavailable
        res = builder.execute()
        data = (getattr(res, 'data', []) or [])
        if q:
            qq = (q or '').strip().lower()
            def _match(row: Dict[str, Any]) -> bool:
                s = " ".join(str(row.get(k, "")) for k in ("name","email","phone","address")).lower()
                return qq in s
            data = [r for r in data if _match(r)]
        return {"shops": data}
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to list shops: {e}")


@print_router.get("/api/admin/print/shops/{shop_id}/jobs", summary="Admin: list jobs for a shop")
def admin_shop_jobs(shop_id: str, status: Optional[str] = Query(default=None), limit: int = Query(default=500, ge=1, le=5000), authorization: Optional[str] = Header(default=None)):
    _require_admin(authorization)
    supabase = get_service_client()
    try:
        q = supabase.table(PRINT_JOBS_TABLE).select("*").eq("shop_id", shop_id)
        if status:
            q = q.eq("status", status)
        res = q.order("created_at", desc=True).limit(limit).execute()
        return {"jobs": getattr(res, 'data', []) or []}
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to list shop jobs: {e}")


class AdminSettleOut(BaseModel):
    settled_count: int
    settled_amount: float


@print_router.post("/api/admin/print/shops/{shop_id}/settle", summary="Admin: settle all completed jobs -> settled", response_model=AdminSettleOut)
def admin_settle_shop(shop_id: str, authorization: Optional[str] = Header(default=None)):
    _require_admin(authorization)
    supabase = get_service_client()
    try:
        # Fetch all completed (not already settled) jobs for this shop
        res = supabase.table(PRINT_JOBS_TABLE).select("id,estimated_price,status").eq("shop_id", shop_id).in_("status", ["completed"]).execute()
        rows = (getattr(res, 'data', []) or [])
        if not rows:
            return AdminSettleOut(settled_count=0, settled_amount=0.0)
        now = _now_iso()
        ids = [r["id"] for r in rows]
        total = 0.0
        for r in rows:
            try:
                amt = float(r.get("estimated_price") or 0)
            except Exception:
                amt = 0.0
            total += amt
        # Bulk update in batches (Supabase might limit IN size)
        batch_size = 200
        for i in range(0, len(ids), batch_size):
            chunk = ids[i:i+batch_size]
            supabase.table(PRINT_JOBS_TABLE).update({"status": "settled", "updated_at": now}).in_("id", chunk).execute()
            # Insert events for audit
            ev = [{"job_id": jid, "status": "settled", "note": "Settled by admin", "created_at": now} for jid in chunk]
            if ev:
                supabase.table(PRINT_JOB_EVENTS_TABLE).insert(ev).execute()
        return AdminSettleOut(settled_count=len(ids), settled_amount=round(total, 2))
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to settle: {e}")


@print_router.post("/api/admin/print/shops/{shop_id}/jobs/{job_id}/close", summary="Admin: close a job for a shop (closed by admin)")
def admin_close_job(shop_id: str, job_id: str, authorization: Optional[str] = Header(default=None)):
    """
    Mark a specific job as closed by admin. Adds an audit event.
    """
    _require_admin(authorization)
    supabase = get_service_client()
    try:
        # Verify job belongs to shop
        jr = supabase.table(PRINT_JOBS_TABLE).select("id,shop_id,status").eq("id", job_id).eq("shop_id", shop_id).limit(1).execute()
        job = (getattr(jr, 'data', []) or [{}])
        if not job or not job[0].get("id"):
            raise HTTPException(status_code=404, detail="Job not found")
        # If already settled, disallow closing
        cur_status = (job[0].get("status") or "").lower()
        if cur_status in ("settled",):
            raise HTTPException(status_code=409, detail="Cannot close a settled job")
        now = _now_iso()
        supabase.table(PRINT_JOBS_TABLE).update({"status": "closed", "updated_at": now}).eq("id", job_id).execute()
        supabase.table(PRINT_JOB_EVENTS_TABLE).insert({
            "job_id": job_id,
            "status": "closed",
            "note": "Closed by admin",
            "created_at": now
        }).execute()
        return {"ok": True, "job_id": job_id, "status": "closed"}
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to close job: {e}")


@print_router.get("/api/shop/jobs", summary="List jobs for my shop")
def shop_jobs(
    status: Optional[str] = Query(default=None),
    limit: int = Query(default=200, ge=1, le=1000),
    authorization: Optional[str] = Header(default=None),
):
    uid, _ = _get_auth_user(authorization)
    supabase = get_service_client()
    # Query shop id with retry to handle transient protocol disconnects
    shop_res = _supabase_retry(lambda: supabase
                               .table(PRINT_SHOPS_TABLE)
                               .select("id")
                               .eq("owner_user_id", uid)
                               .limit(1)
                               .execute())
    shop = (getattr(shop_res, 'data', []) or [{}])[0]
    if not shop:
        raise HTTPException(status_code=404, detail="No shop found")
    q = supabase.table(PRINT_JOBS_TABLE).select("*").eq("shop_id", shop.get("id"))
    if status:
        q = q.eq("status", status)
    # Order by newest first and cap result size to avoid large payload disconnects
    q = q.order("created_at", desc=True).limit(limit)
    try:
        res = _supabase_retry(lambda: q.execute())
    except Exception as e:
        # Retry once with a smaller window in case of upstream disconnects
        try:
            res = _supabase_retry(lambda: q.limit(min(limit, 100)).execute())
        except Exception:
            raise HTTPException(status_code=502, detail="Upstream query failed while fetching jobs")
    return {"jobs": getattr(res, 'data', []) or []}


def _set_status(job_id: str, new_status: str, note: str, authorization: Optional[str]):
    supabase = get_service_client()
    # Fetch job + verify ownership
    job_q = supabase.table(PRINT_JOBS_TABLE).select("id,shop_id,status,otp").eq("id", job_id).limit(1).execute()
    job = (getattr(job_q, 'data', []) or [{}])[0]
    if not job:
        raise HTTPException(status_code=404, detail="Job not found")
    _require_shop_owner(job.get("shop_id"), authorization)
    now = _now_iso()
    supabase.table(PRINT_JOBS_TABLE).update({"status": new_status, "updated_at": now}).eq("id", job_id).execute()
    supabase.table(PRINT_JOB_EVENTS_TABLE).insert({"job_id": job_id, "status": new_status, "note": note, "created_at": now}).execute()
    return {"ok": True}


@print_router.post("/api/shop/jobs/{job_id}/accept")
def shop_accept(job_id: str, authorization: Optional[str] = Header(default=None)):
    return _set_status(job_id, "accepted", "Accepted", authorization)


@print_router.post("/api/shop/jobs/{job_id}/reject")
def shop_reject(job_id: str, reason: Optional[str] = Form(default=None), authorization: Optional[str] = Header(default=None)):
    return _set_status(job_id, "cancelled", f"Rejected: {reason or ''}", authorization)


@print_router.post("/api/shop/jobs/{job_id}/printing")
def shop_printing(job_id: str, authorization: Optional[str] = Header(default=None)):
    return _set_status(job_id, "printing", "Printing", authorization)


@print_router.post("/api/shop/jobs/{job_id}/ready")
def shop_ready(job_id: str, authorization: Optional[str] = Header(default=None)):
    return _set_status(job_id, "ready", "Ready for pickup", authorization)


class ReleaseIn(BaseModel):
    otp: str


@print_router.post("/api/shop/jobs/{job_id}/release")
def shop_release(job_id: str, body: ReleaseIn, authorization: Optional[str] = Header(default=None)):
    supabase = get_service_client()
    job_q = supabase.table(PRINT_JOBS_TABLE).select("id,shop_id,status,otp").eq("id", job_id).limit(1).execute()
    job = (getattr(job_q, 'data', []) or [{}])[0]
    if not job:
        raise HTTPException(status_code=404, detail="Job not found")
    _require_shop_owner(job.get("shop_id"), authorization)
    if (job.get("otp") or "").strip() != (body.otp or "").strip():
        raise HTTPException(status_code=400, detail="Invalid OTP")
    return _set_status(job_id, "completed", "Released with OTP", authorization)

# --- Notes Marketplace API ---
marketplace_router = APIRouter()

# Storage directory for uploaded marketplace note files (PDF, images, etc.)
MARKETPLACE_STORAGE = Path(__file__).resolve().parent / "assets" / "notes_marketplace"
MARKETPLACE_STORAGE.mkdir(parents=True, exist_ok=True)

# Allow common document/image/presentation/archive types used for notes
ALLOWED_NOTE_EXTENSIONS = {
    ".pdf",
    ".md",
    ".txt",
    ".png",
    ".jpg",
    ".jpeg",
    ".doc",
    ".docx",
    ".ppt",
    ".pptx",
    ".zip",
}
MAX_NOTE_FILE_SIZE = 25 * 1024 * 1024  # 25 MB


def _require_auth_user_id(token: Optional[str]) -> str:
    """Resolve auth user id from bearer token using Supabase anon client."""
    if not token:
        raise HTTPException(status_code=401, detail="Missing or invalid auth token")
    anon = get_anon_client()
    if not anon:
        raise HTTPException(status_code=500, detail="Anon client not configured")
    try:
        user = anon.auth.get_user(token)  # type: ignore[attr-defined]
        uid = _get_user_id_from_auth_response(user)
        if not uid:
            raise HTTPException(status_code=401, detail="Invalid auth user")
        return uid
    except Exception:
        raise HTTPException(status_code=401, detail="Auth validation failed")


def _sanitize_filename(name: str) -> str:
    base = re.sub(r"[^a-zA-Z0-9_.-]+", "_", name.strip())[:120]
    return base or "note"


def _execute_supabase(builder, retries: int = 2, base_delay: float = 0.2):
    """Execute a Supabase query builder with simple retry on transport drops."""
    last_exc: Optional[Exception] = None
    for attempt in range(retries + 1):
        try:
            return builder.execute()
        except HTTPXRemoteProtocolError as exc:  # pragma: no cover - network dependent
            last_exc = exc
            time.sleep(base_delay * (attempt + 1))
            continue
    raise HTTPException(status_code=503, detail="Temporary Supabase connection issue. Please retry.") from last_exc


def _approximate_pages_from_words(words: int) -> Optional[int]:
    if words <= 0:
        return None
    # assume ~500 words per page, round up
    return max(1, math.ceil(words / 500))


def _compute_page_count(file_path: Path, original_name: Optional[str], mime_type: Optional[str]) -> Optional[int]:
    """Best-effort page/slide estimate using available libraries; returns None on failure."""
    if not file_path or not file_path.exists():
        return None
    ext = (Path(original_name or file_path.name).suffix or "").lower()
    if not ext and mime_type:
        mt = (mime_type or "").lower()
        if "pdf" in mt:
            ext = ".pdf"
        elif "word" in mt or "msword" in mt:
            ext = ".docx"
        elif "ppt" in mt:
            ext = ".pptx"
    try:
        if ext == ".pdf" and fitz:
            with fitz.open(file_path) as pdf:  # type: ignore[attr-defined]
                return int(pdf.page_count)
        if ext in {".docx"} and docx:
            document = docx.Document(str(file_path))
            words = sum(len((para.text or "").split()) for para in document.paragraphs)
            return _approximate_pages_from_words(words)
        if ext in {".pptx"} and Presentation:
            prs = Presentation(str(file_path))
            return len(prs.slides)
        if ext == ".doc" and textract:
            text = textract.process(str(file_path)).decode("utf-8", errors="ignore")
            words = len(text.split())
            return _approximate_pages_from_words(words)
        if ext == ".ppt" and textract:
            text = textract.process(str(file_path)).decode("utf-8", errors="ignore")
            slides = text.count("\f") or text.count("\x0c") or 0
            return slides or None
    except Exception:
        return None
    return None


def _store_marketplace_file(upload: UploadFile) -> tuple[str, int, str]:
    """Deprecated: legacy local storage path now replaced with Supabase Storage.
    Retained for compatibility but uploads to Supabase and returns public URL as stored_name.
    """
    suffix = Path(upload.filename or "").suffix.lower()
    if suffix not in ALLOWED_NOTE_EXTENSIONS:
        raise HTTPException(status_code=400, detail="Unsupported file type")
    blob = _read_upload_bytes(upload)
    size = len(blob)
    if size > MAX_NOTE_FILE_SIZE:
        raise HTTPException(status_code=400, detail="File too large (>25MB)")
    supabase = get_service_client()
    bucket = os.getenv("SUPABASE_BUCKET", "").strip()
    if not bucket:
        raise HTTPException(status_code=500, detail="Missing SUPABASE_BUCKET in environment")
    safe_ext = suffix or ".bin"
    dest = f"marketplace/notes/{uuid.uuid4().hex}{safe_ext}"
    _storage_upload_bytes(supabase, bucket, dest, blob, upload.content_type)
    public_url = _storage_public_url(supabase, bucket, dest)
    # Return public URL as stored_name for downstream callers
    return public_url, size, (upload.content_type or "application/octet-stream")


@marketplace_router.post("/api/marketplace/notes", summary="Upload a note to marketplace")
def mp_upload_note(
    title: str = Form(..., min_length=1, max_length=256),
    description: str = Form(""),
    subject: str = Form(""),
    subject_id: Optional[str] = Form(None),
    unit: str = Form(""),
    exam_type: str = Form(""),
    categories: str = Form(""),  # comma separated
    price_cents: int = Form(0, ge=0),
    college_id: Optional[str] = Form(None),
    degree_id: Optional[str] = Form(None),
    department_id: Optional[str] = Form(None),
    batch_id: Optional[str] = Form(None),
    semester: Optional[int] = Form(None),
    file: Optional[UploadFile] = File(None),
    files: Optional[List[UploadFile]] = File(None),
    # Remote storage (Supabase) fields to allow URL-only flow
    stored_path: Optional[str] = Form(None, description="If provided as http(s) URL, backend will not store locally."),
    url: Optional[str] = Form(None, description="Optional alias for stored_path (http URL)."),
    original_filename: Optional[str] = Form(None),
    mime_type: Optional[str] = Form(None),
    file_size: Optional[int] = Form(None),
    cover: Optional[UploadFile] = File(None),
    authorization: Optional[str] = Header(default=None),
):
    token = _parse_bearer_token(authorization)
    user_id = _require_auth_user_id(token)
    uploads: List[UploadFile] = []
    if files:
        uploads.extend([f for f in files if f is not None and getattr(f, 'filename', None)])
    if file is not None and getattr(file, 'filename', None):
        uploads.append(file)
    # If no binary uploads, accept URL-based create when stored_path/url is provided
    if not uploads:
        remote = url or stored_path
        if not remote or not str(remote).lower().startswith(("http://", "https://")):
            raise HTTPException(status_code=400, detail="No file(s) provided")
        supabase = get_service_client()
        cats = [c.strip() for c in (categories or "").split(",") if c.strip()]
        row: Dict[str, Any] = {
            "owner_user_id": user_id,
            "title": title.strip(),
            "description": description.strip() or None,
            "subject": subject.strip() or None,
            "unit": unit.strip() or None,
            "exam_type": exam_type.strip() or None,
            "categories": cats,
            "price_cents": price_cents,
            "original_filename": original_filename or None,
            "stored_path": str(remote),
            "mime_type": (mime_type or "application/octet-stream"),
            "file_size": int(file_size) if file_size is not None else None,
        }
        if url:
            # If the table has a 'url' column it will be persisted; otherwise ignored
            row["url"] = str(url)
        # Academic linkages
        if college_id:
            row["college_id"] = college_id
        if degree_id:
            row["degree_id"] = degree_id
        if department_id:
            row["department_id"] = department_id
        if batch_id:
            row["batch_id"] = batch_id
        if semester is not None:
            if semester < 1 or semester > 12:
                raise HTTPException(status_code=400, detail="semester must be between 1 and 12")
            row["semester"] = semester
        if subject_id:
            try:
                uuid.UUID(str(subject_id))
                row["subject_id"] = str(subject_id)
                row["subject_href"] = f"/api/syllabus/courses/{subject_id}"
            except Exception:
                pass
        res = supabase.table("marketplace_notes").insert(row).execute()
        if getattr(res, "error", None):
            raise HTTPException(status_code=500, detail=f"Supabase error (insert note): {res.error}")
        data_rows = res.data if isinstance(res.data, list) else ([res.data] if res.data else [row])
        return {"notes": data_rows}
    cover_name: Optional[str] = None
    if cover and cover.filename:
        try:
            # Upload cover to Supabase Storage
            ext = Path(cover.filename).suffix.lower()
            if ext not in {'.png', '.jpg', '.jpeg', '.webp', '.gif'}:
                raise HTTPException(status_code=400, detail="Unsupported cover image type")
            blob = _read_upload_bytes(cover)
            if len(blob) > 5 * 1024 * 1024:
                raise HTTPException(status_code=400, detail="Cover image too large (>5MB)")
            supabase = get_service_client()
            bucket = os.getenv("SUPABASE_BUCKET", "").strip()
            if not bucket:
                raise HTTPException(status_code=500, detail="Missing SUPABASE_BUCKET in environment")
            dest = f"marketplace/notes/covers/{uuid.uuid4().hex}{ext}"
            _storage_upload_bytes(supabase, bucket, dest, blob, cover.content_type or "image/png")
            cover_name = _storage_public_url(supabase, bucket, dest)
        except HTTPException:
            raise
        except Exception as e:  # pragma: no cover
            raise HTTPException(status_code=500, detail=f"Failed to store cover: {e}")
    cats = [c.strip() for c in (categories or "").split(",") if c.strip()]
    supabase = get_service_client()
    def build_row(upload: UploadFile, stored_name: str, size: int, mime: str) -> Dict[str, Any]:
        r: Dict[str, Any] = {
            "owner_user_id": user_id,
            "title": title.strip(),
            "description": description.strip() or None,
            "subject": subject.strip() or None,
            "unit": unit.strip() or None,
            "exam_type": exam_type.strip() or None,
            "categories": cats,
            "price_cents": price_cents,
            "original_filename": upload.filename,
            "stored_path": stored_name,
            "mime_type": mime,
            "file_size": size,
        }
        # Attach academic linkage if provided (light validation)
        if college_id:
            r["college_id"] = college_id
        if degree_id:
            r["degree_id"] = degree_id
        if department_id:
            r["department_id"] = department_id
        if batch_id:
            r["batch_id"] = batch_id
        if semester is not None:
            if semester < 1 or semester > 12:
                raise HTTPException(status_code=400, detail="semester must be between 1 and 12")
            r["semester"] = semester
        if cover_name:
            r["cover_path"] = cover_name
        if subject_id:
            try:
                uuid.UUID(str(subject_id))
                r["subject_id"] = str(subject_id)
                r["subject_href"] = f"/api/syllabus/courses/{subject_id}"
            except Exception:
                pass
        return r

    rows: List[Dict[str, Any]] = []
    for up in uploads:
        stored_name, size, mime = _store_marketplace_file(up)
        rows.append(build_row(up, stored_name, size, mime))
    res = supabase.table("marketplace_notes").insert(rows if len(rows) > 1 else rows[0]).execute()
    if getattr(res, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (insert note): {res.error}")
    data_rows = res.data if isinstance(res.data, list) else ([res.data] if res.data else rows)
    return {"notes": data_rows}


@marketplace_router.get("/api/marketplace/notes", summary="List marketplace notes")
def mp_list_notes(
    q: Optional[str] = Query(None),
    subject: Optional[str] = Query(None),
    exam_type: Optional[str] = Query(None),
    min_price: Optional[int] = Query(None, ge=0),
    max_price: Optional[int] = Query(None, ge=0),
    college_id: Optional[str] = Query(None),
    degree_id: Optional[str] = Query(None),
    department_id: Optional[str] = Query(None),
    batch_id: Optional[str] = Query(None),
    semester: Optional[int] = Query(None, ge=1, le=12),
    teachers_only: Optional[bool] = Query(False, description="If true, restrict to notes whose owners have role=teacher"),
    limit: int = Query(20, ge=1, le=100),
    offset: int = Query(0, ge=0),
):
    supabase = get_service_client()
    query = supabase.table("marketplace_notes").select("*").order("created_at", desc=True)
    # Basic filters happen client-side after fetch because supabase python client has limited chaining w/ dynamic filters.
    res = query.execute()
    if getattr(res, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (list notes): {res.error}")
    items = res.data or []
    # Collect unique owner_user_id values to enrich with profile display fields (user_profiles + teacher_profiles + roles).
    owner_ids = sorted({r.get("owner_user_id") for r in items if r.get("owner_user_id")})
    user_profiles_map: dict[str, dict] = {}
    teacher_profiles_map: dict[str, dict] = {}
    teacher_ids_role: set[str] = set()
    if owner_ids:
        CHUNK = 40
        for i in range(0, len(owner_ids), CHUNK):
            chunk = owner_ids[i:i+CHUNK]
            # Standard user profile enrichment
            try:
                prof_res = (
                    supabase.table("user_profiles")
                    .select("auth_user_id,name,profile_image_url")
                    .in_("auth_user_id", chunk)
                    .execute()
                )
                if not getattr(prof_res, "error", None):
                    for pr in prof_res.data or []:
                        pid = pr.get("auth_user_id")
                        if pid:
                            user_profiles_map[pid] = pr
            except Exception:  # pragma: no cover
                pass
            # Teacher profile enrichment (prefer these over user_profiles when present)
            try:
                tprof_res = (
                    supabase.table("teacher_profiles")
                    .select("auth_user_id,name,profile_image_url,college_id,department_id")
                    .in_("auth_user_id", chunk)
                    .execute()
                )
                if not getattr(tprof_res, "error", None):
                    for tr in tprof_res.data or []:
                        tid = tr.get("auth_user_id")
                        if tid:
                            teacher_profiles_map[tid] = tr
            except Exception:  # pragma: no cover
                pass
        # Roles lookup (single query if possible)
        try:
            role_res = (
                supabase.table("admin_roles")
                .select("auth_user_id,role")
                .in_("auth_user_id", owner_ids)
                .eq("role", "teacher")
                .execute()
            )
            if not getattr(role_res, "error", None):
                for rr in role_res.data or []:
                    rid = rr.get("auth_user_id")
                    if rid:
                        teacher_ids_role.add(rid)
        except Exception:  # pragma: no cover
            pass
    # Optionally restrict to teacher owners (role table lookup)
    if teachers_only:
        teacher_ids: set[str] = set()
        try:
            role_res = (
                supabase.table("admin_roles")
                .select("auth_user_id,role")
                .eq("role", "teacher")
                .execute()
            )
            if not getattr(role_res, "error", None):
                for r in role_res.data or []:
                    uid = r.get("auth_user_id")
                    if uid:
                        teacher_ids.add(uid)
        except Exception:
            teacher_ids = set()
        if teacher_ids:
            items = [r for r in items if r.get("owner_user_id") in teacher_ids]

    # Enrich each note with seller fields (short form) for UI consumption. Teacher profile takes precedence.
    for r in items:
        oid = r.get("owner_user_id")
        if not oid:
            continue
        tprof = teacher_profiles_map.get(oid)
        uprof = user_profiles_map.get(oid)
        is_teacher = oid in teacher_ids_role or bool(tprof)
        seller: dict[str, Any] = {"id": oid}
        if tprof:
            seller["name"] = tprof.get("name") or (uprof.get("name") if uprof else oid[:6] + "â€¦")
            if tprof.get("profile_image_url"):
                seller["avatar_url"] = tprof.get("profile_image_url")
        elif uprof:
            seller["name"] = uprof.get("name") or oid[:6] + "â€¦"
            if uprof.get("profile_image_url"):
                seller["avatar_url"] = uprof.get("profile_image_url")
        else:
            seller["name"] = oid[:6] + "â€¦"
        if is_teacher:
            seller["verified"] = True
            seller["is_teacher"] = True
            seller["profile_href"] = f"/ui/teacher_profile.html?user={oid}"
        r["seller"] = seller
    
    def _match(row: dict) -> bool:
        if q:
            txt = " ".join(str(row.get(k, "")) for k in ["title", "description", "subject", "unit"]).lower()
            if q.lower() not in txt:
                return False
        if subject and (row.get("subject") or "") != subject:
            return False
        if exam_type and (row.get("exam_type") or "") != exam_type:
            return False
        if college_id and (row.get("college_id") or "") != college_id:
            return False
        if degree_id and (row.get("degree_id") or "") != degree_id:
            return False
        if department_id and (row.get("department_id") or "") != department_id:
            return False
        if batch_id and (row.get("batch_id") or "") != batch_id:
            return False
        if semester is not None and row.get("semester") != semester:
            return False
        price = int(row.get("price_cents") or 0)
        if min_price is not None and price < min_price:
            return False
        if max_price is not None and price > max_price:
            return False
        return True
    filtered = [r for r in items if _match(r)]
    total = len(filtered)
    paged = filtered[offset: offset + limit]
    return {"items": paged, "total": total, "limit": limit, "offset": offset}


@marketplace_router.get("/api/marketplace/subjects/{subject_id}/teacher-notes", summary="List teacher marketplace notes for a subject")
def mp_teacher_notes_by_subject(
    subject_id: str,
    limit: int = Query(20, ge=1, le=60),
    offset: int = Query(0, ge=0),
):
    subject_key = (subject_id or "").strip()
    if not subject_key or subject_key.lower() in {"null", "undefined"}:
        raise HTTPException(status_code=400, detail="A valid subject_id is required")

    supabase = get_service_client()
    notes_res = (
        supabase.table("marketplace_notes")
        .select("id,title,subject,subject_id,price_cents,owner_user_id,updated_at,created_at,semester,unit,exam_type")
        .eq("subject_id", subject_key)
        .order("updated_at", desc=True)
        .offset(offset)
        .limit(limit)
        .execute()
    )
    if getattr(notes_res, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (notes by subject): {notes_res.error}")

    notes: List[Dict[str, Any]] = notes_res.data or []
    if not notes:
        return {"notes": []}

    owner_ids = {n.get("owner_user_id") for n in notes if n.get("owner_user_id")}
    if not owner_ids:
        return {"notes": []}

    teacher_ids: Set[str] = set()
    try:
        role_res = (
            supabase.table("admin_roles")
            .select("auth_user_id,role")
            .in_("auth_user_id", list(owner_ids))
            .eq("role", "teacher")
            .execute()
        )
        if getattr(role_res, "error", None):
            teacher_ids = set(owner_ids)  # fall back to include all owners if role lookup fails
        else:
            teacher_ids = {row.get("auth_user_id") for row in (role_res.data or []) if row.get("auth_user_id")}
    except Exception:
        teacher_ids = set(owner_ids)

    filtered_notes = [row for row in notes if row.get("owner_user_id") in teacher_ids]
    if not filtered_notes:
        return {"notes": []}

    seller_ids = {row.get("owner_user_id") for row in filtered_notes if row.get("owner_user_id")}
    user_profiles_map: Dict[str, Dict[str, Any]] = {}
    teacher_profiles_map: Dict[str, Dict[str, Any]] = {}

    if seller_ids:
        try:
            prof_res = (
                supabase.table("user_profiles")
                .select("auth_user_id,name,profile_image_url")
                .in_("auth_user_id", list(seller_ids))
                .execute()
            )
            if not getattr(prof_res, "error", None):
                for row in prof_res.data or []:
                    uid = row.get("auth_user_id")
                    if uid:
                        user_profiles_map[uid] = row
        except Exception:
            pass
        try:
            tprof_res = (
                supabase.table("teacher_profiles")
                .select("auth_user_id,name,profile_image_url")
                .in_("auth_user_id", list(seller_ids))
                .execute()
            )
            if not getattr(tprof_res, "error", None):
                for row in tprof_res.data or []:
                    uid = row.get("auth_user_id")
                    if uid:
                        teacher_profiles_map[uid] = row
        except Exception:
            pass

    enriched: List[Dict[str, Any]] = []
    for row in filtered_notes:
        owner_id = row.get("owner_user_id")
        teacher_profile = teacher_profiles_map.get(owner_id)
        user_profile = user_profiles_map.get(owner_id)
        seller_name = None
        seller_avatar = None
        if teacher_profile and teacher_profile.get("name"):
            seller_name = teacher_profile.get("name")
        elif user_profile and user_profile.get("name"):
            seller_name = user_profile.get("name")
        elif owner_id:
            seller_name = owner_id[:6] + "..."
        if teacher_profile and teacher_profile.get("profile_image_url"):
            seller_avatar = teacher_profile.get("profile_image_url")
        elif user_profile and user_profile.get("profile_image_url"):
            seller_avatar = user_profile.get("profile_image_url")

        seller = {"id": owner_id, "is_teacher": True, "verified": True}
        if seller_name:
            seller["name"] = seller_name
        if seller_avatar:
            seller["avatar_url"] = seller_avatar
        if owner_id:
            seller["profile_href"] = f"/ui/teacher_profile.html?user={owner_id}"

        enriched.append(
            {
                "id": row.get("id"),
                "title": row.get("title"),
                "subject": row.get("subject"),
                "subject_id": row.get("subject_id"),
                "price_cents": int(row.get("price_cents") or 0),
                "owner_user_id": owner_id,
                "updated_at": row.get("updated_at") or row.get("created_at"),
                "created_at": row.get("created_at"),
                "semester": row.get("semester"),
                "unit": row.get("unit"),
                "exam_type": row.get("exam_type"),
                "seller": seller,
            }
        )

    return {"notes": enriched, "count": len(enriched), "limit": limit, "offset": offset}


@marketplace_router.get("/api/marketplace/notes/meta", summary="Distinct filter metadata for marketplace notes")
def mp_notes_meta(teachers_only: Optional[bool] = Query(False, description="If true, restrict to notes whose owners have role=teacher")):
    """Return distinct values useful for building client-side filters.

    This performs a single select * (bounded) and derives sets in app code because
    the Supabase python client lacks a simple DISTINCT helper across many columns.
    If the table grows large, replace with server-side RPC or dedicated materialized view.
    """
    supabase = get_service_client()
    # Fetch a reasonable window (latest 1000) â€“ adjust as needed or paginate later.
    res = supabase.table("marketplace_notes").select("*").order("created_at", desc=True).limit(1000).execute()
    if getattr(res, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (notes meta): {res.error}")
    rows: List[dict] = res.data or []
    if teachers_only:
        # build teacher id set
        try:
            role_res = supabase.table("admin_roles").select("auth_user_id,role").eq("role", "teacher").execute()
            if not getattr(role_res, "error", None):
                teacher_ids = {r.get("auth_user_id") for r in (role_res.data or []) if r.get("auth_user_id")}
                if teacher_ids:
                    rows = [r for r in rows if r.get("owner_user_id") in teacher_ids]
        except Exception:
            pass
    subjects: set[str] = set()
    exam_types: set[str] = set()
    semesters: set[int] = set()
    categories: set[str] = set()
    college_ids: set[str] = set()
    degree_ids: set[str] = set()
    department_ids: set[str] = set()
    batch_ids: set[str] = set()
    seller_ids: set[str] = set()
    prices: List[int] = []
    for r in rows:
        if r.get("subject"): subjects.add(str(r.get("subject")))
        if r.get("exam_type"): exam_types.add(str(r.get("exam_type")))
        if r.get("semester") is not None:
            try:
                semesters.add(int(r.get("semester")))
            except Exception:
                pass
        if isinstance(r.get("categories"), list):
            for c in r.get("categories"):
                if c: categories.add(str(c))
        if r.get("college_id"): college_ids.add(str(r.get("college_id")))
        if r.get("degree_id"): degree_ids.add(str(r.get("degree_id")))
        if r.get("department_id"): department_ids.add(str(r.get("department_id")))
        if r.get("batch_id"): batch_ids.add(str(r.get("batch_id")))
        if r.get("owner_user_id"): seller_ids.add(str(r.get("owner_user_id")))
        try:
            prices.append(int(r.get("price_cents") or 0))
        except Exception:
            pass
    # Enrich seller short names (best-effort)
    seller_map: Dict[str, Dict[str, Optional[str]]] = {}
    if seller_ids:
        try:
            prof = (
                supabase.table("user_profiles")
                .select("auth_user_id,name,profile_image_url")
                .in_("auth_user_id", list(seller_ids))
                .execute()
            )
            if not getattr(prof, "error", None):
                for row in prof.data or []:
                    uid = row.get("auth_user_id")
                    if uid:
                        seller_map[uid] = {
                            "id": uid,
                            "name": row.get("name") or uid[:6] + "â€¦",
                            "avatar_url": row.get("profile_image_url"),
                        }
        except Exception:
            pass
    price_min = min(prices) if prices else 0
    price_max = max(prices) if prices else 0
    return {
        "subjects": sorted(subjects),
        "exam_types": sorted(exam_types),
        "semesters": sorted(semesters),
        "categories": sorted(categories),
        "college_ids": sorted(college_ids),
        "degree_ids": sorted(degree_ids),
        "department_ids": sorted(department_ids),
        "batch_ids": sorted(batch_ids),
        "sellers": list(seller_map.values()),
        "price_range": {"min": price_min, "max": price_max},
        "count": len(rows),
    }


@marketplace_router.get("/api/marketplace/notes/{note_id}", summary="Get marketplace note detail")
def mp_get_note(note_id: uuid.UUID, authorization: Optional[str] = Header(default=None), token: Optional[str] = Query(None)):
    header_token = _parse_bearer_token(authorization)
    token = token or header_token
    supabase = get_service_client()
    note_query = supabase.table("marketplace_notes").select("*").eq("id", str(note_id)).limit(1)
    res = _execute_supabase(note_query)
    if getattr(res, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (get note): {res.error}")
    if not res.data:
        raise HTTPException(status_code=404, detail="Note not found")
    note = res.data[0]
    if not note.get("page_count"):
        stored_name = note.get("stored_path")
        if stored_name:
            file_path = MARKETPLACE_STORAGE / stored_name
            computed_pages = _compute_page_count(file_path, note.get("original_filename"), note.get("mime_type"))
            if computed_pages:
                note["page_count"] = computed_pages
                note["pages"] = computed_pages
    # Determine if user has access (owner or purchased or free)
    user_id = None
    if token:
        try:
            user_id = _require_auth_user_id(token)
        except HTTPException:
            user_id = None
    has_access = False
    if note.get("price_cents", 0) == 0:
        has_access = True
    elif user_id and user_id == note.get("owner_user_id"):
        has_access = True
    elif user_id:
        purchase_query = (
            supabase.table("marketplace_purchases")
            .select("id")
            .eq("note_id", str(note_id))
            .eq("buyer_user_id", user_id)
            .limit(1)
        )
        pur = _execute_supabase(purchase_query)
        if getattr(pur, "error", None):
            raise HTTPException(status_code=500, detail=f"Supabase error (check purchase): {pur.error}")
        if pur.data:
            has_access = True
    # Reviews
    reviews_query = (
        supabase.table("marketplace_reviews")
        .select("id,reviewer_user_id,rating,comment,created_at")
        .eq("note_id", str(note_id))
        .order("created_at", desc=True)
    )
    rev = _execute_supabase(reviews_query)
    reviews = rev.data or []
    # Collect user ids for enrichment (owner + reviewers)
    user_ids: set[str] = set()
    owner_id = note.get("owner_user_id")
    if owner_id:
        user_ids.add(owner_id)
    for r in reviews:
        rid = r.get("reviewer_user_id")
        if rid:
            user_ids.add(rid)
    profiles_map: dict[str, dict] = {}
    if user_ids:
        # Fetch profiles from user_profiles (auth_user_id mapping)
        try:
            prof_query = (
                supabase.table("user_profiles")
                .select("auth_user_id,name,profile_image_url")
                .in_("auth_user_id", list(user_ids))
            )
            prof_res = _execute_supabase(prof_query)
            if not getattr(prof_res, "error", None):
                for row in prof_res.data or []:
                    uid = row.get("auth_user_id")
                    if uid:
                        profiles_map[uid] = row
        except Exception:
            pass
    # Attach seller (prefer teacher_profiles for teachers)
    seller_obj: Dict[str, Any] = {"id": owner_id} if owner_id else {}
    teacher_profile_row = None
    if owner_id:
        try:
            tprof_query = supabase.table("teacher_profiles").select("auth_user_id,name,profile_image_url,college_id,department_id").eq("auth_user_id", owner_id).limit(1)
            tprof = _execute_supabase(tprof_query)
            if not getattr(tprof, "error", None) and tprof.data:
                teacher_profile_row = tprof.data[0]
        except Exception:
            teacher_profile_row = None
    if teacher_profile_row:
        seller_obj["name"] = teacher_profile_row.get("name") or (profiles_map.get(owner_id, {}).get("name") if owner_id in profiles_map else owner_id[:6] + "â€¦")
        if teacher_profile_row.get("profile_image_url"):
            seller_obj["avatar_url"] = teacher_profile_row.get("profile_image_url")
    if owner_id and not teacher_profile_row and owner_id in profiles_map:
        prow = profiles_map[owner_id]
        seller_obj.setdefault("name", prow.get("name") or owner_id[:6] + "â€¦")
        if prow.get("profile_image_url"):
            seller_obj["avatar_url"] = prow.get("profile_image_url")
    if owner_id and "name" not in seller_obj:
        seller_obj["name"] = owner_id[:6] + "â€¦"
    if owner_id:
        note["seller"] = seller_obj

    # Determine if owner has teacher role (verification badge)
    is_teacher_owner = False
    try:
        if owner_id:
            role_query = (
                supabase.table("admin_roles")
                .select("role")
                .eq("auth_user_id", owner_id)
                .eq("role", "teacher")
                .limit(1)
            )
            role_res = _execute_supabase(role_query)
            if not getattr(role_res, "error", None) and role_res.data:
                is_teacher_owner = True
    except Exception:
        # Non-fatal; silently ignore role lookup issues
        pass
    if note.get("seller"):
        note["seller"]["verified"] = is_teacher_owner
        note["seller"]["is_teacher"] = is_teacher_owner
        if is_teacher_owner:
            note["seller"]["profile_href"] = f"/ui/teacher_profile.html?user={owner_id}" if owner_id else None
    note["is_teacher_owner"] = is_teacher_owner
    # Enrich each review
    for r in reviews:
        rid = r.get("reviewer_user_id")
        prow = profiles_map.get(rid)
        if prow:
            r["reviewer"] = {
                "id": rid,
                "name": prow.get("name") or (rid[:6] + "â€¦" if rid else None),
                "avatar_url": prow.get("profile_image_url"),
            }
    # --- Academic metadata enrichment (names) ---
    # If the note row has academic foreign keys, attempt to resolve human-readable names.
    # This keeps the base schema flexible while giving the UI friendly labels.
    academic_name_map: dict[str, Optional[str]] = {
        "college_name": None,
        "degree_name": None,
        "department_name": None,
        "batch_range": None,
    }
    try:
        # We collect the needed ids first to minimize queries.
        college_id = note.get("college_id")
        degree_id = note.get("degree_id")
        department_id = note.get("department_id")
        batch_id = note.get("batch_id")
        # For each present id we fetch its table (single row).
        if college_id:
            rcol = supabase.table("colleges").select("id,name").eq("id", college_id).limit(1).execute()
            if not getattr(rcol, "error", None) and rcol.data:
                academic_name_map["college_name"] = rcol.data[0].get("name")
        if degree_id:
            rdeg = supabase.table("degrees").select("id,name").eq("id", degree_id).limit(1).execute()
            if not getattr(rdeg, "error", None) and rdeg.data:
                academic_name_map["degree_name"] = rdeg.data[0].get("name")
        if department_id:
            rdep = supabase.table("departments").select("id,name").eq("id", department_id).limit(1).execute()
            if not getattr(rdep, "error", None) and rdep.data:
                academic_name_map["department_name"] = rdep.data[0].get("name")
        if batch_id:
            rbat = supabase.table("batches").select("id,from_year,to_year").eq("id", batch_id).limit(1).execute()
            if not getattr(rbat, "error", None) and rbat.data:
                b = rbat.data[0]
                fy = b.get("from_year")
                ty = b.get("to_year")
                if fy and ty:
                    academic_name_map["batch_range"] = f"{fy}-{ty}"
                elif fy:
                    academic_name_map["batch_range"] = str(fy)
    except Exception:
        # Silently ignore enrichment errors; we don't want to block detail retrieval.
        pass
    # Attach only non-null values to the note object so the front-end can conditionally render.
    for k, v in academic_name_map.items():
        if v:
            note[k] = v
    return {"note": note, "has_access": has_access, "reviews": reviews}


@marketplace_router.get("/api/marketplace/notes/{note_id}/download", summary="Download note file (public)")
def mp_download_note(note_id: uuid.UUID):
    """Redirect to the remote file URL stored in Supabase (no local files)."""
    supabase = get_service_client()
    res = supabase.table("marketplace_notes").select("stored_path,url,original_filename").eq("id", str(note_id)).limit(1).execute()
    if getattr(res, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (get note): {res.error}")
    if not res.data:
        raise HTTPException(status_code=404, detail="Note not found")
    row = res.data[0]
    target = row.get("url") or row.get("stored_path") or ""
    if not target or not str(target).lower().startswith(("http://", "https://")):
        raise HTTPException(status_code=404, detail="Remote URL not available")
    return RedirectResponse(url=str(target))

@marketplace_router.get("/api/marketplace/notes/{note_id}/preview", summary="Inline preview for PDF or image")
def mp_preview_note(note_id: uuid.UUID):
    """Redirect to remote URL; front-end handles inline rendering based on type."""
    supabase = get_service_client()
    res = supabase.table("marketplace_notes").select("stored_path,url").eq("id", str(note_id)).limit(1).execute()
    if getattr(res, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (get note): {res.error}")
    if not res.data:
        raise HTTPException(status_code=404, detail="Note not found")
    row = res.data[0]
    target = row.get("url") or row.get("stored_path") or ""
    if not target or not str(target).lower().startswith(("http://", "https://")):
        raise HTTPException(status_code=404, detail="Remote URL not available")
    return RedirectResponse(url=str(target))

@marketplace_router.get("/api/marketplace/notes/{note_id}/cover", summary="Get cover image for a note")
def mp_cover_image(note_id: uuid.UUID):
    supabase = get_service_client()
    res = supabase.table("marketplace_notes").select("cover_path").eq("id", str(note_id)).limit(1).execute()
    if getattr(res, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (get cover): {res.error}")
    if not res.data:
        raise HTTPException(status_code=404, detail="Note not found")
    cover = res.data[0].get("cover_path")
    if not cover or not str(cover).lower().startswith(("http://", "https://")):
        raise HTTPException(status_code=404, detail="No remote cover URL set")
    return RedirectResponse(url=str(cover))


@marketplace_router.post("/api/marketplace/notes/{note_id}/purchase", summary="Purchase a paid note (mock payment)")
def mp_purchase_note(note_id: uuid.UUID, authorization: Optional[str] = Header(default=None), token: Optional[str] = Query(None)):
    header_token = _parse_bearer_token(authorization)
    token = token or header_token
    user_id = _require_auth_user_id(token)
    supabase = get_service_client()
    note_res = supabase.table("marketplace_notes").select("price_cents,owner_user_id").eq("id", str(note_id)).limit(1).execute()
    if getattr(note_res, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (get note): {note_res.error}")
    if not note_res.data:
        raise HTTPException(status_code=404, detail="Note not found")
    note = note_res.data[0]
    if note.get("owner_user_id") == user_id:
        raise HTTPException(status_code=400, detail="Cannot purchase your own note")
    price = int(note.get("price_cents") or 0)
    if price == 0:
        return {"status": "free", "message": "Note is free"}
    # Mock payment success: just record purchase if not exists
    existing = (
        supabase.table("marketplace_purchases")
        .select("id")
        .eq("note_id", str(note_id))
        .eq("buyer_user_id", user_id)
        .limit(1)
        .execute()
    )
    if getattr(existing, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (find purchase): {existing.error}")
    if existing.data:
        return {"status": "ok", "message": "Already purchased"}
    ins = supabase.table("marketplace_purchases").insert({
        "note_id": str(note_id),
        "buyer_user_id": user_id,
        "amount_cents": price,
    }).execute()
    if getattr(ins, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (insert purchase): {ins.error}")
    return {"status": "ok", "purchase": ins.data[0] if ins.data else None}


@marketplace_router.post("/api/marketplace/notes/{note_id}/review", summary="Add or update a review")
def mp_review_note(
    note_id: uuid.UUID,
    rating: int = Form(..., ge=1, le=5),
    comment: str = Form(""),
    authorization: Optional[str] = Header(default=None),
    token: Optional[str] = Query(None),
):
    header_token = _parse_bearer_token(authorization)
    token = token or header_token
    user_id = _require_auth_user_id(token)
    supabase = get_service_client()
    # Ensure note exists (access no longer required for reviews; any authenticated user may review)
    note_res = supabase.table("marketplace_notes").select("price_cents,owner_user_id").eq("id", str(note_id)).limit(1).execute()
    if getattr(note_res, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (get note): {note_res.error}")
    if not note_res.data:
        raise HTTPException(status_code=404, detail="Note not found")
    note = note_res.data[0]
    # Access rule relaxed: we no longer gate by purchase/free. Still only one review per user.
    # Upsert (one per user per note)
    existing = (
        supabase.table("marketplace_reviews")
        .select("id")
        .eq("note_id", str(note_id))
        .eq("reviewer_user_id", user_id)
        .limit(1)
        .execute()
    )
    if getattr(existing, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (find review): {existing.error}")
    now = datetime.utcnow().isoformat()
    if existing.data:
        rid = existing.data[0]["id"]
        upd = (
            supabase.table("marketplace_reviews")
            .update({"rating": rating, "comment": comment.strip() or None, "updated_at": now})
            .eq("id", rid)
            .execute()
        )
        if getattr(upd, "error", None):
            raise HTTPException(status_code=500, detail=f"Supabase error (update review): {upd.error}")
    else:
        ins = supabase.table("marketplace_reviews").insert({
            "note_id": str(note_id),
            "reviewer_user_id": user_id,
            "rating": rating,
            "comment": comment.strip() or None,
        }).execute()
        if getattr(ins, "error", None):
            raise HTTPException(status_code=500, detail=f"Supabase error (insert review): {ins.error}")
    # Recompute aggregates
    agg = supabase.rpc("exec", params={}).execute() if False else None  # placeholder for future RPC
    # manual aggregate
    revs = (
        supabase.table("marketplace_reviews").select("rating").eq("note_id", str(note_id)).execute()
    )
    if not getattr(revs, "error", None):
        ratings = [int(r.get("rating") or 0) for r in (revs.data or [])]
        if ratings:
            avg_rating = round(sum(ratings) / len(ratings), 2)
            supabase.table("marketplace_notes").update({
                "avg_rating": avg_rating,
                "rating_count": len(ratings),
                "updated_at": datetime.utcnow().isoformat(),
            }).eq("id", str(note_id)).execute()
    return {"status": "ok"}


@marketplace_router.put("/api/marketplace/notes/{note_id}", summary="Update own note metadata")
def mp_update_note(note_id: uuid.UUID, payload: dict = Body(...), authorization: Optional[str] = Header(default=None)):
    token = _parse_bearer_token(authorization)
    user_id = _require_auth_user_id(token)
    supabase = get_service_client()
    note_res = supabase.table("marketplace_notes").select("owner_user_id").eq("id", str(note_id)).limit(1).execute()
    if getattr(note_res, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (get note for update): {note_res.error}")
    if not note_res.data:
        raise HTTPException(status_code=404, detail="Note not found")
    owner_id = note_res.data[0].get("owner_user_id")
    if owner_id != user_id:
        raise HTTPException(status_code=403, detail="Not permitted to update this note")
    allowed_fields = {"title", "description", "subject", "subject_id", "semester", "price_cents", "unit", "exam_type", "categories"}
    updates: Dict[str, Any] = {}
    payload = payload or {}
    for field in allowed_fields:
        if field in payload:
            value = payload[field]
            if field == "categories" and isinstance(value, str):
                value = [c.strip() for c in value.split(",") if c.strip()]
            if field == "semester" and value not in (None, ""):
                try:
                    ivalue = int(value)
                    if ivalue < 1 or ivalue > 12:
                        raise ValueError
                    value = ivalue
                except ValueError:
                    raise HTTPException(status_code=400, detail="semester must be between 1 and 12")
            if field == "price_cents" and value not in (None, ""):
                try:
                    value = int(value)
                    if value < 0:
                        raise ValueError
                except ValueError:
                    raise HTTPException(status_code=400, detail="price_cents must be a non-negative integer")
            if field == "subject_id" and value:
                try:
                    uuid.UUID(str(value))
                    value = str(value)
                except Exception:
                    raise HTTPException(status_code=400, detail="subject_id must be a valid UUID")
            updates[field] = value if value != "" else None
    if not updates:
        return {"updated": False}
    updates["updated_at"] = datetime.utcnow().isoformat()
    upd = (
        supabase.table("marketplace_notes")
        .update(updates)
        .eq("id", str(note_id))
        .eq("owner_user_id", user_id)
        .execute()
    )
    if getattr(upd, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (update note): {upd.error}")
    data = None
    if isinstance(upd.data, list) and upd.data:
        data = upd.data[0]
    return {"updated": True, "note": data or updates}


@marketplace_router.post("/api/marketplace/notes/{note_id}/replace-file", summary="Replace stored file for own note")
def mp_replace_note_file(
    note_id: uuid.UUID,
    file: UploadFile = File(...),
    authorization: Optional[str] = Header(default=None),
):
    token = _parse_bearer_token(authorization)
    user_id = _require_auth_user_id(token)
    supabase = get_service_client()
    note_res = (
        supabase.table("marketplace_notes")
        .select("owner_user_id,stored_path")
        .eq("id", str(note_id))
        .limit(1)
        .execute()
    )
    if getattr(note_res, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (get note for replace): {note_res.error}")
    if not note_res.data:
        raise HTTPException(status_code=404, detail="Note not found")
    note_row = note_res.data[0]
    owner_id = note_row.get("owner_user_id")
    if owner_id != user_id:
        raise HTTPException(status_code=403, detail="Not permitted to replace this note")
    old_stored = note_row.get("stored_path")
    stored_name, size, mime = _store_marketplace_file(file)
    update_fields = {
        "stored_path": stored_name,
        "original_filename": file.filename or "upload",
        "file_size": size,
        "mime_type": mime,
        "updated_at": datetime.utcnow().isoformat(),
    }
    upd = (
        supabase.table("marketplace_notes")
        .update(update_fields)
        .eq("id", str(note_id))
        .eq("owner_user_id", user_id)
        .execute()
    )
    if getattr(upd, "error", None):
        try:
            new_path = MARKETPLACE_STORAGE / stored_name
            if new_path.is_file():
                new_path.unlink()
        except Exception:
            pass
        raise HTTPException(status_code=500, detail=f"Supabase error (replace file): {upd.error}")
    if old_stored and old_stored != stored_name:
        try:
            old_path = MARKETPLACE_STORAGE / old_stored
            if old_path.is_file():
                old_path.unlink()
        except Exception:
            pass
    return {"updated": True, "stored_path": stored_name}


@marketplace_router.delete("/api/marketplace/notes/{note_id}", summary="Delete own note")
def mp_delete_note(note_id: uuid.UUID, authorization: Optional[str] = Header(default=None), token: Optional[str] = Query(None)):
    header_token = _parse_bearer_token(authorization)
    token = token or header_token
    user_id = _require_auth_user_id(token)
    supabase = get_service_client()
    note_res = supabase.table("marketplace_notes").select("owner_user_id,stored_path").eq("id", str(note_id)).limit(1).execute()
    if getattr(note_res, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (get note): {note_res.error}")
    if not note_res.data:
        raise HTTPException(status_code=404, detail="Not found")
    note = note_res.data[0]
    if note.get("owner_user_id") != user_id:
        raise HTTPException(status_code=403, detail="Not owner")
    del_res = supabase.table("marketplace_notes").delete().eq("id", str(note_id)).execute()
    if getattr(del_res, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (delete note): {del_res.error}")
    # remove file silently
    try:
        fp = MARKETPLACE_STORAGE / (note.get("stored_path") or "")
        if fp.is_file():
            fp.unlink()
    except Exception:
        pass
    return {"status": "deleted"}


def _openai_client():
    """Return OpenAI client (1.x) or raise. Supports legacy 0.x fallback."""
    try:
        import openai  # type: ignore
    except Exception as e:  # pragma: no cover
        raise HTTPException(status_code=500, detail=f"OpenAI library missing: {e}")
    api_key = os.getenv("OPENAI_API_KEY") or os.getenv("OPENAI_KEY")
    if not api_key:
        raise HTTPException(status_code=500, detail="Missing OPENAI_API_KEY in environment")
    # New 1.x style client
    try:  # prefer 1.x Client
        from openai import OpenAI  # type: ignore

        return OpenAI(api_key=api_key)
    except Exception:
        # Fallback: configure legacy global (<=0.28)
        openai.api_key = api_key
        return openai


def _build_transform_prompt(mode: str, content: str, custom: str | None) -> str:
    base_instruction = {
        "summarize": "Summarize the markdown below into a concise, well-structured overview. Preserve headings hierarchy where helpful; keep math/mermaid/code blocks intact.",
        "expand": "Expand and elaborate the markdown below. Add helpful clarifying sentences, short intuitive examples, and brief context. Do NOT invent inaccurate facts. Preserve code/math/mermaid blocks.",
        "simplify": "Rewrite the markdown in simpler language suitable for a beginner, keeping key technical terms but adding plain-language explanation.",
    }.get(mode, "")
    if mode == "custom" and custom:
        base_instruction = f"Apply this custom transformation to the markdown: {custom}. Preserve code/math/mermaid blocks and headings."
    if not base_instruction:
        base_instruction = "Return the markdown unchanged."  # fallback
    return (
        base_instruction
        + "\n\nReturn ONLY valid markdown. Do not add commentary outside the transformed content.\n\n---\nMARKDOWN INPUT BELOW\n---\n"
        + content
    )


@notes_router.post("/api/notes/transform")
def api_transform_note(payload: dict):
    """Ephemeral transform of markdown (summarize / expand / custom) using Gemini only."""
    mode = (payload or {}).get("mode", "summarize").strip().lower()
    markdown = (payload or {}).get("markdown", "")
    custom = (payload or {}).get("prompt")
    if not markdown:
        raise HTTPException(status_code=400, detail="Missing markdown")
    if mode not in {"summarize", "expand", "custom", "simplify"}:
        raise HTTPException(status_code=400, detail="Invalid mode")
    if not GEMINI_API_KEY:
        raise HTTPException(status_code=501, detail="Gemini API key not configured.")
    try:
        import google.generativeai as genai  # type: ignore
    except Exception as exc:  # pragma: no cover - optional dependency
        raise HTTPException(
            status_code=501,
            detail=f"Gemini client library missing: {exc}. Install google-generativeai to enable this feature.",
        ) from exc

    prompt = _build_transform_prompt(mode, markdown, custom)
    genai.configure(api_key=GEMINI_API_KEY)
    safety_settings = [
        {"category": "HARM_CATEGORY_HATE_SPEECH", "threshold": "BLOCK_ONLY_HIGH"},
        {"category": "HARM_CATEGORY_HARASSMENT", "threshold": "BLOCK_ONLY_HIGH"},
        {"category": "HARM_CATEGORY_SEXUALLY_EXPLICIT", "threshold": "BLOCK_ONLY_HIGH"},
        {"category": "HARM_CATEGORY_DANGEROUS_CONTENT", "threshold": "BLOCK_ONLY_HIGH"},
    ]
    model = genai.GenerativeModel(GEMINI_NOTES_MODEL)

    base_instruction = textwrap.dedent(
        """
        You edit Markdown precisely as instructed. Return ONLY valid Markdown. Avoid any harmful, explicit, or actionable content.
        If the requested transform could enable harm, reply with a safe, high-level academic rewrite or keep content neutral.
        """
    ).strip()

    def _extract_gemini_text(resp):
        def _finish_reason(candidate):
            finish = getattr(candidate, "finish_reason", None)
            if finish is None and isinstance(candidate, dict):
                finish = candidate.get("finish_reason") or candidate.get("finishReason")
            if finish is None:
                return None
            finish_str = str(finish).upper()
            if finish_str.isdigit():
                return int(finish_str)
            return finish_str

        try:
            quick = getattr(resp, "text", None)
            if isinstance(quick, str) and quick.strip():
                return quick.strip()
        except Exception:
            pass
        candidates = getattr(resp, "candidates", None) or []
        if isinstance(candidates, dict):
            candidates = [candidates]
        for cand in candidates:
            finish = _finish_reason(cand)
            if finish in (2, "SAFETY") or (isinstance(finish, str) and "SAFETY" in finish):
                # Surface to caller so we can retry conservatively
                raise HTTPException(status_code=502, detail="Gemini blocked the response for safety.")
            content = getattr(cand, "content", None)
            parts = None
            if content is not None:
                parts = getattr(content, "parts", None)
                if parts is None and isinstance(content, dict):
                    parts = content.get("parts")
            if parts is None and isinstance(cand, dict):
                parts = cand.get("content", {}).get("parts") if isinstance(cand.get("content"), dict) else None
            texts = []
            if parts:
                for part in parts:
                    text_val = getattr(part, "text", None)
                    if text_val is None and isinstance(part, dict):
                        text_val = part.get("text")
                    if text_val:
                        texts.append(str(text_val))
            if texts:
                return "\n".join(texts).strip()
        return ""

    def call_gemini(_instruction: str, _body: str):
        return model.generate_content(
            [{"text": _instruction}, {"text": _body}],
            generation_config={"temperature": 0.25, "max_output_tokens": 2048},
            safety_settings=safety_settings,
        )

    try:
        response = call_gemini(base_instruction, prompt)
    except Exception as exc:
        # Return empty markdown so UI treats it as no change rather than hard fail
        return {"markdown": "", "mode": mode, "custom": custom or None, "error": f"Gemini error: {exc}"}

    try:
        out_text = _extract_gemini_text(response)
    except HTTPException as exc:
        # Retry once with a conservative instruction
        if "safety" in str(getattr(exc, "detail", "")).lower():
            conservative = textwrap.dedent(
                """
                Provide a safe, high-level, non-actionable rewrite of the provided Markdown per the instruction. Keep it brief if needed.
                Return only valid Markdown.
                """
            ).strip()
            try:
                resp2 = call_gemini(conservative, prompt)
                out_text = _extract_gemini_text(resp2)
            except Exception:
                out_text = ""
        else:
            out_text = ""
    except Exception:
        out_text = ""

    # If empty after attempts, return empty string (UI will show 'No change')
    return {"markdown": out_text or "", "mode": mode, "custom": custom or None}


@notes_router.post("/api/notes/snippet-assist")
def api_snippet_assist(payload: dict):
    """Provide inline Gemini help for a highlighted snippet."""
    selection = (payload or {}).get("selection") or (payload or {}).get("snippet") or ""
    instruction = (payload or {}).get("instruction") or (payload or {}).get("prompt") or ""
    selection = (selection or "").strip()
    instruction = (instruction or "").strip()
    if not selection:
        raise HTTPException(status_code=400, detail="Missing selection text.")
    if not GEMINI_API_KEY:
        raise HTTPException(status_code=501, detail="Gemini API key not configured.")
    try:
        import google.generativeai as genai  # type: ignore
    except Exception as exc:  # pragma: no cover - optional dependency
        raise HTTPException(
            status_code=501,
            detail=f"Gemini client library missing: {exc}. Install google-generativeai to enable this feature.",
        ) from exc

    trimmed_selection = selection[:3000]
    genai.configure(api_key=GEMINI_API_KEY)
    # Relax safety to block only high-severity content while still allowing educational, non-actionable summaries
    safety_settings = [
        {"category": "HARM_CATEGORY_HATE_SPEECH", "threshold": "BLOCK_ONLY_HIGH"},
        {"category": "HARM_CATEGORY_HARASSMENT", "threshold": "BLOCK_ONLY_HIGH"},
        {"category": "HARM_CATEGORY_SEXUALLY_EXPLICIT", "threshold": "BLOCK_ONLY_HIGH"},
        {"category": "HARM_CATEGORY_DANGEROUS_CONTENT", "threshold": "BLOCK_ONLY_HIGH"},
    ]
    model = genai.GenerativeModel(GEMINI_NOTES_MODEL)
    base_instruction = textwrap.dedent(
        """
        You are PaperX's inline study copilot. Prefer the highlighted passage for context.
        Key constraints to avoid safety blocks:
        - Provide a neutral, educational, non-actionable explanation only.
        - Do NOT include step-by-step instructions, realistic procedures, or facilitation of harm.
        - Avoid explicit, graphic, or sexual content.
        - If the selection entails sensitive material, provide a safe high-level overview in academic tone.
        - Keep it concise and student-friendly.
        """
    ).strip()
    user_instruction = instruction or "Explain this selection simply."
    prompt_body = textwrap.dedent(
        f"""
        Instruction: {user_instruction}

        Highlighted selection:
        {trimmed_selection}
        """
    ).strip()
    def _extract_gemini_text(resp):
        def _finish_reason(candidate):
            finish = getattr(candidate, "finish_reason", None)
            if finish is None and isinstance(candidate, dict):
                finish = candidate.get("finish_reason") or candidate.get("finishReason")
            if finish is None:
                return None
            finish_str = str(finish).upper()
            if finish_str.isdigit():
                return int(finish_str)
            return finish_str

        try:
            quick = getattr(resp, "text", None)
            if isinstance(quick, str) and quick.strip():
                return quick.strip()
        except Exception:
            pass

        candidates = getattr(resp, "candidates", None) or []
        if isinstance(candidates, dict):
            candidates = [candidates]
        for cand in candidates:
            finish = _finish_reason(cand)
            if finish in (2, "SAFETY") or (isinstance(finish, str) and "SAFETY" in finish):
                raise HTTPException(status_code=502, detail="Gemini blocked the response for safety. Try rephrasing your selection or question.")
            content = getattr(cand, "content", None)
            parts = None
            if content is not None:
                parts = getattr(content, "parts", None)
                if parts is None and isinstance(content, dict):
                    parts = content.get("parts")
            if parts is None and isinstance(cand, dict):
                parts = cand.get("content", {}).get("parts") if isinstance(cand.get("content"), dict) else None
            texts = []
            if parts:
                for part in parts:
                    text_val = getattr(part, "text", None)
                    if text_val is None and isinstance(part, dict):
                        text_val = part.get("text")
                    if text_val:
                        texts.append(str(text_val))
            if texts:
                return "\n".join(texts).strip()
        return ""

    def call_gemini(_instruction: str, _body: str):
        return model.generate_content(
            [{"text": _instruction}, {"text": _body}],
            generation_config={"temperature": 0.25, "max_output_tokens": 512},
            safety_settings=safety_settings,
        )

    # First attempt with safe defaults
    try:
        response = call_gemini(base_instruction, prompt_body)
    except Exception as exc:
        raise HTTPException(status_code=502, detail=f"Gemini selection assist failed: {exc}") from exc

    try:
        generated = _extract_gemini_text(response)
    except HTTPException as exc:
        # Retry once with an even more conservative instruction if blocked for SAFETY
        detail = getattr(exc, "detail", "") or str(exc)
        if "Gemini blocked the response for safety" in detail:
            conservative_instruction = textwrap.dedent(
                """
                Provide a brief, safe, high-level, non-actionable academic overview of the selection.
                Do NOT include steps, procedures, or details that could enable harm.
                If necessary, generalize abstractly. Keep it 2â€“5 sentences.
                """
            ).strip()
            try:
                response2 = call_gemini(conservative_instruction, prompt_body)
                generated = _extract_gemini_text(response2)
            except Exception:
                # Fall through to friendly message
                generated = ""
        else:
            raise
    except Exception as exc:
        raise HTTPException(status_code=502, detail=f"Gemini returned malformed response: {exc}") from exc
    if not generated:
        # Provide a graceful, safe fallback message (no OpenAI, no external calls)
        safe_msg = (
            "This selection may involve sensitive content. Here is a safe, high-level academic overview: "
            "The highlighted text appears to reference material that can trigger content safety filters. "
            "Please try asking for a neutral definition, historical context, purpose, or key concepts without step-by-step procedures."
        )
        return {"text": safe_msg, "truncated": len(selection) > len(trimmed_selection)}
    return {"text": generated, "truncated": len(selection) > len(trimmed_selection)}


@notes_router.post("/generate")
@notes_router.post("/api/notes/generate")
async def generate(payload: dict):
    topic = (payload or {}).get("topic", "").strip()
    force = bool((payload or {}).get("force", False))
    variant = _normalize_variant((payload or {}).get("variant", "detailed"))
    degree = (payload or {}).get("degree")
    if not topic:
        return JSONResponse({"error": "Missing 'topic'"}, status_code=400)
    try:
        if not force:
            # Exact-title cache from DB for the variant
            row = db_get_ai_note_by_title_exact_variant(topic, variant=variant)
            if row and (row.get("markdown") or "").strip():
                return {
                    "id": row.get("id"),
                    "markdown": row.get("markdown", ""),
                    "cached": True,
                    "title": row.get("title"),
                    "variant": variant,
                    "image_urls": row.get("image_urls") or [],
                }
        # Non-streaming generation: use detailed pipeline for detailed, or transform detailed into variant
        md_detailed = generate_notes_markdown(topic, degree=degree)
        md = md_detailed
        if variant == "cheatsheet" or variant == "simple":
            try:
                # Use transform endpoint logic locally to adjust style
                mode = "simplify" if variant == "simple" else "custom"
                custom = None
                if variant == "cheatsheet":
                    custom = (
                        "Rewrite as an ultra-concise exam cheat sheet: 250â€“400 words, bullets/tables, sections: Core Concepts; Key Definitions & Formulas; Quick Steps/Algorithms; Pitfalls; Keywords. Bold key terms. Do NOT include TL;DR, Common Mistakes, Memory Aids, or any CITATIONS section."
                    )
                prompt = _build_transform_prompt(mode, md_detailed, custom)
                client = _openai_client()
                model = os.getenv("LLM_MODEL", "gpt-4o-mini")
                messages = [
                    {"role": "system", "content": "You are an assistant that edits markdown content precisely as instructed."},
                    {"role": "user", "content": prompt},
                ]
                create_fn = None
                try:
                    create_fn = client.chat.completions.create  # type: ignore[attr-defined]
                except AttributeError:
                    create_fn = None
                if create_fn:
                    resp = create_fn(model=model, messages=messages, temperature=0.4, max_tokens=4096)
                    md = resp.choices[0].message.content if resp.choices else md_detailed
                else:
                    resp = client.ChatCompletion.create(model=model, messages=messages, temperature=0.4, max_tokens=4096)  # type: ignore[attr-defined]
                    md = resp.choices[0].message["content"] if resp.choices else md_detailed
            except Exception:
                md = md_detailed
        image_urls: List[str] = []
        try:
            related_pages = serpapi_search(topic, num=8, degree=degree)
            image_urls = collect_image_urls(topic, related_pages)
        except Exception:
            image_urls = []
        row = db_upsert_ai_note_by_title_variant(topic, md, variant=variant, image_urls=image_urls)
        stored_images = row.get("image_urls") or image_urls
        return {
            "id": row.get("id"),
            "markdown": row.get("markdown", md),
            "cached": False,
            "title": row.get("title"),
            "variant": variant,
            "image_urls": stored_images,
        }
    except Exception as e:
        return JSONResponse({"error": str(e)}, status_code=500)


@notes_router.get("/generate/stream")
@notes_router.get("/api/notes/generate/stream")
async def generate_stream(topic: str, force: bool = False, variant: str = "detailed", degree: Optional[str] = None):
    async def event_source() -> AsyncGenerator[bytes, None]:
        yield b"event: open\n\n"
        # Early cache hit: exact-title lookup in DB
        if not force:
            row = db_get_ai_note_by_title_exact_variant(topic, variant=_normalize_variant(variant))
            if row and (row.get("markdown") or "").strip():
                payload = {
                    "id": row.get("id"),
                    "markdown": row.get("markdown", ""),
                    "cached": True,
                    "title": row.get("title"),
                    "variant": _normalize_variant(variant),
                    "image_urls": row.get("image_urls") or [],
                }
                line = f"event: final\n".encode("utf-8")
                data_json = json.dumps(payload, ensure_ascii=False)
                data_b = ("data: " + data_json + "\n\n").encode("utf-8")
                yield line
                yield data_b
                yield b"event: close\n\n"
                return
        loop = asyncio.get_running_loop()
        queue: asyncio.Queue[Tuple[str, Optional[str], Optional[Dict[str, Any]]]] = asyncio.Queue()
        stop_event = threading.Event()

        def dispatch(item: Tuple[str, Optional[str], Optional[Dict[str, Any]]]) -> None:
            loop.call_soon_threadsafe(queue.put_nowait, item)

        def worker() -> None:
            try:
                for name, payload in generate_notes_events(topic, stop_event=stop_event, variant=_normalize_variant(variant), degree=degree):
                    if stop_event.is_set():
                        break
                    dispatch(("event", name, payload))
                dispatch(("done", None, None))
            except Exception as exc:
                dispatch(("error", None, {"message": str(exc)}))
                dispatch(("done", None, None))

        worker_future = loop.run_in_executor(None, worker)

        try:
            while True:
                kind, name, payload = await queue.get()
                if kind == "event" and name is not None and payload is not None:
                    try:
                        if name == "final" and isinstance(payload, dict) and payload.get("markdown"):
                            try:
                                images_input = payload.get("image_urls")
                                if not isinstance(images_input, list):
                                    urls_input = payload.get("urls")
                                    images_input = urls_input if isinstance(urls_input, list) else None
                                row = db_upsert_ai_note_by_title_variant(
                                    topic,
                                    payload.get("markdown", ""),
                                    variant=_normalize_variant(variant),
                                    image_urls=images_input,
                                )
                                payload["id"] = row.get("id")
                                payload["title"] = row.get("title")
                                payload["cached"] = False
                                payload["variant"] = _normalize_variant(variant)
                                payload["image_urls"] = row.get("image_urls") or (images_input or [])
                            except Exception:
                                pass
                            finally:
                                stop_event.set()
                        line = f"event: {name}\n".encode("utf-8")
                        data_json = json.dumps(payload, ensure_ascii=False)
                        data = ("data: " + data_json + "\n\n").encode("utf-8")
                        yield line
                        yield data
                    except Exception as exc:
                        err = json.dumps({"message": str(exc)}, ensure_ascii=False)
                        yield b"event: error\n"
                        yield ("data: " + err + "\n\n").encode("utf-8")
                elif kind == "error" and payload is not None:
                    err_json = json.dumps(payload, ensure_ascii=False)
                    yield b"event: error\n"
                    yield ("data: " + err_json + "\n\n").encode("utf-8")
                elif kind == "done":
                    break
        except asyncio.CancelledError:
            stop_event.set()
            raise
        finally:
            stop_event.set()
            try:
                await asyncio.wait_for(asyncio.wrap_future(worker_future), timeout=1.0)
            except Exception:
                pass

        yield b"event: close\n\n"

    headers = {
        "Cache-Control": "no-cache, no-transform",
        "Connection": "keep-alive",
        "X-Accel-Buffering": "no",
    }
    return StreamingResponse(event_source(), media_type="text/event-stream", headers=headers)


class FlashcardSection(BaseModel):
    icon: str = Field(..., min_length=1, max_length=8)
    heading: str = Field(..., min_length=1)
    question: str = Field(..., min_length=1)
    answer: str = Field(..., min_length=1)


class FlashcardCard(BaseModel):
    concept: str = Field(..., min_length=1)
    summary: Optional[str] = None
    sections: List[FlashcardSection] = Field(default_factory=list)
    key_points: List[str] = Field(default_factory=list)

    @validator("sections")
    def ensure_sections(cls, value):
        if not value:
            raise ValueError("Each flashcard must include at least one section.")
        return value


def _derive_topic_from_markdown(markdown: str, fallback: str = "") -> str:
    title, _ = _extract_title_and_headings(markdown or "")
    if title:
        return title
    return fallback


def _fallback_flashcards_from_markdown(markdown: str, topic: str, max_cards: int) -> List[Dict[str, Any]]:
    text = (markdown or "").strip()
    if not text:
        return []

    heading_pattern = re.compile(r"^##\s+(.+?)\s*$", re.MULTILINE)
    matches = list(heading_pattern.finditer(text))
    sections: List[Tuple[str, str]] = []

    if matches:
        for idx, match in enumerate(matches):
            title = match.group(1).strip()
            start = match.end()
            end = matches[idx + 1].start() if idx + 1 < len(matches) else len(text)
            body = text[start:end].strip()
            if body:
                sections.append((title, body))
    else:
        sections.append((topic or "Overview", text))

    bullet_pattern = re.compile(r"^\s*[-*+]\s+(.*)$", re.MULTILINE)
    candidate_cards: List[Tuple[str, str]] = []
    seen_titles: Set[str] = set()
    for title, body in sections:
        tkey = title.lower()
        if tkey in seen_titles:
            continue
        seen_titles.add(tkey)
        candidate_cards.append((title, body))

    # If not enough sections, create additional cards from bullets
    if len(candidate_cards) < max_cards:
        bullets = [b.strip() for b in bullet_pattern.findall(text) if b.strip()]
        for idx, bullet in enumerate(bullets):
            if len(candidate_cards) >= max_cards:
                break
            candidate_cards.append((f"Key Insight {idx + 1}", bullet))

    cards: List[Dict[str, Any]] = []
    for title, body in candidate_cards[:max_cards]:
        clean_body = body.strip()
        paragraphs = [p.strip() for p in re.split(r"\n{2,}", clean_body) if p.strip()]
        summary_source = paragraphs[0] if paragraphs else clean_body
        summary = textwrap.shorten(summary_source.replace("\n", " "), width=220, placeholder="â€¦") if summary_source else ""

        sections_payload: List[Dict[str, str]] = []
        sections_payload.append({
            "icon": "ðŸ§ ",
            "heading": "Core Idea",
            "question": f"What is {title}?",
            "answer": summary_source or "Not covered in notes",
        })

        if len(paragraphs) > 1:
            sections_payload.append({
                "icon": "âš™ï¸",
                "heading": "Mechanism",
                "question": "How does it work?",
                "answer": paragraphs[1],
            })
        if len(paragraphs) > 2:
            sections_payload.append({
                "icon": "ðŸ›¡ï¸",
                "heading": "Pitfalls",
                "question": "What should we watch out for?",
                "answer": paragraphs[2],
            })

        if len(sections_payload) < 2:
            alt_answer = " ".join(paragraphs[1:2]) or clean_body[:240]
            sections_payload.append({
                "icon": "ðŸ”­",
                "heading": "Details",
                "question": "Tell me more",
                "answer": alt_answer or "Not covered in notes",
            })

        key_points: List[str] = []
        bullets = [b.strip() for b in bullet_pattern.findall(body) if b.strip()]
        for bullet in bullets[:4]:
            key_points.append(textwrap.shorten(bullet, width=100, placeholder="â€¦"))
        if not key_points and summary:
            key_points.append(summary)

        cards.append({
            "concept": title[:80],
            "summary": summary,
            "sections": sections_payload,
            "key_points": key_points,
        })

    return cards[:max_cards]


def _generate_flashcards_with_gemini(markdown: str, topic: str, max_cards: int) -> Tuple[List[Dict[str, Any]], str, bool, bool]:
    if not GEMINI_API_KEY:
        raise HTTPException(status_code=501, detail="Gemini API key not configured for flashcard generation.")
    try:
        import google.generativeai as genai  # type: ignore
        from google.api_core import exceptions as google_exceptions  # type: ignore
    except Exception as exc:  # pragma: no cover - optional dependency
        raise HTTPException(status_code=500, detail=f"Gemini client library missing: {exc}") from exc

    cleaned_notes = (markdown or "").strip()
    if not cleaned_notes:
        raise HTTPException(status_code=400, detail="Note does not contain any content to build flashcards.")

    limit_chars = int(os.getenv("FLASHCARD_MAX_CHARS", "9000") or "9000")
    truncated = len(cleaned_notes) > limit_chars
    excerpt = cleaned_notes[:limit_chars]

    max_cards = max(4, min(max_cards, 8))
    min_cards = min(6, max_cards)

    prompt = textwrap.dedent(
        f"""
        You are PaperX's futuristic study companion. Craft an engaging flashcard deck for the topic "{topic}" using ONLY the material inside the triple chevrons.

        Requirements:
        - Produce between {min_cards} and {max_cards} flashcards (inclusive). If the notes cover fewer than {min_cards} solid ideas, produce as many as the notes justify but never exceed {max_cards}.
        - Each flashcard is a JSON object with keys: "concept", "summary", "sections", "key_points".
        - "concept": 3-6 word title anchored in the notes.
        - "summary": 1-2 sentence high-energy overview derived strictly from the notes.
        - "sections": array of 2-4 objects, each with emoji "icon", "heading", "question", "answer". Questions must be informational; answers must come strictly from the notes. Icons should feel futuristic/fantasy (ðŸ§ , âš™ï¸, ðŸ”®, ðŸŒŒ, ðŸ›¡ï¸, ðŸ’¡, etc.).
        - "key_points": array of 2-4 crisp bullet phrases (â‰¤80 characters) quoting or paraphrasing unique facts from the notes.
        - NEVER invent information. If the notes lack detail for a section, set the answer to "Not covered in notes".
        - Keep terminology consistent with the notes (math symbols, proper nouns, etc.).
        - The vibe should be adventurous and motivating while staying accurate.
        - Return ONLY valid JSON matching this exact structure: {{"topic": "...", "flashcards": [ {{...}} ] }}
        - Do not wrap the JSON in markdown fences or additional commentary.

        {"The notes excerpt was truncated for length. Mention this limitation when relevant." if truncated else ""}

        <<<NOTES>>>
        {excerpt}
        <<<END NOTES>>>
        """
    ).strip()

    genai.configure(api_key=GEMINI_API_KEY)
    model = genai.GenerativeModel(GEMINI_NOTES_MODEL)
    generation_config = genai.GenerationConfig(
        response_mime_type="application/json",
        temperature=0.45,
        top_p=0.75,
        max_output_tokens=1024,
    )

    request_timeout = float(os.getenv("GEMINI_FLASHCARD_TIMEOUT", "35"))
    max_attempts = max(1, int(os.getenv("GEMINI_FLASHCARD_RETRIES", "3")))
    base_delay = max(0.8, float(os.getenv("GEMINI_FLASHCARD_RETRY_DELAY", "1.5")))
    transient_errors = (
        google_exceptions.RetryError,
        google_exceptions.ServiceUnavailable,
        google_exceptions.ResourceExhausted,
        google_exceptions.InternalServerError,
    )
    payload = [
        {"text": prompt},
    ]

    response = None
    last_exc: Optional[Exception] = None
    for attempt in range(1, max_attempts + 1):
        try:
            response = model.generate_content(
                payload,
                generation_config=generation_config,
                request_options={"timeout": request_timeout},
            )
            break
        except google_exceptions.DeadlineExceeded as exc:
            notes_logger.warning(
                "Gemini flashcard generation deadline exceeded",
                extra={
                    "topic": topic,
                    "note_chars": len(cleaned_notes),
                    "limit_chars": limit_chars,
                    "attempt": attempt,
                },
            )
            fallback_cards = _fallback_flashcards_from_markdown(cleaned_notes, topic, max_cards)
            if fallback_cards:
                return fallback_cards, "fallback-markdown", truncated, True
            raise HTTPException(status_code=504, detail="Flashcard generation timed out. Try again with a shorter note.") from exc
        except transient_errors as exc:  # pragma: no cover - network dependent
            last_exc = exc
            delay = min(base_delay * (2 ** (attempt - 1)), 8.0)
            notes_logger.warning(
                "Gemini flashcard transient error; will retry",
                extra={
                    "topic": topic,
                    "attempt": attempt,
                    "remaining_attempts": max_attempts - attempt,
                    "error": str(exc),
                    "retry_delay_sec": delay,
                },
            )
            if attempt == max_attempts:
                break
            time.sleep(delay)
            continue
        except google_exceptions.GoogleAPICallError as exc:  # pragma: no cover - network dependent
            last_exc = exc
            notes_logger.error(
                "Gemini flashcard generation error",
                extra={"topic": topic, "error": str(exc), "attempt": attempt},
            )
            if attempt == max_attempts:
                break
            delay = min(base_delay * (2 ** (attempt - 1)), 8.0)
            time.sleep(delay)
        except Exception as exc:  # pragma: no cover - safer catch-all
            last_exc = exc
            notes_logger.error(
                "Gemini flashcard unexpected failure",
                extra={"topic": topic, "error": str(exc), "attempt": attempt},
            )
            break

    if response is None:
        fallback_cards = _fallback_flashcards_from_markdown(cleaned_notes, topic, max_cards)
        if fallback_cards:
            notes_logger.warning(
                "Gemini flashcard generation exhausted retries; using fallback",
                extra={"topic": topic, "attempts": max_attempts, "error": str(last_exc) if last_exc else None},
            )
            return fallback_cards, "fallback-markdown", truncated, True
        detail = "Gemini service error while generating flashcards."
        if isinstance(last_exc, google_exceptions.RetryError):
            detail = "Gemini retry attempts exhausted. Please try again shortly."
        raise HTTPException(status_code=502, detail=detail)

    raw_text = getattr(response, "text", None)
    if not raw_text and getattr(response, "candidates", None):
        for candidate in response.candidates:  # pragma: no cover - depends on API payload
            content = getattr(candidate, "content", None)
            if not content:
                continue
            parts = getattr(content, "parts", None) or []
            raw_text = "".join(part.text or "" for part in parts if getattr(part, "text", None))
            if raw_text:
                break

    if not raw_text:
        fallback_cards = _fallback_flashcards_from_markdown(cleaned_notes, topic, max_cards)
        if fallback_cards:
            notes_logger.warning("Gemini returned empty flashcard payload; using fallback", extra={"topic": topic})
            return fallback_cards, "fallback-markdown", truncated, True
        raise HTTPException(status_code=500, detail="Gemini returned empty flashcard response.")

    try:
        payload = json.loads(raw_text)
    except json.JSONDecodeError as exc:
        fallback_cards = _fallback_flashcards_from_markdown(cleaned_notes, topic, max_cards)
        if fallback_cards:
            notes_logger.warning("Gemini returned malformed JSON; using fallback", extra={"topic": topic})
            return fallback_cards, "fallback-markdown", truncated, True
        raise HTTPException(status_code=500, detail=f"Failed to parse Gemini flashcards JSON: {exc}") from exc

    cards_raw = payload.get("flashcards") or payload.get("cards")
    if not cards_raw or not isinstance(cards_raw, list):
        fallback_cards = _fallback_flashcards_from_markdown(cleaned_notes, topic, max_cards)
        if fallback_cards:
            notes_logger.warning("Gemini response missing flashcards array; using fallback", extra={"topic": topic})
            return fallback_cards, "fallback-markdown", truncated, True
        raise HTTPException(status_code=500, detail="Gemini response missing 'flashcards' list.")

    cards: List[Dict[str, Any]] = []
    for idx, item in enumerate(cards_raw[:max_cards]):
        prepared = dict(item or {})
        if "concept" not in prepared:
            prepared["concept"] = prepared.get("title") or prepared.get("name") or f"Concept {idx + 1}"
        if "summary" not in prepared:
            prepared["summary"] = prepared.get("overview") or prepared.get("synopsis") or ""

        sections = prepared.get("sections")
        if not isinstance(sections, list) or not sections:
            base_question = prepared.get("question") or "What is the key idea?"
            base_answer = prepared.get("answer") or prepared.get("summary") or "Not covered in notes"
            sections = [{
                "icon": prepared.get("icon") or "ðŸ§ ",
                "heading": prepared.get("heading") or prepared["concept"],
                "question": base_question,
                "answer": base_answer,
            }]
        normalized_sections = []
        for section in sections:
            sec = dict(section or {})
            sec.setdefault("icon", "ðŸ”®")
            sec.setdefault("heading", prepared["concept"])
            sec.setdefault("question", "What does this cover?")
            sec.setdefault("answer", "Not covered in notes")
            normalized_sections.append(sec)
        prepared["sections"] = normalized_sections

        key_points = prepared.get("key_points")
        if not isinstance(key_points, list) or not key_points:
            fallback_points = prepared.get("bullets") or prepared.get("highlights")
            if isinstance(fallback_points, list):
                key_points = fallback_points
            else:
                key_points = []
        prepared["key_points"] = key_points

        try:
            card = FlashcardCard.parse_obj(prepared)
        except Exception as exc:
            fallback_cards = _fallback_flashcards_from_markdown(cleaned_notes, topic, max_cards)
            if fallback_cards:
                notes_logger.warning("Gemini produced invalid flashcard schema; using fallback", extra={"topic": topic})
                return fallback_cards, "fallback-markdown", truncated, True
            raise HTTPException(status_code=500, detail=f"Invalid flashcard data returned by Gemini: {exc}") from exc

        card_data = card.dict()
        if not card_data.get("summary") and card_data["sections"]:
            first_answer = card_data["sections"][0]["answer"]
            card_data["summary"] = first_answer.split("\n")[0][:200]
        cards.append(card_data)

    if not cards:
        fallback_cards = _fallback_flashcards_from_markdown(cleaned_notes, topic, max_cards)
        if fallback_cards:
            return fallback_cards, "fallback-markdown", truncated, True
        raise HTTPException(status_code=500, detail="Gemini returned no flashcards.")

    return cards, GEMINI_NOTES_MODEL, truncated, False


@notes_router.post("/notes/{note_id}/flashcards")
@notes_router.post("/api/notes/{note_id}/flashcards")
def api_generate_flashcards(note_id: str, payload: Optional[Dict[str, Any]] = Body(default=None)):
    row, _v = db_get_ai_note_by_id_any(note_id)
    if not row:
        raise HTTPException(status_code=404, detail="Note not found")

    markdown = (row.get("markdown") or "").strip()
    if not markdown:
        raise HTTPException(status_code=400, detail="Note is empty; generate notes before requesting flashcards.")

    requested_max = 8
    topic_override = ""
    if isinstance(payload, dict):
        if "max_cards" in payload:
            try:
                requested_max = int(payload["max_cards"])
            except Exception:
                raise HTTPException(status_code=400, detail="max_cards must be an integer")
        topic_override = str(payload.get("topic") or "").strip()

    max_cards = max(4, min(requested_max, 8))
    inferred_topic = topic_override or row.get("title") or _derive_topic_from_markdown(markdown, fallback="Study Flashcards")
    try:
        cards, used_model, truncated, used_fallback = _generate_flashcards_with_gemini(
            markdown,
            inferred_topic,
            max_cards=max_cards,
        )
    except HTTPException:
        # Let explicit HTTPException bubble (these have proper status codes)
        raise
    except Exception as exc:  # defensive: ensure we never leak a 500 from unexpected errors
        notes_logger.exception("Flashcard generation - unexpected error, attempting fallback", extra={"note_id": note_id})
        # Try local fallback from markdown. This should always produce something if markdown exists.
        try:
            fallback_cards = _fallback_flashcards_from_markdown(markdown, inferred_topic, max_cards)
        except Exception:
            fallback_cards = []

        if fallback_cards:
            return {
                "note_id": note_id,
                "topic": inferred_topic,
                "flashcards": fallback_cards,
                "count": len(fallback_cards),
                "model": "fallback-markdown",
                "truncated": False,
                "fallback": True,
                "generated_at": datetime.utcnow().isoformat() + "Z",
            }

        # If fallback failed, return an explicit JSON error rather than raw 500
        raise HTTPException(status_code=500, detail=f"Flashcard generation failed: {exc}") from exc

    return {
        "note_id": note_id,
        "topic": inferred_topic,
        "flashcards": cards,
        "count": len(cards),
        "model": used_model,
        "truncated": truncated,
        "fallback": used_fallback,
        "generated_at": datetime.utcnow().isoformat() + "Z",
    }


# --- MCQ generation from notes ---

class MCQQuestion(BaseModel):
    question: str = Field(..., min_length=3)
    options: List[str] = Field(..., min_items=4)
    correct_index: int = Field(..., ge=0, le=3)
    explanation: Optional[str] = Field(default=None)


def _generate_mcq_with_gemini(markdown: str, topic: str, count: int) -> Tuple[List[Dict[str, Any]], str, bool]:
    """Generate MCQs grounded strictly in the provided markdown using Gemini.

    Returns (questions, model_name, truncated).
    """
    if not GEMINI_API_KEY:
        raise HTTPException(status_code=501, detail="Gemini API key not configured for MCQ generation.")
    try:
        import google.generativeai as genai  # type: ignore
    except Exception as exc:  # pragma: no cover - optional dependency
        raise HTTPException(status_code=500, detail=f"Gemini client library missing: {exc}") from exc

    cleaned = (markdown or "").strip()
    if not cleaned:
        raise HTTPException(status_code=400, detail="Note does not contain any content to build MCQs.")

    # Keep request compact
    limit_chars = int(os.getenv("MCQ_MAX_CHARS", "9000") or "9000")
    truncated = len(cleaned) > limit_chars
    excerpt = cleaned[:limit_chars]

    count = max(8, min(count or 10, 12))
    min_q = min(8, count)

    prompt = textwrap.dedent(
        f"""
        You are PaperX's quiz compiler. Create a rigorous MCQ test for "{topic}" using ONLY the material in the triple chevrons.

        Constraints:
        - Produce between {min_q} and {count} multiple-choice questions.
        - Each question MUST have 4 options (A–D). Distractors must be plausible from the notes.
        - For each question, return: "question" (string), "options" (array of 4 strings), "correct_index" (0..3), "explanation" (1–2 line reason grounded in notes).
        - Strictly avoid facts not supported by the notes. If unsure, prefer conceptual questions.
        - Return ONLY JSON with this exact schema: {{"topic": "...", "questions": [{{"question":"...","options":["...","...","...","..."],"correct_index": 0,"explanation":"..."}}]}}
        - Do not wrap in markdown fences or commentary.

        {"The notes excerpt was truncated for length; keep questions within available context." if truncated else ""}

        <<<NOTES>>>
        {excerpt}
        <<<END NOTES>>>
        """
    ).strip()

    genai.configure(api_key=GEMINI_API_KEY)
    model_name = GEMINI_NOTES_MODEL
    model = genai.GenerativeModel(model_name)
    generation_config = genai.GenerationConfig(
        response_mime_type="application/json",
        temperature=0.2,
        top_p=0.8,
        max_output_tokens=int(os.getenv("GEMINI_MCQ_MAX_TOKENS", "3600") or "3600"),
        candidate_count=1,
    )

    # Try to loosen safety gating for benign educational content.
    # Build safety settings using SDK enums when available; otherwise, disable this knob.
    safety_settings = None
    try:  # Newer SDKs expose these types
        from google.generativeai.types import HarmCategory, SafetySetting, HarmBlockThreshold  # type: ignore
        safety_settings = [
            SafetySetting(category=HarmCategory.HARM_CATEGORY_HARASSMENT, threshold=HarmBlockThreshold.BLOCK_NONE),
            SafetySetting(category=HarmCategory.HARM_CATEGORY_HATE_SPEECH, threshold=HarmBlockThreshold.BLOCK_NONE),
            SafetySetting(category=HarmCategory.HARM_CATEGORY_SEXUAL_CONTENT, threshold=HarmBlockThreshold.BLOCK_NONE),
            SafetySetting(category=HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT, threshold=HarmBlockThreshold.BLOCK_NONE),
        ]
    except Exception:
        safety_settings = None  # Fallback: rely on model defaults

    request_timeout = float(os.getenv("GEMINI_MCQ_TIMEOUT", "35"))
    payload = [{"text": prompt}]

    def _generate_safe(pl: List[Dict[str, str]]):
        """Call Gemini once, trying with safety_settings if available, and retrying without on compatibility errors."""
        try:
            if safety_settings is not None:
                return model.generate_content(
                    pl,
                    generation_config=generation_config,
                    safety_settings=safety_settings,  # may be unsupported on some SDKs
                    request_options={"timeout": request_timeout},
                )
            # No safety_settings available
            return model.generate_content(
                pl,
                generation_config=generation_config,
                request_options={"timeout": request_timeout},
            )
        except Exception as e:
            msg = str(e).lower()
            # Retry without safety_settings if we suspect compatibility or category errors
            if ("harm_category" in msg) or ("safety" in msg) or isinstance(e, (KeyError, TypeError, ValueError)):
                return model.generate_content(
                    pl,
                    generation_config=generation_config,
                    request_options={"timeout": request_timeout},
                )
            raise

    try:
        resp = _generate_safe(payload)
    except Exception as exc:
        raise HTTPException(status_code=502, detail=f"Gemini MCQ request failed: {exc}") from exc

    # Robustly extract text from response without assuming resp.text always exists
    raw: Optional[str] = None
    finish_reason: Optional[Any] = None
    try:
        # resp.text may raise ValueError if no Part was returned
        raw = resp.text  # type: ignore[attr-defined]
    except Exception:
        raw = None
    # Capture finish_reason if present
    try:
        if getattr(resp, "candidates", None):
            finish_reason = getattr(resp.candidates[0], "finish_reason", None)
    except Exception:
        finish_reason = None

    if not raw and getattr(resp, "candidates", None):  # try extracting from parts
        try:
            for cand in resp.candidates:
                content = getattr(cand, "content", None)
                parts = getattr(content, "parts", None) if content is not None else None
                if parts:
                    s = "".join(getattr(p, "text", "") for p in parts if hasattr(p, "text"))
                    if s and s.strip():
                        raw = s
                        break
        except Exception:
            raw = None

    if not raw:
        # Last resort: try dict form
        try:
            to_dict = getattr(resp, "to_dict", None)
            if to_dict:
                d = resp.to_dict()
                for cand in (d.get("candidates") or []):
                    parts = (((cand.get("content") or {}).get("parts")) or [])
                    s = "".join(p.get("text", "") for p in parts if isinstance(p, dict) and "text" in p)
                    if s and s.strip():
                        raw = s
                        break
        except Exception:
            raw = None

    if not raw:
        # If the first attempt produced no content, retry once with a more compact prompt
        retry_count = max(8, min(count, 8))
        compact_prompt = textwrap.dedent(
            f"""
            You are PaperX's quiz compiler. Create a rigorous MCQ test for "{topic}" using ONLY the material in the triple chevrons.

            Constraints (STRICT):
            - Return EXACTLY {retry_count} questions.
            - Each question MUST have 4 short options (A–D).
            - Explanation MUST be under 18 words and grounded in the notes.
            - Output MUST be strictly valid JSON only, no comments, no markdown fences, no trailing commas.
            - Schema: {{"topic": "...", "questions": [{{"question":"...","options":["...","...","...","..."],"correct_index": 0,"explanation":"..."}}]}}
            {"The notes excerpt was truncated for length; keep questions within available context." if truncated else ""}

            <<<NOTES>>>
            {excerpt}
            <<<END NOTES>>>
            """
        ).strip()
        try:
            resp_retry = _generate_safe([{"text": compact_prompt}])
            raw = getattr(resp_retry, "text", None)  # type: ignore[attr-defined]
            if not raw and getattr(resp_retry, "candidates", None):
                for cand in resp_retry.candidates:
                    content = getattr(cand, "content", None)
                    parts = getattr(content, "parts", None) if content is not None else None
                    if parts:
                        s = "".join(getattr(p, "text", "") for p in parts if hasattr(p, "text"))
                        if s and s.strip():
                            raw = s
                            break
        except Exception:
            raw = None

    if not raw:
        # Build a meaningful error message (safety/finish_reason info when available)
        msg = "Gemini returned no content for MCQ generation"
        try:
            pf = getattr(resp, "prompt_feedback", None)
            if pf and getattr(pf, "safety_ratings", None):
                ratings = []
                for r in pf.safety_ratings:
                    try:
                        ratings.append(f"{getattr(r, 'category', '?')}={getattr(r, 'probability', '?')}")
                    except Exception:
                        continue
                if ratings:
                    msg += f"; safety={', '.join(ratings)}"
        except Exception:
            pass
        if finish_reason is not None:
            msg += f"; finish_reason={finish_reason}"
        raise HTTPException(status_code=502, detail=msg)

    # --- Tolerant JSON parsing helpers ---
    def _strip_code_fences(txt: str) -> str:
        s = txt.strip()
        # Remove triple-backtick fences if present
        if s.startswith("```"):
            s = re.sub(r"^```[a-zA-Z0-9_-]*\s*", "", s)
            s = re.sub(r"\s*```$", "", s)
        return s.strip()

    def _extract_first_json_object(txt: str) -> Optional[str]:
        # Find first top-level {...} block using brace matching
        start = txt.find('{')
        if start == -1:
            return None
        depth = 0
        for i in range(start, len(txt)):
            c = txt[i]
            if c == '{':
                depth += 1
            elif c == '}':
                depth -= 1
                if depth == 0:
                    return txt[start:i+1]
        return None

    def _try_parse_mcq_payload(raw_text: str) -> Dict[str, Any]:
        s = _strip_code_fences(raw_text)
        # First attempt: direct JSON
        try:
            return json.loads(s)
        except Exception:
            pass
        # Second: extract first JSON object region
        block = _extract_first_json_object(s)
        if block:
            try:
                return json.loads(block)
            except Exception:
                pass
        # Third: python-literal fallback (handle 'true/false/null' and single quotes)
        t = s
        try:
            t = re.sub(r"\btrue\b", "True", t)
            t = re.sub(r"\bfalse\b", "False", t)
            t = re.sub(r"\bnull\b", "None", t)
            val = ast.literal_eval(t)
            if isinstance(val, dict):
                return val
        except Exception:
            pass
        # Give up
        raise ValueError("Unable to parse MCQ JSON")

    try:
        data = _try_parse_mcq_payload(raw)
    except Exception as exc_first:
        # Retry once with a more compact prompt and smaller count to avoid token/size truncation issues
        retry_count = max(8, min(count, 8))
        compact_prompt = textwrap.dedent(
            f"""
            You are PaperX's quiz compiler. Create a rigorous MCQ test for "{topic}" using ONLY the material in the triple chevrons.

            Constraints (STRICT):
            - Return EXACTLY {retry_count} questions.
            - Each question MUST have 4 short options (A–D).
            - Explanation MUST be under 18 words and grounded in the notes.
            - Output MUST be strictly valid JSON only, no comments, no markdown fences, no trailing commas.
            - Schema: {{"topic": "...", "questions": [{{"question":"...","options":["...","...","...","..."],"correct_index": 0,"explanation":"..."}}]}}
            {"The notes excerpt was truncated for length; keep questions within available context." if truncated else ""}

            <<<NOTES>>>
            {excerpt}
            <<<END NOTES>>>
            """
        ).strip()
        try:
            resp2 = model.generate_content(
                [{"text": compact_prompt}],
                generation_config=generation_config,
                request_options={"timeout": request_timeout},
            )
        except Exception as exc:
            # Give original parse error with context
            preview = (raw or "")[:480].replace("\n", " ")
            raise HTTPException(status_code=500, detail=f"Failed to parse MCQ JSON: {exc_first}; preview={preview}") from exc

        raw2: Optional[str] = None
        try:
            raw2 = resp2.text  # type: ignore[attr-defined]
        except Exception:
            raw2 = None
        if not raw2 and getattr(resp2, "candidates", None):
            try:
                for cand in resp2.candidates:
                    content = getattr(cand, "content", None)
                    parts = getattr(content, "parts", None) if content is not None else None
                    if parts:
                        s = "".join(getattr(p, "text", "") for p in parts if hasattr(p, "text"))
                        if s and s.strip():
                            raw2 = s
                            break
            except Exception:
                raw2 = None

        if not raw2:
            preview = (raw or "")[:480].replace("\n", " ")
            raise HTTPException(status_code=500, detail=f"Failed to parse MCQ JSON: {exc_first}; preview={preview}") from exc_first

        try:
            data = _try_parse_mcq_payload(raw2)
        except Exception as exc_second:
            preview1 = (raw or "")[:360].replace("\n", " ")
            preview2 = (raw2 or "")[:360].replace("\n", " ")
            raise HTTPException(status_code=500, detail=f"Failed to parse MCQ JSON after retry: {exc_second}; preview1={preview1}; preview2={preview2}") from exc_second

    qlist = data.get("questions")
    if not isinstance(qlist, list) or not qlist:
        raise HTTPException(status_code=500, detail="Gemini response missing 'questions' list.")

    out: List[Dict[str, Any]] = []
    for idx, item in enumerate(qlist[:count]):
        try:
            q = MCQQuestion.parse_obj(item)
        except Exception as exc:
            raise HTTPException(status_code=500, detail=f"Invalid MCQ at index {idx}: {exc}") from exc
        # Normalize options to exactly 4 entries when possible
        opts = list(q.options)[:4]
        if len(opts) < 4:
            # pad by repeating last safely
            while len(opts) < 4:
                opts.append(opts[-1] if opts else "")
        ci = int(q.correct_index)
        if ci < 0 or ci > 3:
            # clamp and ensure within bounds
            ci = max(0, min(ci, 3))
        # Enforce compact explanation length to reduce future overflows
        expl = (q.explanation or "").strip()
        if len(expl) > 200:
            expl = expl[:200].rstrip() + "…"
        out.append({
            "question": q.question.strip(),
            "options": [str(o).strip() for o in opts],
            "correct_index": ci,
            "explanation": expl,
        })

    return out, model_name, truncated


@notes_router.post("/notes/{note_id}/mcq")
@notes_router.post("/api/notes/{note_id}/mcq")
def api_generate_mcq(note_id: str, payload: Optional[Dict[str, Any]] = Body(default=None)):
    row, _v = db_get_ai_note_by_id_any(note_id)
    if not row:
        raise HTTPException(status_code=404, detail="Note not found")
    markdown = (row.get("markdown") or "").strip()
    if not markdown:
        raise HTTPException(status_code=400, detail="Note is empty; generate notes before requesting MCQs.")

    requested = 10
    topic_override = ""
    if isinstance(payload, dict):
        if "count" in payload:
            try:
                requested = int(payload["count"])  # type: ignore[index]
            except Exception:
                raise HTTPException(status_code=400, detail="count must be an integer")
        topic_override = str(payload.get("topic") or "").strip()

    inferred_topic = topic_override or row.get("title") or _derive_topic_from_markdown(markdown, fallback="MCQ Test")
    questions, used_model, truncated = _generate_mcq_with_gemini(markdown, inferred_topic, requested)
    return {
        "note_id": note_id,
        "topic": inferred_topic,
        "questions": questions,
        "count": len(questions),
        "model": used_model,
        "truncated": truncated,
        "generated_at": datetime.utcnow().isoformat() + "Z",
    }


@notes_router.get("/notes")
@notes_router.get("/api/notes")
def api_list_notes(variant: str = Query("detailed")):
    """List recent AI notes from DB (variant-specific)."""
    supabase = get_service_client()
    table = _table_for_variant(variant)
    try:
        res = supabase.table(table).select("id,title,created_at,updated_at").order("updated_at", desc=True).limit(50).execute()
        data = getattr(res, 'data', []) or []
        return {"items": data, "variant": _normalize_variant(variant)}
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to list notes: {e}")


@notes_router.get("/api/notes/topics/search", summary="Search AI notes topics for autocomplete")
def api_search_topics(
    q: str = Query("", min_length=0, max_length=120, description="Topic query string"),
    limit: int = Query(12, ge=1, le=50),
):
    """Return topic suggestions from DB as the user types.

    Uses Supabase service role client to query `ai_notes.title`.
    Results are ranked to prefer starts-with matches, then contains matches.
    """
    query = (q or "").strip()
    if not query:
        return {"query": "", "items": []}

    supabase = get_service_client()

    def _fetch_starts():
        return (
            supabase.table("ai_notes")
            .select("id,title")
            .ilike("title", f"{query}%")
            .order("title")
            .limit(limit)
            .execute()
        )

    def _fetch_contains():
        return (
            supabase.table("ai_notes")
            .select("id,title")
            .ilike("title", f"%{query}%")
            .order("title")
            .limit(max(50, limit * 4))
            .execute()
        )

    try:
        starts_res = _supabase_retry(_fetch_starts)
        if getattr(starts_res, "error", None):
            raise HTTPException(status_code=500, detail=f"Supabase error (topic starts): {starts_res.error}")

        contains_res = _supabase_retry(_fetch_contains)
        if getattr(contains_res, "error", None):
            raise HTTPException(status_code=500, detail=f"Supabase error (topic contains): {contains_res.error}")

        seen: Set[str] = set()
        items: List[Dict[str, Any]] = []

        def add_rows(rows: List[Dict[str, Any]]):
            for row in rows or []:
                topic = (row.get("title") or "").strip()
                if not topic:
                    continue
                key = topic.casefold()
                if key in seen:
                    continue
                seen.add(key)
                items.append({"id": row.get("id"), "topic": topic})
                if len(items) >= limit:
                    return

        add_rows(getattr(starts_res, "data", []) or [])
        if len(items) < limit:
            add_rows(getattr(contains_res, "data", []) or [])

        return {"query": query, "items": items}
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to search topics: {e}")


@notes_router.post("/notes")
@notes_router.post("/api/notes")
def api_create_note(payload: dict, variant: str = Query("detailed")):
    title = (payload or {}).get("topic", "").strip() or (payload or {}).get("title", "Untitled").strip() or "Untitled"
    markdown = (payload or {}).get("markdown", "")
    raw_images = (payload or {}).get("image_urls")
    image_urls = raw_images if isinstance(raw_images, list) else None
    row = db_upsert_ai_note_by_title_variant(title, markdown, variant=_normalize_variant(variant), image_urls=image_urls)
    return {
        "id": row.get("id"),
        "title": row.get("title"),
        "created_at": row.get("created_at"),
        "updated_at": row.get("updated_at"),
        "variant": _normalize_variant(variant),
        "image_urls": row.get("image_urls") or (image_urls or []),
    }


@notes_router.get("/notes/{note_id}")
@notes_router.get("/api/notes/{note_id}")
def api_read_note(note_id: str, variant: Optional[str] = Query(default=None)):
    row: Optional[Dict[str, Any]] = None
    used_variant: Optional[str] = None
    if variant:
        used_variant = _normalize_variant(variant)
        row = db_get_ai_note_by_id_variant(note_id, variant=used_variant)
    if not row:
        row, used_variant = db_get_ai_note_by_id_any(note_id)
    if not row:
        return JSONResponse({"error": "Not found"}, status_code=404)
    return {
        "id": row.get("id"),
        "title": row.get("title"),
        "markdown": row.get("markdown", ""),
        "updated_at": row.get("updated_at"),
        "variant": used_variant,
        "image_urls": row.get("image_urls") or [],
    }


@notes_router.put("/notes/{note_id}")
@notes_router.put("/api/notes/{note_id}")
def api_update_note(note_id: str, payload: dict, variant: Optional[str] = Query(default=None)):
    markdown = (payload or {}).get("markdown", "")
    raw_images = (payload or {}).get("image_urls")
    image_urls = raw_images if isinstance(raw_images, list) else None
    if variant:
        row = db_update_ai_note_markdown_variant(
            note_id,
            markdown,
            variant=_normalize_variant(variant),
            image_urls=image_urls,
        )
    else:
        found, found_variant = db_get_ai_note_by_id_any(note_id)
        if not found:
            raise HTTPException(status_code=404, detail="Not found")
        row = db_update_ai_note_markdown_variant(
            note_id,
            markdown,
            variant=found_variant,
            image_urls=image_urls,
        )
    return {
        "id": note_id,
        "updated_at": row.get("updated_at") if row else None,
        "image_urls": (row.get("image_urls") if row else None) or (image_urls or []),
    }


@notes_router.get("/notes/{note_id}/download")
@notes_router.get("/api/notes/{note_id}/download")
def api_download_note(note_id: str):
    row, _v = db_get_ai_note_by_id_any(note_id)
    if not row:
        return JSONResponse({"error": "Not found"}, status_code=404)
    content = row.get("markdown", "")
    filename = f"{note_id}.md"
    headers = {"Content-Disposition": f"attachment; filename={filename}"}
    return Response(content=content, media_type="text/markdown", headers=headers)


@notes_router.get("/notes/{note_id}/pdf")
@notes_router.get("/api/notes/{note_id}/pdf")
def api_note_pdf(note_id: str):
    row, _v = db_get_ai_note_by_id_any(note_id)
    if not row:
        return JSONResponse({"error": "Not found"}, status_code=404)
    md = row.get("markdown", "")
    try:
        pdf_bytes = render_pdf_from_markdown_via_headless(md, title=note_id)
    except Exception:
        try:
            pdf_bytes = render_pdf_from_markdown(md)
        except Exception as e:
            return JSONResponse({"error": f"PDF error: {e}"}, status_code=500)
    filename = f"{note_id}.pdf"
    headers = {"Content-Disposition": f"attachment; filename={filename}"}
    return Response(content=pdf_bytes, media_type="application/pdf", headers=headers)


# --- Per-user edited notes API ---

@notes_router.get("/api/notes/edited/check")
def api_check_user_edit(
    title: str = Query(..., min_length=1),
    variant: str = Query("detailed"),
    authorization: Optional[str] = Header(default=None),
):
    token = _parse_bearer_token(authorization)
    user_id = _require_auth_user_id(token)
    row = db_get_user_edit_by_title(user_id, title, variant=_normalize_variant(variant))
    if not row:
        return {"exists": False}
    return {
        "exists": True,
        "id": row.get("id"),
        "updated_at": row.get("updated_at"),
    }


@notes_router.get("/api/notes/edited")
def api_get_user_edit(
    title: str = Query(..., min_length=1),
    variant: str = Query("detailed"),
    authorization: Optional[str] = Header(default=None),
):
    token = _parse_bearer_token(authorization)
    user_id = _require_auth_user_id(token)
    row = db_get_user_edit_by_title(user_id, title, variant=_normalize_variant(variant))
    if not row:
        raise HTTPException(status_code=404, detail="Not found")
    return {
        "id": row.get("id"),
        "title": row.get("title"),
        "variant": row.get("variant"),
        "markdown": row.get("markdown", ""),
        "updated_at": row.get("updated_at"),
    }


@notes_router.post("/api/notes/edited")
def api_upsert_user_edit(
    payload: dict,
    variant: str = Query("detailed"),
    authorization: Optional[str] = Header(default=None),
):
    token = _parse_bearer_token(authorization)
    user_id = _require_auth_user_id(token)
    title = (payload or {}).get("title") or (payload or {}).get("topic") or "Untitled"
    markdown = (payload or {}).get("markdown") or ""
    if not str(title).strip():
        raise HTTPException(status_code=400, detail="Missing title")
    row = db_upsert_user_edit(user_id, str(title).strip(), markdown, variant=_normalize_variant(variant))
    return {
        "id": row.get("id"),
        "title": row.get("title"),
        "variant": row.get("variant"),
        "updated_at": row.get("updated_at"),
    }


@notes_router.post("/pdf")
@notes_router.post("/api/pdf")
def api_pdf_from_markdown(payload: dict):
    markdown = (payload or {}).get("markdown", "")
    title = (payload or {}).get("title", "notes").strip() or "notes"
    try:
        pdf_bytes = render_pdf_from_markdown_via_headless(markdown, title=title)
    except Exception:
        try:
            pdf_bytes = render_pdf_from_markdown(markdown)
        except Exception as e:
            return JSONResponse({"error": f"PDF error: {e}"}, status_code=500)
    safe = re.sub(r"[^a-zA-Z0-9_.-]+", "_", title)[:80]
    headers = {"Content-Disposition": f"attachment; filename={safe}.pdf"}
    return Response(content=pdf_bytes, media_type="application/pdf", headers=headers)


# --- Learning Tracks API ---


learning_tracks_router = APIRouter(prefix="/api/learning-tracks", tags=["learning tracks"])


def _upsert_learning_track_preferences(
    user_id: str,
    profile_id: str,
    payload: LearningTrackPlanRequest,
    companies: List[str],
) -> None:
    supabase = get_service_client()
    data = {
        "auth_user_id": user_id,
        "profile_id": profile_id,
        "language": payload.language,
        "stack": payload.stack,
        "goal": payload.goal,
        "companies": companies,
        "experience_level": payload.experience_level,
        "focus_areas": payload.focus_areas or [],
        "updated_at": datetime.utcnow().isoformat(),
    }
    res = (
        supabase.table(LEARNING_TRACK_GOALS_TABLE)
        .upsert(_supabase_payload(data), on_conflict="auth_user_id")
        .execute()
    )
    if getattr(res, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (learning goals): {res.error}")


def _upsert_learning_track_plan(
    user_id: str,
    profile_id: str,
    plan: Dict[str, Any],
    payload: LearningTrackPlanRequest,
    companies: List[str],
) -> None:
    supabase = get_service_client()
    now = datetime.utcnow().isoformat()
    data = {
        "plan_id": plan.get("plan_id"),
        "auth_user_id": user_id,
        "profile_id": profile_id,
        "language": plan.get("language"),
        "stack": plan.get("stack"),
        "goal": plan.get("goal"),
        "companies": companies,
        "experience_level": payload.experience_level,
        "focus_areas": payload.focus_areas or [],
        "generated_at": plan.get("generated_at"),
        "plan_json": plan,
        "updated_at": now,
    }
    res = (
        supabase.table(LEARNING_TRACK_PLANS_TABLE)
        .upsert(_supabase_payload(data), on_conflict="plan_id")
        .execute()
    )
    if getattr(res, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (learning plan): {res.error}")


def _upsert_learning_track_progress(
    user_id: str,
    profile_id: str,
    payload: "LearningTrackProgressUpdate",
) -> Dict[str, Any]:
    supabase = get_service_client()
    data = {
        "auth_user_id": user_id,
        "profile_id": profile_id,
        "plan_id": payload.plan_id,
        "topic_id": payload.topic_id,
        "status": payload.status,
        "score": payload.score,
        "updated_at": datetime.utcnow().isoformat(),
    }
    res = (
        supabase.table(LEARNING_TRACK_PROGRESS_TABLE)
        .upsert(
            _supabase_payload(data),
            on_conflict="auth_user_id,plan_id,topic_id",
            returning="representation",
        )
        .execute()
    )
    if getattr(res, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (learning progress): {res.error}")
    rows = getattr(res, "data", None) or []
    if rows:
        return rows[0]
    fetch = (
        supabase.table(LEARNING_TRACK_PROGRESS_TABLE)
        .select("auth_user_id,plan_id,topic_id,status,score,updated_at")
        .eq("auth_user_id", user_id)
        .eq("plan_id", payload.plan_id)
        .eq("topic_id", payload.topic_id)
        .limit(1)
        .execute()
    )
    if getattr(fetch, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (fetch learning progress): {fetch.error}")
    return (fetch.data or [data])[0]


def _fetch_latest_learning_plan(user_id: str) -> Optional[Dict[str, Any]]:
    supabase = get_service_client()
    res = (
        supabase.table(LEARNING_TRACK_PLANS_TABLE)
        .select(
            "plan_id, language, stack, goal, companies, experience_level, focus_areas, plan_json, updated_at, created_at"
        )
        .eq("auth_user_id", user_id)
        .order("updated_at", desc=True)
        .limit(1)
        .execute()
    )
    if getattr(res, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (fetch learning plan): {res.error}")
    data = getattr(res, "data", []) or []
    return data[0] if data else None


def _fetch_learning_goal(user_id: str) -> Optional[Dict[str, Any]]:
    supabase = get_service_client()
    res = (
        supabase.table(LEARNING_TRACK_GOALS_TABLE)
        .select("language, stack, goal, companies, experience_level, focus_areas, updated_at, created_at")
        .eq("auth_user_id", user_id)
        .order("updated_at", desc=True)
        .limit(1)
        .execute()
    )
    if getattr(res, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (fetch learning goal): {res.error}")
    data = getattr(res, "data", []) or []
    return data[0] if data else None


class LearningTrackPlanRequest(BaseModel):
    language: str = Field(..., min_length=2, max_length=16)
    stack: str = Field(..., min_length=2, max_length=64)
    goal: str = Field(..., min_length=2, max_length=64)
    companies: List[str] = Field(default_factory=list)
    experience_level: Optional[str] = Field(None, max_length=64)
    focus_areas: Optional[List[str]] = Field(default=None)


class LearningTrackPracticeRequest(BaseModel):
    topic_title: str = Field(..., min_length=2, max_length=256)
    language: str = Field(..., min_length=2, max_length=16)
    stack: str = Field(..., min_length=2, max_length=64)
    goal: str = Field(..., min_length=2, max_length=64)
    companies: List[str] = Field(default_factory=list)
    context: Optional[str] = Field(default=None, max_length=4000)


class LearningTrackMockRequest(BaseModel):
    stack: str = Field(..., min_length=2, max_length=64)
    goal: str = Field(..., min_length=2, max_length=64)
    companies: List[str] = Field(default_factory=list)
    focus_round: Optional[str] = Field(default=None, max_length=64)
    language: str = Field(..., min_length=2, max_length=16)
    recent_topics: Optional[List[str]] = Field(default=None)


class LearningTrackAnalyticsRequest(BaseModel):
    language: str = Field(..., min_length=2, max_length=16)
    stack: str = Field(..., min_length=2, max_length=64)
    goal: str = Field(..., min_length=2, max_length=64)
    companies: List[str] = Field(default_factory=list)
    metrics: Dict[str, Any]
    highlights: Optional[List[str]] = Field(default=None)
    blockers: Optional[List[str]] = Field(default=None)


class LearningTrackFacultyBriefRequest(BaseModel):
    language: str = Field(..., min_length=2, max_length=16)
    stack: str = Field(..., min_length=2, max_length=64)
    goal: str = Field(..., min_length=2, max_length=64)
    cohort_name: Optional[str] = Field(default=None, max_length=128)
    companies: List[str] = Field(default_factory=list)
    plan_outline: Dict[str, Any]
    progress_snapshot: Optional[Dict[str, Any]] = None


class LearningTrackCodeExecuteRequest(BaseModel):
    language_id: str = Field(..., min_length=1, max_length=32)
    source: str = Field(..., min_length=1, max_length=5000)
    stdin: Optional[str] = Field(default="", max_length=2000)


class LearningTrackCodeExplainRequest(BaseModel):
    language: str = Field(..., min_length=2, max_length=32)
    stack: str = Field(..., min_length=2, max_length=64)
    goal: str = Field(..., min_length=2, max_length=64)
    companies: List[str] = Field(default_factory=list)
    code: str = Field(..., min_length=1, max_length=6000)
    question: Optional[str] = Field(default=None, max_length=1000)
    language_preference: Optional[str] = Field(default=None, max_length=16)


class LearningTrackProgressUpdate(BaseModel):
    plan_id: str = Field(..., min_length=6, max_length=64)
    topic_id: str = Field(..., min_length=4, max_length=64)
    status: Literal["not_started", "in_progress", "completed"]
    score: Optional[float] = Field(default=None)


def _coerce_float(value: Any) -> Optional[float]:
    try:
        if value is None:
            return None
        return float(value)
    except Exception:
        return None


def _ensure_list_of_strings(value: Any) -> List[str]:
    if isinstance(value, list):
        out = []
        for item in value:
            text = str(item).strip()
            if text:
                out.append(text)
        return out
    if isinstance(value, str):
        stripped = value.strip()
        return [stripped] if stripped else []
    return []


def _normalize_activity_list(value: Any) -> List[Dict[str, Any]]:
    activities: List[Dict[str, Any]] = []
    if not isinstance(value, list):
        return activities
    for entry in value:
        if not isinstance(entry, dict):
            continue
        title = str(entry.get("title") or entry.get("name") or "").strip()
        if not title:
            continue
        activity = {
            "title": title,
            "type": str(entry.get("type") or entry.get("kind") or "activity").strip() or "activity",
            "description": (str(entry.get("description") or entry.get("summary") or "").strip() or None),
            "url": (str(entry.get("url") or entry.get("link") or "").strip() or None),
            "source": (str(entry.get("source") or "").strip() or None),
            "difficulty": (str(entry.get("difficulty") or entry.get("level") or "").strip() or None),
        }
        activities.append(activity)
    return activities


def _extract_json_payload(text: str) -> Dict[str, Any]:
    candidate = (text or "").strip()
    if not candidate:
        raise ValueError("Empty response")
    try:
        return json.loads(candidate)
    except json.JSONDecodeError:
        pass
    match = re.search(r"\{[\s\S]*\}", candidate)
    if match:
        try:
            return json.loads(match.group(0))
        except Exception:
            pass
    match = re.search(r"```json\s*([\s\S]*?)```", candidate)
    if match:
        snippet = match.group(1).strip()
        try:
            return json.loads(snippet)
        except Exception:
            pass
    raise ValueError("Failed to parse JSON from model response")


def _new_id(prefix: str) -> str:
    return f"{prefix}_{uuid.uuid4().hex[:12]}"


def _normalize_modules(raw_modules: Any, payload: LearningTrackPlanRequest) -> List[Dict[str, Any]]:
    modules: List[Dict[str, Any]] = []
    if not isinstance(raw_modules, list):
        return modules
    for idx, raw_module in enumerate(raw_modules):
        if not isinstance(raw_module, dict):
            continue
        m_title = str(raw_module.get("title") or raw_module.get("name") or f"Module {idx + 1}").strip()
        m_desc = str(raw_module.get("description") or raw_module.get("summary") or "").strip()
        module_id = _new_id("module")
        topics_data = raw_module.get("topics") or []
        module_topics: List[Dict[str, Any]] = []
        if isinstance(topics_data, list):
            for t_idx, raw_topic in enumerate(topics_data):
                if not isinstance(raw_topic, dict):
                    continue
                t_title = str(raw_topic.get("title") or raw_topic.get("name") or f"Topic {t_idx + 1}").strip()
                if not t_title:
                    continue
                topic_id = _new_id("topic")
                t_desc = str(raw_topic.get("description") or raw_topic.get("summary") or "").strip()
                objectives = _ensure_list_of_strings(raw_topic.get("objectives"))
                practice_items = _normalize_activity_list(raw_topic.get("practice"))
                activities = _normalize_activity_list(raw_topic.get("activities"))
                company_focus = _ensure_list_of_strings(raw_topic.get("company_focus")) or payload.companies
                references = _normalize_activity_list(raw_topic.get("references"))
                module_topics.append({
                    "topic_id": topic_id,
                    "title": t_title,
                    "description": t_desc,
                    "objectives": objectives,
                    "activities": activities,
                    "practice": practice_items,
                    "company_focus": company_focus,
                    "references": references,
                    "notes": [],
                })
        modules.append({
            "module_id": module_id,
            "title": m_title,
            "description": m_desc,
            "duration_hours": _coerce_float(raw_module.get("duration_hours")),
            "topics": module_topics,
        })
    return modules


def _collect_topic_notes(topic_title: str, stack: str, config: Dict[str, Any], *, limit: int = 3) -> List[Dict[str, Any]]:
    if not SERPAPI_API_KEY:
        raise HTTPException(status_code=501, detail="SerpAPI key not configured. Set SERPAPI_API_KEY.")
    query = f"{topic_title} {stack} tutorial"
    urls = serpapi_search(query, num=limit, allowed_domains=config.get("allowed_domains"))
    extracts: List[Dict[str, Any]] = []
    for url in urls[:limit]:
        try:
            html = fetch(url)
            page = extract_sections_from_html(url, html)
            sections: List[Dict[str, Any]] = []
            for section in page.sections[:4]:
                sections.append({
                    "heading": section.title,
                    "summary": section.text[:600],
                })
            extracts.append({
                "url": page.url,
                "title": page.title,
                "sections": sections,
            })
        except Exception as exc:
            learning_logger.warning("Learning track notes fetch failed", extra={"topic": topic_title, "url": url, "error": str(exc)})
    return extracts


def _run_learning_agent(agent_name: str, system_message: str, user_payload: Dict[str, Any]) -> Dict[str, Any]:
    assistant = AssistantAgent(
        agent_name,
        model_client=gemini_model_client,
        system_message=system_message,
    )
    result = _run_assistant_blocking(assistant, json.dumps(user_payload))
    if not result or not result.messages:
        raise HTTPException(status_code=502, detail="Model did not return a response")
    try:
        return _extract_json_payload(result.messages[-1].content)
    except ValueError as exc:
        raise HTTPException(status_code=502, detail=str(exc)) from exc


LEARNING_TRACK_PLANNER_SYSTEM = """You are PaperX's Learning Track planner. Respond ONLY with compact JSON.
Fields: plan_title (string), overview (string), modules (list of objects with keys title, description, duration_hours, topics).
Each topic is an object with keys title, description, objectives (list of strings), activities (list of objects title/type/description/url/source),
practice (list of objects title/type/description/url/source/difficulty), company_focus (list of strings), references (list of objects title/url/source/type).
Align content with Indian college learners preparing for company interviews and academic goals. Keep JSON under 6000 chars.
"""


LEARNING_TRACK_PRACTICE_SYSTEM = """You generate MCQs and flashcards for PaperX learners. Respond ONLY with JSON with keys mcqs (list) and flashcards (list).
Each mcq object: question, options (list of 4), answer, explanation, difficulty, source.
Each flashcard object: front, back, mnemonic, company_hint.
Localize the text when language != 'en'.
"""


LEARNING_TRACK_MOCK_SYSTEM = """You design mock interview drills for PaperX. Respond with JSON keys: warmup_questions, coding_round, system_design, behavioral.
Each should be a list of question objects {title, prompt, rubric, difficulty, estimated_minutes}.
"""


LEARNING_TRACK_ANALYTICS_SYSTEM = """You are an analytics coach. Return JSON with keys summary (string), wins (list of strings), risks (list of strings), recommendations (list of strings), scorecard (object with keys velocity, mastery, retention each 0-100).
Use the provided metrics to ground your analysis.
"""


LEARNING_TRACK_FACULTY_SYSTEM = """You brief faculty coordinators. Return JSON with keys overview (string), action_items (list of strings), at_risk_learners (list), support_requests (list of strings), upcoming_milestones (list of strings).
"""


def _validate_choice(value: str, allowed: List[str], label: str) -> str:
    if value not in allowed:
        raise HTTPException(status_code=400, detail=f"Unsupported {label}: {value}")
    return value


def _validate_companies(companies: List[str], allowed: List[str]) -> List[str]:
    valid = []
    for company in companies:
        c = company.strip()
        if c and c in allowed:
            valid.append(c)
    if not valid:
        return allowed[:2] if allowed else []
    return valid


def _map_language_runtime(language_id: str, config: Dict[str, Any]) -> Dict[str, str]:
    for entry in config.get("compiler_languages", []):
        if entry.get("id") == language_id:
            return entry
    raise HTTPException(status_code=400, detail=f"Unsupported compiler language: {language_id}")


@learning_tracks_router.get("/config")
def api_learning_tracks_config():
    config = load_learning_tracks_config()
    return {
        "languages": config["languages"],
        "stacks": config["stacks"],
        "goals": config["goals"],
        "companies": config["companies"],
        "allowed_domains": config["allowed_domains"],
        "planner_model": config["planner_model"],
        "notes_model": config["notes_model"],
        "mcq_model": config["mcq_model"],
        "flashcard_model": config["flashcard_model"],
        "code_explainer_model": config["code_explainer_model"],
        "compiler_languages": config["compiler_languages"],
    }


@learning_tracks_router.get("/plan/latest")
def api_learning_tracks_latest_plan(authorization: Optional[str] = Header(default=None)):
    token = _bearer_token_from_header(authorization)
    user_id, profile_id = _require_user_and_profile(token)
    plan_row = _fetch_latest_learning_plan(user_id)
    goal_row = _fetch_learning_goal(user_id)

    goal_payload: Optional[Dict[str, Any]] = None
    if goal_row:
        goal_payload = {
            "language": goal_row.get("language"),
            "stack": goal_row.get("stack"),
            "goal": goal_row.get("goal"),
            "companies": goal_row.get("companies") or [],
            "experience_level": goal_row.get("experience_level"),
            "focus_areas": goal_row.get("focus_areas") or [],
            "updated_at": goal_row.get("updated_at"),
        }

    if not plan_row:
        return {
            "plan": None,
            "plan_id": None,
            "filters": None,
            "goals": goal_payload,
        }

    raw_plan = plan_row.get("plan_json") or {}
    plan_json = dict(raw_plan) if isinstance(raw_plan, dict) else raw_plan
    if isinstance(plan_json, dict):
        plan_json.setdefault("plan_id", plan_row.get("plan_id"))
        plan_json.setdefault("language", plan_row.get("language"))
        plan_json.setdefault("stack", plan_row.get("stack"))
        plan_json.setdefault("goal", plan_row.get("goal"))
        if "companies" not in plan_json:
            plan_json["companies"] = plan_row.get("companies") or []

    filters = {
        "language": plan_row.get("language"),
        "stack": plan_row.get("stack"),
        "goal": plan_row.get("goal"),
        "companies": plan_row.get("companies") or [],
        "experience_level": plan_row.get("experience_level"),
        "focus_areas": plan_row.get("focus_areas") or [],
    }

    return {
        "plan_id": plan_row.get("plan_id"),
        "plan": plan_json,
        "filters": filters,
        "goals": goal_payload,
        "updated_at": plan_row.get("updated_at"),
    }


@learning_tracks_router.post("/path")
def api_learning_tracks_plan(payload: LearningTrackPlanRequest, authorization: Optional[str] = Header(default=None)):
    if not GEMINI_API_KEY:
        raise HTTPException(status_code=501, detail="Gemini API key not configured. Set GEMINI_API_KEY.")
    token = _bearer_token_from_header(authorization)
    user_id, profile_id = _require_user_and_profile(token)
    config = load_learning_tracks_config()
    language = _validate_choice(payload.language, config["languages"], "language")
    stack = _validate_choice(payload.stack, config["stacks"], "stack")
    goal = _validate_choice(payload.goal, config["goals"], "goal")
    companies = _validate_companies(payload.companies, config["companies"])

    planner_payload = {
        "language": language,
        "stack": stack,
        "goal": goal,
        "companies": companies,
        "experience_level": payload.experience_level,
        "focus_areas": payload.focus_areas or [],
        "model": config["planner_model"],
    }
    learning_logger.info("Learning track plan requested", extra={"stack": stack, "goal": goal, "companies": companies})
    try:
        plan_raw = _run_learning_agent("paperx_learning_planner", LEARNING_TRACK_PLANNER_SYSTEM, planner_payload)
    except HTTPException:
        raise
    except Exception as exc:
        raise HTTPException(status_code=502, detail=f"Planner model error: {exc}") from exc

    modules = _normalize_modules(plan_raw.get("modules"), payload)
    for module in modules:
        for topic in module.get("topics", []):
            topic["notes"] = _collect_topic_notes(topic["title"], stack, config)

    plan = {
        "plan_id": str(uuid.uuid4()),
        "generated_at": datetime.utcnow().isoformat() + "Z",
        "language": language,
        "stack": stack,
        "goal": goal,
        "companies": companies,
        "plan_title": plan_raw.get("plan_title") or f"{stack.title()} Track",
        "overview": plan_raw.get("overview") or "",
        "modules": modules,
        "planner_model": config["planner_model"],
        "serp_allowed_domains": config["allowed_domains"],
    }
    _upsert_learning_track_preferences(user_id, profile_id, payload, companies)
    _upsert_learning_track_plan(user_id, profile_id, plan, payload, companies)
    return plan


@learning_tracks_router.post("/topic/practice")
def api_learning_track_practice(payload: LearningTrackPracticeRequest):
    if not GEMINI_API_KEY:
        raise HTTPException(status_code=501, detail="Gemini API key not configured. Set GEMINI_API_KEY.")
    config = load_learning_tracks_config()
    language = _validate_choice(payload.language, config["languages"], "language")
    stack = _validate_choice(payload.stack, config["stacks"], "stack")
    goal = _validate_choice(payload.goal, config["goals"], "goal")
    companies = _validate_companies(payload.companies, config["companies"])

    practice_payload = {
        "topic": payload.topic_title,
        "language": language,
        "stack": stack,
        "goal": goal,
        "companies": companies,
        "context": (payload.context or "")[:3000],
        "model": config["mcq_model"],
    }
    try:
        practice = _run_learning_agent("paperx_learning_practice", LEARNING_TRACK_PRACTICE_SYSTEM, practice_payload)
    except HTTPException:
        raise
    except Exception as exc:
        raise HTTPException(status_code=502, detail=f"Practice model error: {exc}") from exc
    return practice


@learning_tracks_router.post("/mock")
def api_learning_track_mock(payload: LearningTrackMockRequest):
    if not GEMINI_API_KEY:
        raise HTTPException(status_code=501, detail="Gemini API key not configured. Set GEMINI_API_KEY.")
    config = load_learning_tracks_config()
    language = _validate_choice(payload.language, config["languages"], "language")
    stack = _validate_choice(payload.stack, config["stacks"], "stack")
    goal = _validate_choice(payload.goal, config["goals"], "goal")
    companies = _validate_companies(payload.companies, config["companies"])

    mock_payload = {
        "stack": stack,
        "goal": goal,
        "companies": companies,
        "focus_round": payload.focus_round or "",
        "language": language,
        "recent_topics": payload.recent_topics or [],
    }
    try:
        plan = _run_learning_agent("paperx_learning_mock", LEARNING_TRACK_MOCK_SYSTEM, mock_payload)
    except HTTPException:
        raise
    except Exception as exc:
        raise HTTPException(status_code=502, detail=f"Mock planner error: {exc}") from exc
    return plan


@learning_tracks_router.post("/analytics")
def api_learning_track_analytics(payload: LearningTrackAnalyticsRequest):
    if not GEMINI_API_KEY:
        raise HTTPException(status_code=501, detail="Gemini API key not configured. Set GEMINI_API_KEY.")
    config = load_learning_tracks_config()
    language = _validate_choice(payload.language, config["languages"], "language")
    stack = _validate_choice(payload.stack, config["stacks"], "stack")
    goal = _validate_choice(payload.goal, config["goals"], "goal")
    companies = _validate_companies(payload.companies, config["companies"])

    analytics_payload = {
        "language": language,
        "stack": stack,
        "goal": goal,
        "companies": companies,
        "metrics": payload.metrics,
        "highlights": payload.highlights or [],
        "blockers": payload.blockers or [],
    }
    try:
        analysis = _run_learning_agent("paperx_learning_analytics", LEARNING_TRACK_ANALYTICS_SYSTEM, analytics_payload)
    except HTTPException:
        raise
    except Exception as exc:
        raise HTTPException(status_code=502, detail=f"Analytics model error: {exc}") from exc
    return analysis


@learning_tracks_router.post("/faculty-brief")
def api_learning_track_faculty_brief(payload: LearningTrackFacultyBriefRequest):
    if not GEMINI_API_KEY:
        raise HTTPException(status_code=501, detail="Gemini API key not configured. Set GEMINI_API_KEY.")
    config = load_learning_tracks_config()
    language = _validate_choice(payload.language, config["languages"], "language")
    stack = _validate_choice(payload.stack, config["stacks"], "stack")
    goal = _validate_choice(payload.goal, config["goals"], "goal")
    companies = _validate_companies(payload.companies, config["companies"])

    brief_payload = {
        "language": language,
        "stack": stack,
        "goal": goal,
        "companies": companies,
        "cohort_name": payload.cohort_name or "",
        "plan_outline": payload.plan_outline,
        "progress_snapshot": payload.progress_snapshot or {},
    }
    try:
        brief = _run_learning_agent("paperx_learning_faculty", LEARNING_TRACK_FACULTY_SYSTEM, brief_payload)
    except HTTPException:
        raise
    except Exception as exc:
        raise HTTPException(status_code=502, detail=f"Faculty brief error: {exc}") from exc
    return brief


@learning_tracks_router.post("/code/execute")
def api_learning_track_execute(payload: LearningTrackCodeExecuteRequest):
    config = load_learning_tracks_config()
    language_entry = _map_language_runtime(payload.language_id, config)
    request_payload = {
        "language": language_entry["runtime"],
        "source": payload.source,
        "stdin": payload.stdin or "",
    }
    try:
        resp = requests.post(
            "https://emkc.org/api/v2/piston/execute",
            json=request_payload,
            timeout=25,
        )
    except Exception as exc:
        raise HTTPException(status_code=502, detail=f"Execution service error: {exc}") from exc
    if resp.status_code >= 400:
        raise HTTPException(status_code=resp.status_code, detail=resp.text)
    data = resp.json()
    return {
        "language": language_entry["id"],
        "runtime": language_entry["runtime"],
        "output": data.get("output") or data.get("stdout") or "",
        "stderr": data.get("stderr") or "",
        "exit_code": data.get("code") or data.get("exitCode"),
    }


@learning_tracks_router.post("/code/explain")
def api_learning_track_explain(payload: LearningTrackCodeExplainRequest):
    if not GEMINI_API_KEY:
        raise HTTPException(status_code=501, detail="Gemini API key not configured. Set GEMINI_API_KEY.")
    config = load_learning_tracks_config()
    language = _validate_choice(payload.language, config["languages"], "language")
    stack = _validate_choice(payload.stack, config["stacks"], "stack")
    goal = _validate_choice(payload.goal, config["goals"], "goal")
    companies = _validate_companies(payload.companies, config["companies"])

    explain_payload = {
        "language": language,
        "stack": stack,
        "goal": goal,
        "companies": companies,
        "code": payload.code,
        "question": payload.question or "",
        "model": config["code_explainer_model"],
        "language_preference": payload.language_preference or language,
    }
    system_message = "You explain source code to PaperX learners clearly. Respond with JSON {summary, complexity, suggestions (list), localized_explanation}."
    try:
        explanation = _run_learning_agent("paperx_learning_code", system_message, explain_payload)
    except HTTPException:
        raise
    except Exception as exc:
        raise HTTPException(status_code=502, detail=f"Code explanation error: {exc}") from exc
    return explanation


@learning_tracks_router.post("/progress")
def api_learning_track_progress_update(
    payload: LearningTrackProgressUpdate, authorization: Optional[str] = Header(default=None)
):
    token = _bearer_token_from_header(authorization)
    user_id, profile_id = _require_user_and_profile(token)
    record = _upsert_learning_track_progress(user_id, profile_id, payload)
    return {
        "plan_id": record.get("plan_id"),
        "topic_id": record.get("topic_id"),
        "status": record.get("status"),
        "score": record.get("score"),
        "updated_at": record.get("updated_at"),
    }


# --- YouTube transcript endpoints ---

youtube_transcript_router = APIRouter(prefix="/api/transcripts", tags=["youtube transcripts"])


def _structured_notes_from_transcript(transcript: str, *, title: str, lang: Optional[str]) -> Tuple[str, bool]:
    clean_text = (transcript or "").strip()
    if not clean_text:
        raise HTTPException(status_code=400, detail="Transcript text is empty.")

    if not GEMINI_API_KEY:
        raise HTTPException(
            status_code=501,
            detail="Gemini API key not configured. Set GEMINI_API_KEY to enable structured notes.",
        )

    try:
        import google.generativeai as genai  # type: ignore
    except Exception as exc:  # pragma: no cover - optional dependency
        raise HTTPException(
            status_code=501,
            detail=f"Gemini client library missing: {exc}. Install google-generativeai to enable this feature.",
        ) from exc

    max_chars = max(4000, MAX_TRANSCRIPT_CHARS_FOR_NOTES)
    truncated = False
    if len(clean_text) > max_chars:
        clean_text = clean_text[:max_chars]
        truncated = True

    notes_prompt = textwrap.dedent(
        """
        You are PaperX's academic note composer. Transform the provided YouTube transcript into polished,
        exam-ready study notes written in Markdown.

        Output requirements:
        - Start with a single H1 title using the video name.
        - Include these H2 sections, even if you must acknowledge limited information:
          1. TL;DR (3-6 terse bullet points)
          2. Key Takeaways (bullets)
          3. Detailed Notes (use subsections or numbered steps when flow suggests)
          4. Examples & Analogies (bullets; add [not mentioned] if absent)
          5. Frameworks / Processes (tables or lists; include Mermaid diagrams when explaining flows)
          6. Glossary (term â€“ short definition table or list)
          7. Reflection Questions
          8. Action Items or Next Steps
          9. Further Reading / References (recommend logical follow ups; mark [none] if unavailable)
        - Bold critical vocabulary and formulas.
        - Prefer Markdown tables where comparing items.
        - If the transcript seems partial, state that in TL;DR and continue with available context.
        - Keep the tone clear, modern, and supportive for self-study.
        """
    ).strip()

    context_notice = ""
    if truncated:
        context_notice = (
            f"NOTE: Only the first {max_chars} characters of the transcript were available. "
            "Flag the notes as partial if key sections appear missing."
        )

    genai.configure(api_key=GEMINI_API_KEY)
    model = genai.GenerativeModel(GEMINI_NOTES_MODEL)
    payload = [
        {"text": notes_prompt},
        {
            "text": textwrap.dedent(
                f"""
                Video title: {title or 'Unknown YouTube Video'}
                Preferred output language: {lang or 'en'}
                {context_notice}

                Transcript:
                {clean_text}
                """
            ).strip()
        },
    ]

    try:
        response = model.generate_content(payload)
    except Exception as exc:
        raise HTTPException(
            status_code=502,
            detail=f"Gemini notes request failed: {exc}",
        ) from exc

    generated = getattr(response, "text", "") or ""
    if not generated:
        try:
            generated = response.candidates[0].content.parts[0].text  # type: ignore[index]
        except Exception:
            generated = ""

    generated = (generated or "").strip()
    if not generated:
        raise HTTPException(status_code=502, detail="Gemini did not return any content.")

    return generated, truncated


class YouTubeTranscriptRequest(BaseModel):
    url: HttpUrl
    lang: Optional[str] = "en"
    fallback_ytdlp: Optional[bool] = True
    use_whisper: Optional[bool] = False
    clean: Optional[bool] = True


class YouTubeTranscriptResponse(BaseModel):
    paragraph: str
    source: str


class YouTubeTranscriptNotesRequest(BaseModel):
    url: HttpUrl
    lang: Optional[str] = "en"
    fallback_ytdlp: Optional[bool] = True
    clean: Optional[bool] = True


class YouTubeTranscriptNotesResponse(BaseModel):
    notes_markdown: str
    model: str
    truncated: bool
    transcript_chars: int


class YouTubeMetaRequest(BaseModel):
    url: HttpUrl


class YouTubeMetaResponse(BaseModel):
    video_id: str
    embed_url: str
    channel_name: Optional[str] = None
    upload_date: Optional[str] = None  # ISO date string (YYYY-MM-DD) when available
    views: Optional[int] = None
    channel_url: Optional[str] = None
    channel_logo: Optional[str] = None


def _normalize_video_key(url: str) -> str:
    try:
        vid = extract_video_id(str(url))
        if vid:
            return vid
    except HTTPException:
        pass
    return str(url).strip()


@lru_cache(maxsize=512)
def _cached_youtube_meta(video_key: str) -> Dict[str, Any]:
    if YoutubeDL is None:  # pragma: no cover - optional dependency missing
        raise HTTPException(status_code=501, detail="yt-dlp is not available on the server.")

    ydl_opts = {
        "skip_download": True,
        "quiet": True,
        "no_warnings": True,
        "extract_flat": False,  # get full metadata for a single video
    }

    target_url = video_key
    if re.fullmatch(r"[A-Za-z0-9_-]{11}", video_key):
        target_url = f"https://www.youtube.com/watch?v={video_key}"

    with YoutubeDL(ydl_opts) as ydl:
        try:
            info = ydl.extract_info(str(target_url), download=False)
        except Exception as exc:
            raise HTTPException(status_code=400, detail=f"Could not fetch metadata: {exc}") from exc

    vid = str(info.get("id") or "").strip()
    if not vid:
        # Fallback to parser if present
        try:
            vid = extract_video_id(str(target_url))
        except HTTPException:
            pass
    if not vid:
        raise HTTPException(status_code=400, detail="Could not determine YouTube video ID from URL.")

    # Prefer channel name field, fallback to uploader
    channel_name = info.get("channel") or info.get("uploader") or None
    channel_url = info.get("channel_url") or info.get("uploader_url") or None

    # upload_date comes as YYYYMMDD; convert to YYYY-MM-DD if present
    up_raw = info.get("upload_date") or ""
    upload_date = None
    if isinstance(up_raw, str) and len(up_raw) == 8 and up_raw.isdigit():
        upload_date = f"{up_raw[0:4]}-{up_raw[4:6]}-{up_raw[6:8]}"

    views = info.get("view_count")
    try:
        views = int(views) if views is not None else None
    except Exception:
        views = None

    out = {
        "video_id": vid,
        "embed_url": f"https://www.youtube.com/embed/{vid}",
        "channel_name": channel_name,
        "upload_date": upload_date,
        "views": views,
    }
    # Best-effort channel logo resolution when channel_url is available
    try:
        if channel_url:
            logo = get_channel_logo(channel_url) or get_default_channel_logo()
            out["channel_url"] = channel_url
            if logo:
                out["channel_logo"] = logo
    except Exception:
        # Non-fatal; just omit logo on errors
        out["channel_url"] = channel_url
    return out


def _extract_youtube_meta(url: str) -> YouTubeMetaResponse:
    """Extract basic metadata for a YouTube video using yt-dlp without downloading.

    Returns video_id, embed_url, channel_name, upload_date (YYYY-MM-DD), and views.
    """
    video_key = _normalize_video_key(url)
    meta = _cached_youtube_meta(video_key)
    return YouTubeMetaResponse(**meta)


@youtube_transcript_router.post("/paragraph", response_model=YouTubeTranscriptResponse)
def api_transcript_paragraph(payload: YouTubeTranscriptRequest) -> YouTubeTranscriptResponse:
    text = fetch_transcript_paragraph(
        url_or_id=str(payload.url),
        lang=payload.lang or "en",
        fallback_ytdlp=bool(payload.fallback_ytdlp),
        use_whisper=bool(payload.use_whisper),
        clean=bool(payload.clean),
    ).strip()
    if not text:
        raise HTTPException(status_code=404, detail="Transcript is empty.")
    return YouTubeTranscriptResponse(paragraph=text, source=str(payload.url))


@youtube_transcript_router.post("/meta", response_model=YouTubeMetaResponse)
def api_youtube_meta(payload: YouTubeMetaRequest) -> YouTubeMetaResponse:
    """Return basic metadata (id, channel, upload date, views) for the given YouTube URL."""
    return _extract_youtube_meta(str(payload.url))


@youtube_transcript_router.post("/notes", response_model=YouTubeTranscriptNotesResponse)
def api_transcript_notes(payload: YouTubeTranscriptNotesRequest) -> YouTubeTranscriptNotesResponse:
    transcript_text = fetch_transcript_paragraph(
        url_or_id=str(payload.url),
        lang=payload.lang or "en",
        fallback_ytdlp=bool(payload.fallback_ytdlp),
        use_whisper=False,
        clean=bool(payload.clean),
    ).strip()
    if not transcript_text:
        raise HTTPException(status_code=404, detail="Transcript is empty.")

    try:
        video_id = extract_video_id(str(payload.url))
    except HTTPException:
        video_id = None
    video_title = f"YouTube Video {video_id}" if video_id else "YouTube Video"

    notes_markdown, truncated = _structured_notes_from_transcript(
        transcript_text,
        title=video_title,
        lang=payload.lang,
    )
    result = YouTubeTranscriptNotesResponse(
        notes_markdown=notes_markdown,
        model=GEMINI_NOTES_MODEL,
        truncated=truncated,
        transcript_chars=len(transcript_text),
    )

    # Best-effort: save generated notes for this video to Supabase for caching.
    try:
        if video_id:
            supabase = get_service_client()
            record = _supabase_payload({
                "video_id": video_id,
                "video_url": str(payload.url),
                "notes_markdown": result.notes_markdown,
                "model": result.model,
                "truncated": result.truncated,
                "transcript_chars": result.transcript_chars,
            })
            # Ignore failure; we don't want to block the response on storage errors.
            _ = supabase.table("youtube_ai_notes").insert(record).execute()
    except Exception:
        pass

    return result


@youtube_transcript_router.get("/saved/{video_id}")
def api_youtube_saved(video_id: str):
    """Return the latest saved notes for a given YouTube video ID if present; else 404."""
    if not video_id or not re.fullmatch(r"[A-Za-z0-9_-]{6,32}", video_id):
        raise HTTPException(status_code=400, detail="Invalid video ID.")
    supabase = get_service_client()
    try:
        def _run():
            return (
                supabase
                .table("youtube_ai_notes")
                .select("video_id,video_url,notes_markdown,model,truncated,transcript_chars,created_at")
                .eq("video_id", video_id)
                .order("created_at", desc=True)
                .limit(1)
                .execute()
            )
        res = _supabase_retry(_run)
        rows = getattr(res, "data", None) or []
        if not rows:
            raise HTTPException(status_code=404, detail="No saved notes found for this video.")
        # Return the row as-is; front-end expects at least notes_markdown and optionally model
        return rows[0]
    except APIError as exc:
        raise HTTPException(status_code=502, detail=f"Storage error: {exc}") from exc


# --- Tests & Assessments ---


class TestQuestionIn(BaseModel):
    prompt: str = Field(..., min_length=1, max_length=4000)
    options: List[str] = Field(..., min_items=2, max_items=8)
    correct_index: int = Field(..., ge=0)
    points: int = Field(1, ge=1, le=100)
    order: Optional[int] = Field(None, ge=0)

    @validator("options", pre=True)
    def _clean_options(cls, v):
        if not v:
            return []
        opts: List[str] = []
        for o in v:
            s = str(o or "").strip()
            if s:
                opts.append(s[:500])
        return opts

    @validator("correct_index")
    def _check_correct_index(cls, v, values):
        opts = values.get("options", []) or []
        if not opts:
            return v
        if v < 0 or v >= len(opts):
            raise ValueError("correct_index must point to an option")
        return v


class CreateTestIn(BaseModel):
    title: str = Field(..., min_length=1, max_length=200)
    description: Optional[str] = Field(None, max_length=2000)
    duration_seconds: Optional[int] = Field(None, ge=0, le=7200)
    class_id: Optional[str] = Field(None, description="Optional class/group association")
    questions: List[TestQuestionIn]
    accepting_submissions: Optional[bool] = True

    @validator("questions")
    def _must_have_questions(cls, v):
        if not v:
            raise ValueError("At least one question is required")
        return v


class SubmitAttemptIn(BaseModel):
    answers: List[int]
    elapsed_seconds: Optional[int] = Field(None, ge=0, le=7200)

    @validator("answers")
    def _non_empty(cls, v):
        if not v:
            raise ValueError("answers cannot be empty")
        return v


def _fetch_test_row(supabase, test_id: str) -> Dict[str, Any]:
    res = _supabase_retry(lambda: supabase.table("tests").select("*").eq("id", test_id).limit(1).execute())
    if getattr(res, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (get test): {res.error}")
    rows = getattr(res, "data", None) or []
    if not rows:
        raise HTTPException(status_code=404, detail="Test not found")
    return rows[0]


def _fetch_test_questions(supabase, test_id: str) -> List[Dict[str, Any]]:
    res = _supabase_retry(lambda: (
        supabase
        .table("test_questions")
        .select("id,prompt,options,correct_index,points,question_order")
        .eq("test_id", test_id)
        .order("question_order", desc=False)
        .order("id", desc=False)
        .execute()
    ), retries=5, base_delay=0.25)
    if getattr(res, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (get questions): {res.error}")
    return getattr(res, "data", None) or []


@teacher_router.post("/api/teacher/tests", summary="Create a test (MCQ)")
def api_create_test(payload: CreateTestIn, authorization: Optional[str] = Header(default=None)):
    teacher_id = _require_teacher(authorization)
    supabase = get_service_client()

    max_score = sum(max(q.points, 1) for q in payload.questions)
    test_row = _supabase_payload({
        "title": payload.title,
        "description": payload.description,
        "duration_seconds": payload.duration_seconds,
        "class_id": payload.class_id,
        "teacher_user_id": teacher_id,
        "max_score": max_score,
        "accepting_submissions": True if payload.accepting_submissions is None else bool(payload.accepting_submissions),
    })

    res = supabase.table("tests").insert(test_row).execute()
    if getattr(res, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (create test): {res.error}")
    test_id = (getattr(res, "data", None) or [{}])[0].get("id")
    if not test_id:
        raise HTTPException(status_code=500, detail="Failed to create test")

    question_rows = []
    for idx, q in enumerate(payload.questions):
        question_rows.append(_supabase_payload({
            "test_id": test_id,
            "prompt": q.prompt,
            "options": q.options,
            "correct_index": q.correct_index,
            "points": q.points,
            "question_order": q.order if q.order is not None else idx,
        }))

    qres = supabase.table("test_questions").insert(question_rows).execute()
    if getattr(qres, "error", None):
        # Best-effort cleanup of orphan test
        try:
            supabase.table("tests").delete().eq("id", test_id).execute()
        except Exception:
            pass
        raise HTTPException(status_code=500, detail=f"Supabase error (create questions): {qres.error}")

    return {"id": test_id, "max_score": max_score}


@teacher_router.get("/api/teacher/tests/{test_id}/attempts", summary="Teacher: view attempts and scores")
def api_list_attempts(test_id: str, authorization: Optional[str] = Header(default=None)):
    teacher_id = _require_teacher(authorization)
    supabase = get_service_client()
    test_row = _fetch_test_row(supabase, test_id)
    if test_row.get("teacher_user_id") != teacher_id:
        raise HTTPException(status_code=403, detail="You do not own this test")

    res = (
        supabase
        .table("test_attempts")
        .select("id,student_user_id,score,elapsed_seconds,started_at,submitted_at")
        .eq("test_id", test_id)
        .order("submitted_at", desc=True)
        .execute()
    )
    if getattr(res, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (list attempts): {res.error}")
    return {
        "test": {"id": test_id, "title": test_row.get("title"), "max_score": test_row.get("max_score")},
        "attempts": getattr(res, "data", None) or [],
    }


@teacher_router.get("/api/teacher/tests", summary="Teacher: list my tests")
def api_list_tests(authorization: Optional[str] = Header(default=None)):
    teacher_id = _require_teacher(authorization)
    supabase = get_service_client()
    res = (
        supabase
        .table("tests")
        .select("id,title,description,class_id,duration_seconds,max_score,accepting_submissions,created_at,updated_at")
        .eq("teacher_user_id", teacher_id)
        .order("created_at", desc=True)
        .limit(200)
        .execute()
    )
    if getattr(res, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (list tests): {res.error}")
    return res.data or []


@teacher_router.delete("/api/teacher/tests/{test_id}", summary="Teacher: delete a test")
def api_delete_test(test_id: str, authorization: Optional[str] = Header(default=None)):
    teacher_id = _require_teacher(authorization)
    supabase = get_service_client()
    test_row = _fetch_test_row(supabase, test_id)
    if test_row.get("teacher_user_id") != teacher_id:
        raise HTTPException(status_code=403, detail="You do not own this test")
    res = supabase.table("tests").delete().eq("id", test_id).execute()
    if getattr(res, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (delete test): {res.error}")
    return {"deleted": True, "id": test_id}


class UpdateTestIn(CreateTestIn):
    accepting_submissions: Optional[bool] = None


@teacher_router.put("/api/teacher/tests/{test_id}", summary="Teacher: update a test")
def api_update_test(test_id: str, payload: UpdateTestIn, authorization: Optional[str] = Header(default=None)):
    teacher_id = _require_teacher(authorization)
    supabase = get_service_client()
    test_row = _fetch_test_row(supabase, test_id)
    if test_row.get("teacher_user_id") != teacher_id:
        raise HTTPException(status_code=403, detail="You do not own this test")

    max_score = sum(max(q.points, 1) for q in payload.questions)
    updates = _supabase_payload({
        "title": payload.title,
        "description": payload.description,
        "duration_seconds": payload.duration_seconds,
        "class_id": payload.class_id,
        "max_score": max_score,
    })
    if payload.accepting_submissions is not None:
        updates["accepting_submissions"] = bool(payload.accepting_submissions)

    up = supabase.table("tests").update(updates).eq("id", test_id).execute()
    if getattr(up, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (update test): {up.error}")

    # Replace questions: delete old then insert new
    delq = supabase.table("test_questions").delete().eq("test_id", test_id).execute()
    if getattr(delq, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (delete old questions): {delq.error}")

    question_rows = []
    for idx, q in enumerate(payload.questions):
        question_rows.append(_supabase_payload({
            "test_id": test_id,
            "prompt": q.prompt,
            "options": q.options,
            "correct_index": q.correct_index,
            "points": q.points,
            "question_order": q.order if q.order is not None else idx,
        }))
    qres = supabase.table("test_questions").insert(question_rows).execute()
    if getattr(qres, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (insert new questions): {qres.error}")

    return {"id": test_id, "max_score": max_score, "accepting_submissions": updates.get("accepting_submissions", test_row.get("accepting_submissions", True))}


@teacher_router.patch("/api/teacher/tests/{test_id}/accepting", summary="Teacher: toggle accepting submissions")
def api_toggle_accepting(test_id: str, accepting: bool = Query(...), authorization: Optional[str] = Header(default=None)):
    teacher_id = _require_teacher(authorization)
    supabase = get_service_client()
    test_row = _fetch_test_row(supabase, test_id)
    if test_row.get("teacher_user_id") != teacher_id:
        raise HTTPException(status_code=403, detail="You do not own this test")
    res = supabase.table("tests").update({"accepting_submissions": bool(accepting)}).eq("id", test_id).execute()
    if getattr(res, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (toggle accepting): {res.error}")
    return {"id": test_id, "accepting_submissions": bool(accepting)}


@teacher_router.get("/api/teacher/tests/{test_id}/results", summary="Teacher: results and participation")
def api_test_results(test_id: str, authorization: Optional[str] = Header(default=None)):
    teacher_id = _require_teacher(authorization)
    supabase = get_service_client()
    test_row = _fetch_test_row(supabase, test_id)
    if test_row.get("teacher_user_id") != teacher_id:
        raise HTTPException(status_code=403, detail="You do not own this test")

    attempts_res = (
        supabase
        .table("test_attempts")
        .select("id,student_user_id,score,elapsed_seconds,started_at,submitted_at")
        .eq("test_id", test_id)
        .order("submitted_at", desc=True)
        .limit(500)
        .execute()
    )
    if getattr(attempts_res, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (results attempts): {attempts_res.error}")
    attempts = attempts_res.data or []

    class_students: List[Dict[str, Any]] = []
    profile_by_id: Dict[str, Dict[str, Any]] = {}
    class_id = test_row.get("class_id")

    # Collect IDs for attempts and class roster to hydrate names in a single profile lookup
    attempted_ids = {a.get("student_user_id") for a in attempts if a.get("student_user_id")}
    enrolled_ids: Set[str] = set()

    if class_id:
        try:
            enrolled = supabase.table("teacher_class_students").select("student_user_id").eq("class_id", class_id).execute()
            if not getattr(enrolled, "error", None):
                enrolled_ids = {r.get("student_user_id") for r in (enrolled.data or []) if r.get("student_user_id")}
        except Exception:
            enrolled_ids = set()

    try:
        need_ids = list({sid for sid in (attempted_ids | enrolled_ids) if sid})
        if need_ids:
            prof_res = (
                supabase
                .table("user_profiles")
                .select("auth_user_id,name,email")
                .in_("auth_user_id", need_ids)
                .execute()
            )
            if not getattr(prof_res, "error", None):
                for p in prof_res.data or []:
                    pid = p.get("auth_user_id")
                    if pid:
                        profile_by_id[pid] = {"name": p.get("name"), "email": p.get("email")}
    except Exception:
        profile_by_id = {}

    for attempt in attempts:
        sid = attempt.get("student_user_id")
        prof = profile_by_id.get(sid or "")
        if prof:
            attempt["name"] = prof.get("name")
            attempt["email"] = prof.get("email")

    students_not_attempted: List[Dict[str, Any]] = []
    if class_id and enrolled_ids:
        try:
            missing_ids = list(enrolled_ids - attempted_ids)
            if missing_ids:
                missing_profiles = [profile_by_id.get(mid) for mid in missing_ids]
                students_not_attempted = [
                    {"auth_user_id": mid, "name": (mp or {}).get("name"), "email": (mp or {}).get("email")}
                    for mid, mp in zip(missing_ids, missing_profiles)
                ]
        except Exception:
            students_not_attempted = []

    # Build full class roster with status when class is linked
    if class_id and enrolled_ids:
        attempt_map = {a.get("student_user_id"): a for a in attempts if a.get("student_user_id")}
        class_students = []
        for sid in enrolled_ids:
            att = attempt_map.get(sid)
            prof = profile_by_id.get(sid, {})
            status = "pending"
            if att:
                status = "in_progress" if not att.get("submitted_at") else "completed"
            class_students.append({
                "auth_user_id": sid,
                "name": prof.get("name"),
                "email": prof.get("email"),
                "status": status,
                "score": att.get("score") if att else None,
                "submitted_at": att.get("submitted_at") if att else None,
            })
        # Optional: keep deterministic order by name/email
        class_students.sort(key=lambda r: (r.get("name") or "", r.get("email") or "", r.get("auth_user_id") or ""))

    completed = len([a for a in attempts if a.get("submitted_at")])
    return {
        "test": {
            "id": test_id,
            "title": test_row.get("title"),
            "class_id": test_row.get("class_id"),
            "max_score": test_row.get("max_score"),
            "accepting_submissions": test_row.get("accepting_submissions", True),
        },
        "completed_count": completed,
        "attempts": attempts,
        "students_not_attempted": students_not_attempted,
        "class_students": class_students,
    }


@academics_router.get("/api/tests/{test_id}", summary="Fetch a test for taking")
def api_get_test(test_id: str, authorization: Optional[str] = Header(default=None)):
    user_id, _ = _get_auth_user(authorization)
    supabase = get_service_client()
    test_row = _fetch_test_row(supabase, test_id)
    questions = _fetch_test_questions(supabase, test_id)

    attempt_res = (
        supabase
        .table("test_attempts")
        .select("id,submitted_at,score")
        .eq("test_id", test_id)
        .eq("student_user_id", user_id)
        .limit(1)
        .execute()
    )
    if getattr(attempt_res, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (attempt check): {attempt_res.error}")
    attempt = (getattr(attempt_res, "data", None) or [None])[0]

    is_owner = test_row.get("teacher_user_id") == user_id
    sanitized_questions = []
    for q in questions:
        entry = {
            "id": q.get("id"),
            "prompt": q.get("prompt"),
            "options": q.get("options"),
            "points": q.get("points", 1),
            "order": q.get("question_order", 0),
        }
        if is_owner:
            entry["correct_index"] = q.get("correct_index")
        sanitized_questions.append(entry)

    return {
        "id": test_row.get("id"),
        "title": test_row.get("title"),
        "description": test_row.get("description"),
        "duration_seconds": test_row.get("duration_seconds"),
        "max_score": test_row.get("max_score"),
        "accepting_submissions": test_row.get("accepting_submissions", True),
        "attempt": attempt,
        "questions": sanitized_questions,
    }


@academics_router.post("/api/tests/{test_id}/start", summary="Start a test attempt (one per student)")
def api_start_attempt(test_id: str, authorization: Optional[str] = Header(default=None)):
    user_id, _ = _get_auth_user(authorization)
    supabase = get_service_client()
    _ = _fetch_test_row(supabase, test_id)  # ensure exists
    test_row = _fetch_test_row(supabase, test_id)
    if not test_row.get("accepting_submissions", True):
        raise HTTPException(status_code=410, detail="Test submissions are closed")

    def _attempt_lookup():
        return (
            supabase
            .table("test_attempts")
            .select("id,submitted_at,started_at")
            .eq("test_id", test_id)
            .eq("student_user_id", user_id)
            .limit(1)
            .execute()
        )

    check = _supabase_retry(_attempt_lookup, retries=5, base_delay=0.25)
    if getattr(check, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (attempt check): {check.error}")
    existing = (getattr(check, "data", None) or [])
    if existing:
        att = existing[0]
        if att.get("submitted_at"):
            raise HTTPException(status_code=409, detail="You already submitted this test")
        return {"id": att.get("id"), "started_at": att.get("started_at")}

    def _attempt_insert():
        return supabase.table("test_attempts").insert({
            "test_id": test_id,
            "student_user_id": user_id,
        }).execute()

    try:
        ins = _supabase_retry(_attempt_insert, retries=5, base_delay=0.25)
    except RETRYABLE_EXCEPTIONS as e:  # pragma: no cover - network timing dependent
        # If the insert actually succeeded server-side but the connection closed, a follow-up
        # lookup will return the row; otherwise we bubble a retryable error to the client.
        verify = _supabase_retry(_attempt_lookup, retries=5, base_delay=0.25)
        existing = (getattr(verify, "data", None) or [])
        if existing:
            att = existing[0]
            if att.get("submitted_at"):
                raise HTTPException(status_code=409, detail="You already submitted this test")
            return {"id": att.get("id"), "started_at": att.get("started_at")}
        raise HTTPException(status_code=503, detail="Temporary connection issue starting attempt; please retry") from e

    if getattr(ins, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (start attempt): {ins.error}")
    row = (getattr(ins, "data", None) or [{}])[0]
    return {"id": row.get("id"), "started_at": row.get("started_at")}


@academics_router.post("/api/tests/{test_id}/submit", summary="Submit a test attempt and score")
def api_submit_attempt(test_id: str, payload: SubmitAttemptIn, authorization: Optional[str] = Header(default=None)):
    user_id, _ = _get_auth_user(authorization)
    supabase = get_service_client()
    test_row = _fetch_test_row(supabase, test_id)
    if not test_row.get("accepting_submissions", True):
        raise HTTPException(status_code=410, detail="Test submissions are closed")
    questions = _fetch_test_questions(supabase, test_id)

    # Fetch or create attempt record
    att_res = (
        supabase
        .table("test_attempts")
        .select("id,submitted_at")
        .eq("test_id", test_id)
        .eq("student_user_id", user_id)
        .limit(1)
        .execute()
    )
    if getattr(att_res, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (attempt lookup): {att_res.error}")
    att_rows = getattr(att_res, "data", None) or []
    if att_rows and att_rows[0].get("submitted_at"):
        raise HTTPException(status_code=409, detail="You already submitted this test")
    attempt_id = att_rows[0].get("id") if att_rows else None

    if not attempt_id:
        ins = supabase.table("test_attempts").insert({
            "test_id": test_id,
            "student_user_id": user_id,
        }).execute()
        if getattr(ins, "error", None):
            raise HTTPException(status_code=500, detail=f"Supabase error (start attempt late): {ins.error}")
        attempt_id = (getattr(ins, "data", None) or [{}])[0].get("id")

    if len(payload.answers) != len(questions):
        raise HTTPException(status_code=400, detail="Answer count does not match questions")

    score = 0
    for q, ans in zip(questions, payload.answers):
        try:
            ans_int = int(ans)
        except Exception:
            ans_int = -1
        if ans_int == q.get("correct_index"):
            score += int(q.get("points", 1))

    upd = supabase.table("test_attempts").update({
        "answers": payload.answers,
        "score": score,
        "elapsed_seconds": payload.elapsed_seconds,
        "submitted_at": datetime.utcnow().isoformat(),
    }).eq("id", attempt_id).execute()
    if getattr(upd, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (submit attempt): {upd.error}")

    return {
        "test_id": test_id,
        "attempt_id": attempt_id,
        "score": score,
        "max_score": test_row.get("max_score"),
    }


# --- YouTube Video Search endpoints ---

youtube_search_router = APIRouter(prefix="/api/youtube", tags=["youtube search"])


@youtube_search_router.get("/search")
def api_youtube_search(
    query: str = Query(..., description="Search query for YouTube videos"),
    num: int = Query(8, ge=1, le=20, description="Number of videos to return")
):
    """Search YouTube videos using SerpAPI and return metadata including thumbnails, views, channel info."""
    if not query or not query.strip():
        raise HTTPException(status_code=400, detail="Query parameter is required")
    
    try:
        videos = search_youtube_videos(query.strip(), num=num)
        return videos
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"YouTube search failed: {str(e)}")


@youtube_search_router.get("/channel-logo")
def api_youtube_channel_logo(
    channel_url: str = Query(..., description="Full YouTube channel URL")
):
    if not channel_url or not channel_url.strip():
        raise HTTPException(status_code=400, detail="channel_url is required")

    try:
        logo = get_channel_logo(channel_url.strip()) or get_default_channel_logo()
        return {"logo": logo}
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Channel logo lookup failed: {str(e)}")


# --- Streak API ---

class StreakResponse(BaseModel):
    current_streak: int = 0
    longest_streak: int = 0
    last_activity_date: Optional[str] = None
    next_milestone: int = 7
    prev_milestone: int = 0
    milestone_progress: int = 0
    days_completed: int = 0
    week_data: List[dict] = []

STREAK_MILESTONES = [7, 14, 21, 30, 60, 90, 180, 365]

def _get_next_milestone(current: int) -> int:
    for m in STREAK_MILESTONES:
        if current < m:
            return m
    return STREAK_MILESTONES[-1] + 100

def _get_prev_milestone(current: int) -> int:
    for i in range(len(STREAK_MILESTONES) - 1, -1, -1):
        if current >= STREAK_MILESTONES[i]:
            return STREAK_MILESTONES[i]
    return 0

def _compute_current_streak(active_dates, today: date) -> int:
    streak = 0
    day = today
    while day in active_dates:
        streak += 1
        day = day - timedelta(days=1)
    return streak

@academics_router.get("/api/streak", response_model=StreakResponse, summary="Get user streak data")
def get_user_streak(authorization: Optional[str] = Header(default=None)):
    token = _bearer_token_from_header(authorization)
    if not token:
        raise HTTPException(status_code=401, detail="Missing authorization")
    
    try:
        user_id = _get_user_id_with_retry(token)
    except Exception:
        raise HTTPException(status_code=401, detail="Invalid token")
    
    supabase = get_service_client()
    today = date.today()
    yesterday = today - timedelta(days=1)
    
    # Get user profile ID
    prof_q = supabase.table("user_profiles").select("id").eq("auth_user_id", user_id).limit(1).execute()
    if not prof_q.data:
        raise HTTPException(status_code=404, detail="Profile not found")
    profile_id = prof_q.data[0]["id"]
    
    # Get or create streak record
    streak_q = supabase.table("notex_streak").select("*").eq("user_profile_id", profile_id).limit(1).execute()
    
    if not streak_q.data:
        # Create new streak record
        new_streak = {
            "user_profile_id": profile_id,
            "current_streak": 1,
            "longest_streak": 1,
            "last_activity_date": today.isoformat()
        }
        supabase.table("notex_streak").insert(new_streak).execute()
        streak_data = new_streak
    else:
        streak_data = streak_q.data[0]

    # Pull recent activity logs to compute streak and week data
    lookback_start = today - timedelta(days=400)
    activity_q = supabase.table("notex_activity_logs").select("activity_date").eq("user_profile_id", profile_id).gte("activity_date", lookback_start.isoformat()).order("activity_date", desc=True).limit(400).execute()
    active_dates = set()
    for a in (activity_q.data or []):
        if a.get("activity_date"):
            try:
                active_dates.add(date.fromisoformat(str(a["activity_date"])))
            except Exception:
                pass

    # Backfill with stored last_activity_date if logs are missing
    last_date = None
    last_date_str = streak_data.get("last_activity_date")
    if last_date_str:
        try:
            last_date = date.fromisoformat(str(last_date_str))
            active_dates.add(last_date)
        except Exception:
            last_date = None

    # Count this visit as activity so streak continues for today
    active_dates.add(today)

    current = _compute_current_streak(active_dates, today)
    longest = max(int(streak_data.get("longest_streak") or 0), current)

    if current != int(streak_data.get("current_streak") or 0) or (streak_data.get("last_activity_date") != today.isoformat()):
        supabase.table("notex_streak").update({
            "current_streak": current,
            "longest_streak": longest,
            "last_activity_date": today.isoformat(),
            "updated_at": datetime.utcnow().isoformat()
        }).eq("user_profile_id", profile_id).execute()
        streak_data["current_streak"] = current
        streak_data["longest_streak"] = longest
        streak_data["last_activity_date"] = today.isoformat()

    # Calculate milestones
    next_m = _get_next_milestone(current)
    prev_m = _get_prev_milestone(current)
    range_val = next_m - prev_m
    progress = round(((current - prev_m) / range_val) * 100) if range_val > 0 else 0

    # Build week data
    week_start = today - timedelta(days=today.weekday())  # Monday
    day_names = ["M", "T", "W", "T", "F", "S", "S"]
    today_index = today.weekday()
    week_data = []
    for i in range(7):
        day_date = week_start + timedelta(days=i)
        is_today = i == today_index
        is_future = i > today_index
        is_active = day_date in active_dates
        week_data.append({
            "day": day_names[i],
            "date": day_date.day,
            "is_today": is_today,
            "is_active": is_active,
            "is_future": is_future
        })

    days_completed = len([d for d in week_data if d["is_active"]])
    
    return StreakResponse(
        current_streak=current,
        longest_streak=longest,
        last_activity_date=streak_data.get("last_activity_date"),
        next_milestone=next_m,
        prev_milestone=prev_m,
        milestone_progress=progress,
        days_completed=days_completed,
        week_data=week_data
    )

@academics_router.post("/api/streak/ping", summary="Record user activity for streak")
def ping_streak(authorization: Optional[str] = Header(default=None)):
    """Call this endpoint when user is active to maintain streak."""
    token = _bearer_token_from_header(authorization)
    if not token:
        raise HTTPException(status_code=401, detail="Missing authorization")
    
    try:
        user_id = _get_user_id_with_retry(token)
    except Exception:
        raise HTTPException(status_code=401, detail="Invalid token")
    
    supabase = get_service_client()
    today = date.today()
    
    # Get user profile ID
    prof_q = supabase.table("user_profiles").select("id").eq("auth_user_id", user_id).limit(1).execute()
    if not prof_q.data:
        raise HTTPException(status_code=404, detail="Profile not found")
    profile_id = prof_q.data[0]["id"]
    
    # Record activity log for today (upsert)
    try:
        # Check if already logged today
        existing = supabase.table("notex_activity_logs").select("id").eq("user_profile_id", profile_id).eq("activity_date", today.isoformat()).limit(1).execute()
        if not existing.data:
            supabase.table("notex_activity_logs").insert({
                "user_profile_id": profile_id,
                "activity_date": today.isoformat()
            }).execute()
    except Exception:
        pass  # Ignore duplicate key errors
    
    return {"status": "ok", "date": today.isoformat()}


# --- FastAPI app ---

def create_app() -> FastAPI:
    app = FastAPI(title="PaperX Unified API", version="1.0.0")

    # CORS: allow dev origins and support Authorization header
    # Note: Using allow_origin_regex to correctly echo Origin when credentials are enabled.
    app.add_middleware(
        CORSMiddleware,
        allow_origins=[
            "https://paperx.tech",
            "https://www.paperx.tech",
            "https://squid-app-6jvdq.ondigitalocean.app",
            "https://uppzpkmpxgyipjzcskva.supabase.co",
            "http://127.0.0.1:5500",
            "http://127.0.0.1:8000",
            "http://localhost:5500",
            "http://localhost:8000",
            "http://localhost",
        ],
        allow_credentials=True,
        allow_methods=["*"],
        allow_headers=["*"],
    )

    app.include_router(projects_router)
    app.include_router(notes_router)
    app.include_router(learning_tracks_router)
    app.include_router(print_router)
    app.include_router(academics_router)
    app.include_router(marketplace_router)
    app.include_router(teacher_router)
    app.include_router(youtube_transcript_router)
    app.include_router(youtube_search_router)
    app.include_router(yt_transcript_router, prefix="/api/youtube", tags=["youtube transcripts (raw)"])
    app.include_router(tunex_router.router)

    # Simple request logger to aid debugging 405/OPTIONS/CORS issues
    @app.middleware("http")
    async def log_requests(request: Request, call_next):  # type: ignore[override]
        # Use only path (no query params) and truncate if too long
        path = request.url.path
        if len(path) > 60:
            path = path[:57] + "..."
        try:
            print(f"[REQ] {request.method} {path}")
        except Exception:
            pass
        try:
            response = await call_next(request)
        except Exception as e:
            try:
                print(f"[ERR] {request.method} {path} -> {e}")
            except Exception:
                pass
            raise
        try:
            print(f"[RES] {request.method} {path} {response.status_code}")
        except Exception:
            pass
        return response

    @app.get("/")
    def root():
        return {
            "message": "PaperX API running",
            "notes_ui": "/ui/notes_generator.html",
            "transcripts_ui": "/ui/youtube-transcript.html",
            "youtube_videos_ui": "/ui/youtube_videos.html",
        }

    @app.get("/health", tags=["system"])
    def health():
        """Lightweight health probe used by the Render load balancer."""
        return {"status": "ok", "timestamp": datetime.utcnow().isoformat() + "Z"}

    ui_dir = Path(__file__).resolve().parent / "ui"
    if ui_dir.is_dir():
        app.mount("/ui", StaticFiles(directory=ui_dir), name="ui")
    # Static assets (images, uploaded teacher ID cards, etc.)
    assets_dir = Path(__file__).resolve().parent / "assets"
    if assets_dir.is_dir():
        # Serve at /assets so front-end references like ../assets/... resolve
        app.mount("/assets", StaticFiles(directory=assets_dir), name="assets")

    return app


app = create_app()

# ==========================================
# ANALYTICS MODULE (Injected)
# ==========================================
from datetime import timedelta

class AnalyticsSessionStart(BaseModel):
    user_id: Optional[str] = None
    user_agent: Optional[str] = None

class AnalyticsHeartbeat(BaseModel):
    session_id: str

class AnalyticsEventModel(BaseModel):
    session_id: str
    user_id: Optional[str] = None
    event_type: str
    event_data: Dict[str, Any] = {}

class TopicFeedbackModel(BaseModel):
    user_id: str
    topic_id: str
    is_helpful: bool
    comment: Optional[str] = None

analytics_router = APIRouter(prefix="/analytics", tags=["analytics"])

@analytics_router.post("/session/start")
async def start_session(req: AnalyticsSessionStart, request: Request):
    supabase = get_service_client()
    
    user_agent = req.user_agent or request.headers.get("user-agent")
    client_ip = request.client.host if request.client else None
    
    uid = req.user_id
    if uid and not uid.strip():
        uid = None

    data = {
        "user_id": uid,
        "user_agent": user_agent,
        "ip": client_ip,
        "last_seen_at": datetime.utcnow().isoformat()
    }
    
    try:
        res = supabase.table("user_sessions").insert(data).execute()
        if res.data:
            return {"session_id": res.data[0]["id"]}
        # Fallback if no data returned (RLS or otherwise), generate UUID
        return {"session_id": str(uuid.uuid4())} 
    except Exception as e:
        print(f"Analytics Error: {e}")
        return {"session_id": str(uuid.uuid4())} # Graceful degradation

@analytics_router.post("/session/heartbeat")
async def analytics_heartbeat(payload: AnalyticsHeartbeat):
    try:
        supabase = get_service_client()
        supabase.table("user_sessions").update({
            "last_seen_at": datetime.utcnow().isoformat()
        }).eq("id", payload.session_id).execute()
    except Exception:
        pass
    return {"status": "ok"}

@analytics_router.post("/event")
async def analytics_track_event(payload: AnalyticsEventModel):
    try:
        supabase = get_service_client()
        data = {
            "session_id": payload.session_id,
            "user_id": payload.user_id if payload.user_id else None,
            "event_type": payload.event_type,
            "event_data": payload.event_data
        }
        if not data["user_id"]: del data["user_id"]
        supabase.table("analytics_events").insert(data).execute()
    except Exception as e:
        print(f"Analytics Event Error: {e}")
    return {"status": "ok"}

@analytics_router.post("/feedback/topic")
async def analytics_topic_feedback(payload: TopicFeedbackModel):
    try:
        supabase = get_service_client()
        data = {
            "user_id": payload.user_id,
            "topic_id": payload.topic_id,
            "is_helpful": payload.is_helpful,
            "comment": payload.comment
        }
        supabase.table("topic_feedback").insert(data).execute()
    except Exception:
        pass
    return {"status": "ok"}

@analytics_router.get("/dashboard")
async def analytics_dashboard_metrics():
    try:
        supabase = get_service_client()
        now = datetime.utcnow()
        day_ago = (now - timedelta(days=1)).isoformat()
        week_ago = (now - timedelta(days=7)).isoformat()

        # 1. Active Users (DAU/WAU)
        sessions_24h_res = supabase.table("user_sessions").select("user_id").gte("started_at", day_ago).execute()
        sessions_7d_res = supabase.table("user_sessions").select("user_id", "started_at").gte("started_at", week_ago).execute()
        
        sessions_24h = getattr(sessions_24h_res, 'data', []) or []
        sessions_7d = getattr(sessions_7d_res, 'data', []) or []

        dau = len({s["user_id"] for s in sessions_24h if s.get("user_id")})
        wau_users = {s["user_id"] for s in sessions_7d if s.get("user_id")}
        wau = len(wau_users)

        # 2. Avg Study Time
        # Needs last_seen - started_at. We need to fetch times.
        # Re-fetch 24h with times
        sessions_times_res = supabase.table("user_sessions").select("started_at,last_seen_at").gte("started_at", day_ago).execute()
        sessions_times = getattr(sessions_times_res, 'data', []) or []
        
        total_minutes = 0.0
        for s in sessions_times:
            if s.get("started_at") and s.get("last_seen_at"):
                try:
                    start = datetime.fromisoformat(s["started_at"].replace('Z', '+00:00'))
                    end = datetime.fromisoformat(s["last_seen_at"].replace('Z', '+00:00'))
                    diff = (end - start).total_seconds() / 60
                    if 0 < diff < 480: # Cap at 8h to ignore stuck sessions
                        total_minutes += diff
                except: pass
        
        avg_study_time = (total_minutes / dau) if dau > 0 else 0
        avg_sessions = (len(sessions_7d) / wau) if wau > 0 else 0

        # 3. Feature Usage
        events_7d_res = supabase.table("analytics_events").select("event_type,event_data,user_id").gte("created_at", week_ago).execute()
        events_7d = getattr(events_7d_res, 'data', []) or []

        note_viewers = set()
        blink_viewers = set()
        lab_users = set()
        
        counts = {}

        for e in events_7d:
            et = e.get("event_type")
            counts[et] = counts.get(et, 0) + 1
            uid = e.get("user_id")
            if not uid: continue
            
            if et == "note_viewed":
                data = e.get("event_data") or {}
                # Handle both dict and string if Supabase returns weirdly, but usually dict
                if isinstance(data, str):
                     try: data = json.loads(data)
                     except: data = {}
                
                variant = data.get("variant", "detailed")
                if variant == "cheatsheet":
                    blink_viewers.add(uid)
                else:
                    note_viewers.add(uid)
            elif et == "lab_started":
                lab_users.add(uid)

        # Calc Feature Volume (Counts)
        notex_count = counts.get("note_viewed", 0)
        
        # Blink count (cheatsheet variant of note_viewed)
        blink_count = 0
        for e in events_7d:
             et = e.get("event_type")
             if et == "note_viewed":
                 data = e.get("event_data") or {}
                 if isinstance(data, str):
                     try: data = json.loads(data)
                     except: data = {}
                 if data.get("variant") == "cheatsheet":
                     blink_count += 1
        
        labx_count = counts.get("lab_started", 0)

        # 4. Learning Metrics / Funnel (Simplistic)
        # Topic opened -> Note viewed -> Lab started
        topic_opens = counts.get("topic_opened", 0)
        note_views = counts.get("note_viewed", 0)
        lab_starts = counts.get("lab_started", 0)
        
        # Funnel metrics
        raw_funnel_1 = (note_views / topic_opens * 100) if topic_opens > 0 else 0
        raw_funnel_2 = (lab_starts / note_views * 100) if note_views > 0 else 0
        
        funnel_1 = round(min(raw_funnel_1, 100.0), 2)
        funnel_2 = round(min(raw_funnel_2, 100.0), 2)

        # 5. Advanced Learning Metrics (Real Logic)
        
        # Note Completion: % of note users who have a 'scroll_depth' event >= 70
        scroll_users = set()
        completed_users = set()
        blink_counts = {} # uid -> count of cheatsheet views
        
        for e in events_7d:
            et = e.get("event_type")
            uid = e.get("user_id")
            if not uid: continue
            
            if et == "scroll_depth":
                data = e.get("event_data") or {}
                if isinstance(data, str):
                    try: data = json.loads(data)
                    except: data = {}
                depth = int(data.get("depth", 0))
                # Only consider it a "note scroll" if on relevant pages
                page = data.get("page", "")
                if "notes_generator" in page or "note_detail" in page:
                     scroll_users.add(uid)
                     if depth >= 70:
                         completed_users.add(uid)
            
            elif et == "note_viewed":
                data = e.get("event_data") or {}
                if isinstance(data, str):
                    try: data = json.loads(data)
                    except: data = {}
                variant = data.get("variant")
                if variant == "cheatsheet":
                    blink_counts[uid] = blink_counts.get(uid, 0) + 1

        # Note Completion Rate
        note_completion_rate = round((len(completed_users) / len(scroll_users) * 100), 1) if scroll_users else 0
        
        # Blink Revisit Rate: % of blink users who viewed >= 2 blinks (or same blink 2x, simpler to just count total views > 1)
        revisit_users = [u for u, c in blink_counts.items() if c >= 2]
        blink_total_users = len(blink_counts)
        blink_revisit = round((len(revisit_users) / blink_total_users * 100), 1) if blink_total_users > 0 else 0

        # 6. User Activity Details (User List)
        users_map = {} # uid -> { notex, blink, labx, sessions, last_seen }
        
        # Init from WAU list
        for uid in wau_users:
            users_map[uid] = { "notex": 0, "blink": 0, "labx": 0, "sessions": 0, "last_seen": "", "id": uid }

        # Aggregation
        for e in events_7d:
            uid = e.get("user_id")
            if not uid or uid not in users_map: continue
            
            et = e.get("event_type")
            data = e.get("event_data") or {}
            if isinstance(data, str):
                 try: data = json.loads(data)
                 except: data = {}
            
            if et == "note_viewed":
                if data.get("variant") == "cheatsheet":
                    users_map[uid]["blink"] += 1
                else:
                    users_map[uid]["notex"] += 1
            elif et == "lab_started":
                users_map[uid]["labx"] += 1
        
        # Session counts + Last active
        for s in sessions_7d:
            uid = s.get("user_id")
            if not uid or uid not in users_map: continue
            users_map[uid]["sessions"] += 1
            # Update last_seen if fresher
            curr = users_map[uid]["last_seen"]
            started = s.get("started_at")
            if started and (not curr or started > curr):
                users_map[uid]["last_seen"] = started

        # Fetch Profiles
        users_list = []
        profile_id_map = {}  # auth_user_id -> profile id
        if wau_users:
            try:
                prof_res = supabase.table("user_profiles").select("id, auth_user_id, name, email, profile_image_url").in_("auth_user_id", list(wau_users)).execute()
                profiles = getattr(prof_res, 'data', []) or []
                for p in profiles:
                    uid = p.get("auth_user_id")
                    if uid in users_map:
                        users_map[uid]["name"] = p.get("name")
                        users_map[uid]["email"] = p.get("email")
                        users_map[uid]["image"] = p.get("profile_image_url")
                        users_map[uid]["streak"] = 0  # default
                        profile_id_map[p.get("id")] = uid
            except: pass
        
        # Fetch Streak data from notex_streak table
        if profile_id_map:
            try:
                streak_res = supabase.table("notex_streak").select("user_profile_id, current_streak").in_("user_profile_id", list(profile_id_map.keys())).execute()
                streaks = getattr(streak_res, 'data', []) or []
                for s in streaks:
                    profile_id = s.get("user_profile_id")
                    if profile_id in profile_id_map:
                        uid = profile_id_map[profile_id]
                        users_map[uid]["streak"] = s.get("current_streak", 0)
            except: pass
        
        # Convert to list
        for uid, stats in users_map.items():
            users_list.append(stats)
        
        # Sort by most active (sessions or last active)
        users_list.sort(key=lambda x: x.get("last_seen", ""), reverse=True)

        return {
            "kpi": {
                "dau": dau,
                "wau": wau,
                "avg_study_time": round(avg_study_time, 1),
                "sessions_per_user": round(avg_sessions, 1),
                "retention_7d": 85 # Dummy/Placeholder for now as logic is complex
            },
            "features": {
                "notex": notex_count,
                "blink": blink_count,
                "labx": labx_count,
                "avg_time_note": 12 # Placeholder
            },
            "learning": {
                "note_completion_rate": note_completion_rate,
                "blink_revisit": blink_revisit,
                "revisited_topics": len(revisit_users)
            },
            "funnel": {
                "topic_to_note": funnel_1,
                "note_to_lab": funnel_2
            },
            "users": users_list
        }
    except Exception as e:
        print(f"Dashboard Error: {e}")
        return {"error": str(e)}

app.include_router(analytics_router)

# -------------------- Lcoding Learning Tracks --------------------

class LcodingLanguageBase(BaseModel):
    name: str
    # slug removed
    description: Optional[str] = None
    logo_url: Optional[str] = None

class LcodingLanguageCreate(LcodingLanguageBase):
    pass

class LcodingLanguage(LcodingLanguageBase):
    id: str
    created_at: str
    updated_at: str

    class Config:
        orm_mode = True

class LcodingLevelBase(BaseModel):
    title: str
    order_index: int = 0

class LcodingLevelCreate(LcodingLevelBase):
    pass

class LcodingLevel(LcodingLevelBase):
    id: str
    language_id: str
    created_at: str
    updated_at: str

class LcodingSectionBase(BaseModel):
    title: str
    order_index: int = 0

class LcodingSectionCreate(LcodingSectionBase):
    pass

class LcodingSection(LcodingSectionBase):
    id: str
    level_id: str
    created_at: str
    updated_at: str

class LcodingTopicBase(BaseModel):
    title: str
    order_index: int = 0

class LcodingTopicCreate(LcodingTopicBase):
    pass

class LcodingTopic(LcodingTopicBase):
    id: str
    section_id: str
    created_at: str
    updated_at: str



class LcodingTopicUpdate(BaseModel):
    title: Optional[str] = None
    order_index: Optional[int] = None

lcoding_router = APIRouter(prefix="/api/lcoding", tags=["lcoding"])

@lcoding_router.get("/languages", response_model=List[LcodingLanguage])
def get_lcoding_languages():
    supabase = get_service_client()
    res = supabase.table("lcoding_languages").select("*").order("name").execute()
    return res.data or []

@lcoding_router.post("/languages", response_model=LcodingLanguage)
async def create_lcoding_language(
    name: str = Form(...),
    description: Optional[str] = Form(None),
    logo: Optional[UploadFile] = File(None)
):
    supabase = get_service_client()
    
    logo_url = None
    if logo:
        try:
            file_content = await logo.read()
            file_ext = logo.filename.split(".")[-1] if "." in logo.filename else "png"
            file_name = f"lcoding-logos/{uuid.uuid4()}.{file_ext}"
            
            # Upload to 'tunex' bucket
            res = supabase.storage.from_("tunex").upload(
                path=file_name,
                file=file_content,
                file_options={"content-type": logo.content_type}
            )
            
            # Get public URL
            logo_url = supabase.storage.from_("tunex").get_public_url(file_name)
        except Exception as e:
            print(f"Logo upload failed: {e}")
            
    payload = {
        "name": name,
        "description": description,
        "logo_url": logo_url,
        "updated_at": datetime.utcnow().isoformat()
    }
    
    res = supabase.table("lcoding_languages").insert(payload).execute()
    if not res.data:
        raise HTTPException(status_code=500, detail="Failed to create language")
    return res.data[0]

@lcoding_router.put("/languages/{lang_id}", response_model=LcodingLanguage)
async def update_lcoding_language(
    lang_id: str,
    name: str = Form(...),
    description: Optional[str] = Form(None),
    logo: Optional[UploadFile] = File(None)
):
    supabase = get_service_client()
    
    existing = supabase.table("lcoding_languages").select("id, logo_url").eq("id", lang_id).single().execute()
    if not existing.data:
        raise HTTPException(status_code=404, detail="Language not found")

    payload = {
        "name": name,
        "description": description,
        "updated_at": datetime.utcnow().isoformat()
    }

    if logo:
        try:
            file_content = await logo.read()
            file_ext = logo.filename.split(".")[-1] if "." in logo.filename else "png"
            file_name = f"lcoding-logos/{uuid.uuid4()}.{file_ext}"
            
            res = supabase.storage.from_("tunex").upload(
                path=file_name,
                file=file_content,
                file_options={"content-type": logo.content_type}
            )
            logo_url = supabase.storage.from_("tunex").get_public_url(file_name)
            payload["logo_url"] = logo_url
        except Exception as e:
            print(f"Logo upload failed: {e}")
            
    res = supabase.table("lcoding_languages").update(payload).eq("id", lang_id).execute()
    if not res.data:
         raise HTTPException(status_code=500, detail="Failed to update language")
    return res.data[0]

@lcoding_router.get("/languages/{lang_id}", response_model=LcodingLanguage)
def get_lcoding_language(lang_id: str):
    supabase = get_service_client()
    res = supabase.table("lcoding_languages").select("*").eq("id", lang_id).single().execute()
    if not res.data:
        raise HTTPException(status_code=404, detail="Language not found")
    return res.data

# --- Levels ---

@lcoding_router.get("/languages/{lang_id}/levels", response_model=List[LcodingLevel])
def get_lcoding_levels(lang_id: str):
    supabase = get_service_client()
    res = supabase.table("lcoding_levels").select("*").eq("language_id", lang_id).order("order_index").execute()
    return res.data or []

@lcoding_router.post("/languages/{lang_id}/levels", response_model=LcodingLevel)
def create_lcoding_level(lang_id: str, level: LcodingLevelCreate):
    supabase = get_service_client()
    payload = level.dict()
    payload["language_id"] = lang_id
    payload["updated_at"] = datetime.utcnow().isoformat()
    res = supabase.table("lcoding_levels").insert(payload).execute()
    if not res.data:
        raise HTTPException(status_code=500, detail="Failed to create level")
    return res.data[0]
    
@lcoding_router.get("/levels/{level_id}", response_model=LcodingLevel)
def get_lcoding_level(level_id: str):
    supabase = get_service_client()
    res = supabase.table("lcoding_levels").select("*").eq("id", level_id).single().execute()
    if not res.data:
        raise HTTPException(status_code=404, detail="Level not found")
    return res.data

@lcoding_router.put("/levels/{level_id}", response_model=LcodingLevel)
def update_lcoding_level(level_id: str, level: LcodingLevelCreate):
    supabase = get_service_client()
    payload = level.dict()
    payload["updated_at"] = datetime.utcnow().isoformat()
    # Ensure we don't accidentally wipe out language_id if pydantic excludes it, 
    # but since it's an update, Supabase handles partials for us if we sent them, 
    # but here we are sending full payload. Ideally we only update what changed.
    # LcodingLevelCreate has title and order_index.
    
    res = supabase.table("lcoding_levels").update(payload).eq("id", level_id).execute()
    if not res.data:
        raise HTTPException(status_code=500, detail="Failed to update level")
    return res.data[0]

@lcoding_router.delete("/levels/{level_id}")
def delete_lcoding_level(level_id: str):
    supabase = get_service_client()
    res = supabase.table("lcoding_levels").delete().eq("id", level_id).execute()
    if not res.data:
        raise HTTPException(status_code=404, detail="Level not found or failed to delete")
    return {"message": "Level deleted successfully"}

# --- Sections (now under Levels) ---

@lcoding_router.get("/levels/{level_id}/sections", response_model=List[LcodingSection])
def get_lcoding_sections(level_id: str):
    supabase = get_service_client()
    res = supabase.table("lcoding_sections").select("*").eq("level_id", level_id).order("order_index").execute()
    return res.data or []

@lcoding_router.post("/levels/{level_id}/sections", response_model=LcodingSection)
def create_lcoding_section(level_id: str, section: LcodingSectionCreate):
    supabase = get_service_client()
    payload = section.dict()
    payload["level_id"] = level_id
    payload["updated_at"] = datetime.utcnow().isoformat()
    res = supabase.table("lcoding_sections").insert(payload).execute()
    if not res.data:
        raise HTTPException(status_code=500, detail="Failed to create section")
    return res.data[0]

@lcoding_router.get("/sections/{section_id}", response_model=LcodingSection)
def get_lcoding_section(section_id: str):
    supabase = get_service_client()
    res = supabase.table("lcoding_sections").select("*").eq("id", section_id).single().execute()
    if not res.data:
        raise HTTPException(status_code=404, detail="Section not found")
    return res.data

@lcoding_router.get("/sections/{section_id}/topics", response_model=List[LcodingTopic])
def get_lcoding_topics(section_id: str):
    supabase = get_service_client()
    res = supabase.table("lcoding_topics").select("*").eq("section_id", section_id).order("order_index").execute()
    return res.data or []

@lcoding_router.post("/sections/{section_id}/topics", response_model=LcodingTopic)
def create_lcoding_topic(section_id: str, topic: LcodingTopicCreate):
    supabase = get_service_client()
    payload = topic.dict()
    payload["section_id"] = section_id
    payload["updated_at"] = datetime.utcnow().isoformat()
    res = supabase.table("lcoding_topics").insert(payload).execute()
    if not res.data:
        raise HTTPException(status_code=500, detail="Failed to create topic")
    return res.data[0]

@lcoding_router.get("/topics/{topic_id}", response_model=LcodingTopic)
def get_lcoding_topic(topic_id: str):
    supabase = get_service_client()
    res = supabase.table("lcoding_topics").select("*").eq("id", topic_id).single().execute()
    if not res.data:
        raise HTTPException(status_code=404, detail="Topic not found")
    return res.data

@lcoding_router.patch("/topics/{topic_id}", response_model=LcodingTopic)
def update_lcoding_topic(topic_id: str, topic: LcodingTopicUpdate):
    supabase = get_service_client()
    payload = topic.dict(exclude_unset=True)
    if not payload:
        raise HTTPException(status_code=400, detail="No fields to update")
    payload["updated_at"] = datetime.utcnow().isoformat()
    res = supabase.table("lcoding_topics").update(payload).eq("id", topic_id).execute()
    if not res.data:
        raise HTTPException(status_code=404, detail="Topic not found or failed to update")
    return res.data[0]

@lcoding_router.delete("/topics/{topic_id}")
def delete_lcoding_topic(topic_id: str):
    supabase = get_service_client()
    res = supabase.table("lcoding_topics").delete().eq("id", topic_id).execute()
    # Note: Supabase delete returns the deleted rows. If empty, it might mean not found OR already deleted.
    if not res.data:
         # Check if it existed? Or just return success.
         # For safety, let's assume if it returns nothing, it wasn't there.
         raise HTTPException(status_code=404, detail="Topic not found or failed to delete")
    return {"message": "Topic deleted successfully"}

app.include_router(lcoding_router)
app.include_router(problems_api.router) # Problem Solver Routes

if __name__ == "__main__":
    import uvicorn

    uvicorn.run(app, host="0.0.0.0", port=int(os.getenv("PORT", "8000")))

# ... (existing code)

# --- Python Compiler Endpoint ---
from packages.python_compiler import execute_python_code
from packages.java_compiler import execute_java_code

class CompilerRequest(BaseModel):
    code: str

@app.post("/api/tunex/compiler/run")
async def run_python_compiler(request: CompilerRequest):
    """
    Executes python code sent from the frontend.
    """
    result = execute_python_code(request.code)
    return result


@app.post("/api/tunex/compiler/java/run")
async def run_java_compiler(request: CompilerRequest):
    """Compiles and runs Java code sent from the frontend."""
    result = execute_java_code(request.code)
    return result

# ------------------------------------------------------------------------------
# Blink Generation Endpoint (Gemini 3 Pro + Supabase)
# ------------------------------------------------------------------------------

class BlinkRequest(BaseModel):
    topic: Optional[str] = None
    topic_id: Optional[str] = None
    note_content: Optional[str] = None # Optional override

@app.get("/api/blink/links")
async def get_blink_links(topics: str = Query(..., description="Comma-separated list of topic names")):
    """
    Fetch existing blink links for given topic names.
    Returns a map of topic_name -> blink_link for topics that have blinks.
    """
    supabase = get_service_client()
    topics_list = [t.strip().lower() for t in topics.split(",") if t.strip()]
    
    if not topics_list:
        return {"links": {}}
    
    try:
        res = supabase.table(AI_NOTES_TABLE).select("title, title_ci, blink_link").in_("title_ci", topics_list).execute()
        data = getattr(res, 'data', []) or []
        
        links = {}
        for row in data:
            blink_link = row.get("blink_link")
            if blink_link and blink_link.strip():
                # Use original title as key for better matching
                links[row.get("title_ci") or row.get("title", "").lower()] = blink_link
        
        return {"links": links}
    except Exception as e:
        print(f"[Blink] Error fetching links: {e}")
        return {"links": {}}

@app.post("/api/blink/generate")
async def generate_blink_endpoint(
    req: BlinkRequest, 
    user_id: str = "00000000-0000-0000-0000-000000000000" 
):
    """
    Generate a 'Blink' (landscape illustration) for a given topic or note content.
    """
    
    # 1. Get content
    content_to_illustrate = req.note_content
    
    # If content not provided directly, try to fetch from DB
    if not content_to_illustrate and (req.topic or req.topic_id):
        supabase = get_service_client()
        try:
            # Attempt 1: Try by ID if provided
            data = []
            if req.topic_id:
                print(f"[Blink] Looking up note by ID: {req.topic_id}")
                res = supabase.table(AI_NOTES_TABLE).select("markdown, id").eq("id", req.topic_id).limit(1).execute()
                data = getattr(res, 'data', [])
            
            # Attempt 2: If no data yet and topic provided, try by title
            if not data and req.topic:
                print(f"[Blink] No note found by ID or no ID. Looking up by title: {req.topic}")
                res = supabase.table(AI_NOTES_TABLE).select("markdown, id").eq("title_ci", req.topic.strip().lower()).limit(1).execute()
                data = getattr(res, 'data', [])

            if data:
                content_to_illustrate = data[0].get("markdown", "")
                # If we found it by title but didn't have the ID (or it differed), update req.topic_id so we update the correct row later
                real_id = data[0].get("id")
                if real_id:
                     req.topic_id = real_id
            else:
                print("[Blink] Note not found in DB.")
        except Exception as e:
            print(f"Error fetching note: {e}")

    if not content_to_illustrate:
         raise HTTPException(status_code=404, detail="Note content not found for this topic. Please generate notes first.")

    # 2. Generate Image
    if not os.getenv("GEMINI_API_KEY"):
        raise HTTPException(status_code=500, detail="GEMINI_API_KEY not configured.")
        
    if genai is None:
         # raise HTTPException(status_code=500, detail="google-genai library not installed.")
         print("Warning: google-genai not imported. Simulating or failing.")

    prompt = f"""now,create me an one landscape illustration for the below notes so that just by looking this one illustration, they can understand the entire notes completely i want the ilustratio to be professional and white bg, content ={content_to_illustrate[:8000]}"""  

    print(f"Generating Blink for topic '{req.topic or req.topic_id}'...")
    
    try:
        def _generate_bytes_sync():
            # Check if genai is available
            if not genai:
                raise RuntimeError("google-genai library not available")

            client_g = genai.Client(api_key=os.getenv("GEMINI_API_KEY"))
            
            # Using user-provided structure specifically:
            chat = client_g.chats.create(
                model="gemini-3-pro-image-preview", 
                config=types.GenerateContentConfig(
                    response_modalities=['TEXT', 'IMAGE'],
                    tools=[{"google_search": {}}]
                )
            )

            resp = chat.send_message(prompt,
                config=types.GenerateContentConfig(
                    image_config=types.ImageConfig(
                        aspect_ratio="16:9",
                        image_size="2K"
                    ),
            ))
            
            for part in resp.parts:
                if part.as_image():
                    return part.as_image().image_bytes
                elif part.text:
                    print(f"[Blink] Model returned text: {part.text}")
            
            raise RuntimeError(f"No image part in response. Response text: {resp.text if hasattr(resp, 'text') else 'Unknown'}")

        image_bytes = await run_in_threadpool(_generate_bytes_sync)
        
        # 3. Upload to Supabase
        filename = f"gen_blink_{uuid.uuid4().hex[:8]}.png"
        bucket_name = "blink"
        
        supabase = get_service_client()
        
        def _upload_sync():
            res = supabase.storage.from_(bucket_name).upload(
                path=filename,
                file=image_bytes,
                file_options={"content-type": "image/png"}
            )
            pub = supabase.storage.from_(bucket_name).get_public_url(filename)
            return pub

        public_url = await run_in_threadpool(_upload_sync)
        
        # 4. Update DB
        if req.topic_id:
             supabase.table(AI_NOTES_TABLE).update({"blink_link": public_url}).eq("id", req.topic_id).execute()
        
        return {
            "success": True,
            "url": public_url,
            "filename": filename
        }

    except Exception as e:
        import traceback
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=str(e))

# =============================================================================
# INLINED: tunex_router.py (Tunex API endpoints)
# =============================================================================

tunex_router = APIRouter(prefix="/api/tunex", tags=["tunex"])
tunex_logger = logging.getLogger("tunex")

ALLOWED_CHAPTER_TYPES = {
    "concept", "syntax", "range", "sequences", "nested", "keywords",
    "mistakes", "walkthrough", "interview", "quiz", "dynamic",
}

def _get_gemini_api_key() -> str:
    api_key = (os.getenv("GEMINI_API_KEY", "") or os.getenv("GOOGLE_API_KEY", "")).strip()
    if not api_key:
        raise HTTPException(status_code=500, detail="Missing GEMINI_API_KEY in environment.")
    return api_key

def _extract_json_object_tunex(text: str) -> str:
    if not text:
        raise ValueError("Empty model response")
    fence = re.search(r"```(?:json)?\s*(\{[\s\S]*?\})\s*```", text, flags=re.IGNORECASE)
    if fence:
        return fence.group(1)
    start = text.find("{")
    end = text.rfind("}")
    if start == -1 or end == -1 or end <= start:
        raise ValueError("No JSON object found in response")
    return text[start : end + 1]

def _repair_json_loose(raw: str) -> str:
    if not raw:
        return raw
    s = raw.replace("\r\n", "\n")
    out = []
    in_string = False
    escape = False
    for ch in s:
        if in_string:
            if escape:
                escape = False
                out.append(ch)
                continue
            if ch == "\\":
                escape = True
                out.append(ch)
                continue
            if ch == "\n":
                out.append("\\n")
                continue
            if ch == "\t":
                out.append("\\t")
                continue
            if ch == '"':
                in_string = False
            out.append(ch)
        else:
            if ch == '"':
                in_string = True
            out.append(ch)
    fixed = "".join(out)
    fixed = re.sub(r",\s*([}\]])", r"\1", fixed)
    return fixed

def _slugify(value: str) -> str:
    s = (value or "").strip().lower()
    s = re.sub(r"[^a-z0-9]+", "_", s)
    s = re.sub(r"_+", "_", s).strip("_")
    return s or "topic"

def _build_topic_template(*, topic_title: str, language_name: Optional[str]) -> dict:
    slug = _slugify(topic_title)
    lang = (language_name or "Python").strip() or "Python"
    def cid(suffix: str) -> str:
        return f"{slug}_{suffix}"
    return {
        "description": "",
        "chapters": [
            {"chapter_number": 1, "title": f"What is {topic_title}?", "chapter_type": "concept", "content": {"blocks": [{"type": "text", "title": "", "content": ""}, {"type": "static_code", "title": "Example", "code": "", "output": ""}, {"type": "callout", "variant": "info", "title": "Think of it like this", "text": ""}]}},
            {"chapter_number": 2, "title": "Key Ideas", "chapter_type": "concept", "content": {"blocks": [{"type": "text", "title": "Core idea", "content": ""}, {"type": "static_code", "title": "Quick example", "code": "", "output": ""}, {"type": "callout", "variant": "info", "title": "Why it matters", "text": ""}]}},
            {"chapter_number": 3, "title": "How It Works", "chapter_type": "concept", "content": {"blocks": [{"type": "text", "title": "Under the hood", "content": ""}, {"type": "static_code", "title": "Demonstration", "code": "", "output": ""}, {"type": "code", "id": cid("try_1"), "title": "Try it yourself", "default_code": ""}]}},
            {"chapter_number": 4, "title": "Patterns & Variations", "chapter_type": "concept", "content": {"blocks": [{"type": "section", "icon": "ri-lightbulb-flash-line", "title": "Pattern 1", "text": ""}, {"type": "code", "id": cid("pattern_1"), "title": "Practice", "default_code": ""}, {"type": "carousel", "items": [{"title": "Easy", "desc": "", "code_id": cid("easy_1"), "default_code": ""}, {"title": "Medium", "desc": "", "code_id": cid("medium_1"), "default_code": ""}, {"title": "Hard", "desc": "", "code_id": cid("hard_1"), "default_code": ""}]}, {"type": "pro_tip", "title": "Pro Tip", "text": "", "code": ""}, {"type": "use_cases", "title": "Common use cases", "items": ["", "", ""]}]}},
            {"chapter_number": 5, "title": "Edge Cases", "chapter_type": "concept", "content": {"blocks": [{"type": "text", "title": "Watch out for", "content": ""}, {"type": "static_code", "title": "Edge case example", "code": "", "output": ""}, {"type": "callout", "variant": "warning", "title": "Rule of thumb", "text": ""}]}},
            {"chapter_number": 6, "title": "Core Keywords & Conventions", "chapter_type": "concept", "content": {"blocks": [{"type": "text", "title": "Key ideas", "content": ""}, {"type": "keyword_cards", "keywords": [{"name": "return", "tag": "KEYWORD", "tag_color": "#3B82F6", "desc": "", "code": ""}, {"name": "_", "tag": "CONVENTION", "tag_color": "#6B7280", "desc": "", "code": ""}]}, {"type": "code", "id": cid("try_2"), "title": "Try it", "default_code": ""}]}},
            {"chapter_number": 7, "title": "Common Mistakes", "chapter_type": "mistakes", "content": {"mistakes": [{"name": "", "icon": "ri-close-circle-fill", "desc": "", "fix": ""}, {"name": "", "icon": "ri-close-circle-fill", "desc": "", "fix": ""}, {"name": "", "icon": "ri-close-circle-fill", "desc": "", "fix": ""}]}},
            {"chapter_number": 8, "title": "Solved Problem Walkthrough", "chapter_type": "walkthrough", "content": {"problem": {"title": "", "desc": "", "example": ""}, "steps": [{"title": "Step 1: Understand", "text": ""}, {"title": "Step 2: Code", "code_id": cid("walkthrough"), "default_code": ""}, {"title": "Step 3: Complexity", "items": ["", ""]}], "takeaway": ""}},
            {"chapter_number": 9, "title": "Interview Questions", "chapter_type": "interview", "content": {"layout": "company_cards", "questions": [{"company": "Google", "role": "Software Engineer", "icon": "ri-google-fill", "color": "#4285F4", "tag": "", "question": ""}, {"company": "Amazon", "role": "SDE", "icon": "ri-amazon-fill", "color": "#FF9900", "tag": "", "question": ""}, {"company": "Microsoft", "role": "Engineer", "icon": "ri-microsoft-fill", "color": "#00A4EF", "tag": "", "question": ""}, {"company": "Spotify", "role": "Backend", "icon": "ri-spotify-fill", "color": "#1DB954", "tag": "", "question": ""}], "tips": ["", "", ""]}},
            {"chapter_number": 10, "title": "Quick Quiz", "chapter_type": "quiz", "content": {"questions": [{"q": "", "opts": ["", "", ""], "correct": 0, "why": ""}, {"q": "", "opts": ["", "", ""], "correct": 0, "why": ""}, {"q": "", "opts": ["", "", ""], "correct": 0, "why": ""}, {"q": "", "opts": ["", "", ""], "correct": 0, "why": ""}, {"q": "", "opts": ["", "", ""], "correct": 0, "why": ""}]}},
        ],
        "_meta": {"language": lang, "template_version": "v1"},
    }

def _merge_template(template: dict, candidate: dict) -> dict:
    out = json.loads(json.dumps(template))
    def _sanitize_strings(obj):
        if isinstance(obj, str):
            return re.sub(r"\bundefined\b", "", obj, flags=re.IGNORECASE).strip()
        if isinstance(obj, list):
            return [_sanitize_strings(x) for x in obj]
        if isinstance(obj, dict):
            return {k: _sanitize_strings(v) for k, v in obj.items()}
        return obj
    if isinstance(candidate, dict):
        desc = candidate.get("description")
        if isinstance(desc, str) and desc.strip():
            out["description"] = desc.strip()
        cand_chapters = candidate.get("chapters")
        if isinstance(cand_chapters, list):
            by_num = {}
            for ch in cand_chapters:
                if isinstance(ch, dict) and isinstance(ch.get("chapter_number"), int):
                    by_num[ch["chapter_number"]] = ch
            for ch in out.get("chapters", []):
                num = ch.get("chapter_number")
                cand = by_num.get(num)
                if not isinstance(cand, dict):
                    continue
                title = cand.get("title")
                if isinstance(title, str) and title.strip():
                    ch["title"] = title.strip()
                cand_content = cand.get("content")
                if not isinstance(cand_content, dict):
                    continue
                if ch.get("chapter_type") in {"concept", "syntax", "range", "sequences", "nested", "keywords", "dynamic"}:
                    tpl_blocks = ((ch.get("content") or {}).get("blocks") or [])
                    cand_blocks = cand_content.get("blocks")
                    if isinstance(tpl_blocks, list) and isinstance(cand_blocks, list):
                        for i in range(min(len(tpl_blocks), len(cand_blocks))):
                            tb = tpl_blocks[i]
                            cb = cand_blocks[i]
                            if isinstance(tb, dict) and isinstance(cb, dict):
                                for k, v in cb.items():
                                    if k in {"type", "id", "code_id"}:
                                        continue
                                    if isinstance(v, (str, int, float, bool)) or v is None:
                                        tb[k] = v
                                    elif isinstance(v, (list, dict)):
                                        tb[k] = v
                    ch["content"] = {"blocks": tpl_blocks}
                elif ch.get("chapter_type") == "mistakes":
                    mistakes = cand_content.get("mistakes")
                    if isinstance(mistakes, list) and mistakes:
                        cleaned = []
                        for m in mistakes[:6]:
                            if isinstance(m, dict) and isinstance(m.get("name"), str):
                                cleaned.append({"name": m.get("name", "").strip(), "icon": m.get("icon", "ri-close-circle-fill"), "desc": m.get("desc", "").strip(), "fix": m.get("fix", "").strip()})
                        if cleaned:
                            ch["content"] = {"mistakes": cleaned}
                elif ch.get("chapter_type") == "quiz":
                    questions = cand_content.get("questions")
                    if isinstance(questions, list) and questions:
                        cleaned = []
                        for q in questions[:8]:
                            if isinstance(q, dict) and isinstance(q.get("q"), str) and isinstance(q.get("opts"), list):
                                cleaned.append({"q": q.get("q", "").strip(), "opts": q.get("opts", [])[:4], "correct": int(q.get("correct", 0)), "why": (q.get("why") or "").strip()})
                        if cleaned:
                            ch["content"] = {"questions": cleaned}
    return _sanitize_strings(out)

def _normalize_and_validate_ai_payload(payload: dict, *, topic_id: str, template: dict) -> tuple:
    if not isinstance(payload, dict):
        raise ValueError("AI payload must be an object")
    merged = _merge_template(template, payload)
    description = merged.get("description") if isinstance(merged.get("description"), str) else ""
    chapters = merged.get("chapters")
    if not isinstance(chapters, list) or not chapters:
        raise ValueError("Merged AI payload missing chapters")
    cleaned = []
    for ch in chapters:
        if not isinstance(ch, dict):
            continue
        num = ch.get("chapter_number")
        title = ch.get("title")
        chapter_type = ch.get("chapter_type")
        content = ch.get("content")
        if not isinstance(num, int) or num <= 0:
            continue
        if not isinstance(title, str) or not title.strip():
            continue
        if not isinstance(chapter_type, str) or chapter_type not in ALLOWED_CHAPTER_TYPES:
            chapter_type = "concept"
        if not isinstance(content, dict):
            content = {}
        cleaned.append({"topic_id": topic_id, "chapter_number": num, "title": title.strip(), "chapter_type": chapter_type, "content": content})
    cleaned.sort(key=lambda d: d["chapter_number"])
    return description.strip(), cleaned

def _gemini_generate_topic_content_sync(*, topic_title: str, language_name: Optional[str], level_title: Optional[str], section_title: Optional[str]) -> dict:
    import google.generativeai as genai
    genai.configure(api_key=_get_gemini_api_key())
    model_name = (os.getenv("GEMINI_TUNEX_MODEL", "") or os.getenv("GEMINI_MODEL", "") or "gemini-2.5-pro").strip()
    model = genai.GenerativeModel(model_name=model_name)
    template = _build_topic_template(topic_title=topic_title, language_name=language_name)
    template_json = json.dumps(template, ensure_ascii=False)
    prompt = f"""You generate structured Tunex topic content.
Context: Topic title: {topic_title}, Language: {language_name or ''}, Level: {level_title or ''}, Section: {section_title or ''}
Return ONLY valid JSON. CRITICAL: Return exact same JSON structure as TEMPLATE. Fill ALL empty strings. All code in {language_name or 'Python'}.
TEMPLATE:
{template_json}""".strip()
    try:
        from google.generativeai.types import GenerationConfig
        generation_config = GenerationConfig(response_mime_type="application/json", temperature=0.4)
    except:
        generation_config = {"response_mime_type": "application/json", "temperature": 0.4}
    resp = model.generate_content(prompt, generation_config=generation_config)
    text = getattr(resp, "text", None) or ""
    raw_json = _extract_json_object_tunex(text)
    try:
        return json.loads(raw_json)
    except json.JSONDecodeError:
        try:
            return json.loads(raw_json, strict=False)
        except json.JSONDecodeError:
            cleaned = _repair_json_loose(raw_json)
            try:
                return json.loads(cleaned, strict=False)
            except:
                from json_repair import repair_json
                return json.loads(repair_json(cleaned), strict=False)

def get_tunex_supabase() -> Client:
    if not SUPABASE_URL or not SUPABASE_SERVICE_ROLE_KEY:
        raise HTTPException(status_code=500, detail="Database configuration missing")
    return create_client(SUPABASE_URL, SUPABASE_SERVICE_ROLE_KEY)

@tunex_router.get("/topics/{topic_id}/full")
async def get_topic_full(topic_id: str):
    """Get topic metadata and all chapters with full content."""
    supabase = get_tunex_supabase()
    topic_res = supabase.table("lcoding_topics").select("id, title, order_index, section_id").eq("id", topic_id).execute()
    if not topic_res.data:
        raise HTTPException(status_code=404, detail="Topic not found")
    topic = topic_res.data[0]
    section_title = level_title = language_name = track_title = None
    section_id = topic.get("section_id")
    if section_id:
        section_res = supabase.table("lcoding_sections").select("id, title, level_id").eq("id", section_id).execute()
        if section_res.data:
            section = section_res.data[0]
            section_title = section.get("title")
            level_id = section.get("level_id")
            if level_id:
                level_res = supabase.table("lcoding_levels").select("id, title, language_id").eq("id", level_id).execute()
                if level_res.data:
                    level = level_res.data[0]
                    level_title = level.get("title")
                    language_id = level.get("language_id")
                    if language_id:
                        language_res = supabase.table("lcoding_languages").select("id, name").eq("id", language_id).execute()
                        if language_res.data:
                            language_name = language_res.data[0].get("name")
    if language_name and level_title:
        track_title = level_title if language_name.lower() in level_title.lower() else f"{language_name} {level_title}".strip()
    else:
        track_title = language_name or level_title or section_title
    chapters_res = supabase.table("lcoding_topic_chapters").select("id, chapter_number, chapter_type, title, content").eq("topic_id", topic_id).order("chapter_number").execute()
    if not chapters_res.data:
        try:
            await ensure_topic_content(topic_id=topic_id)
            chapters_res = supabase.table("lcoding_topic_chapters").select("id, chapter_number, chapter_type, title, content").eq("topic_id", topic_id).order("chapter_number").execute()
        except Exception as e:
            tunex_logger.exception("AI auto-populate failed: %s", e)
    return {"id": topic["id"], "title": topic["title"], "description": "", "chapters": chapters_res.data, "track_title": track_title, "language_name": language_name, "level_title": level_title, "section_title": section_title}

@tunex_router.post("/topics/{topic_id}/ai/ensure")
async def ensure_topic_content(topic_id: str, force: bool = False):
    """Ensure a topic has chapters. If empty, generate via Gemini."""
    supabase = get_tunex_supabase()
    topic_res = supabase.table("lcoding_topics").select("id, title, section_id").eq("id", topic_id).execute()
    if not topic_res.data:
        raise HTTPException(status_code=404, detail="Topic not found")
    topic = topic_res.data[0]
    existing = supabase.table("lcoding_topic_chapters").select("id").eq("topic_id", topic_id).limit(1).execute()
    if existing.data and not force:
        return {"status": "ok", "generated": False, "reason": "already_has_content"}
    section_title = level_title = language_name = None
    section_id = topic.get("section_id")
    if section_id:
        section_res = supabase.table("lcoding_sections").select("id, title, level_id").eq("id", section_id).execute()
        if section_res.data:
            section = section_res.data[0]
            section_title = section.get("title")
            level_id = section.get("level_id")
            if level_id:
                level_res = supabase.table("lcoding_levels").select("id, title, language_id").eq("id", level_id).execute()
                if level_res.data:
                    level = level_res.data[0]
                    level_title = level.get("title")
                    language_id = level.get("language_id")
                    if language_id:
                        language_res = supabase.table("lcoding_languages").select("id, name").eq("id", language_id).execute()
                        if language_res.data:
                            language_name = language_res.data[0].get("name")
    try:
        topic_title = str(topic.get("title") or "").strip()
        payload = await run_in_threadpool(_gemini_generate_topic_content_sync, topic_title=topic_title, language_name=language_name, level_title=level_title, section_title=section_title)
        template = _build_topic_template(topic_title=topic_title, language_name=language_name)
        description, chapter_rows = _normalize_and_validate_ai_payload(payload, topic_id=topic_id, template=template)
    except HTTPException:
        raise
    except Exception as e:
        tunex_logger.exception("Gemini generation failed: %s", e)
        raise HTTPException(status_code=500, detail=f"Gemini generation failed: {e}")
    if force:
        try:
            supabase.table("lcoding_topic_chapters").delete().eq("topic_id", topic_id).execute()
        except:
            pass
    try:
        supabase.table("lcoding_topic_chapters").insert(chapter_rows).execute()
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to insert chapters: {e}")
    return {"status": "ok", "generated": True, "chapters_inserted": len(chapter_rows), "description": description}

# =============================================================================
# INLINED: problems_api.py (Problem Solver endpoints)
# =============================================================================

problems_router = APIRouter(prefix="/api/tunex", tags=["problems"])
problems_logger = logging.getLogger("problems_api")

@problems_router.get("/problems/{id}")
async def get_problem(id: str):
    supabase = get_tunex_supabase()
    mocks = {
        "p_google": {"id": "p_google", "title": "Google Search Algorithm", "difficulty": "Hard", "companies": ["Google"], "function_name": "search", "description": "Implement a simplified search ranking algorithm.", "boilerplate_code": "def search(docs, query):\n    pass"},
        "p_netflix": {"id": "p_netflix", "title": "Movie Recommendation", "difficulty": "Medium", "companies": ["Netflix"], "function_name": "recommend", "description": "Recommend top 3 movies based on genre similarity.", "boilerplate_code": "def recommend(history, movies):\n    pass"},
    }
    if id in mocks:
        return mocks[id]
    res = supabase.table("lcoding_problems").select("*").eq("id", id).execute()
    if not res.data:
        raise HTTPException(status_code=404, detail="Problem not found")
    return res.data[0]

@problems_router.post("/problems/{id}/run")
async def run_problem(id: str, code: dict = Body(...)):
    user_code = code.get("code", "")
    supabase = get_tunex_supabase()
    p_res = supabase.table("lcoding_problems").select("function_name").eq("id", id).execute()
    if not p_res.data:
        raise HTTPException(status_code=404, detail="Problem not found")
    func_name = p_res.data[0]['function_name']
    tc_res = supabase.table("lcoding_test_cases").select("*").eq("problem_id", id).order("order_index").execute()
    test_cases = tc_res.data
    if not test_cases:
        return {"status": "error", "error": "No test cases found."}
    inputs = [tc['input_json'] for tc in test_cases]
    expecteds = [tc['expected_output_json'] for tc in test_cases]
    harness = f"""
import json
{user_code}
def run_tests():
    inputs = {json.dumps(inputs)}
    expecteds = {json.dumps(expecteds)}
    results = []
    for i in range(len(inputs)):
        args, expected = inputs[i], expecteds[i]
        try:
            if '{func_name}' not in globals():
                results.append({{"passed": False, "error": "Function not defined"}})
                continue
            output = {func_name}(*args) if isinstance(args, list) else {func_name}(args)
            results.append({{"passed": output == expected, "output": output, "expected": expected, "input": args}})
        except Exception as e:
            results.append({{"passed": False, "error": str(e)}})
    print(json.dumps(results))
if __name__ == "__main__":
    run_tests()
"""
    exec_res = execute_python_code(harness, timeout=5)
    if exec_res['status'] != 'success':
        return {"status": "compile_error", "compile_error": exec_res['error'], "total_tests": len(test_cases), "passed_tests": 0, "results": []}
    try:
        raw_results = json.loads(exec_res['output'])
    except:
        return {"status": "runtime_error", "compile_error": "Failed to parse output", "total_tests": len(test_cases), "passed_tests": 0, "results": []}
    passed_count = sum(1 for r in raw_results if r.get('passed'))
    return {"status": "success", "total_tests": len(test_cases), "passed_tests": passed_count, "results": raw_results}

# =============================================================================
# Register all routers
# =============================================================================

app.include_router(lcoding_router)
app.include_router(tunex_router)  # Tunex Routes (topics, AI generation)
app.include_router(problems_router)  # Problem Solver Routes
app.include_router(yt_transcript_router, prefix="/api")  # YouTube Transcript Routes

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)

