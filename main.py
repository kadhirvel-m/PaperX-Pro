"""Single-file consolidated FastAPI app for PaperX."""

from __future__ import annotations

import asyncio
import enum
import io
import json
import logging
import os
import random
import re
import requests
import sys
import textwrap
import time
import uuid
import math
import ast
from dataclasses import dataclass, field
from datetime import datetime, date, timezone
from decimal import Decimal
from functools import lru_cache
from pathlib import Path
from typing import Any, AsyncGenerator, Dict, Iterator, List, Literal, Optional, Set, Tuple
from urllib.parse import quote, urlparse
import threading
import concurrent.futures

# New imports for using google.genai as requested
try:
    from google import genai
    from google.genai import types
except ImportError:
    genai = None
    types = None


from autogen_agentchat.agents import AssistantAgent
from autogen_core.models import ModelInfo
from autogen_ext.models.openai import OpenAIChatCompletionClient
from bs4 import BeautifulSoup
from dotenv import load_dotenv
from fastapi import APIRouter, Body, FastAPI, File, Form, Header, HTTPException, Query, UploadFile, WebSocket, WebSocketDisconnect, Request
from starlette.concurrency import run_in_threadpool
from fastapi.middleware.cors import CORSMiddleware
from fastapi.middleware.gzip import GZipMiddleware
from fastapi.responses import FileResponse, JSONResponse, Response, StreamingResponse, RedirectResponse
from fastapi.staticfiles import StaticFiles
try:
    import httpx  # Optional: used for catching RemoteProtocolError from underlying HTTP calls
    from httpx import RemoteProtocolError as HTTPXRemoteProtocolError  # type: ignore
except Exception:  # pragma: no cover
    httpx = None
    class HTTPXRemoteProtocolError(Exception):
        pass

try:
    import fitz  # type: ignore
except Exception:  # pragma: no cover
    fitz = None  # type: ignore

try:
    import docx  # type: ignore
except Exception:  # pragma: no cover
    docx = None  # type: ignore

try:
    import PyPDF2  # type: ignore
except Exception:  # pragma: no cover
    PyPDF2 = None  # type: ignore

try:
    from pptx import Presentation  # type: ignore
except Exception:  # pragma: no cover
    Presentation = None  # type: ignore

try:
    import textract  # type: ignore
except Exception:  # pragma: no cover
    textract = None  # type: ignore
from markdownify import markdownify as md
from pydantic import BaseModel, Field, HttpUrl, validator, root_validator
from rapidfuzz import fuzz
from serpapi import GoogleSearch
from supabase import Client, create_client
from postgrest.exceptions import APIError
from packages import tunex_router
from packages import problems_api # New Problem Solver API

# Inlined imports
import html
import socket
import unicodedata
import tempfile
import regex as rx
from urllib.error import HTTPError, URLError
from urllib.request import Request as UrllibRequest, urlopen
from concurrent.futures import ThreadPoolExecutor
from threading import Lock
from youtube_transcript_api import (
    YouTubeTranscriptApi,
    TranscriptsDisabled,
    NoTranscriptFound,
    VideoUnavailable,
)
from fastapi.responses import PlainTextResponse
try:
    # Used for fetching YouTube video metadata (channel, views, etc.)
    from yt_dlp import YoutubeDL  # type: ignore
except Exception:  # pragma: no cover
    YoutubeDL = None  # type: ignore

# Load .env from this module's directory to avoid CWD issues
try:
    _env_path = (Path(__file__).resolve().parent / ".env")
    # override=True so a valid file value isn't shadowed by a stale OS env
    load_dotenv(dotenv_path=str(_env_path), override=True)
except Exception:
    # Fallback to default discovery
    load_dotenv()

# ==========================================
# INLINED PACKAGES: youtube_video & yt_transcript
# ==========================================

# --- youtube_video.py content ---

def _parse_video_id(url: str) -> Optional[str]:
    """Extract video ID from YouTube URL."""
    if not url:
        return None
    # Already just an ID
    if re.match(r'^[A-Za-z0-9_-]{11}$', url):
        return url
    # Extract from various URL formats
    patterns = [
        r'(?:youtube\.com\/watch\?v=|youtu\.be\/|youtube\.com\/embed\/)([A-Za-z0-9_-]{11})',
        r'youtube\.com\/shorts\/([A-Za-z0-9_-]{11})',
    ]
    for pattern in patterns:
        match = re.search(pattern, url)
        if match:
            return match.group(1)
    return None


def _format_duration(seconds: Optional[int]) -> str:
    """Format duration in seconds to MM:SS or HH:MM:SS."""
    if seconds is None:
        return ""
    try:
        total_seconds = int(round(float(seconds)))
    except (TypeError, ValueError):
        return ""
    if total_seconds <= 0:
        return ""
    hours, remainder = divmod(total_seconds, 3600)
    minutes, secs = divmod(remainder, 60)
    if hours > 0:
        return f"{hours}:{minutes:02d}:{secs:02d}"
    return f"{minutes}:{secs:02d}"


def _format_views(views: Optional[int]) -> str:
    """Format view count with commas."""
    if views is None:
        return ""
    if views >= 1_000_000:
        return f"{views / 1_000_000:.1f}M views"
    elif views >= 1_000:
        return f"{views / 1_000:.1f}K views"
    return f"{views:,} views"


_channel_logo_cache: Dict[str, str] = {}
_channel_logo_lock = Lock()
_DEFAULT_CHANNEL_LOGO = "https://www.youtube.com/s/desktop/94838207/img/favicon_144x144.png"


def _normalize_channel_logo(logo_url: str) -> str:
    """Downscale large channel logo URLs to a smaller size."""
    if not logo_url or not isinstance(logo_url, str):
        return ""
    return re.sub(r"=s\d+-c", "=s88-c", logo_url, count=1)


def _fetch_channel_logo(channel_page_url: Optional[str]) -> str:
    """
    Fetch the channel avatar URL by scraping the channel page's open graph metadata.
    Results are cached per-channel to avoid repeated network requests.
    """
    if not channel_page_url:
        return ""

    channel_page_url = channel_page_url.strip()
    if not channel_page_url:
        return ""

    with _channel_logo_lock:
        cached = _channel_logo_cache.get(channel_page_url)
    if cached is not None:
        return cached

    try:
        req = UrllibRequest(
            channel_page_url,
            headers={
                "User-Agent": (
                    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
                    "AppleWebKit/537.36 (KHTML, like Gecko) "
                    "Chrome/116.0 Safari/537.36"
                )
            },
        )
        with urlopen(req, timeout=4) as response:
            charset = response.headers.get_content_charset() or "utf-8"
            html_text = response.read().decode(charset, errors="ignore")
    except (ValueError, HTTPError, URLError, TimeoutError, socket.timeout):
        with _channel_logo_lock:
            _channel_logo_cache[channel_page_url] = ""
        return ""

    logo_match = re.search(
        r'<meta[^>]+property=["\']og:image["\'][^>]*content=["\']([^"\']+)["\']',
        html_text,
        flags=re.IGNORECASE,
    )
    if not logo_match:
        with _channel_logo_lock:
            _channel_logo_cache[channel_page_url] = ""
        return ""

    logo_url = html.unescape(logo_match.group(1))
    with _channel_logo_lock:
        _channel_logo_cache[channel_page_url] = logo_url
    return logo_url


def get_channel_logo(channel_page_url: Optional[str]) -> str:
    """Public helper to resolve and normalize a channel logo."""
    return _normalize_channel_logo(_fetch_channel_logo(channel_page_url)) or ""


def get_default_channel_logo() -> str:
    return _DEFAULT_CHANNEL_LOGO


def search_youtube_videos(query: str, num: int = 8, *, prefetch_logos: bool = False) -> List[Dict[str, str]]:
    """
    Search YouTube videos using yt-dlp.
    
    Args:
        query: Search query string
        num: Number of results to return (default 8, max 20)
    
    Returns:
        List of video dictionaries with title, link, channel, views, duration, thumbnail
    """
    if not YoutubeDL:
        raise ImportError("yt-dlp is not installed. Install it with: pip install yt-dlp")
    
    if not query or not query.strip():
        return []
    
    # Limit results
    num = max(1, min(num, 20))
    
    # yt-dlp options for searching
    ydl_opts = {
        'quiet': True,
        'no_warnings': True,
        'extract_flat': True,  # Don't download, just extract metadata
        'force_generic_extractor': False,
        'default_search': 'ytsearch',  # Use YouTube search
        'format': 'best',
        'noplaylist': True,
        'playlistend': num,
        'cachedir': False,
    }
    
    videos: List[Dict[str, str]] = []
    
    try:
        with YoutubeDL(ydl_opts) as ydl:
            # Search for videos (ytsearch{num}:{query})
            search_query = f"ytsearch{num}:{query}"
            result = ydl.extract_info(search_query, download=False)
            
            if not result or 'entries' not in result:
                return []
            
            entries = result.get('entries', [])
            limited_entries = entries[:num]

            if prefetch_logos:
                unique_pages: List[str] = []
                for entry in limited_entries:
                    channel_page = (entry.get('channel_url') or entry.get('uploader_url') or "").strip()
                    if channel_page and channel_page not in unique_pages:
                        unique_pages.append(channel_page)

                with _channel_logo_lock:
                    cached_pages = set(_channel_logo_cache.keys())
                missing_pages = [
                    page for page in unique_pages
                    if page and page not in cached_pages
                ]
                if missing_pages:
                    max_workers = min(6, len(missing_pages))
                    try:
                        with ThreadPoolExecutor(max_workers=max_workers) as executor:
                            list(executor.map(_fetch_channel_logo, missing_pages))
                    except RuntimeError:
                        for page in missing_pages:
                            _fetch_channel_logo(page)

            for entry in limited_entries:
                if not entry:
                    continue
                
                # Extract video information
                video_id = entry.get('id', '')
                title = entry.get('title', '').strip()
                channel = entry.get('channel') or entry.get('uploader') or 'YouTube'
                
                # Duration
                duration_sec = entry.get('duration')
                duration = _format_duration(duration_sec)
                
                # Views
                view_count = entry.get('view_count')
                views = _format_views(view_count)
                
                # Thumbnail - prefer maxresdefault, then hq720
                thumbnail = entry.get('thumbnail', '')
                if not thumbnail and video_id:
                    # Fallback to standard YouTube thumbnail URLs
                    thumbnail = f"https://i.ytimg.com/vi/{video_id}/maxresdefault.jpg"
                
                # Video URL
                video_url = entry.get('url', '')
                if not video_url and video_id:
                    video_url = f"https://www.youtube.com/watch?v={video_id}"
                
                # Channel thumbnail/logo (scraped from channel page metadata)
                channel_page = (entry.get('channel_url') or entry.get('uploader_url') or "").strip()
                channel_logo = get_channel_logo(channel_page) if prefetch_logos else ""
                final_logo = channel_logo or _DEFAULT_CHANNEL_LOGO
                
                # Only add if we have essential data
                if title and video_url and thumbnail:
                    videos.append({
                        "title": title,
                        "link": video_url,
                        "channel": channel.strip() if channel else "YouTube",
                        "views": views,
                        "duration": duration,
                        "thumbnail": thumbnail,
                        "channel_logo": final_logo,
                        "channel_logo_is_default": final_logo == _DEFAULT_CHANNEL_LOGO,
                        "channel_page": channel_page,
                    })
    
    except Exception as e:
        # Log error but return empty list instead of raising
        print(f"Error searching YouTube videos: {e}")
        return []
    
    return videos

# --- yt_transcript.py content ---

# ---------------------------------------------------------------------
# Public API
# ---------------------------------------------------------------------
def fetch_transcript_paragraph(
    url_or_id: str,
    lang: str = "en",
    *,
    fallback_ytdlp: bool = True,
    use_whisper: bool = False,
    clean: bool = True,
) -> str:
    """
    Returns the entire transcript as a single cleaned paragraph (UTF-8 string).
    No timestamps. No SRT/VTT. Raises HTTPException on failure (FastAPI-friendly).
    """
    video_id = extract_video_id(url_or_id)
    preferred = [lang] if lang else ["en"]

    # 1) Official transcript
    transcript, _lang_code = try_youtube_transcript_api(video_id, preferred)

    # 2) yt-dlp auto captions
    if transcript is None and fallback_ytdlp:
        vtt_path, _ytdlp_lang = try_yt_dlp_captions(video_id, preferred)
        if vtt_path:
            transcript = load_vtt_to_segments(vtt_path)

    # 3) Whisper fallback (optional)
    if transcript is None and use_whisper:
        text = transcribe_with_whisper(video_id, lang_hint=lang)
        if not text:
            raise HTTPException(404, "Transcription failed (Whisper returned empty text).")
        transcript = [{"start": 0.0, "duration": 0.0, "text": text}]

    if transcript is None:
        raise HTTPException(404, "No transcript/captions available (or disabled) for this video.")

    if clean:
        # strip tags, compact word-level repeats, de-dup near-duplicates
        cleaned = []
        for seg in transcript:
            t = strip_inline_tags(seg.get("text", ""))
            if not t:
                continue
            t = compact_repetitions(t)
            cleaned.append({"start": float(seg.get("start", 0.0)),
                            "duration": float(seg.get("duration", 0.0)),
                            "text": t})
        transcript = dedupe_and_merge_segments(cleaned)
    else:
        # just normalize structure
        transcript = [
            {"start": float(seg.get("start", 0.0)),
             "duration": float(seg.get("duration", 0.0)),
             "text": seg.get("text", "")}
            for seg in (transcript or [])
        ]

    # Render a single paragraph
    return render_paragraph(transcript)


# ---------------------------------------------------------------------
# Optional FastAPI Router (plug-and-play)
#   from transcript_paragraph import router
#   app.include_router(router, prefix="/api")
# GET /api/transcript.txt?url=...&lang=en&fallback_ytdlp=true&use_whisper=false&clean=true
# ---------------------------------------------------------------------
yt_transcript_router = APIRouter()

class _Params(BaseModel):
    url: HttpUrl
    lang: Optional[str] = "en"
    fallback_ytdlp: Optional[bool] = True
    use_whisper: Optional[bool] = False
    clean: Optional[bool] = True

@yt_transcript_router.get("/transcript.txt")
def get_transcript_txt(
    url: str = Query(..., description="YouTube video URL (or ID)"),
    lang: str = Query("en"),
    fallback_ytdlp: bool = Query(True),
    use_whisper: bool = Query(False),
    clean: bool = Query(True),
):
    text = fetch_transcript_paragraph(
        url_or_id=url,
        lang=lang,
        fallback_ytdlp=fallback_ytdlp,
        use_whisper=use_whisper,
        clean=clean,
    )
    return PlainTextResponse(text, media_type="text/plain; charset=utf-8")


# ---------------------------------------------------------------------
# Helpers: ID parsing
# ---------------------------------------------------------------------
YOUTUBE_ID_PATTERNS = [
    r"(?:v=|/v/|/embed/|/shorts/|youtu\.be/)([A-Za-z0-9_-]{11})",
    r"^([A-Za-z0-9_-]{11})$",
]

def extract_video_id(url_or_id: str) -> str:
    for pat in YOUTUBE_ID_PATTERNS:
        m = re.search(pat, url_or_id)
        if m:
            return m.group(1)
    token = (url_or_id or "").strip()
    if re.fullmatch(r"[A-Za-z0-9_-]{11}", token):
        return token
    raise HTTPException(400, "Could not parse YouTube video ID from the provided URL or ID.")


# ---------------------------------------------------------------------
# Cleaning utilities
# ---------------------------------------------------------------------
TAG_TS_RE        = rx.compile(r"<\d{2}:\d{2}:\d{2}(?:\.\d{1,3})?>", flags=rx.I)
TAG_C_OPEN_RE    = rx.compile(r"<c(?:\.[^>]*)?>", flags=rx.I)
TAG_C_CLOSE_RE   = rx.compile(r"</c>", flags=rx.I)
BRACKET_NOISE_RE = rx.compile(r"\[(?:music|applause|__|noise|silence)\]", flags=rx.I)
WS_RE            = rx.compile(r"[ \t\u00A0]+")

def strip_inline_tags(text: str) -> str:
    t = TAG_TS_RE.sub("", text)
    t = TAG_C_OPEN_RE.sub("", t)
    t = TAG_C_CLOSE_RE.sub("", t)
    t = html.unescape(t)
    t = BRACKET_NOISE_RE.sub("", t)
    t = t.replace("_", " ")
    t = WS_RE.sub(" ", t).strip()
    return t

def normalize_for_compare(text: str) -> str:
    t = strip_inline_tags(text)
    t = unicodedata.normalize("NFKC", t)
    t = rx.sub(r"[^\p{L}\p{N}\s.,!?;:']", "", t)
    t = WS_RE.sub(" ", t).strip().lower()
    return t

def smart_sentence_join(chunks: List[str]) -> str:
    raw = " ".join(chunks)
    raw = WS_RE.sub(" ", raw).strip()
    raw = re.sub(r"\s+([.,!?;:])", r"\1", raw)
    return raw

# Tokenize/untokenize for repetition compaction
TOKEN_RE = rx.compile(r"\p{L}+\p{M}*|\d+|[^\s\p{L}\p{N}]", rx.UNICODE)

def _tokens(s: str) -> List[str]:
    return TOKEN_RE.findall(s)

def _untokenize(tokens: List[str]) -> str:
    out = []
    for i, tok in enumerate(tokens):
        if i > 0 and rx.match(r"[\p{L}\p{N}]", tok) and rx.match(r"[\p{L}\p{N}]", tokens[i-1]):
            out.append(" ")
        out.append(tok)
    return "".join(out)

def compact_repetitions(text: str, max_ngram: int = 12, min_chars_per_span: int = 4) -> str:
    """
    Removes consecutive duplicated spans like:
    'hello everyone welcome ... hello everyone welcome ...'
    Works token-wise, preferring longest repeated spans up to max_ngram.
    """
    if not text or len(text) < 2:
        return text

    # quick stutter fix: 'the the', 'and and'
    text = rx.sub(r"\b(\p{L}+)\s+\1\b", r"\1", text, flags=rx.IGNORECASE)

    toks = _tokens(text)
    i = 0
    out: List[str] = []

    while i < len(toks):
        matched = False
        max_n = min(max_ngram, (len(toks) - i) // 2)
        for n in range(max_n, 0, -1):
            a = toks[i:i+n]
            b = toks[i+n:i+2*n]
            if not a or not b:
                continue
            if a == b:
                span_txt = _untokenize(a)
                if len(rx.sub(r"\s+", "", span_txt)) >= min_chars_per_span:
                    j = i + n
                    while j + n <= len(toks) and toks[j:j+n] == a:
                        j += n
                    out.extend(a)  # keep one copy
                    i = j
                    matched = True
                    break
        if not matched:
            out.append(toks[i])
            i += 1

    s = _untokenize(out)
    s = rx.sub(r"\s+([.,!?;:])", r" \1", s)
    s = rx.sub(r"([(\[{])\s+", r"\1", s)
    s = rx.sub(r"\s+([)\]}])", r"\1", s)
    s = rx.sub(r"\s{2,}", " ", s).strip()
    return s


# ---------------------------------------------------------------------
# Source fetchers: official API, yt-dlp
# ---------------------------------------------------------------------
def prefer_language_candidates(preferred_langs: List[str]) -> List[str]:
    expanded: List[str] = []
    for lang in preferred_langs:
        expanded.append(lang)
        if "-" in lang:
            base = lang.split("-")[0]
            if base not in expanded:
                expanded.append(base)
    # common fallbacks
    for fb in ["en", "en-US", "en-GB", "en-IN"]:
        if fb not in expanded:
            expanded.append(fb)
    return expanded

def _convert_transcript_to_dicts(transcript_list) -> List[dict]:
    """Convert FetchedTranscriptSnippet objects to dicts for backwards compatibility."""
    result = []
    for item in transcript_list:
        # Handle both new FetchedTranscriptSnippet objects and old dicts
        if hasattr(item, 'text'):
            result.append({
                "text": item.text,
                "start": item.start,
                "duration": item.duration,
            })
        elif isinstance(item, dict):
            result.append(item)
        else:
            # Fallback: try to convert to dict
            result.append(dict(item))
    return result


def try_youtube_transcript_api(video_id: str, langs: List[str]) -> Tuple[Optional[List[dict]], Optional[str]]:
    try:
        # youtube-transcript-api v1.2+ requires instantiation
        ytt = YouTubeTranscriptApi()
        listing = ytt.list(video_id)
    except (TranscriptsDisabled, NoTranscriptFound, VideoUnavailable):
        return None, None
    except Exception:
        return None, None

    for lang in prefer_language_candidates(langs):
        try:
            tr = listing.find_manually_created_transcript([lang])
            return _convert_transcript_to_dicts(tr.fetch()), tr.language_code
        except Exception:
            pass
        try:
            tr = listing.find_generated_transcript([lang])
            return _convert_transcript_to_dicts(tr.fetch()), tr.language_code
        except Exception:
            pass

    # as a last resort, try translatable
    for tr in listing:
        try:
            if tr.is_translatable:
                for lang in prefer_language_candidates(langs):
                    try:
                        return _convert_transcript_to_dicts(tr.translate(lang).fetch()), lang
                    except Exception:
                        continue
        except Exception:
            continue

    try:
        first = next(iter(listing))
        return _convert_transcript_to_dicts(first.fetch()), first.language_code
    except Exception:
        return None, None

def try_yt_dlp_captions(video_id: str, langs: List[str]) -> Tuple[Optional[str], Optional[str]]:
    tempdir = tempfile.mkdtemp(prefix="ytcapt_")
    outtmpl = os.path.join(tempdir, "%(id)s.%(ext)s")
    ydl_opts = {
        "skip_download": True,
        "writesubtitles": True,
        "writeautomaticsub": True,
        "subtitleslangs": prefer_language_candidates(langs),
        "subtitlesformat": "vtt",
        "outtmpl": outtmpl,
        "quiet": True,
        "no_warnings": True,
    }
    url = f"https://www.youtube.com/watch?v={video_id}"
    with YoutubeDL(ydl_opts) as ydl:
        try:
            ydl.extract_info(url, download=True)
        except Exception:
            return None, None

    vtts = [os.path.join(tempdir, f) for f in os.listdir(tempdir) if f.endswith(".vtt")]
    if not vtts:
        return None, None

    prefs = prefer_language_candidates(langs)
    chosen = None; chosen_lang = None
    for lang in prefs:
        for p in vtts:
            if re.search(rf"\.{re.escape(lang)}\.vtt$", os.path.basename(p)):
                chosen, chosen_lang = p, lang
                break
        if chosen:
            break
    if not chosen:
        chosen = vtts[0]
        m = re.search(r"\.([a-zA-Z-]{2,})\.vtt$", os.path.basename(chosen))
        chosen_lang = m.group(1) if m else None
    return chosen, chosen_lang


# ---------------------------------------------------------------------
# VTT parsing and merging (for yt-dlp captions)
# ---------------------------------------------------------------------
def parse_vtt_timestamp(ts: str) -> float:
    parts = ts.split(":")
    if len(parts) == 3:
        h, m, s = parts
    elif len(parts) == 2:
        h = "0"; m, s = parts
    else:
        return 0.0
    if "." in s:
        sec, ms = s.split("."); ms = int(ms.ljust(3, "0")[:3])
    else:
        sec, ms = s, 0
    return int(h) * 3600 + int(m) * 60 + int(sec) + ms / 1000.0

def load_vtt_to_segments(vtt_path: str) -> List[dict]:
    with open(vtt_path, "r", encoding="utf-8", errors="ignore") as f:
        lines = [ln.rstrip("\n") for ln in f]

    segs: List[dict] = []
    i = 0; n = len(lines)
    while i < n:
        line = lines[i].strip(); i += 1
        if not line or line.upper() == "WEBVTT":
            continue
        # optional cue index
        if re.match(r"^\d+\s*$", line):
            if i < n:
                line = lines[i].strip(); i += 1
        if "-->" in line:
            times = line.split("-->")
            if len(times) != 2:
                while i < n and lines[i].strip():
                    i += 1
                continue
            start_s = parse_vtt_timestamp(times[0].strip())
            end_s   = parse_vtt_timestamp(times[1].strip())
            cue = []
            while i < n and lines[i].strip() != "":
                cue.append(lines[i]); i += 1
            while i < n and lines[i].strip() == "":
                i += 1
            text_raw = " ".join(cue)
            text_clean = strip_inline_tags(text_raw)
            if text_clean:
                segs.append({"start": start_s, "duration": max(0.0, end_s - start_s), "text": text_clean})
    return segs

def dedupe_and_merge_segments(segments: List[dict]) -> List[dict]:
    out: List[dict] = []
    for seg in segments:
        t = (seg.get("text") or "").strip()
        if not t:
            continue
        norm = normalize_for_compare(t)
        if not norm:
            continue

        if out:
            last = out[-1]
            last_norm = last.get("_norm")
            if norm == last_norm:
                # merge consecutive identicals
                last_end = last["start"] + last["duration"]
                new_end  = seg["start"] + seg["duration"]
                if seg["start"] <= last_end + 0.2:
                    last["duration"] = max(last["duration"], new_end - last["start"])
                continue
            if t.lower() == last["text"].lower() and seg["start"] <= (last["start"] + last["duration"] + 0.5):
                # near-dupe contiguous lowercased text
                last_end = last["start"] + last["duration"]
                new_end  = seg["start"] + seg["duration"]
                last["duration"] = max(last["duration"], new_end - last["start"])
                continue

        seg = dict(seg)
        seg["_norm"] = norm
        out.append(seg)

    for s in out:
        s.pop("_norm", None)
    return out


# ---------------------------------------------------------------------
# Final paragraph render
# ---------------------------------------------------------------------
def render_paragraph(transcript: List[dict]) -> str:
    chunks = [seg["text"].strip() for seg in transcript if seg.get("text")]
    text = smart_sentence_join(chunks)
    # final pass to squash rolling echoes across cue boundaries
    return compact_repetitions(text) + "\n"


# ---------------------------------------------------------------------
# Optional Whisper fallback
# ---------------------------------------------------------------------
def transcribe_with_whisper(video_id: str, lang_hint: Optional[str] = None) -> str:
    try:
        import whisper  # pip install openai-whisper; requires ffmpeg
    except Exception as e:
        raise HTTPException(
            501,
            f"Whisper not available ({e}). Install with `pip install openai-whisper` and ensure ffmpeg is installed."
        )

    tmp = tempfile.mkdtemp(prefix="whisp_")
    url = f"https://www.youtube.com/watch?v={video_id}"
    ydl_opts = {
        "format": "bestaudio/best",
        "outtmpl": os.path.join(tmp, "%(id)s.%(ext)s"),
        "quiet": True, "no_warnings": True,
        "postprocessors": [
            {"key": "FFmpegExtractAudio", "preferredcodec": "mp3", "preferredquality": "192"}
        ],
    }
    with YoutubeDL(ydl_opts) as ydl:
        info = ydl.extract_info(url, download=True)
        base = ydl.prepare_filename(info)
        audio_path = os.path.splitext(base)[0] + ".mp3"
        if not os.path.exists(audio_path):
            audio_path = base

    model = whisper.load_model("small")
    kw = {}
    if lang_hint:
        kw["language"] = lang_hint.split("-")[0]
    result = model.transcribe(audio_path, **kw)
    text = (result.get("text") or "").strip()
    return compact_repetitions(text)

# --- End of inlined packages ---

# --- Supabase helpers ---


supabase_logger = logging.getLogger("paperx.supabase")
supabase_logger.setLevel(logging.INFO)

# Truncate long uvicorn access log messages
class TruncateLogFilter(logging.Filter):
    def filter(self, record):
        if hasattr(record, 'args') and record.args:
            # Truncate the message if it contains a very long URL
            args = list(record.args) if isinstance(record.args, tuple) else [record.args]
            new_args = []
            for arg in args:
                if isinstance(arg, str) and len(arg) > 100:
                    new_args.append(arg[:97] + "...")
                else:
                    new_args.append(arg)
            record.args = tuple(new_args)
        return True

# Apply filter to uvicorn.access logger
uvicorn_access_logger = logging.getLogger("uvicorn.access")
uvicorn_access_logger.addFilter(TruncateLogFilter())

SUPABASE_URL = os.getenv("SUPABASE_URL", "").strip()
SUPABASE_SERVICE_ROLE_KEY = os.getenv("SUPABASE_SERVICE_ROLE_KEY", "").strip()
SUPABASE_ANON_KEY = os.getenv("SUPABASE_ANON_KEY", "").strip()

if not SUPABASE_URL or not SUPABASE_SERVICE_ROLE_KEY:
    raise RuntimeError("Missing SUPABASE_URL or SUPABASE_SERVICE_ROLE_KEY in environment.")


@lru_cache()
def get_service_client() -> Client:
    return create_client(SUPABASE_URL, SUPABASE_SERVICE_ROLE_KEY)


@lru_cache()
def get_anon_client() -> Optional[Client]:
    if not SUPABASE_ANON_KEY:
        supabase_logger.warning("Missing SUPABASE_ANON_KEY; auth-dependent routes will be disabled.")
        return None
    return create_client(SUPABASE_URL, SUPABASE_ANON_KEY)


def _to_supabase_json(value: Any) -> Any:
    """Recursively coerce common Python types (UUID, datetime, set, etc.) into JSON-serializable forms."""
    if value is None:
        return None
    if isinstance(value, uuid.UUID):
        return str(value)
    if isinstance(value, Decimal):
        return float(value)
    if isinstance(value, (datetime, date)):
        return value.isoformat()
    if isinstance(value, enum.Enum):
        return value.value
    if isinstance(value, BaseModel):
        return _to_supabase_json(value.dict(exclude_none=True))
    if isinstance(value, set):
        return [_to_supabase_json(v) for v in value]
    if isinstance(value, (list, tuple)):
        return [_to_supabase_json(v) for v in value]
    if isinstance(value, dict):
        return {k: _to_supabase_json(v) for k, v in value.items() if v is not None}
    return value


def _supabase_payload(raw: Dict[str, Any]) -> Dict[str, Any]:
    """Return a JSON-serializable dict suitable for Supabase from a Pydantic .dict() payload."""
    return {k: _to_supabase_json(v) for k, v in raw.items() if v is not None}

# --- Auth helpers ---
def _bearer_token_from_header(authorization: Optional[str]) -> Optional[str]:
    if not authorization:
        return None
    try:
        parts = str(authorization).split()
        if len(parts) >= 2 and parts[0].lower() == 'bearer':
            return parts[1]
        return str(authorization)
    except Exception:
        return None

# --- Resiliency helpers for Supabase/httpx transient protocol errors ---
# Some users have observed intermittent httpcore.RemoteProtocolError("Server disconnected") coming
# from underlying HTTP/2 (or connection reuse) when performing rapid successive metadata lookups.
# These are typically transient (connection closed between frames). We add a lightweight retry
# wrapper so endpoint handlers can reattempt idempotent read queries without failing the whole request.
RETRYABLE_EXCEPTIONS: tuple = ()
try:  # HTTPXRemoteProtocolError already imported conditionally at top
    if httpx is not None:
        RETRYABLE_EXCEPTIONS = (HTTPXRemoteProtocolError, httpx.RemoteProtocolError)  # type: ignore
    else:
        RETRYABLE_EXCEPTIONS = (HTTPXRemoteProtocolError,)  # type: ignore
except Exception:  # pragma: no cover
    pass

def _supabase_retry(fn, *, retries: int = 3, base_delay: float = 0.35):
    """Execute a zero-arg callable returning a Supabase response with simple exponential backoff.

    Only catches protocol-level disconnection errors that are safe to retry for idempotent SELECT/IN queries.
    """
    for attempt in range(retries):
        try:
            return fn()
        except RETRYABLE_EXCEPTIONS as e:  # pragma: no cover - network timing dependent
            if attempt == retries - 1:
                raise
            time.sleep(base_delay * (2 ** attempt))



PROJECTS_TABLE = os.getenv("PROJECTS_TABLE") or os.getenv("SUPABASE_PROJECTS_TABLE") or "projects"
PROJECT_APPLICATIONS_TABLE = os.getenv("PROJECT_APPLICATIONS_TABLE") or "project_applications"
PROJECT_COLLAB_TABLE = os.getenv("PROJECT_COLLAB_TABLE") or "project_collab_messages"
SKILL_TESTS_TABLE = os.getenv("SKILL_TESTS_TABLE") or "skill_tests"
SKILL_VERIFICATIONS_TABLE = os.getenv("SKILL_VERIFICATIONS_TABLE") or "skill_verifications"
PROJECTS_BUCKET = (os.getenv("SUPABASE_PROJECTS_BUCKET") or os.getenv("SUPABASE_BUCKET") or "").strip()
PROJECT_MEDIA_EXTENSIONS = {
    "cover": {".png", ".jpg", ".jpeg", ".webp", ".gif"},
    "gallery": {".png", ".jpg", ".jpeg", ".webp", ".gif", ".mp4", ".mov", ".webm"},
}

# --- Learning track table names ---

LEARNING_TRACK_GOALS_TABLE = os.getenv("LEARNING_TRACK_GOALS_TABLE", "learning_track_goals")
LEARNING_TRACK_PLANS_TABLE = os.getenv("LEARNING_TRACK_PLANS_TABLE", "learning_track_plans")
LEARNING_TRACK_PROGRESS_TABLE = os.getenv("LEARNING_TRACK_PROGRESS_TABLE", "learning_track_progress")

# --- Print/Orders table names ---
PRINT_SHOPS_TABLE = os.getenv("PRINT_SHOPS_TABLE") or "print_shops"
PRINT_PRICING_TABLE = os.getenv("PRINT_PRICING_TABLE") or "print_pricing"
PRINT_PRINTERS_TABLE = os.getenv("PRINT_PRINTERS_TABLE") or "print_printers"
PRINT_JOBS_TABLE = os.getenv("PRINT_JOBS_TABLE") or "print_jobs"
PRINT_JOB_EVENTS_TABLE = os.getenv("PRINT_JOB_EVENTS_TABLE") or "print_job_events"

# --- AI model clients ---

openai_model_client = OpenAIChatCompletionClient(
    model="gpt-4o-mini",
    api_key=os.environ.get("OPENAI_API_KEY")
)

deepseek_model_client =  OpenAIChatCompletionClient(
    base_url="https://openrouter.ai/api/v1",
    model="deepseek/deepseek-r1-0528:free",
    api_key = os.environ.get("OPENROUTER_API_KEY"),
    model_info=ModelInfo(
        vision=True,
        function_calling=True,
        json_output=True,
        family="unknown",
        structured_output=True
    )
)

gemini_model_client = OpenAIChatCompletionClient(
    base_url=os.getenv("GEMINI_BASE_URL", "https://generativelanguage.googleapis.com/v1beta/openai/"),
    model="gemini-2.5-flash",
    api_key=(os.getenv("GEMINI_API_KEY", "") or "").strip(),
    model_info=ModelInfo(
        vision=True,
        function_calling=True,
        json_output=True,
        family="unknown",
        structured_output=True
    )
)

# --- Notes storage ---

BASE_DIR = os.path.abspath(os.path.dirname(__file__))
NOTES_DIR = os.path.join(BASE_DIR, "notes")
os.makedirs(NOTES_DIR, exist_ok=True)
os.makedirs(NOTES_DIR, exist_ok=True)

# DB table for AI notes (title + markdown)
AI_NOTES_TABLE = os.getenv("AI_NOTES_TABLE", "ai_notes")
AI_NOTES_CHEATSHEET_TABLE = os.getenv("AI_NOTES_CHEATSHEET_TABLE", "ai_notes_cheatsheet")
AI_NOTES_SIMPLE_TABLE = os.getenv("AI_NOTES_SIMPLE_TABLE", "ai_notes_simple")
AI_NOTES_USER_EDITS_TABLE = os.getenv("AI_NOTES_USER_EDITS_TABLE", "ai_notes_user_edits")

VALID_NOTE_VARIANTS = {"detailed", "cheatsheet", "simple"}


def _normalize_course_type(course_type: Optional[str]) -> Optional[str]:
    if course_type is None:
        return None
    value = str(course_type).strip().lower()
    if value in {"maths", "theorey", "practical"}:
        return value
    return None

def _normalize_variant(variant: Optional[str]) -> str:
    v = (variant or "detailed").strip().lower()
    return v if v in VALID_NOTE_VARIANTS else "detailed"

def _table_for_variant(variant: Optional[str]) -> str:
    v = _normalize_variant(variant)
    if v == "cheatsheet":
        return AI_NOTES_CHEATSHEET_TABLE
    if v == "simple":
        return AI_NOTES_SIMPLE_TABLE
    return AI_NOTES_TABLE

def db_get_ai_note_by_title_exact(title: str) -> Optional[Dict[str, Any]]:
    """Fetch a detailed-variant note by exact title. Backward-compat wrapper."""
    return db_get_ai_note_by_title_exact_variant(title, variant="detailed")

def db_get_ai_note_by_title_exact_variant(title: str, *, variant: Optional[str] = None) -> Optional[Dict[str, Any]]:
    """Fetch a note by exact title (case-insensitive) from the table for the given variant."""
    supabase = get_service_client()
    t = (title or "").strip()
    if not t:
        return None
    table = _table_for_variant(variant)
    try:
        res = (
            supabase.table(table)
            .select("id,title,markdown,image_urls,created_at,updated_at")
            .eq("title_ci", t.lower())
            .limit(1)
            .execute()
        )
        data = getattr(res, 'data', []) or []
        return data[0] if data else None
    except Exception:
        return None

def db_upsert_ai_note_by_title(
    title: str,
    markdown: str,
    image_urls: Optional[List[str]] = None,
) -> Dict[str, Any]:
    """Insert or update a detailed-variant note by title; returns the stored row."""
    return db_upsert_ai_note_by_title_variant(title, markdown, variant="detailed", image_urls=image_urls)

def db_upsert_ai_note_by_title_variant(
    title: str,
    markdown: str,
    *,
    variant: Optional[str] = None,
    image_urls: Optional[List[str]] = None,
) -> Dict[str, Any]:
    """Insert or update a note by title in the variant-specific table; returns the stored row."""
    supabase = get_service_client()
    now = datetime.utcnow().isoformat()
    payload = {
        "title": (title or "").strip() or "Untitled",
        "markdown": markdown or "",
        "updated_at": now,
    }
    if image_urls is not None:
        cleaned_images = [str(url).strip() for url in image_urls if str(url or "").strip()]
        payload["image_urls"] = cleaned_images
    table = _table_for_variant(variant)
    try:
        res = supabase.table(table).upsert(payload, on_conflict="title_ci", returning="representation").execute()
        if getattr(res, 'error', None):
            raise Exception(res.error)
        row = (getattr(res, 'data', []) or [{}])[0]
        if row:
            return row
        # Fallback: refetch by title_ci
        ref = (
            supabase.table(table)
            .select("id,title,markdown,image_urls,created_at,updated_at")
            .eq("title_ci", payload["title"].lower())
            .limit(1)
            .execute()
        )
        data = getattr(ref, 'data', []) or []
        if not data:
            raise RuntimeError("Failed to upsert ai_note")
        return data[0]
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"DB save failed: {e}")

def db_get_ai_note_by_id(note_id: str) -> Optional[Dict[str, Any]]:
    """Backward compat: search only detailed table."""
    return db_get_ai_note_by_id_variant(note_id, variant="detailed")

def db_get_ai_note_by_id_variant(note_id: str, *, variant: Optional[str] = None) -> Optional[Dict[str, Any]]:
    supabase = get_service_client()
    table = _table_for_variant(variant)
    try:
        res = (
            supabase.table(table)
            .select("id,title,markdown,image_urls,created_at,updated_at")
            .eq("id", note_id)
            .limit(1)
            .execute()
        )
        data = getattr(res, 'data', []) or []
        return data[0] if data else None
    except Exception:
        return None

def db_get_ai_note_by_id_any(note_id: str) -> Tuple[Optional[Dict[str, Any]], Optional[str]]:
    """Search all variant tables for the given id. Returns (row, variant)."""
    for v in ("detailed", "cheatsheet", "simple"):
        row = db_get_ai_note_by_id_variant(note_id, variant=v)
        if row:
            return row, v
    return None, None

def db_update_ai_note_markdown(
    note_id: str,
    markdown: str,
    image_urls: Optional[List[str]] = None,
) -> Optional[Dict[str, Any]]:
    """Backward compat: update only detailed table."""
    return db_update_ai_note_markdown_variant(note_id, markdown, variant="detailed", image_urls=image_urls)

def db_update_ai_note_markdown_variant(
    note_id: str,
    markdown: str,
    *,
    variant: Optional[str] = None,
    image_urls: Optional[List[str]] = None,
) -> Optional[Dict[str, Any]]:
    supabase = get_service_client()
    table = _table_for_variant(variant)
    try:
        update_payload: Dict[str, Any] = {"markdown": markdown or "", "updated_at": datetime.utcnow().isoformat()}
        if image_urls is not None:
            update_payload["image_urls"] = [str(url).strip() for url in image_urls if str(url or "").strip()]
        res = supabase.table(table).update(update_payload).eq("id", note_id).execute()
        if getattr(res, 'error', None):
            raise Exception(res.error)
        out = db_get_ai_note_by_id_variant(note_id, variant=variant)
        return out
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"DB update failed: {e}")


# --- Per-user edited notes helpers ---

def db_get_user_edit_by_title(user_id: str, title: str, *, variant: Optional[str] = None) -> Optional[Dict[str, Any]]:
    supabase = get_service_client()
    v = _normalize_variant(variant or "detailed")
    t = (title or "").strip()
    if not user_id or not t:
        return None
    try:
        res = (
            supabase.table(AI_NOTES_USER_EDITS_TABLE)
            .select("id,title,variant,markdown,created_at,updated_at")
            .eq("user_id", str(user_id))
            .eq("title_ci", t.lower())
            .eq("variant", v)
            .limit(1)
            .execute()
        )
        data = getattr(res, 'data', []) or []
        return data[0] if data else None
    except Exception:
        return None


def db_upsert_user_edit(user_id: str, title: str, markdown: str, *, variant: Optional[str] = None) -> Dict[str, Any]:
    supabase = get_service_client()
    v = _normalize_variant(variant or "detailed")
    now = datetime.utcnow().isoformat()
    payload = {
        "user_id": str(user_id),
        "title": (title or "").strip() or "Untitled",
        "variant": v,
        "markdown": markdown or "",
        "updated_at": now,
    }
    try:
        res = (
            supabase
            .table(AI_NOTES_USER_EDITS_TABLE)
            .upsert(payload, on_conflict="user_id,title_ci,variant", returning="representation")
            .execute()
        )
        if getattr(res, 'error', None):
            raise Exception(res.error)
        row = (getattr(res, 'data', []) or [{}])[0]
        if row:
            return row
        # Fallback: refetch by composite key
        ref = (
            supabase.table(AI_NOTES_USER_EDITS_TABLE)
            .select("id,title,variant,markdown,created_at,updated_at")
            .eq("user_id", str(user_id))
            .eq("title_ci", payload["title"].lower())
            .eq("variant", v)
            .limit(1)
            .execute()
        )
        data = getattr(ref, 'data', []) or []
        if not data:
            raise RuntimeError("Failed to upsert user edit")
        return data[0]
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"DB save failed: {e}")


def _slugify_topic(topic: str) -> str:
    s = re.sub(r"[^a-zA-Z0-9_.-]+", "-", (topic or "").strip()).strip("-")
    if not s:
        s = "note"
    return s[:60]


def _note_id(topic: str) -> str:
    slug = _slugify_topic(topic)
    ts = int(time.time() * 1000)
    short = uuid.uuid4().hex[:6]
    return f"{slug}-{ts}-{short}"


def _note_path(note_id: str) -> str:
    return os.path.join(NOTES_DIR, f"{note_id}.md")


def save_note(topic: str, markdown: str) -> Dict[str, str]:
    nid = _note_id(topic)
    path = _note_path(nid)
    with open(path, "w", encoding="utf-8") as f:
        f.write(markdown or "")
    st = os.stat(path)
    return {
        "id": nid,
        "topic": topic,
        "path": path,
        "created_at": datetime.fromtimestamp(st.st_mtime).isoformat(),
        "size": st.st_size,
    }


def read_note(note_id: str) -> Dict[str, str]:
    path = _note_path(note_id)
    if not os.path.isfile(path):
        raise FileNotFoundError("Note not found")
    with open(path, "r", encoding="utf-8") as f:
        md = f.read()
    st = os.stat(path)
    return {
        "id": note_id,
        "markdown": md,
        "updated_at": datetime.fromtimestamp(st.st_mtime).isoformat(),
        "size": st.st_size,
    }


def update_note(note_id: str, markdown: str) -> Dict[str, str]:
    path = _note_path(note_id)
    if not os.path.isfile(path):
        raise FileNotFoundError("Note not found")
    with open(path, "w", encoding="utf-8") as f:
        f.write(markdown or "")
    st = os.stat(path)
    return {
        "id": note_id,
        "updated_at": datetime.fromtimestamp(st.st_mtime).isoformat(),
        "size": st.st_size,
    }


def list_notes() -> List[Dict[str, str]]:
    items: List[Dict[str, str]] = []
    if not os.path.isdir(NOTES_DIR):
        return items
    for name in os.listdir(NOTES_DIR):
        if not name.endswith(".md"):
            continue
        nid = name[:-3]
        p = os.path.join(NOTES_DIR, name)
        st = os.stat(p)
        topic = None
        try:
            with open(p, "r", encoding="utf-8") as f:
                for line in f:
                    if line.startswith("# "):
                        topic = line[2:].strip()
                        break
        except Exception:
            pass
        items.append({
            "id": nid,
            "topic": topic,
            "updated_at": datetime.fromtimestamp(st.st_mtime).isoformat(),
            "size": st.st_size,
        })
    items.sort(key=lambda x: x["updated_at"], reverse=True)
    return items


def note_path(note_id: str) -> str:
    """Expose file path for download endpoints."""
    return _note_path(note_id)


# -------------------- Fuzzy/Semantic search helpers --------------------

def _normalize_text(s: str) -> str:
    s = (s or "").lower()
    s = re.sub(r"[^a-z0-9\s]", " ", s)
    s = re.sub(r"\s+", " ", s).strip()
    return s


def _extract_title_and_headings(md: str) -> Tuple[Optional[str], List[str]]:
    title: Optional[str] = None
    headings: List[str] = []
    for raw in (md or "").splitlines():
        line = raw.strip()
        if line.startswith("# "):
            h = line[2:].strip()
            if not title:
                title = h
            headings.append(h)
        elif line.startswith("## "):
            headings.append(line[3:].strip())
        elif line.startswith("### "):
            headings.append(line[4:].strip())
    return title, headings


def _score_query_against_note(query: str, note_id: str) -> Tuple[int, Dict[str, str]]:
    path = _note_path(note_id)
    try:
        with open(path, "r", encoding="utf-8") as f:
            md = f.read()
    except Exception:
        return 0, {"id": note_id}

    title, headings = _extract_title_and_headings(md)

    # Derive slug part from note_id like: slug-<ts>-<short>
    slug_part = note_id
    parts = note_id.rsplit("-", 2)
    if len(parts) == 3:
        slug_part = parts[0]

    q = _normalize_text(query)
    scores: List[int] = []

    if title:
        scores.append(fuzz.token_set_ratio(q, _normalize_text(title)))
    if headings:
        best_heading = max((fuzz.token_set_ratio(q, _normalize_text(h)) for h in headings), default=0)
        scores.append(best_heading)
    if slug_part:
        scores.append(fuzz.token_set_ratio(q, _normalize_text(slug_part)))

    # Also compare with first 300 chars of body (after dropping headings)
    body = []
    for raw in md.splitlines():
        if raw.strip().startswith("#"):
            continue
        body.append(raw)
        if len(body) > 60:
            break
    body_text = _normalize_text(" ".join(body))[:300]
    if body_text:
        scores.append(fuzz.token_set_ratio(q, body_text))

    score = max(scores) if scores else 0
    return score, {
        "id": note_id,
        "title": title or None,
        "slug": slug_part,
        "path": path,
    }


def search_notes(query: str, limit: int = 5) -> List[Dict[str, object]]:
    """Return best-matching notes for a textual query using fuzzy scoring.

    Each result includes: id, score, title, path, slug.
    """
    results: List[Tuple[int, Dict[str, str]]] = []
    for name in os.listdir(NOTES_DIR):
        if not name.endswith(".md"):
            continue
        nid = name[:-3]
        score, meta = _score_query_against_note(query, nid)
        results.append((score, meta))

    results.sort(key=lambda x: x[0], reverse=True)
    out: List[Dict[str, object]] = []
    for s, meta in results[:limit]:
        m = dict(meta)
        m["score"] = int(s)
        out.append(m)
    return out


def find_existing_note_for_topic(topic: str, threshold: int = 82) -> Optional[Dict[str, object]]:
    """Find a cached note for a topic if similarity exceeds threshold.

    Returns dict with: id, score, title, path; or None if no match.
    """
    candidates = search_notes(topic, limit=3)
    if not candidates:
        return None
    best = candidates[0]
    if best.get("score", 0) >= threshold:
        return best
    return None

# --- PDF rendering ---

try:
    from markdown import markdown as md_to_html
except Exception:  # pragma: no cover - optional if headless is used
    md_to_html = None
try:
    from xhtml2pdf import pisa
except Exception:  # pragma: no cover - optional if headless is used
    pisa = None
try:
    from playwright.sync_api import sync_playwright
except Exception:  # pragma: no cover - optional
    sync_playwright = None


BASE_CSS = """
<style>
  @page { size: A4; margin: 1.2cm; }
  body { font-family: DejaVu Sans, Arial, sans-serif; font-size: 11pt; color: #111827; }
  h1, h2, h3 { color: #0f172a; }
  h1 { font-size: 22pt; margin: 0 0 10px 0; }
  h2 { font-size: 16pt; margin: 14px 0 8px 0; }
  h3 { font-size: 13pt; margin: 10px 0 6px 0; }
  p, li { line-height: 1.4; }
  pre, code { font-family: DejaVu Sans Mono, Consolas, monospace; font-size: 9pt; }
  pre { background: #0b1020; color: #e5e7eb; padding: 10px; border-radius: 6px; }
  code { background: #eef1ff; padding: 1px 3px; border-radius: 4px; }
  table { width: 100%; border-collapse: collapse; margin: 8px 0; }
  th, td { border: 1px solid #e5e7eb; padding: 6px; }
  blockquote { border-left: 3px solid #93c5fd; padding-left: 8px; color: #374151; }
  .small { color: #6b7280; font-size: 9pt; }
</style>
"""


def markdown_to_html(markdown_text: str) -> str:
    if md_to_html is None:
        raise RuntimeError("markdown library not installed")
    html_body = md_to_html(markdown_text or "", extensions=[
        'extra', 'admonition', 'codehilite', 'tables', 'toc'
    ])
    return f"<html><head>{BASE_CSS}</head><body>{html_body}</body></html>"


def render_pdf_from_markdown(markdown_text: str) -> bytes:
    """Render a PDF byte stream from a Markdown string.

    Uses markdownâ†’HTML and xhtml2pdf. Returns PDF bytes or raises RuntimeError on failure.
    """
    if pisa is None:
        raise RuntimeError("xhtml2pdf not installed")
    html = markdown_to_html(markdown_text)
    pdf_io = io.BytesIO()
    result = pisa.CreatePDF(io.StringIO(html), dest=pdf_io, encoding='utf-8')
    if result.err:
        raise RuntimeError("Failed to generate PDF")
    return pdf_io.getvalue()


# ---------------- WYSIWYG via headless Chromium (best for KaTeX/Mermaid) ----------------

def render_pdf_via_headless(url: str, wait_selector: str = "#output", timeout_ms: int = 20000) -> bytes:
    """Open a URL in headless Chromium and print to PDF. Requires playwright + browsers installed.

    Install: pip install playwright; playwright install
    """
    if sync_playwright is None:
        raise RuntimeError("playwright not installed")
    with sync_playwright() as p:
        browser = p.chromium.launch()
        try:
            page = browser.new_page()
            page.goto(url, wait_until="networkidle")
            # wait for primary content to appear
            try:
                page.wait_for_selector(wait_selector, timeout=timeout_ms)
            except Exception:
                pass
            # give KaTeX/mermaid a brief window to render
            try:
                page.wait_for_selector('.katex', timeout=3000)
            except Exception:
                time.sleep(0.5)
            pdf_bytes = page.pdf(format="A4", print_background=True, margin={
                'top': '0.6in', 'bottom': '0.6in', 'left': '0.6in', 'right': '0.6in'
            })
            return pdf_bytes
        finally:
            browser.close()


def render_pdf_from_markdown_via_headless(markdown_text: str, title: str = "Notes") -> bytes:
    """Render a PDF by loading a minimal HTML page that renders MD + KaTeX client-side.

    Uses Playwright to set page content with CDN assets for Marked, DOMPurify, KaTeX.
    """
    if sync_playwright is None:
        raise RuntimeError("playwright not installed")
    md_json = json.dumps(markdown_text or "")
    title_esc = json.dumps(title or "Notes")
    html = f"""
<!doctype html>
<html>
  <head>
    <meta charset=\"utf-8\" />
    <title>{title_esc}</title>
    {BASE_CSS}
    <style>
      body {{ background: #ffffff; }}
      main {{ max-width: 800px; margin: 0 auto; }}
    </style>
    <link rel=\"stylesheet\" href=\"https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/katex.min.css\" />
    <script src=\"https://cdn.jsdelivr.net/npm/marked/marked.min.js\"></script>
    <script src=\"https://cdn.jsdelivr.net/npm/dompurify@3.1.6/dist/purify.min.js\"></script>
    <script defer src=\"https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/katex.min.js\"></script>
    <script defer src=\"https://cdn.jsdelivr.net/npm/katex@0.16.11/dist/contrib/auto-render.min.js\"></script>
  </head>
  <body>
    <main>
      <article id=\"output\"></article>
    </main>
    <script>
      const md = {md_json};
      const html = DOMPurify.sanitize(marked.parse(md));
      const out = document.getElementById('output');
      out.innerHTML = html;
      function doRenderMath() {{
        if (typeof renderMathInElement === 'function') {{
          renderMathInElement(out, {{
            delimiters: [
              {{ left: '$$', right: '$$', display: true }},
              {{ left: '$', right: '$', display: false }},
              {{ left: '\\(', right: '\\)', display: false }},
              {{ left: '\\[', right: '\\]', display: true }}
            ],
            throwOnError: false,
            ignoredTags: ['script','noscript','style','textarea','pre','code']
          }});
        }} else {{ setTimeout(doRenderMath, 50); }}
      }}
      doRenderMath();
    </script>
  </body>
</html>
"""
    with sync_playwright() as p:
        browser = p.chromium.launch()
        try:
            page = browser.new_page()
            page.set_content(html, wait_until="load")
            try:
                page.wait_for_selector('.katex', timeout=5000)
            except Exception:
                time.sleep(0.5)
            pdf_bytes = page.pdf(format="A4", print_background=True, margin={
                'top': '0.6in', 'bottom': '0.6in', 'left': '0.6in', 'right': '0.6in'
            })
            return pdf_bytes
        finally:
            browser.close()

# --- Notes generation ---



notes_logger = logging.getLogger("paperx.notes")
if not notes_logger.handlers:
    handler = logging.StreamHandler(sys.stdout)
    fmt = logging.Formatter("%(asctime)s [%(levelname)s] %(name)s: %(message)s")
    handler.setFormatter(fmt)
    notes_logger.addHandler(handler)
notes_logger.setLevel(logging.INFO)

learning_logger = logging.getLogger("paperx.learning_tracks")
if not learning_logger.handlers:
    l_handler = logging.StreamHandler(sys.stdout)
    l_fmt = logging.Formatter("%(asctime)s [%(levelname)s] %(name)s: %(message)s")
    l_handler.setFormatter(l_fmt)
    learning_logger.addHandler(l_handler)
learning_logger.setLevel(logging.INFO)

OPENAI_MODEL = os.getenv("OPENAI_MODEL", "gpt-4o-mini")
OPENAI_API_KEY = (os.getenv("OPENAI_API_KEY", "") or "").strip()
SERPAPI_API_KEY = (os.getenv("SERPAPI_API_KEY", "") or "").strip()
SERPAPI_ENABLED = os.getenv("ENABLE_SERPAPI", "true").strip().lower() in {"1", "true", "yes", "on"}
SERPAPI_TIMEOUT_SEC = float(os.getenv("SERPAPI_TIMEOUT_SEC", "8"))
GEMINI_API_KEY = (os.getenv("GEMINI_API_KEY", "") or "").strip()
GEMINI_NOTES_MODEL = os.getenv("GEMINI_NOTES_MODEL", "gemini-3-pro-preview")
MAX_TRANSCRIPT_CHARS_FOR_NOTES = int(os.getenv("TRANSCRIPT_NOTES_MAX_CHARS", "20000"))

# Default domains for notes/web search when DB has no config yet
DEFAULT_ALLOWED_DOMAINS = [
    "geeksforgeeks.org",
    "tutorialspoint.com",
    "scaler.com",
    "byjus.com",
    "wikipedia.org",
    "tpointtech.com",
]

# Table to store degree-specific allowed domains
DEGREE_ALLOWED_DOMAINS_TABLE = os.getenv("DEGREE_ALLOWED_DOMAINS_TABLE", "degree_allowed_domains")


LEARNING_TRACK_DEFAULT_LANGUAGES = [
    "en",
    "ta",
    "hi",
    "es",
]

LEARNING_TRACK_DEFAULT_STACKS = [
    "python",
    "javascript",
    "java",
    "dsa",
    "react",
]

LEARNING_TRACK_DEFAULT_GOALS = [
    "company_interview",
    "full_stack",
    "semester_prep",
]

LEARNING_TRACK_DEFAULT_COMPANIES = [
    "tcs",
    "zoho",
    "infosys",
    "product",
]

LEARNING_TRACK_DEFAULT_COMPILER_LANGUAGES = [
    {"id": "python", "runtime": "python3", "name": "Python 3"},
    {"id": "javascript", "runtime": "javascript", "name": "JavaScript (Node.js)"},
    {"id": "java", "runtime": "java", "name": "Java 17"},
    {"id": "cpp", "runtime": "cpp", "name": "C++ 17"},
    {"id": "go", "runtime": "go", "name": "Go"},
]


def _parse_env_list(name: str, default: List[str]) -> List[str]:
    raw = os.getenv(name, "")
    if not raw:
        return list(default)
    try:
        parts = [item.strip() for item in re.split(r"[,|]", raw) if item.strip()]
    except Exception:
        return list(default)
    return parts or list(default)


def _parse_env_json_array(name: str, default: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    raw = os.getenv(name, "").strip()
    if not raw:
        return [dict(item) for item in default]
    try:
        data = json.loads(raw)
        if isinstance(data, list):
            cleaned: List[Dict[str, Any]] = []
            for entry in data:
                if isinstance(entry, dict) and entry.get("id") and entry.get("runtime"):
                    cleaned.append({
                        "id": str(entry["id"]).strip(),
                        "runtime": str(entry["runtime"]).strip(),
                        "name": str(entry.get("name") or entry["id"]).strip(),
                    })
            if cleaned:
                return cleaned
    except Exception:
        pass
    return [dict(item) for item in default]


@lru_cache()
def load_learning_tracks_config() -> Dict[str, Any]:
    allowed_domains = _parse_env_list("LEARNING_TRACK_ALLOWED_DOMAINS", DEFAULT_ALLOWED_DOMAINS)
    languages = _parse_env_list("LEARNING_TRACK_LANGUAGES", LEARNING_TRACK_DEFAULT_LANGUAGES)
    stacks = _parse_env_list("LEARNING_TRACK_STACKS", LEARNING_TRACK_DEFAULT_STACKS)
    goals = _parse_env_list("LEARNING_TRACK_GOALS", LEARNING_TRACK_DEFAULT_GOALS)
    companies = _parse_env_list("LEARNING_TRACK_COMPANIES", LEARNING_TRACK_DEFAULT_COMPANIES)
    compiler_languages = _parse_env_json_array("LEARNING_TRACK_COMPILER_LANGUAGES", LEARNING_TRACK_DEFAULT_COMPILER_LANGUAGES)
    planner_model = os.getenv("LEARNING_TRACK_PLANNER_MODEL", os.getenv("GEMINI_PLANNER_MODEL", "gemini-2.5-flash"))
    flashcard_model = os.getenv("LEARNING_TRACK_FLASHCARD_MODEL", GEMINI_NOTES_MODEL)
    code_explainer_model = os.getenv("LEARNING_TRACK_CODE_MODEL", GEMINI_NOTES_MODEL)
    mcq_model = os.getenv("LEARNING_TRACK_MCQ_MODEL", GEMINI_NOTES_MODEL)
    return {
        "languages": languages,
        "stacks": stacks,
        "goals": goals,
        "companies": companies,
        "allowed_domains": allowed_domains,
        "compiler_languages": compiler_languages,
        "planner_model": planner_model,
        "flashcard_model": flashcard_model,
        "code_explainer_model": code_explainer_model,
        "mcq_model": mcq_model,
        "notes_model": GEMINI_NOTES_MODEL,
    }

HEADERS = {
    "User-Agent": "Mozilla/5.0 (PaperX; +https://example.com) AppleWebKit/537.36 "
                  "(KHTML, like Gecko) Chrome/124.0 Safari/537.36"
}

REQ_TIMEOUT = 25
MAX_FETCH_BYTES = int(os.getenv("PAPERX_FETCH_MAX_BYTES", "5000000"))  # 5 MB safety cap per fetch


@dataclass
class SectionChunk:
    title: str
    text: str
    url: str
    quote: str = ""


@dataclass
class PageExtract:
    url: str
    title: str
    sections: List[SectionChunk] = field(default_factory=list)


def _normalize_degree_key(degree: Optional[str]) -> Optional[str]:
    if not degree:
        return None
    s = re.sub(r"[^a-zA-Z0-9]+", "", degree).upper()
    return s or None


def db_get_allowed_domains_for_degree(degree: Optional[str]) -> List[str]:
    """Fetch enabled allowed domains for the normalized degree key.

    Returns empty list if none configured (caller may fallback to DEFAULT).
    """
    key = _normalize_degree_key(degree)
    if not key:
        return []
    supabase = get_service_client()
    try:
        res = (
            supabase.table(DEGREE_ALLOWED_DOMAINS_TABLE)
            .select("domain,enabled")
            .eq("degree_key", key)
            .eq("enabled", True)
            .order("domain")
            .execute()
        )
        rows = getattr(res, "data", []) or []
        out = []
        for r in rows:
            d = (r.get("domain") or "").strip()
            if d:
                out.append(d)
        return out
    except Exception:
        return []


def is_allowed(url: str, allowed_domains: Optional[List[str]] = None) -> bool:
    """Check if URL's host matches any allowed domain.

    If allowed_domains is empty/None, allow all (no restriction).
    """
    try:
        host = urlparse(url).netloc.lower()
        domains = allowed_domains or []
        if not domains:
            return True
        return any(host.endswith(d) for d in domains)
    except Exception:
        return False


def serpapi_search(topic: str, num: int = 10, *, degree: Optional[str] = None, allowed_domains: Optional[List[str]] = None) -> List[str]:
    """Search Google via SerpAPI.

    Preference order for domain restriction:
      1) allowed_domains param if provided (non-empty)
      2) domains fetched from DB by degree
      3) DEFAULT_ALLOWED_DOMAINS

    Fallback: If no URLs matched the domain filter, retry WITHOUT any site filter (top web results).
    """
    if not SERPAPI_ENABLED:
        notes_logger.info("SerpAPI disabled; skipping search", extra={"topic": topic})
        return []
    if not SERPAPI_API_KEY:
        notes_logger.warning("SerpAPI key missing; skipping search", extra={"topic": topic})
        return []
    # Resolve domains according to priority
    domains: List[str] = []
    if allowed_domains:
        domains = [d for d in allowed_domains if d]
    elif degree:
        domains = db_get_allowed_domains_for_degree(degree)
    if not domains:
        domains = list(DEFAULT_ALLOWED_DOMAINS)

    site_filter = " OR ".join([f"site:{d}" for d in domains]) if domains else ""
    q = f"{topic} ({site_filter})" if site_filter else topic
    params = {
        "engine": "google",
        "q": q,
        "num": min(20, max(5, num)),
        "hl": "en",
        "safe": "active",
        "api_key": SERPAPI_API_KEY,
    }
    notes_logger.info("SerpAPI search start", extra={
        "topic": topic,
        "query": q,
        "allowed_domains": domains,
        "api_key_present": bool(SERPAPI_API_KEY),
    })
    def _fetch(p):
        return GoogleSearch(p).get_dict()

    results: Dict[str, Any] = {}
    try:
        with concurrent.futures.ThreadPoolExecutor(max_workers=1) as pool:
            fut = pool.submit(_fetch, params)
            results = fut.result(timeout=SERPAPI_TIMEOUT_SEC)
    except concurrent.futures.TimeoutError:
        notes_logger.error("SerpAPI search timed out", extra={"topic": topic, "timeout_sec": SERPAPI_TIMEOUT_SEC})
        return []
    except Exception as exc:
        notes_logger.error("SerpAPI search failed", exc_info=exc)
        return []
    urls: List[str] = []
    if not results:
        notes_logger.warning("SerpAPI returned empty response", extra={"topic": topic})
    else:
        err = results.get("error")
        if err:
            notes_logger.error("SerpAPI error for topic '%s': %s", topic, err)
        else:
            notes_logger.debug(
                "SerpAPI search metadata",
                extra={
                    "topic": topic,
                    "organic_count": len(results.get("organic_results") or []),
                    "related_questions": len(results.get("related_questions") or []),
                },
            )

            for item in (results.get("organic_results") or []):
                link = item.get("link")
                if link and is_allowed(link, domains):
                    urls.append(link)
            # Also parse related if available
            for item in (results.get("related_questions") or []):
                for src in (item.get("sources") or []):
                    link = src.get("link")
                    if link and is_allowed(link, domains):
                        urls.append(link)

    # Dedup preserve order
    seen: Set[str] = set()
    filtered: List[str] = []
    for u in urls:
        if u not in seen:
            seen.add(u)
            filtered.append(u)

    if filtered:
        notes_logger.info(
            "SerpAPI search success",
            extra={"topic": topic, "selected_urls": filtered[: min(3, len(filtered))], "total": len(filtered), "domains": domains},
        )
        return filtered[:num]

    # Fallback: re-run without site restriction to get top results
    notes_logger.warning("No URLs matched allowed domains; falling back to unrestricted search", extra={"topic": topic})
    try:
        with concurrent.futures.ThreadPoolExecutor(max_workers=1) as pool:
            fut2 = pool.submit(_fetch, {**params, "q": topic})
            results2 = fut2.result(timeout=SERPAPI_TIMEOUT_SEC)
    except concurrent.futures.TimeoutError:
        notes_logger.error("SerpAPI fallback timed out", extra={"topic": topic, "timeout_sec": SERPAPI_TIMEOUT_SEC})
        return []
    except Exception:
        return []
    urls2: List[str] = []
    for item in (results2.get("organic_results") or []):
        link = item.get("link")
        if link:
            urls2.append(link)
    # dedup preserve order
    seen2: Set[str] = set()
    out2: List[str] = []
    for u in urls2:
        if u not in seen2:
            seen2.add(u)
            out2.append(u)
    return out2[:num]


def fetch(url: str) -> str:
    """Fetch a URL with a strict size cap to avoid memory blow-ups from large assets.

    Uses streaming downloads and enforces a configurable max byte budget
    (PAPERX_FETCH_MAX_BYTES, default 5 MB). Raises RuntimeError on size
    overflow so callers can treat it as a fetch failure.
    """
    with requests.get(url, headers=HEADERS, timeout=REQ_TIMEOUT, stream=True) as r:
        r.raise_for_status()

        # Fast reject if Content-Length header is present and too large
        try:
            content_len = int(r.headers.get("Content-Length", "0"))
            if content_len and content_len > MAX_FETCH_BYTES:
                raise RuntimeError(f"Remote content too large ({content_len} bytes)")
        except ValueError:
            # Ignore malformed header; fall back to streamed limit below
            pass

        chunks: List[bytes] = []
        total = 0
        for chunk in r.iter_content(chunk_size=8192):
            if not chunk:
                continue
            total += len(chunk)
            if total > MAX_FETCH_BYTES:
                raise RuntimeError(f"Remote content exceeded max size ({MAX_FETCH_BYTES} bytes cap)")
            chunks.append(chunk)

    body = b"".join(chunks)
    encoding = r.encoding or "utf-8"
    try:
        return body.decode(encoding, errors="replace")
    except Exception:
        return body.decode("utf-8", errors="replace")


# -------------------- Image helpers --------------------
LOGO_HINTS = [
    "logo", "favicon", "icon", "sprite", "brandmark", "watermark",
    "placeholder", "default", "avatar", "badge", "mark"
]

def looks_like_logo(url: str) -> bool:
    u = url.lower()
    if any(h in u for h in LOGO_HINTS):
        return True
    # very small images by filename hints
    if re.search(r"[=_-](?:16|24|32|48|64)(?:x(?:16|24|32|48|64))?\.(?:png|jpg|jpeg|gif|webp)$", u):
        return True
    return False

def is_reasonable_image_url(url: str) -> bool:
    if not url:
        return False
    if looks_like_logo(url):
        return False
    return any(url.lower().endswith(ext) for ext in [".png", ".jpg", ".jpeg", ".webp", ".gif", ".svg"])

def serpapi_image_urls(topic: str, num: int = 10) -> List[str]:
    params = {
        "engine": "google",
        "q": topic,
        "tbm": "isch",
        "num": min(20, max(5, num)),
        "safe": "active",
        "api_key": SERPAPI_API_KEY,
        "hl": "en",
    }
    try:
        search = GoogleSearch(params)
        results = search.get_dict()
    except Exception:
        return []
    urls: List[str] = []
    for item in (results.get("images_results") or []):
        link = item.get("original") or item.get("thumbnail") or item.get("link")
        if link and is_reasonable_image_url(link):
            urls.append(link)
    # dedup
    seen = set()
    out: List[str] = []
    for u in urls:
        if u not in seen:
            seen.add(u)
            out.append(u)
    return out[:num]


def normalize_text(txt: str) -> str:
    txt = re.sub(r"\s+", " ", txt).strip()
    return txt


def extract_sections_from_html(url: str, html: str) -> PageExtract:
    """Extract title and sectioned text by H1/H2/H3."""
    soup = BeautifulSoup(html, "html.parser")
    title = soup.title.get_text(strip=True) if soup.title else url
    # remove nav/aside/footer/scripts
    for tag in soup(["script", "style", "noscript", "header", "footer", "nav", "form", "aside"]):
        tag.decompose()

    # identify content area heuristically
    main = soup.find(["article", "main"]) or soup.body or soup

    # Gather headings and their content
    sections: List[SectionChunk] = []
    headings = main.find_all(["h1", "h2", "h3"])
    if not headings:
        # fallback: big paragraphs
        text = normalize_text(main.get_text(" "))
        if text:
            sections.append(SectionChunk(title="Content", text=text, url=url))
        return PageExtract(url=url, title=title, sections=sections)

    for i, h in enumerate(headings):
        h_title = normalize_text(h.get_text(" "))
        content_parts = []
        for sib in h.next_siblings:
            if getattr(sib, "name", None) in ["h1", "h2", "h3"]:
                break
            if getattr(sib, "name", None) in ["p", "ul", "ol", "pre", "code", "table", "div"]:
                content_parts.append(sib.get_text(" ", strip=True))
            elif isinstance(sib, str):
                content_parts.append(str(sib).strip())
        content = normalize_text(" ".join([c for c in content_parts if c]))
        if h_title and content:
            # extract a short quote (first 200 chars) to attach as citation span
            quote = content[:200]
            sections.append(SectionChunk(title=h_title, text=content, url=url, quote=quote))
    return PageExtract(url=url, title=title, sections=sections)


def extract_image_urls_from_html(url: str, html: str, max_images: int = 10) -> List[str]:
    soup = BeautifulSoup(html, "html.parser")
    # remove non-content containers to reduce boilerplate images
    for tag in soup(["script", "style", "noscript", "header", "footer", "nav", "form", "aside"]):
        tag.decompose()
    images: List[str] = []
    for img in soup.find_all("img"):
        src = img.get("src") or img.get("data-src") or img.get("data-original")
        if not src:
            continue
        # resolve relative URLs
        if src.startswith("//"):
            src = f"https:{src}"
        elif src.startswith("/"):
            parsed = urlparse(url)
            src = f"{parsed.scheme}://{parsed.netloc}{src}"
        alt_text = (img.get("alt") or "").lower()
        if any(k in alt_text for k in ["logo", "icon", "favicon"]):
            continue
        if is_reasonable_image_url(src):
            images.append(src)
        if len(images) >= max_images:
            break
    # dedup preserve order
    seen = set()
    out: List[str] = []
    for u in images:
        if u not in seen:
            seen.add(u)
            out.append(u)
    return out[:max_images]


def unify_section_titles(pages: List[PageExtract]) -> List[str]:
    """
    Build a merged set of section titles observed across pages.
    Use fuzzy grouping to avoid duplicates (e.g., 'Advantages' vs 'Pros').
    """
    titles = []
    for p in pages:
        for s in p.sections:
            titles.append(s.title)

    # fuzzy dedup
    merged: List[str] = []
    for t in titles:
        if not t:
            continue
        if not merged:
            merged.append(t)
            continue
        scores = [fuzz.token_set_ratio(t.lower(), m.lower()) for m in merged]
        if max(scores) >= 85:
            # treat as same; skip adding
            continue
        merged.append(t)

    # Always ensure compulsory blocks exist as "virtual" targets
    compulsory = ["Introduction", "Examples", "Conclusion"]
    for c in compulsory:
        if all(fuzz.token_set_ratio(c.lower(), m.lower()) < 85 for m in merged):
            merged.append(c)
    return merged


def assemble_context_for_llm(pages: List[PageExtract], merged_titles: List[str], topic: str) -> str:
    """
    Build a compact, source-quoted context the model can use.
    """
    lines = [f"Topic: {topic}", "", "SOURCE EXCERPTS (keep factual grounding):"]
    for p in pages:
        lines.append(f"\n### {p.title}\nURL: {p.url}")
        for s in p.sections[:8]:  # keep compact
            lines.append(f"- [{s.title}] {s.quote}â€¦")
    lines.append("\nMERGED SECTION TITLES CANDIDATE ORDER:")
    for t in merged_titles:
        lines.append(f"- {t}")
    return "\n".join(lines)


SYSTEM_INSTRUCTIONS = """You are a senior educational writer building accurate, well-structured notes for college students in India.

PRIMARY GOAL:
Produce notes that are **accurate, complete, and strictly relevant** to the given topic.  
Use the provided context **only where it is clearly correct and directly applicable**.  
If any part of the context is **irrelevant, weakly related, outdated, or incorrect**, **IGNORE it completely** and rely on your **own expert knowledge** instead.

CRITICAL RULES:
- If information is missing, **generate it yourself accurately**.
- Use ONLY content that genuinely matches the topic.
- Do NOT force‑fit unrelated context.
- Do NOT include phrases like *"needs review"*, *"may vary"*, or *"depends"*.
- Ensure **conceptual correctness suitable for university exams**.
- Respect section headings actually observed on the referenced pages. You may merge similar headings (e.g., Advantages/Pros).
- You MUST also include these blocks even if not present: Introduction, Examples, Conclusion.
- Keep explanations concise but complete; use bullet points where helpful.
- Include at least one Mermaid diagram when process/relationships are relevant.
- Every non-obvious claim MUST carry an inline citation like [GFG], [TP], [Scaler], [Wiki], or [TPT] mapped in the CITATIONS section.
- Prefer plain text + Mermaid diagrams; do not embed external images.
- Bold important keywords, symbols, and technical terms using Markdown **double asterisks**. Examples: **epsilon-greedy (ε-greedy)**, **Markov Decision Process (MDP)**, parameters like **θ**, **γ**, **α**, algorithm names like **Q-learning**.

OUTPUT FORMAT (STRICT):
Return a single Markdown document with:
1) A title line: '# <Topic>'
2) For each merged section title (after light normalization), a '## <Section>' block
3) Use bullet points, short paragraphs, tables when appropriate (GitHub MD)
4) Mermaid diagram(s) in fenced code blocks: ```mermaid ... ```
5) A final '## CITATIONS' list mapping labels to URLs with short quoted spans

If sources contradict, mark the line with [conflict] and keep both with citations.

Keep it under ~900–1200 words unless the topic is inherently longer.
"""


def build_agent() -> AssistantAgent:
    model_client = gemini_model_client
    assistant = AssistantAgent(
        "paperx_notes_agent",
        model_client=model_client,
        system_message=SYSTEM_INSTRUCTIONS,
    )
    return assistant


# --- Helper: safely run async assistant from sync code (works inside running event loop) ---

def _run_assistant_blocking(assistant: AssistantAgent, user_prompt: str):
    async def _coro():
        return await assistant.run(task=user_prompt)
    try:
        # If we're in a running loop (e.g., FastAPI), offload to a thread
        asyncio.get_running_loop()
        import concurrent.futures
        with concurrent.futures.ThreadPoolExecutor(max_workers=1) as ex:
            return ex.submit(lambda: asyncio.run(_coro())).result()
    except RuntimeError:
        # No running loop (CLI), safe to use asyncio.run directly
        return asyncio.run(_coro())


def generate_notes_markdown(topic: str, *, degree: Optional[str] = None) -> str:
    # 1) Search
    notes_logger.info("generate_notes_markdown:start", extra={"topic": topic})
    urls = serpapi_search(topic, num=10, degree=degree)
    if not urls:
        notes_logger.error("generate_notes_markdown:no_urls", extra={"topic": topic})
        raise RuntimeError("No results from allowed domains.")

    # 2) Fetch & extract
    pages: List[PageExtract] = []
    for u in urls[:6]:  # keep it tight
        try:
            html = fetch(u)
            pages.append(extract_sections_from_html(u, html))
            notes_logger.debug("Fetched page", extra={"topic": topic, "url": u})
        except Exception as e:
            # ignore broken pages
            notes_logger.warning(
                "Fetch failed", extra={"topic": topic, "url": u, "error": str(e)}
            )
            continue

    if not pages:
        notes_logger.error("generate_notes_markdown:no_pages", extra={"topic": topic})
        raise RuntimeError("Failed to extract any pages.")

    # 3) Merge section titles from sources
    merged_titles = unify_section_titles(pages)
    notes_logger.debug(
        "Merged titles",
        extra={"topic": topic, "titles": merged_titles[: min(5, len(merged_titles))]},
    )

    # 4) Build context for LLM
    context = assemble_context_for_llm(pages, merged_titles, topic)

    # 5) Call AutoGen Assistant
    assistant = build_agent()
    user_prompt = f"""
You will compose comprehensive, exam-ready Markdown notes for the topic "{topic}".

PRIMARY GOAL:
Produce notes that are **accurate, complete, and strictly relevant** to the given topic.  
Use the provided context **only where it is clearly correct and directly applicable**.  
If any part of the context is **irrelevant, weakly related, outdated, or incorrect**, **IGNORE it completely** and rely on your **own expert knowledge** instead.

Context:
{context}

Instructions:
STRICT CONTENT RULES:
- Use ONLY content that genuinely matches the topic.
- Do NOT force‑fit unrelated context.
- Do NOT include phrases like *"needs review"*, *"may vary"*, or *"depends"*.
- If information is missing, **generate it yourself accurately**.
- Ensure **conceptual correctness suitable for university exams**.
- No hallucinated references; cite only well‑known, credible sources.
- Maintain a confident academic tone.

STRUCTURE & DEPTH:
- Write **DETAILED, THOROUGH notes** suitable for scoring high marks.
- Expand explanations with examples, formulas, workflows, and comparisons where appropriate.
- Normalize headings lightly (choose the most standard academic term).
- Maintain logical flow from basics → core concepts → advanced ideas → applications.


MANDATORY SECTIONS TO INCLUDE (if applicable to the topic):
- **Introduction**: Comprehensive overview with context and importance
- **Need / Why It Is Required**: What problem does it solve? Why was it developed?
- **Definition / Core Concept**: Clear, precise technical definition of the "{topic}"

FORMATTING RULES:
- Output must be in **Markdown**.
- Start with: `# {topic}`
- **Bold important terms, symbols, equations, and definitions** (use consistently, avoid overuse).
- Use bullet points, tables, and sub‑headings for clarity.

CITATIONS:
- Add a final section: **## CITATIONS**
- Map citation labels to URLs, using only trusted sources:
  - [GFG] – GeeksforGeeks
  - [TPT] – TutorialsPoint
  - [Scaler] – Scaler Topics
  - [Wiki] – Wikipedia
  - [TP] – Trusted textbooks / official documentation
- Inline‑cite like:  
  "... explanation ... [GFG]" or "... definition ... [Wiki]"

TARGET LENGTH:
- **900–1200 words**, unless the topic strictly requires less.

QUALITY BAR:
The output should be **exam‑ready**, **self‑contained**, and **require no further corrections**.

Start with '# {topic}' and then the sections in a logical order.
"""
    # Use safe runner to support both CLI and FastAPI contexts
    result = _run_assistant_blocking(assistant, user_prompt)
    content = result.messages[-1].content
    notes_logger.info("generate_notes_markdown:success", extra={"topic": topic, "length": len(content)})
    return content


# ---------------- New: Streaming events generator for UI/API ----------------

def _build_variant_user_prompt(context: str, topic: str, variant: str) -> str:
    v = _normalize_variant(variant)
    if v == "cheatsheet":
        return f"""
You will compose an EXAM-READY CHEAT SHEET for the topic "{topic}" based ONLY on the source excerpts below.

Context:
{context}

Output rules (STRICT):
- Keep it ultra concise (≈ 250-400 words). Use bullets and tables.
- Start with a single H1: '# {topic} — Cheat Sheet'.
- Sections (H2):
  1) Core Concepts (5–10 bullets, crisp one-liners)
  2) Key Definitions & Formulas (bullets; inline math where relevant)
  3) Quick Steps / Algorithms (bulleted steps)
  4) Pitfalls / Gotchas (3–6 bullets)
  5) Keywords (comma-separated list)
- Bold key terms and symbols with **...**. Prefer compact phrasing over full sentences.
- If any fact is uncertain, mark [needs review].
 - Do NOT include sections titled 'TL;DR', 'Common Mistakes', or 'Memory Aids'.
 - Do NOT include a 'CITATIONS' section or any citation list.
""".strip()
    if v == "simple":
        return f"""
You will write a SIMPLE, EASY-TO-UNDERSTAND set of notes for "{topic}" using ONLY the source excerpts below.

Context:
{context}

Output rules (STRICT):
- Target length: 600–900 words, plain language, short sentences.
- Start with '# {topic} — Simple Notes'.
- Structure with logical H2 sections, including: Introduction, Concepts, Examples, Conclusion.
- Explain in everyday words without dumbing down definitions.
- Use bullets and small tables where helpful.
- Bold important terms with **...**.
- Include a final '## CITATIONS' section with label→URL list for the sources you used.
""".strip()
    # default detailed prompt - comprehensive and thorough
    return f"""
You will compose comprehensive, exam-ready Markdown notes for the topic "{topic}".

Context:
{context}

PRIMARY GOAL:
Produce notes that are **accurate, complete, and strictly relevant** to the given topic.  
Use the provided context **only where it is clearly correct and directly applicable**.  
If any part of the context is **irrelevant, weakly related, outdated, or incorrect**, **IGNORE it completely** and rely on your **own expert knowledge** instead.

Context:
{context}

Instructions:
STRICT CONTENT RULES:
- Use ONLY content that genuinely matches the topic.
- Do NOT force‑fit unrelated context.
- Do NOT include phrases like *"needs review"*, *"may vary"*, or *"depends"*.
- If information is missing, **generate it yourself accurately**.
- Ensure **conceptual correctness suitable for university exams**.
- No hallucinated references; cite only well‑known, credible sources.
- Maintain a confident academic tone.

STRUCTURE & DEPTH:
- Write **DETAILED, THOROUGH notes** suitable for scoring high marks.
- Expand explanations with examples, formulas, workflows, and comparisons where appropriate.
- Normalize headings lightly (choose the most standard academic term).
- Maintain logical flow from basics → core concepts → advanced ideas → applications.


MANDATORY SECTIONS TO INCLUDE (if applicable to the topic):
- **Introduction**: Comprehensive overview with context and importance
- **Need / Why It Is Required**: What problem does it solve? Why was it developed?
- **Definition / Core Concept**: Clear, precise technical definition of the "{topic}"

FORMATTING RULES:
- Output must be in **Markdown**.
- Start with: `# {topic}`
- **Bold important terms, symbols, equations, and definitions** (use consistently, avoid overuse).
- Use bullet points, tables, and sub‑headings for clarity.

CITATIONS:
- Add a final section: **## CITATIONS**
- Map citation labels to URLs, using only trusted sources:
  - [GFG] – GeeksforGeeks
  - [TPT] – TutorialsPoint
  - [Scaler] – Scaler Topics
  - [Wiki] – Wikipedia
  - [TP] – Trusted textbooks / official documentation
- Inline‑cite like:  
  "... explanation ... [GFG]" or "... definition ... [Wiki]"

TARGET LENGTH:
- **900–1200 words**, unless the topic strictly requires less.

QUALITY BAR:
The output should be **exam‑ready**, **self‑contained**, and **require no further corrections**.
Start with '# {topic}' and then the sections in a logical order.
""".strip()

def generate_notes_events(topic: str, *, stop_event: Optional[threading.Event] = None, variant: str = "detailed", degree: Optional[str] = None) -> Iterator[Tuple[str, Dict[str, Any]]]:
    """Yield (event_name, payload) tuples describing real-time progress and final output.

    Events emitted in order (names):
      - start
      - search_results
      - fetch_start / fetch_done / fetch_error
      - merged_titles
      - context_ready
      - llm_start / llm_done
      - images
      - final (includes markdown + image_urls)
      - error (early termination on fatal error)
    """
    try:
        if stop_event and stop_event.is_set():
            return
        dyn_domains = db_get_allowed_domains_for_degree(degree) if degree else []
        if not dyn_domains:
            dyn_domains = list(DEFAULT_ALLOWED_DOMAINS)
        yield ("start", {"topic": topic, "allowed_domains": dyn_domains, "degree": degree})
        if stop_event and stop_event.is_set():
            return
        urls = serpapi_search(topic, num=10, degree=degree)
        if stop_event and stop_event.is_set():
            return
        if not urls:
            yield ("error", {"message": "No results from allowed domains."})
            return
        yield ("search_results", {"urls": urls})

        # Fetch & extract
        pages: List[PageExtract] = []
        for u in urls[:6]:
            if stop_event and stop_event.is_set():
                return
            yield ("fetch_start", {"url": u})
            try:
                html = fetch(u)
                page = extract_sections_from_html(u, html)
                pages.append(page)
                yield ("fetch_done", {"url": u, "title": page.title, "sections": len(page.sections)})
            except Exception as e:
                yield ("fetch_error", {"url": u, "error": str(e)})

        if not pages:
            yield ("error", {"message": "Failed to extract any pages."})
            return

        if stop_event and stop_event.is_set():
            return
        merged_titles = unify_section_titles(pages)
        yield ("merged_titles", {"titles": merged_titles})

        if stop_event and stop_event.is_set():
            return
        context = assemble_context_for_llm(pages, merged_titles, topic)
        yield ("context_ready", {"chars": len(context)})

        assistant = build_agent()
        user_prompt = _build_variant_user_prompt(context, topic, variant)
        yield ("llm_start", {})
        try:
            # Use safe runner in case we're under FastAPI's loop
            result = _run_assistant_blocking(assistant, user_prompt)
            content = result.messages[-1].content
        except Exception as e:
            yield ("error", {"message": f"LLM error: {e}"})
            return
        yield ("llm_done", {"md_chars": len(content)})

        if stop_event and stop_event.is_set():
            return
        # Images from SERP + pages
        related_pages = urls[:8]
        image_urls = collect_image_urls(topic, related_pages, stop_event=stop_event)
        if stop_event and stop_event.is_set():
            return
        yield ("images", {"count": len(image_urls), "image_urls": image_urls})

        yield ("final", {"markdown": content, "image_urls": image_urls})
    except Exception as e:
        # last-resort catch to keep stream alive with an error
        yield ("error", {"message": str(e)})


def collect_image_urls(topic: str, page_urls: List[str], stop_event: Optional[threading.Event] = None) -> List[str]:
    if stop_event and stop_event.is_set():
        return []
    urls: List[str] = []
    # 1) From SerpAPI image search - REMOVED per user request
    # urls.extend(serpapi_image_urls(topic, num=12))
    if stop_event and stop_event.is_set():
        return urls
    # 2) From parsed webpages
    for u in page_urls[:6]:
        if stop_event and stop_event.is_set():
            break
        try:
            html = fetch(u)
        except Exception:
            continue
        urls.extend(extract_image_urls_from_html(u, html, max_images=6))
    # deduplicate while preserving order
    seen = set()
    out: List[str] = []
    for link in urls:
        if link not in seen:
            seen.add(link)
            out.append(link)
    return out[:20]


def save_md(topic: str, md_text: str) -> str:
    safe = re.sub(r"[^a-zA-Z0-9_.-]+", "_", topic.strip())[:80]
    fname = f"{safe}.md"
    with open(fname, "w", encoding="utf-8") as f:
        f.write(md_text)
    return os.path.abspath(fname)


def main():
    if len(sys.argv) < 2:
        print("Usage: python paperx_notes.py \"<topic>\"")
        sys.exit(1)

    topic = sys.argv[1].strip()
    degree = sys.argv[2].strip() if len(sys.argv) > 2 else None
    resolved_domains = db_get_allowed_domains_for_degree(degree) if degree else []
    if not resolved_domains:
        resolved_domains = list(DEFAULT_ALLOWED_DOMAINS)
    
    print(f"[Paper X] Generating notes for topic: {topic}\n"
          f"Degree: {degree or '-'}\n"
          f"Allowed domains: {', '.join(resolved_domains)}\n")

    md_text = generate_notes_markdown(topic, degree=degree)
    path = save_md(topic, md_text)
    print(md_text)
    # Collect related images without downloading (URLs only)
    related_pages = serpapi_search(topic, num=8, degree=degree)
    image_urls = collect_image_urls(topic, related_pages)
    if image_urls:
        print("\n---\nRelated image URLs (filtered, no logos):")
        for i, u in enumerate(image_urls, 1):
            print(f"{i}. {u}")
    print("\n---\nSaved:", path)


if __name__ == "__main__":
    main()

# --- Academics schemas ---



class BatchIn(BaseModel):
    from_year: int = Field(..., ge=1950, le=2100, alias="from")
    to_year: int = Field(..., ge=1950, le=2100, alias="to")

    @validator("to_year")
    def check_range(cls, v, values):
        f = values.get("from_year")
        if f is not None and v < f:
            raise ValueError("to_year must be >= from_year")
        return v


class DepartmentCreateIn(BaseModel):
    name: str
    batches: List[BatchIn]

    @validator("name")
    def normalize_name(cls, v: str):
        value = (v or "").strip()
        if not value:
            raise ValueError("Department name required.")
        return value.upper()

    @validator("batches")
    def ensure_batches(cls, v: List[BatchIn]):
        if not v:
            raise ValueError("Add at least one batch range.")
        seen: set[Tuple[int, int]] = set()
        unique: List[BatchIn] = []
        for batch in v:
            pair = (batch.from_year, batch.to_year)
            if pair in seen:
                continue
            seen.add(pair)
            unique.append(batch)
        return unique


class DegreeCreateIn(BaseModel):
    name: str = Field(..., min_length=2, max_length=256)
    level: Optional[str] = Field(None, max_length=64)
    duration_years: Optional[int] = Field(None, ge=1, le=10)
    departments: List[DepartmentCreateIn]

    @validator("name")
    def trim_degree_name(cls, v: str):
        value = (v or "").strip()
        if not value:
            raise ValueError("Degree name required.")
        return value

    @validator("departments")
    def ensure_departments(cls, v: List[DepartmentCreateIn]):
        if not v:
            raise ValueError("Add at least one department to the degree.")
        seen: set[str] = set()
        for dept in v:
            key = dept.name.upper()
            if key in seen:
                raise ValueError(f"Duplicate department '{dept.name}' in the same degree.")
            seen.add(key)
        return v


class CollegeCreateIn(BaseModel):
    college_name: str = Field(..., min_length=2, max_length=256)
    degrees: Optional[List[DegreeCreateIn]] = None
    departments: Optional[List[str]] = None  # legacy payload support
    batches: Optional[List[BatchIn]] = None  # legacy payload support

    @validator("college_name")
    def trim_college(cls, v: str):
        value = (v or "").strip()
        if not value:
            raise ValueError("College name required.")
        return value

    @root_validator(pre=True)
    def coerce_legacy_payload(cls, values):
        if not values:
            return values
        data = dict(values)
        if data.get("degrees"):
            return data
        legacy_departments = [
            (d or "").strip() for d in (data.get("departments") or []) if (d or "").strip()
        ]
        if legacy_departments:
            legacy_batches_raw = data.get("batches") or []
            if not legacy_batches_raw:
                raise ValueError("Provide batch ranges when using legacy departments payload.")
            coerced_batches = []
            for item in legacy_batches_raw:
                if isinstance(item, BatchIn):
                    coerced_batches.append(item.dict(by_alias=True))
                elif isinstance(item, dict):
                    if "from" in item and "to" in item:
                        coerced_batches.append({"from": item["from"], "to": item["to"]})
                    elif "from_year" in item and "to_year" in item:
                        coerced_batches.append({"from": item["from_year"], "to": item["to_year"]})
                    else:
                        raise ValueError("Invalid batch entry in legacy payload.")
                else:
                    raise ValueError("Invalid batch entry in legacy payload.")
            data["degrees"] = [
                {
                    "name": "B.Tech",
                    "level": None,
                    "duration_years": None,
                    "departments": [
                        {"name": dept, "batches": coerced_batches}
                        for dept in legacy_departments
                    ],
                }
            ]
            return data
        raise ValueError("Provide at least one degree with departments and batch ranges.")

    @validator("degrees")
    def ensure_degrees(cls, v: Optional[List[DegreeCreateIn]]):
        if not v:
            raise ValueError("At least one degree is required.")
        seen: set[str] = set()
        for degree in v:
            key = degree.name.strip().lower()
            if key in seen:
                raise ValueError(f"Duplicate degree '{degree.name}'.")
            seen.add(key)
        return v


class College(BaseModel):
    id: uuid.UUID
    name: str
    logo_url: Optional[str] = None


class CollegeNameOnlyIn(BaseModel):
    name: str = Field(..., min_length=2, max_length=256)

    @validator("name")
    def trim_name(cls, v: str):
        value = (v or "").strip()
        if not value:
            raise ValueError("College name required.")
        return value


class DepartmentWithBatchesOut(BaseModel):
    id: uuid.UUID
    name: str
    batches: List[BatchIn]


class DepartmentSimpleBase(BaseModel):
    name: str = Field(..., min_length=2, max_length=256)
    batches: Optional[List[BatchIn]] = None

    @validator("name")
    def normalize_name(cls, v: str):
        value = (v or "").strip()
        if not value:
            raise ValueError("Department name required.")
        return value.upper()

    @validator("batches")
    def dedupe_batches(cls, v: Optional[List[BatchIn]]):
        if v is None:
            return None
        seen: set[Tuple[int, int]] = set()
        unique: List[BatchIn] = []
        for batch in v:
            pair = (batch.from_year, batch.to_year)
            if pair in seen:
                continue
            seen.add(pair)
            unique.append(batch)
        return unique


class DepartmentSimpleCreateIn(DepartmentSimpleBase):
    pass


class DepartmentSimpleUpdateIn(DepartmentSimpleBase):
    pass


class DegreeOut(BaseModel):
    id: uuid.UUID
    name: str
    level: Optional[str] = None
    duration_years: Optional[int] = None
    departments: List[DepartmentWithBatchesOut]


class DegreeSimpleCreateIn(BaseModel):
    name: str = Field(..., min_length=2, max_length=256)
    level: Optional[str] = Field(None, max_length=64)
    duration_years: Optional[int] = Field(None, ge=1, le=10)

    @validator("name")
    def trim_name(cls, v: str):
        value = (v or "").strip()
        if not value:
            raise ValueError("Degree name required.")
        return value

    @validator("level")
    def normalize_level(cls, v: Optional[str]):
        if v is None:
            return None
        value = v.strip()
        return value or None


class CollegeFullOut(BaseModel):
    id: uuid.UUID
    name: str
    logo_url: Optional[str] = None
    degrees: List[DegreeOut]
    departments: List[str]
    batches: List[BatchIn]


class DepartmentOut(BaseModel):

    id: uuid.UUID
    name: str


class BatchWithIdOut(BaseModel):
    id: uuid.UUID
    from_year: int
    to_year: int


class UserAuth(BaseModel):
    email: str
    password: str


class SignupFullIn(BaseModel):
    name: str
    gender: Optional[str] = Field(None, pattern=r"^(female|male|other)$")
    phone: Optional[str] = Field(None, min_length=10, max_length=15)
    email: str
    password: str
    college: str
    department: str
    batch_from: int = Field(..., ge=1950, le=2100)
    batch_to: int = Field(..., ge=1950, le=2100)
    semester: int = Field(..., ge=1, le=12)
    regno: str

    @validator("department")
    def uppercase_dept(cls, v):
        return (v or "").upper()

    @validator("regno")
    def uppercase_regno(cls, v):
        return (v or "").upper()

    @validator("batch_to")
    def batch_years_valid(cls, v, values):
        f = values.get("batch_from")
        if f and v < f:
            raise ValueError("batch_to must be >= batch_from")
        return v


class TopicIn(BaseModel):
    topic: str = Field(..., min_length=1)

    @validator("topic", pre=True)
    def _strip_topic(cls, value: Any):  # noqa: N805
        if value is None:
            raise ValueError("Topic cannot be empty")
        if isinstance(value, str):
            trimmed = value.strip()
            if not trimmed:
                raise ValueError("Topic cannot be empty")
            return trimmed
        trimmed = str(value).strip()
        if not trimmed:
            raise ValueError("Topic cannot be empty")
        return trimmed


class TopicUpsertIn(TopicIn):
    """Topic payload for unit create/update.

    When `id` is provided, the server updates that existing topic row.
    When `id` is omitted, the server inserts a new topic row.
    """

    id: Optional[uuid.UUID] = None


class UnitIn(BaseModel):
    unit_title: str = Field(..., min_length=1)
    topics: List[TopicIn]

    @validator("topics")
    def non_empty_topics(cls, v):
        return v or []


class SyllabusCourseIn(BaseModel):
    batch_id: uuid.UUID
    semester: int = Field(..., ge=1, le=12)
    course_code: str = Field(..., min_length=1, max_length=64)
    title: str = Field(..., min_length=1, max_length=256)
    type: Optional[str] = Field(default="practical")
    units: List[UnitIn]

    @validator("type", pre=True, always=True)
    def _normalize_type(cls, value: Any):  # noqa: N805
        allowed = {"maths", "theorey", "practical"}
        if value is None:
            return "practical"
        if isinstance(value, str):
            v = value.strip().lower()
            return v if v in allowed else "practical"
        return "practical"


class TopicOut(BaseModel):
    id: uuid.UUID
    topic: str
    order_in_unit: int
    image_url: Optional[str] = None
    video_url: Optional[str] = None
    ppt_url: Optional[str] = None
    lab_url: Optional[str] = None
    unit_id: Optional[uuid.UUID] = None
    course_id: Optional[uuid.UUID] = None
    course_type: Optional[str] = None


class UnitOut(BaseModel):
    id: uuid.UUID
    unit_title: str
    order_in_course: int
    topics: List[TopicOut]


class SyllabusCourseOut(BaseModel):
    id: uuid.UUID
    batch_id: uuid.UUID
    semester: int
    course_code: str
    title: str
    type: Optional[str] = None
    units: List[UnitOut]


class SyllabusCourseSummaryOut(BaseModel):
    id: uuid.UUID
    batch_id: uuid.UUID
    semester: int
    course_code: str
    title: str
    type: Optional[str] = None


class SyllabusCourseSimpleBase(BaseModel):
    semester: int = Field(..., ge=1, le=12)
    course_code: str = Field(..., min_length=1, max_length=64)
    title: str = Field(..., min_length=1, max_length=256)
    type: Optional[str] = Field(default="practical")

    @validator("course_code", "title", pre=True)
    def _strip_text(cls, value: Any):  # noqa: N805
        if value is None:
            raise ValueError("Value is required")
        if isinstance(value, str):
            trimmed = value.strip()
            if not trimmed:
                raise ValueError("Value is required")
            return trimmed
        return str(value)

    @validator("course_code")
    def _uppercase_code(cls, value: str):  # noqa: N805
        return value.upper()

    @validator("type", pre=True, always=True)
    def _normalize_type(cls, value: Any):  # noqa: N805
        allowed = {"maths", "theorey", "practical"}
        if value is None:
            return "practical"
        if isinstance(value, str):
            v = value.strip().lower()
            return v if v in allowed else "practical"
        return "practical"


class SyllabusCourseSimpleCreateIn(SyllabusCourseSimpleBase):
    pass


class SyllabusCourseSimpleUpdateIn(SyllabusCourseSimpleBase):
    pass


class UnitTopicsIn(BaseModel):
    unit_title: str = Field(..., min_length=1, max_length=256)
    topics: List[TopicUpsertIn] = Field(default_factory=list)

    @validator("unit_title", pre=True)
    def _normalize_unit_title(cls, value: Any):  # noqa: N805
        if value is None:
            raise ValueError("Unit title cannot be empty")
        if isinstance(value, str):
            trimmed = value.strip()
            if not trimmed:
                raise ValueError("Unit title cannot be empty")
            return trimmed
        trimmed = str(value).strip()
        if not trimmed:
            raise ValueError("Unit title cannot be empty")
        return trimmed

    @validator("topics", pre=True)
    def _ensure_topics_list(cls, value: Any):  # noqa: N805
        if value is None:
            return []
        return value


class BatchResolveIn(BaseModel):
    college_id: uuid.UUID
    dept_name: str
    from_year: int = Field(..., ge=1950, le=2100)
    to_year: int = Field(..., ge=1950, le=2100)

    @validator("dept_name")
    def uppercase_dept_public(cls, v):
        return (v or "").upper()

    @validator("to_year")
    def check_years_public(cls, v, values):
        f = values.get("from_year")
        if f and v < f:
            raise ValueError("to_year must be >= from_year")
        return v


class ParseSyllabusIn(BaseModel):
    text: str = Field(..., min_length=10)
    course_code: Optional[str] = None
    title: Optional[str] = None


class ParsedSyllabusOut(BaseModel):
    course_code: str
    title: str
    units: List[UnitIn]

# --- Academics service ---


try:
    from supabase_auth.errors import AuthRetryableError, AuthApiError  # type: ignore
except Exception:  # pragma: no cover
    class AuthRetryableError(Exception):
        pass

    class AuthApiError(Exception):
        pass


def upsert_college(name: str) -> uuid.UUID:
    supabase = get_service_client()
    res = supabase.table("colleges").select("id").eq("name", name).limit(1).execute()
    if getattr(res, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (find college): {res.error}")
    if res.data:
        return uuid.UUID(res.data[0]["id"])

    ins = supabase.table("colleges").insert({"name": name}).execute()
    if getattr(ins, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (insert college): {ins.error}")

    res2 = supabase.table("colleges").select("id").eq("name", name).limit(1).execute()
    if getattr(res2, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (refetch college): {res2.error}")
    if not res2.data:
        raise HTTPException(status_code=500, detail="Failed to fetch inserted college.")
    return uuid.UUID(res2.data[0]["id"])


def sync_degree_hierarchy(college_id: uuid.UUID, degrees: List[DegreeCreateIn]) -> None:
    if not degrees:
        return
    supabase = get_service_client()

    degree_res = (
        supabase.table("degrees")
        .select("id,name,level,duration_years")
        .eq("college_id", str(college_id))
        .execute()
    )
    if getattr(degree_res, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (list degrees): {degree_res.error}")

    degree_map: Dict[str, Dict[str, Any]] = {}
    for row in degree_res.data or []:
        key = (row.get("name") or "").strip().lower()
        if key:
            degree_map[key] = row

    dept_res = (
        supabase.table("departments")
        .select("id,name,degree_id")
        .eq("college_id", str(college_id))
        .execute()
    )
    if getattr(dept_res, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (list departments): {dept_res.error}")
    dept_map: Dict[str, Dict[str, Any]] = {}
    for row in dept_res.data or []:
        name = (row.get("name") or "").upper()
        if name:
            dept_map[name] = row

    batch_res = (
        supabase.table("batches")
        .select("department_id,from_year,to_year")
        .eq("college_id", str(college_id))
        .execute()
    )
    if getattr(batch_res, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (list batches): {batch_res.error}")
    batches_by_department: Dict[str, set[Tuple[int, int]]] = {}
    for row in batch_res.data or []:
        dept_id = row.get("department_id")
        if not dept_id:
            continue
        from_year = row.get("from_year")
        to_year = row.get("to_year")
        if from_year is None or to_year is None:
            continue
        batches_by_department.setdefault(dept_id, set()).add((int(from_year), int(to_year)))

    for degree in degrees:
        degree_key = degree.name.strip().lower()
        degree_row = degree_map.get(degree_key)
        if degree_row:
            degree_id = uuid.UUID(degree_row["id"])
            updates: Dict[str, Any] = {}
            if degree_row.get("level") != degree.level:
                updates["level"] = degree.level
            if degree_row.get("duration_years") != degree.duration_years:
                updates["duration_years"] = degree.duration_years
            if updates:
                upd = supabase.table("degrees").update(updates).eq("id", str(degree_id)).execute()
                if getattr(upd, "error", None):
                    raise HTTPException(status_code=500, detail=f"Supabase error (update degree): {upd.error}")
                degree_row.update(updates)
        else:
            payload = {
                "college_id": str(college_id),
                "name": degree.name,
                "level": degree.level,
                "duration_years": degree.duration_years,
            }
            ins = supabase.table("degrees").insert(payload).execute()
            if getattr(ins, "error", None):
                raise HTTPException(status_code=500, detail=f"Supabase error (insert degree): {ins.error}")
            if ins.data:
                degree_row = ins.data[0]
            else:
                refetch = (
                    supabase.table("degrees")
                    .select("id,name,level,duration_years")
                    .eq("college_id", str(college_id))
                    .eq("name", degree.name)
                    .limit(1)
                    .execute()
                )
                if getattr(refetch, "error", None) or not refetch.data:
                    raise HTTPException(status_code=500, detail="Failed to insert degree.")
                degree_row = refetch.data[0]
            degree_id = uuid.UUID(degree_row["id"])
            degree_map[degree_key] = degree_row

        degree_id_str = str(degree_id)

        for department in degree.departments:
            dept_key = department.name  # already upper-case from validation
            dept_row = dept_map.get(dept_key)
            if dept_row:
                dept_id = uuid.UUID(dept_row["id"])
                if dept_row.get("degree_id") != degree_id_str:
                    upd = (
                        supabase.table("departments")
                        .update({"degree_id": degree_id_str})
                        .eq("id", str(dept_id))
                        .execute()
                    )
                    if getattr(upd, "error", None):
                        raise HTTPException(status_code=500, detail=f"Supabase error (update department): {upd.error}")
                    dept_row["degree_id"] = degree_id_str
            else:
                payload = {
                    "college_id": str(college_id),
                    "degree_id": degree_id_str,
                    "name": dept_key,
                }
                ins = supabase.table("departments").insert(payload).execute()
                if getattr(ins, "error", None):
                    raise HTTPException(status_code=500, detail=f"Supabase error (insert department): {ins.error}")
                if ins.data:
                    dept_row = ins.data[0]
                else:
                    refetch = (
                        supabase.table("departments")
                        .select("id,name,degree_id")
                        .eq("college_id", str(college_id))
                        .eq("name", dept_key)
                        .limit(1)
                        .execute()
                    )
                    if getattr(refetch, "error", None) or not refetch.data:
                        raise HTTPException(status_code=500, detail="Failed to insert department.")
                    dept_row = refetch.data[0]
                dept_map[dept_key] = dept_row
                dept_id = uuid.UUID(dept_row["id"])
                batches_by_department[str(dept_id)] = set()
            dept_id = uuid.UUID(dept_row["id"])
            dept_id_str = str(dept_id)
            existing_pairs = batches_by_department.setdefault(dept_id_str, set())
            to_insert = []
            for batch in department.batches:
                pair = (batch.from_year, batch.to_year)
                if pair in existing_pairs:
                    continue
                to_insert.append(
                    {
                        "college_id": str(college_id),
                        "department_id": dept_id_str,
                        "from_year": batch.from_year,
                        "to_year": batch.to_year,
                    }
                )
                existing_pairs.add(pair)
            if to_insert:
                ins_batches = supabase.table("batches").insert(to_insert).execute()
                if getattr(ins_batches, "error", None):
                    raise HTTPException(status_code=500, detail=f"Supabase error (insert batches): {ins_batches.error}")


def get_college_full(college_id: uuid.UUID):
    supabase = get_service_client()
    college = (
        supabase.table("colleges")
        .select("id,name,logo_url")
        .eq("id", str(college_id))
        .single()
        .execute()
    )
    if getattr(college, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (get college): {college.error}")
    if not college.data:
        raise HTTPException(status_code=404, detail="College not found.")

    degree_rows = (
        supabase.table("degrees")
        .select("id,name,level,duration_years")
        .eq("college_id", str(college_id))
        .order("name")
        .execute()
    )
    if getattr(degree_rows, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (get degrees): {degree_rows.error}")

    dept_rows = (
        supabase.table("departments")
        .select("id,name,degree_id")
        .eq("college_id", str(college_id))
        .order("name")
        .execute()
    )
    if getattr(dept_rows, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (get departments): {dept_rows.error}")

    dept_lookup: Dict[str, Dict[str, Any]] = {}
    dept_by_degree: Dict[str, List[Dict[str, Any]]] = {}
    for row in dept_rows.data or []:
        dept_id_str = row["id"]
        dept_info = {
            "id": uuid.UUID(dept_id_str),
            "name": row.get("name"),
            "batches": [],
        }
        dept_lookup[dept_id_str] = dept_info
        degree_id_str = row.get("degree_id")
        if degree_id_str:
            dept_by_degree.setdefault(degree_id_str, []).append(dept_info)

    for bucket in dept_by_degree.values():
        bucket.sort(key=lambda item: (item["name"] or "").upper())

    batch_rows = (
        supabase.table("batches")
        .select("department_id,from_year,to_year")
        .eq("college_id", str(college_id))
        .order("from_year")
        .execute()
    )
    if getattr(batch_rows, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (get batches): {batch_rows.error}")

    flat_seen: set[Tuple[int, int]] = set()
    dept_seen: Dict[str, set[Tuple[int, int]]] = {}
    flat_batches: List[BatchIn] = []
    for row in batch_rows.data or []:
        dept_id_str = row.get("department_id")
        from_year = row.get("from_year")
        to_year = row.get("to_year")
        if from_year is None or to_year is None:
            continue
        pair = (int(from_year), int(to_year))
        if pair not in flat_seen:
            flat_seen.add(pair)
            flat_batches.append(BatchIn(**{"from": pair[0], "to": pair[1]}))
        if not dept_id_str or dept_id_str not in dept_lookup:
            continue
        dept_pairs = dept_seen.setdefault(dept_id_str, set())
        if pair in dept_pairs:
            continue
        dept_pairs.add(pair)
        dept_lookup[dept_id_str]["batches"].append(BatchIn(**{"from": pair[0], "to": pair[1]}))

    for dept_info in dept_lookup.values():
        dept_info["batches"].sort(key=lambda b: (b.from_year, b.to_year))

    degree_data = sorted(degree_rows.data or [], key=lambda row: (row.get("name") or "").lower())
    degrees_out: List[DegreeOut] = []
    for row in degree_data:
        degree_id_str = row["id"]
        departments_out = [
            DepartmentWithBatchesOut(
                id=dept["id"],
                name=dept["name"],
                batches=dept["batches"],
            )
            for dept in dept_by_degree.get(degree_id_str, [])
        ]
        degrees_out.append(
            DegreeOut(
                id=uuid.UUID(degree_id_str),
                name=row.get("name"),
                level=row.get("level"),
                duration_years=row.get("duration_years"),
                departments=departments_out,
            )
        )

    department_names = sorted({(row.get("name") or "").upper() for row in (dept_rows.data or []) if row.get("name")})

    return {
        "id": uuid.UUID(college.data["id"]),
        "name": college.data["name"],
        "logo_url": college.data.get("logo_url"),
        "degrees": degrees_out,
        "departments": department_names,
        "batches": flat_batches,
    }


def _locate_department_from_snapshot(
    college_snapshot: Dict[str, Any],
    degree_id: uuid.UUID,
    department_id: uuid.UUID,
) -> DepartmentWithBatchesOut:
    degrees = college_snapshot.get("degrees") if isinstance(college_snapshot, dict) else []
    if degrees is None:
        degrees = []
    target_degree_id = uuid.UUID(str(degree_id))
    target_department_id = uuid.UUID(str(department_id))

    for degree in degrees:
        deg_id = getattr(degree, "id", None)
        if deg_id is None and isinstance(degree, dict):
            deg_id = degree.get("id")
        if deg_id is None:
            continue
        if str(deg_id) != str(target_degree_id):
            continue

        departments = getattr(degree, "departments", None)
        if departments is None and isinstance(degree, dict):
            departments = degree.get("departments")
        if not departments:
            break

        for dept in departments:
            dept_id = getattr(dept, "id", None)
            if dept_id is None and isinstance(dept, dict):
                dept_id = dept.get("id")
            if dept_id is None or str(dept_id) != str(target_department_id):
                continue

            name = getattr(dept, "name", None)
            if name is None and isinstance(dept, dict):
                name = dept.get("name")

            batches = getattr(dept, "batches", None)
            if batches is None and isinstance(dept, dict):
                batches = dept.get("batches")
            batch_models: List[BatchIn] = []
            for batch in batches or []:
                if isinstance(batch, BatchIn):
                    batch_models.append(batch)
                elif isinstance(batch, dict):
                    frm = batch.get("from") or batch.get("from_year")
                    to = batch.get("to") or batch.get("to_year")
                    if frm is not None and to is not None:
                        batch_models.append(BatchIn(**{"from": frm, "to": to}))
            return DepartmentWithBatchesOut(
                id=target_department_id,
                name=name,
                batches=batch_models,
            )

    raise HTTPException(status_code=404, detail="Department not found in college snapshot.")


def _extract_access_token(res) -> Optional[str]:
    try:
        session = getattr(res, "session", None) or (res.get("session") if isinstance(res, dict) else None)
        if session:
            token = getattr(session, "access_token", None) or (session.get("access_token") if isinstance(session, dict) else None)
            if token:
                return token
    except Exception:
        pass
    return None


def _get_user_id_from_auth_response(res) -> Optional[str]:
    try:
        user = getattr(res, "user", None) or (res.get("user") if isinstance(res, dict) else None)
        if user:
            uid = getattr(user, "id", None) or (user.get("id") if isinstance(user, dict) else None)
            return uid
    except Exception:
        pass
    return None


def _resolve_college_id_by_name(college_name: str) -> uuid.UUID:
    supabase = get_service_client()
    q = (
        supabase.table("colleges")
        .select("id")
        .eq("name", college_name)
        .limit(1)
        .execute()
    )
    if getattr(q, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (find college): {q.error}")
    if not q.data:
        return upsert_college(college_name)
    return uuid.UUID(q.data[0]["id"])


def _resolve_department_id(college_id: uuid.UUID, dept_name: str) -> uuid.UUID:
    supabase = get_service_client()
    dep_q = (
        supabase.table("departments")
        .select("id")
        .eq("college_id", str(college_id))
        .eq("name", dept_name.upper())
        .limit(1)
        .execute()
    )
    if getattr(dep_q, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (find department): {dep_q.error}")
    if not dep_q.data:
        raise HTTPException(status_code=404, detail="Department not found for college")
    return uuid.UUID(dep_q.data[0]["id"])


def _get_or_create_batch_id(
    college_id: uuid.UUID, department_id: uuid.UUID, from_year: int, to_year: int
) -> uuid.UUID:
    supabase = get_service_client()
    sel = (
        supabase.table("batches")
        .select("id")
        .eq("college_id", str(college_id))
        .eq("department_id", str(department_id))
        .eq("from_year", from_year)
        .eq("to_year", to_year)
        .limit(1)
        .execute()
    )
    if getattr(sel, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (find batch): {sel.error}")
    if sel.data:
        return uuid.UUID(sel.data[0]["id"])

    ins = (
        supabase.table("batches")
        .insert(
            {
                "college_id": str(college_id),
                "department_id": str(department_id),
                "from_year": from_year,
                "to_year": to_year,
            }
        )
        .execute()
    )
    if getattr(ins, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (insert batch): {ins.error}")

    ref = (
        supabase.table("batches")
        .select("id")
        .eq("college_id", str(college_id))
        .eq("department_id", str(department_id))
        .eq("from_year", from_year)
        .eq("to_year", to_year)
        .limit(1)
        .execute()
    )
    if getattr(ref, "error", None) or not ref.data:
        raise HTTPException(status_code=500, detail=f"Supabase error (refetch batch): {getattr(ref, 'error', None)}")
    return uuid.UUID(ref.data[0]["id"])


def _ensure_department_batches(
    college_id: uuid.UUID, department_id: uuid.UUID, batches: Optional[List[BatchIn]]
) -> None:
    if not batches:
        return

    supabase = get_service_client()
    existing = (
        supabase.table("batches")
        .select("from_year,to_year")
        .eq("college_id", str(college_id))
        .eq("department_id", str(department_id))
        .execute()
    )
    if getattr(existing, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (list department batches): {existing.error}")

    existing_pairs: set[Tuple[int, int]] = set()
    for row in getattr(existing, "data", []) or []:
        from_year = row.get("from_year")
        to_year = row.get("to_year")
        if from_year is None or to_year is None:
            continue
        existing_pairs.add((int(from_year), int(to_year)))

    to_insert: List[Dict[str, Any]] = []
    for batch in batches:
        pair = (batch.from_year, batch.to_year)
        if pair in existing_pairs:
            continue
        to_insert.append(
            {
                "college_id": str(college_id),
                "department_id": str(department_id),
                "from_year": batch.from_year,
                "to_year": batch.to_year,
            }
        )
        existing_pairs.add(pair)

    if to_insert:
        ins = supabase.table("batches").insert(to_insert).execute()
        if getattr(ins, "error", None):
            raise HTTPException(status_code=500, detail=f"Supabase error (insert department batches): {ins.error}")


def _clean_lines(text: str) -> List[str]:
    lines = [ln.strip() for ln in (text or "").splitlines()]
    return [ln for ln in lines if ln]


def _naive_extract_improved(text: str) -> dict:
    """Heuristic syllabus parser that:
    - Detects unit headers like "UNIT I â€“ Title:" and extracts unit title cleanly
    - Splits topics by commas and spaced dashes (" - ", " â€“ ", " â€” ")
    - Skips non-topic sections (TOTAL PERIODS, Text Books, Reference Books, Content Beyond Syllabus)
    - Attempts to guess course_code and title from a header line (e.g., "AI PE703 DEEP REINFORCEMENT LEARNING 3 - -")
    """
    lines = _clean_lines(text)

    # Merge hyphenated line breaks like "ar-" + "ray" => "array"
    def _dehyphenate(ls: List[str]) -> List[str]:
        out: List[str] = []
        i = 0
        while i < len(ls):
            cur = ls[i]
            if cur.endswith('-') and i + 1 < len(ls):
                nxt = ls[i + 1]
                # If next line starts with letters, merge tokens
                if re.match(r"^[A-Za-z]", nxt or ""):
                    merged = cur[:-1] + nxt.lstrip()
                    out.append(merged)
                    i += 2
                    continue
            out.append(cur)
            i += 1
        return out

    lines = _dehyphenate(lines)
    units: List[dict] = []
    current_unit: Optional[dict] = None
    seen_first_unit = False
    end_reached = False

    # Match UNIT headers even when there is no space before the numeral, e.g., "UNITV"
    unit_pat = re.compile(r"^(?:unit|module|chapter)(?=\s*[ivxlcdm\d])\s*([ivxlcdm]+|\d+)?", re.IGNORECASE)

    skip_topic_pat = re.compile(
        r"^(TOTAL\s+PERIODS|TEXT\s*BOOKS|REFERENCE\s*BOOKS|CONTENT\s+BEYOND\s+SYLLABUS|SUBJECT\s+CODE|SUBJECT\s+NAME|LECTURES|TUTORIALS|PRACTICALS?)\b",
        re.IGNORECASE,
    )
    skip_intro_head_pat = re.compile(
        r"^(COURSE\s+PRE[- ]?REQUISITE|COURSE\s+OBJECTIVES|COURSE\s+OUTCOMES)\b",
        re.IGNORECASE,
    )

    course_code: Optional[str] = None
    course_title: Optional[str] = None
    code_line_pat = re.compile(r"\b([A-Z]{2,4}\s*[A-Z]{0,3}\d{2,4}[A-Z]?)\b[\s,:-]+(.+)$")

    def split_topics(text_line: str) -> List[str]:
        cleaned = re.sub(r"^([\-*â€¢Â·ï¿½?ï¿½]+|\d+[.)])\s*", "", text_line).strip()
        if not cleaned or skip_topic_pat.search(cleaned):
            return []
        parts = re.split(r"\s*,\s*|\s+[–—-]\s+|-(?=[A-Z(])", cleaned)
        out: List[str] = []
        for p in parts:
            t = p.strip().strip(".;, ")
            if t and not skip_topic_pat.search(t):
                out.append(t)
        return out

    for ln in lines:
        if end_reached:
            break
        if course_code is None:
            mcode = code_line_pat.search(ln)
            if mcode:
                course_code = mcode.group(1).strip()
                tail = mcode.group(2)
                tail = re.sub(r"\b\d+\s*[â€“â€”-]\s*[â€“â€”-].*$", "", tail).strip()
                if tail:
                    course_title = tail.strip("-â€“â€”:; .") or None

        # Stop parsing after end-of-syllabus markers appear
        if re.match(r"^(TOTAL\s+PERIODS|TEXT\s*BOOKS|REFERENCE\s*BOOKS|CONTENT\s+BEYOND\s+SYLLABUS)\b", ln, flags=re.IGNORECASE):
            end_reached = True
            break

        # Before first unit header, ignore all lines from intro sections
        if not seen_first_unit:
            if skip_intro_head_pat.match(ln):
                # skip heading
                continue
            # Also skip bullet lines in intro (most have bullets like â€¢ or start with uppercase sentences)
            if unit_pat.match(ln):
                # fall through to create the first unit
                pass
            else:
                # Ignore until first real unit header
                continue

        um = unit_pat.match(ln)
        if um:
            # remove the leading keyword + numeral (with or without space), plus any immediate separators
            after = re.sub(r"^(?:unit|module|chapter)\s*([ivxlcdm]+|\d+)?\s*[:â€“â€”-]?\s*", "", ln, flags=re.IGNORECASE)
            parts = re.split(r"[:â€“â€”-]", after, maxsplit=1)
            title_part = (parts[1] if len(parts) > 1 else parts[0]).strip()
            title_main, title_rest = (title_part.split(":", 1) + [""])[:2]
            title_main = title_main.strip().strip("-â€“â€”:; ") or "Unit"

            current_unit = {"unit_title": title_main, "topics": []}
            units.append(current_unit)
            seen_first_unit = True
            if title_rest:
                for tp in split_topics(title_rest):
                    current_unit["topics"].append({"topic": tp})
            continue

        # Uppercase heading with colon indicates a new unit (e.g., BASIC PROBABILITY: ...)
        mhead = re.match(r"^([A-Z][A-Z0-9 ,\-/&().+]+?):\s*(.*)$", ln)
        if mhead and not skip_topic_pat.search(ln):
            title_main = (mhead.group(1) or "").strip().strip("-:; .")
            title_rest = (mhead.group(2) or "").strip()
            if title_main and len(title_main) >= 4:
                current_unit = {"unit_title": title_main, "topics": []}
                units.append(current_unit)
                seen_first_unit = True
                if title_rest:
                    for tp in split_topics(title_rest):
                        current_unit["topics"].append({"topic": tp})
                continue
        # Uppercase heading with no colon also indicates a unit (e.g., "STACK,QUEUE AND LINKED LISTS")
        mhead_nc = re.match(r"^([A-Z][A-Z0-9 ,\-/&().+]+)$", ln)
        if mhead_nc and not skip_topic_pat.search(ln):
            title_main = (mhead_nc.group(1) or "").strip().strip("-:; .")
            if title_main and len(title_main) >= 4:
                current_unit = {"unit_title": title_main, "topics": []}
                units.append(current_unit)
                seen_first_unit = True
                continue
        if not current_unit:
            # Should not happen now as we skip lines until the first unit header
            continue

        # Skip hour-only lines or stray numbers
        if re.match(r"^\(?\s*\d+\s*Hrs?\.?\s*\)?$", ln, flags=re.IGNORECASE) or ln.strip().isdigit():
            continue
        for tp in split_topics(ln):
            current_unit["topics"].append({"topic": tp})

    return {"course_code": course_code, "title": course_title, "units": units}


def _split_subject_sections(text: str) -> List[Dict[str, str]]:
    """Split a full-semester syllabus text into subject sections.

    Heuristic: a subject starts at a line that looks like a course code followed by a title, e.g.,
      "AI PE703 DEEP REINFORCEMENT LEARNING 3 - -"

    Returns list of dicts with keys: code, title, text.
    """
    lines = [ln.rstrip() for ln in (text or "").splitlines()]
    sections: List[Dict[str, str]] = []
    # Accept lines where code is followed by optional title on same line
    code_line_pat = re.compile(r"^\s*([A-Z]{2,4}\s*[A-Z]{0,3}\d{2,4}[A-Z]?)\b(?:[\s,:-]+(.+))?$")
    anchors: List[Tuple[int, str, Optional[str]]] = []
    for idx, raw in enumerate(lines):
        ln = raw.strip()
        m = code_line_pat.search(ln)
        if not m:
            continue
        code = m.group(1).strip()
        tail = (m.group(2) or "").strip()
        if tail:
            tail = re.sub(r"\b\d+\s*[â€“â€”-]\s*[â€“â€”-].*$", "", tail).strip()
        title = tail.strip("-â€“â€”:; .") if tail else None
        if code:
            anchors.append((idx, code, title))
    if not anchors:
        return []
    # Helper: find a reasonable title if missing on the code line
    def infer_title(start_idx: int) -> Optional[str]:
        # Look ahead a few lines for the subject name (skip meta headers)
        skip_pat = re.compile(r"^(subject\s+code|subject\s+name|lectures|tutorials|practical|course\s+pre|course\s+objectives|course\s+outcomes)\b", re.IGNORECASE)
        collected: List[str] = []
        for j in range(start_idx + 1, min(start_idx + 8, len(lines))):
            cand = lines[j].strip().strip("-â€“â€”:; .")
            if not cand or skip_pat.search(cand):
                continue
            # stop if we hit a unit header
            if re.match(r"^(?:unit|module|chapter)(?=\s*[ivxlcdm\d])", cand, flags=re.IGNORECASE):
                break
            collected.append(cand)
            # if the next piece starts with uppercase and the first chunk looks incomplete (endswith AND), join one more line
            if len(collected) == 1 and j + 1 < len(lines):
                nxt = lines[j + 1].strip().strip("-â€“â€”:; .")
                if nxt and re.match(r"^[A-Z]", nxt) and collected[0].upper().endswith(" AND"):
                    collected.append(nxt)
            break
        if collected:
            return " ".join(collected).strip()
        return None

    for i, (start, code, title) in enumerate(anchors):
        end = anchors[i + 1][0] if i + 1 < len(anchors) else len(lines)
        seg = "\n".join(lines[start:end]).strip()
        if not seg:
            continue
        ttl = title or infer_title(start)
        sections.append({"code": code, "title": (ttl or "").strip(), "text": seg})
    return sections

def _naive_extract(text: str) -> dict:
    lines = _clean_lines(text)
    units: List[dict] = []
    current_unit: Optional[dict] = None

    # Match UNIT headers even when there is no space before the numeral, e.g., "UNITV"
    unit_pat = re.compile(r"^(?:unit|module|chapter)(?=\s*[ivxlcdm\d])\s*([ivxlcdm]+|\d+)?", re.IGNORECASE)

    for ln in lines:
        if unit_pat.match(ln):
            title = ln
            current_unit = {"unit_title": title, "topics": []}
            units.append(current_unit)
            continue
        if not current_unit:
            current_unit = {"unit_title": "Unit 1", "topics": []}
            units.append(current_unit)
        topic = re.sub(r"^([\-*â€¢]+|\d+[.)])\s*", "", ln).strip()
        if topic:
            current_unit["topics"].append({"topic": topic})

    return {"course_code": None, "title": None, "units": units}


def _normalize_parsed_struct(parsed: dict, hints: dict) -> ParsedSyllabusOut:
    text_cc = hints.get("course_code") if hints else None
    text_title = hints.get("title") if hints else None

    units_in: List[UnitIn] = []
    for u in (parsed.get("units") or []):
        title = u.get("unit_title") or u.get("title") or "Unit"
        topics_src = u.get("topics") or []
        topics_in = []
        for t in topics_src:
            if isinstance(t, dict):
                tp = t.get("topic") or t.get("title") or t.get("text")
            else:
                tp = str(t)
            if tp:
                topics_in.append(TopicIn(topic=tp))
        units_in.append(UnitIn(unit_title=title, topics=topics_in))

    cc = text_cc or (parsed.get("course_code") or "") or "UNKNOWN"
    ttl = text_title or (parsed.get("title") or "") or "Untitled Course"
    return ParsedSyllabusOut(course_code=cc, title=ttl, units=units_in)


GEMINI_PARSE_MODEL = os.getenv("GEMINI_PARSE_MODEL", "gemini-2.5-flash").strip()


SYLLABUS_AI_PARSE_PROMPT = """Analyze this university syllabus/curriculum document and extract the structure as JSON.

This is a typical university syllabus with the following structure:
- Subject Code (e.g., 25UMAT21, CS101, AI PE703)
- Subject Title (e.g., DIFFERENTIAL EQUATIONS & TRANSFORMS)
- Course Prerequisites, Objectives, Outcomes (CO1-CO5) - skip these sections
- SYLLABUS section with UNIT I through UNIT V (or more)
- Each unit has: Unit number, Title, Topics, and Hours (e.g., 12)
- Text Books, Reference Books sections at the end - skip these

Extract all subjects/courses with their units and topics. Return ONLY valid JSON in this exact format:
{
  "subjects": [
    {
      "name": "Subject Title Here",
      "code": "SUBJECT_CODE or null if not found",
      "units": [
        {
          "name": "Unit Title (e.g., ORDINARY DIFFERENTIAL EQUATIONS)",
          "unit_number": 1,
          "topics": [
            {"name": "Topic 1 - should be a specific concept", "order_index": 0},
            {"name": "Topic 2", "order_index": 1}
          ]
        }
      ]
    }
  ]
}

RULES:
1. Extract ALL subjects mentioned in the document
2. For each subject, extract ALL units/modules (usually UNIT I, II, III, IV, V)
3. For each unit, extract ALL topics - split by commas, dashes, or line breaks
4. SKIP sections: Course Prerequisites, Course Objectives, Course Outcomes (CO1-CO5), Text Books, Reference Books, Total Periods
5. If unit numbers are not explicit, infer them from order (1, 2, 3...)
6. If subject codes are not present, set code to null
7. Clean up topic names - remove leading bullets, numbers, or special characters
8. Return ONLY the JSON, no markdown formatting or explanation

Document text:
"""


def _gemini_parse(text: str, hints: dict) -> Optional[dict]:
    if not GEMINI_API_KEY:
        return None
    try:
        import google.generativeai as genai
    except Exception:
        return None

    try:
        genai.configure(api_key=GEMINI_API_KEY)
        model = genai.GenerativeModel(GEMINI_PARSE_MODEL or "gemini-2.5-flash")
        
        # Limit the text length to keep latency low
        max_chars = int(os.getenv("GEMINI_PARSE_MAX_CHARS", "100000"))
        safe_text = (text or "")[:max_chars]
        
        # Use the improved syllabus parsing prompt
        full_prompt = SYLLABUS_AI_PARSE_PROMPT + safe_text
        
        resp = model.generate_content(full_prompt)
        raw = getattr(resp, "text", None)
        if not raw:
            try:
                raw = resp.candidates[0].content.parts[0].text
            except Exception:
                raw = None
        if not raw:
            return None

        # Extract JSON from response (might be wrapped in markdown code blocks)
        m = re.search(r"```(?:json)?\s*([\s\S]*?)```", raw)
        jtxt = m.group(1) if m else raw
        
        parsed = json.loads(jtxt)
        
        # Convert new format to expected format if it has 'subjects' key
        if "subjects" in parsed and parsed["subjects"]:
            # Take the first subject and convert to old format
            first_subject = parsed["subjects"][0]
            units = []
            for unit in first_subject.get("units", []):
                topics = []
                for topic in unit.get("topics", []):
                    topic_name = topic.get("name") if isinstance(topic, dict) else str(topic)
                    if topic_name:
                        topics.append({"topic": topic_name})
                units.append({
                    "unit_title": unit.get("name") or f"Unit {unit.get('unit_number', 1)}",
                    "topics": topics
                })
            return {
                "course_code": first_subject.get("code"),
                "title": first_subject.get("name"),
                "units": units
            }
        
        return parsed
    except Exception as e:
        print(f"[GEMINI_PARSE] Error: {e}")
        return None


async def _gemini_parse_with_timeout(text: str, hints: dict, timeout_s: float = 18.0) -> Optional[dict]:
    try:
        return await asyncio.wait_for(run_in_threadpool(_gemini_parse, text, hints), timeout=timeout_s)
    except Exception:
        return None


LAB_KEYWORD_PATTERN = re.compile(r"\b(lab|laboratory|practical|sessional|experiment|experiments)\b", re.IGNORECASE)


def _contains_lab(text: Optional[str]) -> bool:
    if not text:
        return False
    return bool(LAB_KEYWORD_PATTERN.search(text))


def _filter_lab_units(units: List[UnitIn]) -> List[UnitIn]:
    filtered: List[UnitIn] = []
    for unit in units or []:
        title = unit.unit_title or ""
        if _contains_lab(title):
            continue
        clean_topics: List[TopicIn] = []
        for topic in unit.topics or []:
            topic_text = getattr(topic, "topic", None)
            if not topic_text or _contains_lab(topic_text):
                continue
            clean_topics.append(TopicIn(topic=topic_text))
        if not clean_topics:
            continue
        filtered.append(UnitIn(unit_title=title, topics=clean_topics))
    return filtered


def parse_syllabus(payload: ParseSyllabusIn) -> ParsedSyllabusOut:
    text = (payload.text or "").strip()
    if len(text) < 10:
        raise HTTPException(status_code=400, detail="Text too short")
    hints = {"course_code": payload.course_code, "title": payload.title}

    # Always use the improved heuristic parser for stability
    parsed = _naive_extract_improved(text)

    norm = _normalize_parsed_struct(parsed, hints)
    if not norm.units:
        raise HTTPException(status_code=422, detail="Could not extract any units or topics")
    return norm


def _pdf_bytes_to_text(data: bytes) -> str:
    text = ""
    # Prefer PyMuPDF (fitz) for accuracy
    if data and fitz is not None:  # type: ignore[attr-defined]
        try:
            with fitz.open(stream=data, filetype="pdf") as doc:  # type: ignore[attr-defined]
                chunks: List[str] = []
                for page in doc:
                    try:
                        chunks.append(page.get_text("text"))
                    except Exception:
                        continue
                text = "\n".join(chunks).strip()
        except Exception:
            text = ""
    if not text and textract is not None:
        try:
            out = textract.process(io.BytesIO(data), extension='pdf')  # type: ignore
            try:
                text = out.decode('utf-8', errors='ignore').strip()
            except Exception:
                text = (out or b"").decode('latin-1', errors='ignore').strip()
        except Exception:
            text = ""
    if not text and PyPDF2 is not None:
        try:
            reader = PyPDF2.PdfReader(io.BytesIO(data))  # type: ignore[attr-defined]
            chunks: List[str] = []
            for page in reader.pages:
                try:
                    page_text = page.extract_text()  # type: ignore[attr-defined]
                except Exception:
                    page_text = None
                if page_text:
                    chunks.append(page_text)
            text = "\n".join(chunks).strip()
        except Exception:
            text = ""
    if not text:
        # Very last fallback: naive bytes decode
        try:
            text = data.decode('utf-8', errors='ignore')
        except Exception:
            text = ""
    return text


# NOTE: The /api/syllabus/upload route is registered later, after academics_router is created.


def signup_user(user):
    anon_client = get_anon_client()
    if not anon_client:
        raise HTTPException(status_code=500, detail="Server missing SUPABASE_ANON_KEY")
    try:
        res = anon_client.auth.sign_up({"email": user.email, "password": user.password})
        # Extract session data for persistent login
        session = getattr(res, "session", None)
        access_token = getattr(session, "access_token", None) if session else None
        refresh_token = getattr(session, "refresh_token", None) if session else None
        expires_in = getattr(session, "expires_in", 3600) if session else 3600
        return {
            "message": "Signup initiated",
            "access_token": access_token,
            "refresh_token": refresh_token,
            "expires_in": expires_in,
        }
    except Exception as e:
        supabase_logger.exception("Signup error")
        raise HTTPException(status_code=400, detail=str(e))


def login_user(user):
    anon_client = get_anon_client()
    if not anon_client:
        raise HTTPException(status_code=500, detail="Server missing SUPABASE_ANON_KEY")
    try:
        res = anon_client.auth.sign_in_with_password({"email": user.email, "password": user.password})
        # Extract session data for persistent login
        session = getattr(res, "session", None)
        access_token = getattr(session, "access_token", None) if session else None
        refresh_token = getattr(session, "refresh_token", None) if session else None
        expires_in = getattr(session, "expires_in", 3600) if session else 3600
        if not access_token:
            raise HTTPException(status_code=401, detail="Login failed: no session returned")
        return {
            "message": "Login successful",
            "access_token": access_token,
            "refresh_token": refresh_token,
            "expires_in": expires_in,
        }
    except HTTPException:
        raise
    except Exception:
        supabase_logger.exception("Login failed")
        raise HTTPException(status_code=401, detail="Invalid credentials or login failed")


def signup_full_user(payload):
    anon_client = get_anon_client()
    if not anon_client:
        raise HTTPException(status_code=500, detail="Server missing SUPABASE_ANON_KEY")
    try:
        auth_res = anon_client.auth.sign_up({"email": payload.email, "password": payload.password})
        user_id = _get_user_id_from_auth_response(auth_res)
        if not user_id:
            raise HTTPException(status_code=400, detail="Failed to create auth user")
        access_token = _extract_access_token(auth_res)

        college_id = _resolve_college_id_by_name(payload.college)
        department_id = _resolve_department_id(college_id, payload.department)
        batch_id = _get_or_create_batch_id(college_id, department_id, payload.batch_from, payload.batch_to)

        supabase = get_service_client()
        ins = (
            supabase.table("user_profiles")
            .insert(
                {
                    "auth_user_id": user_id,
                    "name": payload.name,
                    "gender": payload.gender,
                    "phone": payload.phone,
                    "email": payload.email,
                    "college_id": str(college_id),
                    "department_id": str(department_id),
                    "batch_id": str(batch_id),
                    "batch_from": payload.batch_from,
                    "batch_to": payload.batch_to,
                    "semester": payload.semester,
                    "regno": payload.regno,
                }
            )
            .execute()
        )
        if getattr(ins, "error", None):
            raise HTTPException(status_code=500, detail=f"Supabase error (insert profile): {ins.error}")

        prof_q = (
            supabase.table("user_profiles")
            .select("id")
            .eq("auth_user_id", user_id)
            .limit(1)
            .execute()
        )
        profile_id = prof_q.data[0]["id"] if prof_q.data else None
        return {
            "message": "Signup complete",
            "access_token": access_token,
            "user_id": user_id,
            "profile_id": profile_id,
        }
    except HTTPException:
        raise
    except Exception as e:
        supabase_logger.exception("Signup full error")
        raise HTTPException(status_code=500, detail=f"Unexpected error: {e}")


def get_current_user_profile(token: Optional[str]):
    anon_client = get_anon_client()
    if not anon_client:
        raise HTTPException(status_code=500, detail="Server missing SUPABASE_ANON_KEY")
    if not token:
        raise HTTPException(status_code=401, detail="Missing or invalid Authorization header")

    try:
        user_id = _get_user_id_with_retry(token)
        supabase = get_service_client()

        # 1. Fetch Profile WITH embedded College, Dept, Batch data in ONE request
        # Requires FKs: user_profiles.college_id -> colleges.id, etc.
        try:
            prof_q = _supabase_retry(
                lambda: (
                    supabase.table("user_profiles")
                    .select("*, colleges(id,name), departments(id,name), batches(id,from_year,to_year)")
                    .eq("auth_user_id", user_id)
                    .limit(1)
                    .execute()
                )
            )
        except HTTPXRemoteProtocolError as exc:
            supabase_logger.warning("/api/me profile fetch transient protocol error: %s", exc)
            raise HTTPException(status_code=503, detail="Upstream temporarily unavailable. Please retry.")
        
        if getattr(prof_q, "error", None):
            raise HTTPException(status_code=500, detail=f"Supabase error (get profile): {prof_q.error}")
        if not prof_q.data:
            raise HTTPException(status_code=404, detail="Profile not found")
        
        prof = prof_q.data[0]
        profile_id = prof.get("id")

        # Extract joined data or fallback to None
        college_data = prof.get("colleges")  # dict or list depending on cardinality (should be dict if FK is 1:1/M:1) -> actually Supabase returns dict for M:1
        department_data = prof.get("departments")
        batch_data = prof.get("batches")

        # Normalize potentially scalar or list return from join
        college = college_data if isinstance(college_data, dict) else (college_data[0] if isinstance(college_data, list) and college_data else None)
        department = department_data if isinstance(department_data, dict) else (department_data[0] if isinstance(department_data, list) and department_data else None)
        batch = None
        if batch_data:
            raw_batch = batch_data if isinstance(batch_data, dict) else (batch_data[0] if isinstance(batch_data, list) else None)
            if raw_batch:
                batch = {"id": raw_batch.get("id"), "from": raw_batch.get("from_year"), "to": raw_batch.get("to_year")}

        # 2. Parallel Fetch of Related Data (Experience, Education, etc.)
        # We use a ThreadPool to run these read-only fetches concurrently
        related_results = {}
        fetch_specs = [
            ("experiences", "user_experiences", [("order_index", False), ("start_date", True), ("created_at", False)]),
            ("education_entries", "user_education", [("order_index", False), ("created_at", False)]),
            ("certification_entries", "user_certifications", [("order_index", False), ("issue_date", True), ("created_at", False)]),
            ("portfolio_projects", "user_portfolio_projects", [("order_index", False), ("start_date", True), ("created_at", False)]),
            ("publication_entries", "user_publications", [("order_index", False), ("publication_date", True), ("created_at", False)]),
        ]

        if profile_id:
            import concurrent.futures
            with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:
                future_map = {
                    executor.submit(_fetch_profile_related, profile_id, table, sort_keys): key
                    for key, table, sort_keys in fetch_specs
                }
                for future in concurrent.futures.as_completed(future_map):
                    key = future_map[future]
                    try:
                        related_results[key] = future.result()
                    except Exception as e:
                        supabase_logger.error(f"Failed to fetch related {key}: {e}")
                        related_results[key] = []
        else:
             for key, _, _ in fetch_specs:
                 related_results[key] = []

        # Unpack related data
        related_education = related_results.get("education_entries", [])
        
        # Education Overrides (Semester, batch derived from text, etc.)
        final_semester = prof.get("semester")
        final_regno = prof.get("regno")
        derived_batch_years = None
        
        if related_education:
            # Sort by current_semester then order_index
            sem_sorted = [e for e in related_education if isinstance(e, dict)]
            if sem_sorted:
                sem_sorted.sort(key=lambda r: (r.get("current_semester") or 0, r.get("order_index") or 0), reverse=True)
                primary_edu = sem_sorted[0]
                if primary_edu.get("current_semester"):
                    final_semester = primary_edu.get("current_semester")
                if primary_edu.get("regno"):
                    final_regno = primary_edu.get("regno")
                
                # Text-based resolution fallbacks (only if direct IDs missing)
                # We must attempt to resolve IDs to support batch lookup
                if not college:
                    edu_school = primary_edu.get("school")
                    if edu_school:
                        try:
                            # Attempt resolution
                            cq2 = supabase.table("colleges").select("id,name").eq("name", edu_school).limit(1).execute()
                            if not getattr(cq2, "error", None) and cq2.data:
                                college = {"id": cq2.data[0]["id"], "name": cq2.data[0]["name"]}
                            else:
                                college = {"id": None, "name": edu_school}
                        except Exception:
                            college = {"id": None, "name": edu_school}
                
                if not department:
                    edu_dept = primary_edu.get("department")
                    if edu_dept:
                        try:
                            # Attempt resolution - requires college_id if we want to be strict, or just name? 
                            # DB schema: departments usually linked to college.
                            # Original logic matched name AND college_id if available. 
                            dept_q = supabase.table("departments").select("id,name").eq("name", (edu_dept or "").upper())
                            if college and college.get("id"):
                                dept_q = dept_q.eq("college_id", college.get("id"))
                            dq2 = dept_q.limit(1).execute()
                            
                            if not getattr(dq2, "error", None) and dq2.data:
                                department = {"id": dq2.data[0]["id"], "name": dq2.data[0]["name"]}
                            else:
                                department = {"id": None, "name": (edu_dept or "").upper()}
                        except Exception:
                            department = {"id": None, "name": (edu_dept or "").upper()}

                if not batch:
                    edu_batch_range = primary_edu.get("batch_range")
                    if edu_batch_range and isinstance(edu_batch_range, str):
                        import re as _re
                        years_full = _re.findall(r"\b(\d{4})\b", edu_batch_range)
                        if len(years_full) >= 2:
                            try:
                                from_year = int(years_full[0])
                                to_year = int(years_full[1])
                                batch = {"id": None, "from": from_year, "to": to_year}
                                derived_batch_years = (from_year, to_year)
                            except Exception:
                                pass

        # 3. Optimized Syllabus Fetch (N+1 -> 1 query)
        # Fetch Courses -> embedded Units -> embedded Topics
        syllabus = []
        effective_batch_id = prof.get("batch_id") or (batch.get("id") if batch else None)
        
        # Try to resolve batch ID from derived years if missing
        if not effective_batch_id and derived_batch_years and college and college.get("id") and department and department.get("id"):
             # This is a rare edge case, keeping the single lookup is fine, or arguably skip to save time.
             # We'll keep it but optimize slightly.
             try:
                 fy, ty = derived_batch_years
                 bq2 = supabase.table("batches").select("id").eq("college_id", college["id"]).eq("department_id", department["id"]).eq("from_year", fy).eq("to_year", ty).limit(1).execute()
                 if bq2.data:
                     effective_batch_id = bq2.data[0]["id"]
             except Exception:
                 pass

        if effective_batch_id and final_semester:
            try:
                # Deep query: courses -> units -> topics
                syllabus_query = (
                    supabase.table("syllabus_courses")
                    .select("id,course_code,title,semester,type, syllabus_units(id,unit_title,order_in_course, syllabus_topics(id,topic,order_in_unit,image_url,lab_url))")
                    .eq("batch_id", effective_batch_id)
                    .eq("semester", final_semester)
                    .execute()
                )
                
                if not getattr(syllabus_query, "error", None) and syllabus_query.data:
                    # Sort in memory
                    courses_data = syllabus_query.data
                    courses_data.sort(key=lambda c: c.get("course_code") or "")
                    
                    for cr in courses_data:
                        raw_units = cr.get("syllabus_units") or []
                        # Sort units
                        raw_units.sort(key=lambda u: u.get("order_in_course") or 0)
                        
                        clean_units = []
                        for u in raw_units:
                            raw_topics = u.get("syllabus_topics") or []
                            # Sort topics
                            raw_topics.sort(key=lambda t: t.get("order_in_unit") or 0)
                            
                            clean_topics = [
                                {
                                    "id": t.get("id"),
                                    "topic": t.get("topic"),
                                    "order_in_unit": t.get("order_in_unit"),
                                    "image_url": t.get("image_url"),
                                    "lab_url": t.get("lab_url"),
                                }
                                for t in raw_topics
                            ]
                            clean_units.append({
                                "id": u.get("id"),
                                "unit_title": u.get("unit_title"),
                                "order_in_course": u.get("order_in_course"),
                                "topics": clean_topics
                            })
                            
                        syllabus.append({
                            "id": cr.get("id"),
                            "course_code": cr.get("course_code"),
                            "title": cr.get("title"),
                            "semester": cr.get("semester"),
                            "type": cr.get("type"),
                            "units": clean_units
                        })
            except Exception as e:
                supabase_logger.warning(f"Syllabus fetch error: {e}")

        return {
            "profile": {
                "id": prof["id"],
                "auth_user_id": prof["auth_user_id"],
                "email": prof.get("email"),
                "name": prof.get("name"),
                "gender": prof.get("gender"),
                "phone": prof.get("phone"),
                "semester": final_semester,
                "regno": final_regno,
                "profile_image_url": prof.get("profile_image_url"),
                "resume_url": prof.get("resume_url"),
                "headline": prof.get("headline"),
                "location": prof.get("location"),
                "dob": prof.get("dob"),
                "linkedin": prof.get("linkedin"),
                "github": prof.get("github"),
                "leetcode": prof.get("leetcode"),
                "portfolio_url": prof.get("portfolio_url"),
                "website": prof.get("website"),
                "twitter": prof.get("twitter"),
                "instagram": prof.get("instagram"),
                "medium": prof.get("medium"),
                "verification_score": prof.get("verification_score"),
                "bio": prof.get("bio"),
                "technologies": prof.get("technologies"),
                "skills": prof.get("skills"),
                "certifications": prof.get("certifications"),
                "languages": prof.get("languages"),
                "interests": prof.get("interests"),
                "project_info": prof.get("project_info"),
                "publications": prof.get("publications"),
                "achievements": prof.get("achievements"),
                "experience": prof.get("experience"),
                "experiences": related_results.get("experiences", []),
                "education_entries": related_education,
                "certification_entries": related_results.get("certification_entries", []),
                "portfolio_projects": related_results.get("portfolio_projects", []),
                "publication_entries": related_results.get("publication_entries", []),
                "college": college,
                "department": department,
                "batch": batch,
            },
            "syllabus": syllabus,
        }

    except HTTPException:
        raise
    except Exception as e:
        msg = f"{e}".lower()
        if "invalid jwt" in msg or "token is malformed" in msg or "unable to parse" in msg:
            raise HTTPException(status_code=401, detail="Invalid or malformed token")
        supabase_logger.exception("/api/me error")
        raise HTTPException(status_code=500, detail=f"Unexpected error: {e}")


def upsert_syllabus_course(payload: SyllabusCourseIn) -> SyllabusCourseOut:
    supabase = get_service_client()
    existing = _supabase_retry(lambda: (
        supabase.table("syllabus_courses")
        .select("id,title,type")
        .eq("batch_id", str(payload.batch_id))
        .eq("semester", payload.semester)
        .eq("course_code", payload.course_code)
        .limit(1)
        .execute()
    ))
    if getattr(existing, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (find course): {existing.error}")
    if existing.data:
        course_id = uuid.UUID(existing.data[0]["id"])
        existing_type = existing.data[0].get("type")
        desired_type = payload.type or "practical"
        if existing.data[0].get("title") != payload.title or existing_type != desired_type:
            upd = _supabase_retry(lambda: (
                supabase.table("syllabus_courses")
                .update({"title": payload.title, "type": desired_type})
                .eq("id", str(course_id))
                .execute()
            ))
            if getattr(upd, "error", None):
                raise HTTPException(status_code=500, detail=f"Supabase error (update course): {upd.error}")
    else:
        ins = _supabase_retry(lambda: (
            supabase.table("syllabus_courses")
            .insert(
                {
                    "batch_id": str(payload.batch_id),
                    "semester": payload.semester,
                    "course_code": payload.course_code,
                    "title": payload.title,
                    "type": payload.type or "practical",
                }
            )
            .execute()
        ))
        if getattr(ins, "error", None):
            raise HTTPException(status_code=500, detail=f"Supabase error (insert course): {ins.error}")
        course_id = uuid.UUID(ins.data[0]["id"]) if ins.data else None

    # Fetch units/topics after ensuring course exists
    units = []  # placeholder; actual unit sync handled elsewhere
    return SyllabusCourseOut(
        id=course_id,
        batch_id=payload.batch_id,
        semester=payload.semester,
        course_code=payload.course_code,
        title=payload.title,
        type=payload.type,
        units=units,
    )


def sync_units_and_topics(course_id: uuid.UUID, units: List[UnitIn]) -> List[UnitOut]:
    supabase = get_service_client()
    ures = (
        supabase.table("syllabus_units")
        .select("id,unit_title")
        .eq("course_id", str(course_id))
        .execute()
    )
    if getattr(ures, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (list units): {ures.error}")
    unit_map: Dict[str, uuid.UUID] = {row["unit_title"]: uuid.UUID(row["id"]) for row in (ures.data or [])}

    for order_idx, u in enumerate(units):
        uid = unit_map.get(u.unit_title)
        if uid is None:
            ins = (
                supabase.table("syllabus_units")
                .insert(
                    {
                        "course_id": str(course_id),
                        "unit_title": u.unit_title,
                        "order_in_course": order_idx,
                    }
                )
                .execute()
            )
            if getattr(ins, "error", None):
                raise HTTPException(status_code=500, detail=f"Supabase error (insert unit): {ins.error}")
            ref = (
                supabase.table("syllabus_units")
                .select("id")
                .eq("course_id", str(course_id))
                .eq("unit_title", u.unit_title)
                .limit(1)
                .execute()
            )
            if getattr(ref, "error", None) or not ref.data:
                raise HTTPException(status_code=500, detail=f"Supabase error (refetch unit): {getattr(ref, 'error', None)}")
            uid = uuid.UUID(ref.data[0]["id"])
            unit_map[u.unit_title] = uid
        else:
            upd = (
                supabase.table("syllabus_units")
                .update({"order_in_course": order_idx})
                .eq("id", str(uid))
                .execute()
            )
            if getattr(upd, "error", None):
                raise HTTPException(status_code=500, detail=f"Supabase error (update unit order): {upd.error}")

        tres = (
            supabase.table("syllabus_topics")
            .select("id,topic")
            .eq("unit_id", str(uid))
            .execute()
        )
        if getattr(tres, "error", None):
            raise HTTPException(status_code=500, detail=f"Supabase error (list topics): {tres.error}")
        topic_map: Dict[str, uuid.UUID] = {row["topic"]: uuid.UUID(row["id"]) for row in (tres.data or [])}
        for t_order, t in enumerate(u.topics or []):
            tid = topic_map.get(t.topic)
            if tid is None:
                tins = (
                    supabase.table("syllabus_topics")
                    .insert(
                        {
                            "unit_id": str(uid),
                            "topic": t.topic,
                            "order_in_unit": t_order,
                        }
                    )
                    .execute()
                )
                if getattr(tins, "error", None):
                    raise HTTPException(status_code=500, detail=f"Supabase error (insert topic): {tins.error}")
                tref = (
                    supabase.table("syllabus_topics")
                    .select("id")
                    .eq("unit_id", str(uid))
                    .eq("topic", t.topic)
                    .limit(1)
                    .execute()
                )
                if getattr(tref, "error", None) or not tref.data:
                    raise HTTPException(status_code=500, detail=f"Supabase error (refetch topic): {getattr(tref, 'error', None)}")
                tid = uuid.UUID(tref.data[0]["id"])
                topic_map[t.topic] = tid
            else:
                tupd = (
                    supabase.table("syllabus_topics")
                    .update({"order_in_unit": t_order})
                    .eq("id", str(tid))
                    .execute()
                )
                if getattr(tupd, "error", None):
                    raise HTTPException(status_code=500, detail=f"Supabase error (update topic order): {tupd.error}")

    units_rows = (
        supabase.table("syllabus_units")
        .select("id,unit_title,order_in_course")
        .eq("course_id", str(course_id))
        .order("order_in_course")
        .execute()
    )
    if getattr(units_rows, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (get units): {units_rows.error}")
    out_units: List[UnitOut] = []
    for ur in (units_rows.data or []):
        uid = uuid.UUID(ur["id"])
        tops = (
            supabase.table("syllabus_topics")
            .select("id,topic,order_in_unit,image_url,video_url,ppt_url,lab_url")
            .eq("unit_id", str(uid))
            .order("order_in_unit")
            .execute()
        )
        if getattr(tops, "error", None):
            raise HTTPException(status_code=500, detail=f"Supabase error (get topics): {tops.error}")
        out_units.append(
            UnitOut(
                id=uid,
                unit_title=ur["unit_title"],
                order_in_course=ur["order_in_course"],
                topics=[
                    TopicOut(
                        id=uuid.UUID(tr["id"]),
                        topic=tr["topic"],
                        order_in_unit=tr["order_in_unit"],
                        image_url=tr.get("image_url"),
                        video_url=tr.get("video_url"),
                        ppt_url=tr.get("ppt_url"),
                        lab_url=tr.get("lab_url"),
                    )
                    for tr in (tops.data or [])
                ],
            )
        )
    return out_units


def load_course_with_units(course_id: uuid.UUID) -> SyllabusCourseOut:
    supabase = get_service_client()

    def _safe_exec(builder, label: str):
        # Lightweight retry loop so transient disconnects (RemoteProtocolError, etc.) don't crash the endpoint
        retries = 3
        delay = 0.15
        last_err = None
        for attempt in range(retries):
            try:
                res = builder.execute()
                break
            except Exception as e:  # httpx / network-level errors are fine to retry
                last_err = e
                if attempt == retries - 1:
                    raise HTTPException(status_code=503, detail=f"Supabase error ({label}): {e}")
                time.sleep(delay * (attempt + 1))
        if getattr(res, "error", None):
            raise HTTPException(status_code=503, detail=f"Supabase error ({label}): {res.error}")
        return res

    course_builder = (
        supabase.table("syllabus_courses")
        .select("id,batch_id,semester,course_code,title,type")
        .eq("id", str(course_id))
        .single()
    )
    course_q = _safe_exec(course_builder, "get course")
    if not course_q.data:
        raise HTTPException(status_code=404, detail="Course not found")

    units_builder = (
        supabase.table("syllabus_units")
        .select("id,unit_title,order_in_course")
        .eq("course_id", str(course_id))
        .order("order_in_course")
    )
    units_rows = _safe_exec(units_builder, "get units")

    units_out: List[UnitOut] = []
    for unit_row in units_rows.data or []:
        unit_id = uuid.UUID(unit_row["id"])
        topics_builder = (
            supabase.table("syllabus_topics")
            .select("id,topic,order_in_unit,image_url,video_url,ppt_url,lab_url")
            .eq("unit_id", str(unit_id))
            .order("order_in_unit")
        )
        topics_rows = _safe_exec(topics_builder, "get topics")
        units_out.append(
            UnitOut(
                id=unit_id,
                unit_title=unit_row["unit_title"],
                order_in_course=unit_row["order_in_course"],
                topics=[
                    TopicOut(
                        id=uuid.UUID(topic_row["id"]),
                        topic=topic_row["topic"],
                        order_in_unit=topic_row["order_in_unit"],
                        image_url=topic_row.get("image_url"),
                        video_url=topic_row.get("video_url"),
                        ppt_url=topic_row.get("ppt_url"),
                        lab_url=topic_row.get("lab_url"),
                    )
                    for topic_row in (topics_rows.data or [])
                ],
            )
        )

    data = course_q.data
    return SyllabusCourseOut(
        id=uuid.UUID(data["id"]),
        batch_id=uuid.UUID(data["batch_id"]),
        semester=int(data["semester"]),
        course_code=data.get("course_code"),
        title=data.get("title"),
        type=data.get("type"),
        units=units_out,
    )


def load_unit_with_topics(unit_id: uuid.UUID) -> UnitOut:
    supabase = get_service_client()
    unit_res = (
        supabase.table("syllabus_units")
        .select("id,unit_title,order_in_course")
        .eq("id", str(unit_id))
        .limit(1)
        .execute()
    )
    if getattr(unit_res, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (get unit): {unit_res.error}")
    if not unit_res.data:
        raise HTTPException(status_code=404, detail="Unit not found")

    unit_row = unit_res.data[0]
    topics_res = (
        supabase.table("syllabus_topics")
        .select("id,topic,order_in_unit,image_url,video_url,ppt_url,lab_url")
        .eq("unit_id", str(unit_id))
        .order("order_in_unit")
        .execute()
    )
    if getattr(topics_res, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (get unit topics): {topics_res.error}")

    return UnitOut(
        id=unit_id,
        unit_title=unit_row.get("unit_title"),
        order_in_course=int(unit_row.get("order_in_course", 0)),
        topics=[
            TopicOut(
                id=uuid.UUID(topic_row["id"]),
                topic=topic_row.get("topic"),
                order_in_unit=int(topic_row.get("order_in_unit", 0)),
                image_url=topic_row.get("image_url"),
                video_url=topic_row.get("video_url"),
                ppt_url=topic_row.get("ppt_url"),
                lab_url=topic_row.get("lab_url"),
            )
            for topic_row in (topics_res.data or [])
        ],
    )


def resolve_or_create_batch(
    college_id: uuid.UUID, dept_name: str, from_year: int, to_year: int
) -> BatchWithIdOut:
    department_id = _resolve_department_id(college_id, dept_name)
    batch_id = _get_or_create_batch_id(college_id, department_id, from_year, to_year)
    return BatchWithIdOut(id=batch_id, from_year=from_year, to_year=to_year)


# --------------------- Progress tracking services ---------------------------

def _get_user_id_with_retry(token: str, retries: int = 3, base_delay: float = 0.25) -> str:
    anon_client = get_anon_client()
    if not anon_client:
        raise HTTPException(status_code=500, detail="Server missing SUPABASE_ANON_KEY")
    last_exc: Optional[Exception] = None
    retryable_auth_errors: tuple[Any, ...] = (AuthRetryableError,)
    if httpx is not None:
        retryable_auth_errors = retryable_auth_errors + (httpx.RemoteProtocolError,)  # type: ignore
        if HTTPXRemoteProtocolError not in retryable_auth_errors:
            retryable_auth_errors = retryable_auth_errors + (HTTPXRemoteProtocolError,)  # type: ignore
    else:
        retryable_auth_errors = retryable_auth_errors + (HTTPXRemoteProtocolError,)

    for attempt in range(retries):
        try:
            auth_user = anon_client.auth.get_user(token)
            user = getattr(auth_user, "user", None) or (auth_user.get("user") if isinstance(auth_user, dict) else None)
            user_id = getattr(user, "id", None) or (user.get("id") if isinstance(user, dict) else None)
            if not user_id:
                raise HTTPException(status_code=401, detail="Invalid token or user not found")
            return user_id
        except AuthApiError as e:
            msg = getattr(e, "message", None) or str(e) or "Invalid or expired session"
            raise HTTPException(status_code=401, detail=msg)
        except retryable_auth_errors as e:  # type: ignore
            last_exc = e
            time.sleep(base_delay * (attempt + 1))
            continue
        except Exception:
            raise
    supabase_logger.warning("Auth get_user retry exhausted: %s", last_exc)
    raise HTTPException(status_code=503, detail="Authentication service temporarily unavailable; please retry")


def _require_user_and_profile(token: Optional[str]) -> tuple[str, str]:
    anon_client = get_anon_client()
    if not anon_client:
        raise HTTPException(status_code=500, detail="Server missing SUPABASE_ANON_KEY")
    if not token:
        raise HTTPException(status_code=401, detail="Missing or invalid Authorization header")
    user_id = _get_user_id_with_retry(token)
    supabase = get_service_client()
    prof_q = (
        supabase.table("user_profiles").select("id").eq("auth_user_id", user_id).limit(1).execute()
    )
    if getattr(prof_q, "error", None) or not prof_q.data:
        raise HTTPException(status_code=404, detail="Profile not found")
    profile_id = prof_q.data[0]["id"]
    return user_id, profile_id


def _ensure_user_and_profile(token: Optional[str]) -> tuple[str, str]:
    """Ensure there is a user_profiles row for the authenticated user.

    Returns (auth_user_id, profile_id). Creates a minimal row if missing.
    """
    anon_client = get_anon_client()
    if not anon_client:
        raise HTTPException(status_code=500, detail="Server missing SUPABASE_ANON_KEY")
    if not token:
        raise HTTPException(status_code=401, detail="Missing or invalid Authorization header")

    user_id = _get_user_id_with_retry(token)
    supabase = get_service_client()

    # Try existing profile first
    prof_q = (
        supabase.table("user_profiles").select("id").eq("auth_user_id", user_id).limit(1).execute()
    )
    if getattr(prof_q, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (get profile): {prof_q.error}")
    if prof_q.data:
        return user_id, prof_q.data[0]["id"]

    # Create a minimal profile using email/name from auth metadata when available
    email = None
    name = None
    try:
        auth_user = anon_client.auth.get_user(token)
        user_obj = getattr(auth_user, "user", None) or (auth_user.get("user") if isinstance(auth_user, dict) else None)
        email = (getattr(user_obj, "email", None) or (user_obj.get("email") if isinstance(user_obj, dict) else None))
        meta = (getattr(user_obj, "user_metadata", None) or (user_obj.get("user_metadata") if isinstance(user_obj, dict) else None)) or {}
        try:
            name = meta.get("full_name") or meta.get("name") or meta.get("preferred_username")
        except Exception:
            name = None
    except Exception:
        pass

    ins = (
        supabase.table("user_profiles")
        .insert({
            "auth_user_id": user_id,
            "email": email,
            "name": name,
        })
        .execute()
    )
    if getattr(ins, "error", None):
        # If a race created it already, proceed to refetch; otherwise fail
        err_txt = str(ins.error)
        if "duplicate" not in err_txt.lower():
            raise HTTPException(status_code=500, detail=f"Supabase error (create profile): {ins.error}")

    prof_q2 = (
        supabase.table("user_profiles").select("id").eq("auth_user_id", user_id).limit(1).execute()
    )
    if getattr(prof_q2, "error", None) or not prof_q2.data:
        raise HTTPException(status_code=500, detail=f"Supabase error (refetch profile): {getattr(prof_q2, 'error', None)}")
    return user_id, prof_q2.data[0]["id"]


def _get_profile_me(token: Optional[str]):
    # Ensure a profile exists (creates a minimal one for OAuth users)
    _, profile_id = _ensure_user_and_profile(token)
    supabase = get_service_client()
    prof_q = (
        supabase.table("user_profiles")
        .select(
            "id,auth_user_id,email,name,gender,phone,semester,regno,profile_image_url,resume_url,bio,linkedin,github,leetcode,specializations,projects,"\
            "headline,location,dob,portfolio_url,website,twitter,instagram,medium,verification_score,"
            "technologies,skills,certifications,languages,interests,project_info,publications,achievements,experience"
        )
        .eq("id", profile_id)
        .single()
        .execute()
    )
    if getattr(prof_q, "error", None) or not prof_q.data:
        raise HTTPException(status_code=404, detail="Profile not found")

    profile = dict(prof_q.data)
    profile_id_str = profile.get("id")
    if profile_id_str:
        profile["experiences"] = _fetch_profile_related(
            profile_id_str,
            "user_experiences",
            [("order_index", False), ("start_date", True), ("created_at", False)],
        )
        profile["education_entries"] = _fetch_profile_related(
            profile_id_str,
            "user_education",
            [("order_index", False), ("created_at", False)],
        )
        profile["certification_entries"] = _fetch_profile_related(
            profile_id_str,
            "user_certifications",
            [("order_index", False), ("issue_date", True), ("created_at", False)],
        )
        profile["portfolio_projects"] = _fetch_profile_related(
            profile_id_str,
            "user_portfolio_projects",
            [("order_index", False), ("start_date", True), ("created_at", False)],
        )
        profile["publication_entries"] = _fetch_profile_related(
            profile_id_str,
            "user_publications",
            [("order_index", False), ("publication_date", True), ("created_at", False)],
        )

    # Derive academic info strictly from education entries (FK columns if present)
    college = None
    department = None
    batch = None
    derived_semester = profile.get("semester")
    derived_regno = profile.get("regno")
    primary_edu = None
    edu_entries = profile.get("education_entries") or []
    if edu_entries:
        # choose highest current_semester, else first
        typed = [e for e in edu_entries if isinstance(e, dict)]
        if typed:
            typed.sort(key=lambda r: (r.get("current_semester") or 0, -(r.get("order_index") or 0)), reverse=True)
            primary_edu = typed[0]
    if primary_edu:
        if primary_edu.get("current_semester"):
            derived_semester = primary_edu.get("current_semester")
        if primary_edu.get("regno"):
            derived_regno = primary_edu.get("regno")
        supabase = get_service_client()
        # Prefer FK ids on education row (added by normalization) if present
        college_id = primary_edu.get("college_id")
        department_id = primary_edu.get("department_id")
        batch_id = primary_edu.get("batch_id")
        if college_id:
            cq = supabase.table("colleges").select("id,name").eq("id", college_id).limit(1).execute()
            if not getattr(cq, "error", None) and cq.data:
                college = {"id": cq.data[0]["id"], "name": cq.data[0]["name"]}
        if department_id:
            dq = supabase.table("departments").select("id,name").eq("id", department_id).limit(1).execute()
            if not getattr(dq, "error", None) and dq.data:
                department = {"id": dq.data[0]["id"], "name": dq.data[0]["name"]}
        if batch_id:
            bq = supabase.table("batches").select("id,from_year,to_year").eq("id", batch_id).limit(1).execute()
            if not getattr(bq, "error", None) and bq.data:
                batch = {"id": bq.data[0]["id"], "from": bq.data[0]["from_year"], "to": bq.data[0]["to_year"]}
        # Fallback on legacy text if FK not available
        if not college and primary_edu.get("school"):
            college = {"id": None, "name": primary_edu.get("school")}
        if not department and primary_edu.get("department"):
            department = {"id": None, "name": (primary_edu.get("department") or "").upper()}
        if not batch and primary_edu.get("batch_range"):
            import re as _re
            years = _re.findall(r"\b(\d{4})\b", primary_edu.get("batch_range") or "")
            if len(years) >= 2:
                try:
                    batch = {"id": None, "from": int(years[0]), "to": int(years[1])}
                except Exception:
                    pass
    profile["college"] = college
    profile["department"] = department
    profile["batch"] = batch
    profile["semester"] = derived_semester
    profile["regno"] = derived_regno
    return profile


def _clean_media_items(media_items: Optional[List[Dict[str, Any]]]) -> Optional[List[Dict[str, Any]]]:
    cleaned: List[Dict[str, Any]] = []
    for raw in media_items or []:
        if not isinstance(raw, dict):
            continue
        url = _strip_or_none(raw.get("url"))
        if not url:
            continue
        cleaned.append(
            {
                "kind": _strip_or_none(raw.get("kind")),
                "url": url,
                "title": _strip_or_none(raw.get("title")),
            }
        )
    return cleaned or None


def _prepare_experience_rows(rows: Optional[List[Dict[str, Any]]]) -> List[Dict[str, Any]]:
    prepared: List[Dict[str, Any]] = []
    now_iso = datetime.utcnow().isoformat()
    for idx, row in enumerate(rows or []):
        if not isinstance(row, dict):
            continue
        title = _strip_or_none(row.get("title"))
        start_date = _date_or_none(row.get("start_date"))
        if not title or not start_date:
            continue
        prepared_row: Dict[str, Any] = {
            "title": title,
            "employment_type": _strip_or_none(row.get("employment_type")),
            "company": _strip_or_none(row.get("company")),
            "company_logo_url": _strip_or_none(row.get("company_logo_url")),
            "location": _strip_or_none(row.get("location")),
            "location_type": _strip_or_none(row.get("location_type")),
            "start_date": start_date,
            "end_date": _date_or_none(row.get("end_date")),
            "is_current": bool(row.get("is_current")),
            "description": _strip_or_none(row.get("description")),
            "order_index": idx,
            "updated_at": now_iso,
        }
        media_items = row.get("media")
        cleaned_media = _clean_media_items(media_items if isinstance(media_items, list) else None)
        if cleaned_media is not None:
            prepared_row["media"] = cleaned_media
        row_id = _strip_or_none(row.get("id"))
        if not row_id:
            row_id = str(uuid.uuid4())
        prepared_row["id"] = row_id
        prepared.append(prepared_row)
    return prepared


def _prepare_education_rows(rows: Optional[List[Dict[str, Any]]]) -> List[Dict[str, Any]]:
    """Prepare revised education rows.

    Legacy keys (field_of_study, start_date, end_date) ignored after schema change.
    """
    prepared: List[Dict[str, Any]] = []
    now_iso = datetime.utcnow().isoformat()
    for idx, row in enumerate(rows or []):
        if not isinstance(row, dict):
            continue
        school = _strip_or_none(row.get("school"))
        if not school:
            continue
        # Attempt FK resolution when ids missing but names present.
        college_id_val = _strip_or_none(row.get("college_id"))
        degree_id_val = _strip_or_none(row.get("degree_id"))
        department_id_val = _strip_or_none(row.get("department_id"))
        batch_id_val = _strip_or_none(row.get("batch_id"))
        try:
            if not college_id_val and school:
                # Upsert college by name
                college_uuid = upsert_college(school)
                college_id_val = str(college_uuid)
            # Resolve or create degree if name provided and degree_id missing
            degree_name_candidate = _strip_or_none(row.get("degree"))
            if college_id_val and not degree_id_val and degree_name_candidate:
                supabase = get_service_client()
                deg_q = supabase.table("degrees").select("id").eq("college_id", college_id_val).eq("name", degree_name_candidate).limit(1).execute()
                if not getattr(deg_q, "error", None) and deg_q.data:
                    degree_id_val = deg_q.data[0]["id"]
                else:
                    ins_deg = supabase.table("degrees").insert({
                        "college_id": college_id_val,
                        "name": degree_name_candidate,
                    }).execute()
                    if not getattr(ins_deg, "error", None):
                        ref_deg = supabase.table("degrees").select("id").eq("college_id", college_id_val).eq("name", degree_name_candidate).limit(1).execute()
                        if not getattr(ref_deg, "error", None) and ref_deg.data:
                            degree_id_val = ref_deg.data[0]["id"]
            if college_id_val and not department_id_val:
                dept_name_candidate = _strip_or_none(row.get("department"))
                if dept_name_candidate:
                    try:
                        department_uuid = _resolve_department_id(uuid.UUID(college_id_val), dept_name_candidate)
                        department_id_val = str(department_uuid)
                    except HTTPException:
                        # Department not found; create minimal department without degree context
                        supabase = get_service_client()
                        dep_payload = {"college_id": college_id_val, "name": dept_name_candidate.upper()}
                        if degree_id_val:
                            dep_payload["degree_id"] = degree_id_val
                        ins_dep = supabase.table("departments").insert(dep_payload).execute()
                        if not getattr(ins_dep, "error", None):
                            ref_dep = supabase.table("departments").select("id").eq("college_id", college_id_val).eq("name", dept_name_candidate.upper()).limit(1).execute()
                            if not getattr(ref_dep, "error", None) and ref_dep.data:
                                department_id_val = ref_dep.data[0]["id"]
            if college_id_val and department_id_val and not batch_id_val:
                batch_range_raw = _strip_or_none(row.get("batch_range"))
                if batch_range_raw and "-" in batch_range_raw:
                    try:
                        yr_from = int(batch_range_raw.split("-",1)[0])
                        yr_to = int(batch_range_raw.split("-",1)[1])
                        batch_uuid = _get_or_create_batch_id(uuid.UUID(college_id_val), uuid.UUID(department_id_val), yr_from, yr_to)
                        batch_id_val = str(batch_uuid)
                    except Exception:
                        pass
        except Exception:
            # Fail soft; continue without FK resolution if anything goes wrong
            pass
        prepared_row: Dict[str, Any] = {
            "school": school,
            "degree": _strip_or_none(row.get("degree")),
            "department": _strip_or_none(row.get("department")),
            "batch_range": _strip_or_none(row.get("batch_range")),
            "section": _strip_or_none(row.get("section")),
            "regno": _strip_or_none(row.get("regno")),
            "current_semester": row.get("current_semester") if isinstance(row.get("current_semester"), int) else None,
            "grade": _strip_or_none(row.get("grade")),
            "activities": _strip_or_none(row.get("activities")),
            "description": _strip_or_none(row.get("description")),
            "order_index": idx,
            "updated_at": now_iso,
        }
        # Optional new FK id fields propagated from frontend (already resolved or chosen)
        for fk_key in ("college_id", "degree_id", "department_id", "batch_id"):
            val = _strip_or_none(row.get(fk_key))
            if val:
                prepared_row[fk_key] = val
        # Include resolved ones if not provided originally
        if college_id_val and "college_id" not in prepared_row:
            prepared_row["college_id"] = college_id_val
        if degree_id_val and "degree_id" not in prepared_row:
            prepared_row["degree_id"] = degree_id_val
        if department_id_val and "department_id" not in prepared_row:
            prepared_row["department_id"] = department_id_val
        if batch_id_val and "batch_id" not in prepared_row:
            prepared_row["batch_id"] = batch_id_val
        row_id = _strip_or_none(row.get("id"))
        if not row_id:
            row_id = str(uuid.uuid4())
        prepared_row["id"] = row_id
        prepared.append(prepared_row)
    return prepared


def _prepare_certification_rows(rows: Optional[List[Dict[str, Any]]]) -> List[Dict[str, Any]]:
    prepared: List[Dict[str, Any]] = []
    now_iso = datetime.utcnow().isoformat()
    for idx, row in enumerate(rows or []):
        if not isinstance(row, dict):
            continue
        name = _strip_or_none(row.get("name"))
        if not name:
            continue
        prepared_row: Dict[str, Any] = {
            "name": name,
            "issuing_org": _strip_or_none(row.get("issuing_org")),
            "issue_date": _date_or_none(row.get("issue_date")),
            "expiration_date": _date_or_none(row.get("expiration_date")),
            "does_not_expire": bool(row.get("does_not_expire")),
            "credential_id": _strip_or_none(row.get("credential_id")),
            "credential_url": _strip_or_none(row.get("credential_url")),
            "description": _strip_or_none(row.get("description")),
            "order_index": idx,
            "updated_at": now_iso,
        }
        row_id = _strip_or_none(row.get("id"))
        if not row_id:
            row_id = str(uuid.uuid4())
        prepared_row["id"] = row_id
        prepared.append(prepared_row)
    return prepared


def _prepare_project_rows(rows: Optional[List[Dict[str, Any]]]) -> List[Dict[str, Any]]:
    prepared: List[Dict[str, Any]] = []
    now_iso = datetime.utcnow().isoformat()
    for idx, row in enumerate(rows or []):
        if not isinstance(row, dict):
            continue
        name = _strip_or_none(row.get("name"))
        if not name:
            continue
        tech_stack_raw = row.get("tech_stack")
        tech_stack_list: Optional[List[str]] = None
        if isinstance(tech_stack_raw, list):
            tech_stack_list = [s for s in (_strip_or_none(item) for item in tech_stack_raw) if s]
        elif isinstance(tech_stack_raw, str):
            tech_stack_list = [s for s in (_strip_or_none(part) for part in tech_stack_raw.split(",")) if s]
        team_raw = row.get("team")
        team_list: Optional[List[Dict[str, Any]]] = None
        if isinstance(team_raw, list):
            normalized: List[Dict[str, Any]] = []
            for entry in team_raw:
                if isinstance(entry, dict):
                    name_val = _strip_or_none(entry.get("name"))
                    if name_val:
                        normalized.append(
                            {
                                "name": name_val,
                                "role": _strip_or_none(entry.get("role")),
                                "profile_url": _strip_or_none(entry.get("profile_url")),
                                "user_id": _strip_or_none(entry.get("user_id")),
                            }
                        )
                else:
                    item_name = _strip_or_none(entry)
                    if item_name:
                        normalized.append({"name": item_name})
            team_list = normalized or None
        elif isinstance(team_raw, str):
            members = [s for s in (_strip_or_none(part) for part in team_raw.split(",")) if s]
            if members:
                team_list = [{"name": member} for member in members]
        prepared_row: Dict[str, Any] = {
            "name": name,
            "associated_experience_id": _strip_or_none(row.get("associated_experience_id")),
            "associated_education_id": _strip_or_none(row.get("associated_education_id")),
            "start_date": _date_or_none(row.get("start_date")),
            "end_date": _date_or_none(row.get("end_date")),
            "url": _strip_or_none(row.get("url")),
            "description": _strip_or_none(row.get("description")),
            "order_index": idx,
            "updated_at": now_iso,
        }
        if tech_stack_list is not None:
            prepared_row["tech_stack"] = tech_stack_list
        if team_list is not None:
            prepared_row["team"] = team_list
        row_id = _strip_or_none(row.get("id"))
        if not row_id:
            row_id = str(uuid.uuid4())
        prepared_row["id"] = row_id
        prepared.append(prepared_row)
    return prepared


def _prepare_publication_rows(rows: Optional[List[Dict[str, Any]]]) -> List[Dict[str, Any]]:
    prepared: List[Dict[str, Any]] = []
    now_iso = datetime.utcnow().isoformat()
    for idx, row in enumerate(rows or []):
        if not isinstance(row, dict):
            continue
        title = _strip_or_none(row.get("title"))
        if not title:
            continue
        authors_raw = row.get("authors")
        authors_list: Optional[List[str]] = None
        if isinstance(authors_raw, list):
            authors_list = [s for s in (_strip_or_none(part) for part in authors_raw) if s]
        elif isinstance(authors_raw, str):
            authors_list = [s for s in (_strip_or_none(part) for part in authors_raw.split(",")) if s]
        prepared_row: Dict[str, Any] = {
            "title": title,
            "publisher": _strip_or_none(row.get("publisher")),
            "publication_date": _date_or_none(row.get("publication_date")),
            "url": _strip_or_none(row.get("url")),
            "abstract": _strip_or_none(row.get("abstract")),
            "order_index": idx,
            "updated_at": now_iso,
        }
        if authors_list is not None:
            prepared_row["authors"] = authors_list
        row_id = _strip_or_none(row.get("id"))
        if not row_id:
            row_id = str(uuid.uuid4())
        prepared_row["id"] = row_id
        prepared.append(prepared_row)
    return prepared


def _sync_profile_collection(profile_id: str, table: str, rows: List[Dict[str, Any]]):
    supabase = get_service_client()
    existing_q = (
        supabase.table(table)
        .select("id")
        .eq("user_profile_id", profile_id)
        .execute()
    )
    if getattr(existing_q, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (fetch {table}): {existing_q.error}")
    existing_ids = {row["id"] for row in (existing_q.data or []) if row.get("id")}
    incoming_ids = {row["id"] for row in rows if row.get("id")}
    to_delete = list(existing_ids - incoming_ids)

    delete_res = None
    existing_rows = [row for row in rows if row.get("id")]
    new_rows = [row for row in rows if not row.get("id")]

    if existing_rows:
        payload = [{**row, "user_profile_id": profile_id} for row in existing_rows]
        upsert = (
            supabase.table(table)
            .upsert(payload, on_conflict="id")
            .execute()
        )
        if getattr(upsert, "error", None):
            raise HTTPException(status_code=500, detail=f"Supabase error (upsert {table}): {upsert.error}")

    if new_rows:
        insert_payload = [{**row, "user_profile_id": profile_id} for row in new_rows]
        insert_res = supabase.table(table).insert(insert_payload).execute()
        if getattr(insert_res, "error", None):
            raise HTTPException(status_code=500, detail=f"Supabase error (insert {table}): {insert_res.error}")

    if to_delete:
        delete_res = (
            supabase.table(table)
            .delete()
            .in_("id", to_delete)
            .execute()
        )
        if getattr(delete_res, "error", None):
            raise HTTPException(status_code=500, detail=f"Supabase error (cleanup {table}): {delete_res.error}")

    if not rows and existing_ids and not to_delete:
        delete_res = (
            supabase.table(table)
            .delete()
            .eq("user_profile_id", profile_id)
            .execute()
        )
        if getattr(delete_res, "error", None):
            raise HTTPException(status_code=500, detail=f"Supabase error (clear {table}): {delete_res.error}")

    if delete_res is not None and getattr(delete_res, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (delete {table}): {delete_res.error}")


def _fetch_profile_related(profile_id: str, table: str, order_by: Optional[List[Tuple[str, bool]]] = None) -> List[Dict[str, Any]]:
    supabase = get_service_client()
    def _build_query():
        q = supabase.table(table).select("*").eq("user_profile_id", profile_id)
        if order_by:
            for column, desc in order_by:
                q = q.order(column, desc=bool(desc))
        return q
    try:
        res = _supabase_retry(lambda: _build_query().execute())
    except HTTPXRemoteProtocolError as exc:  # pragma: no cover - network dependent
        # Treat as empty on transient disconnects so /api/me still works
        supabase_logger.warning("profile related fetch '%s' transient protocol error: %s", table, exc)
        res = type("_R", (), {"data": [], "error": None})()
    if getattr(res, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (fetch {table}): {res.error}")
    items: List[Dict[str, Any]] = []
    for raw in res.data or []:
        if not isinstance(raw, dict):
            continue
        item = dict(raw)
        item.pop("user_profile_id", None)
        for key in ("created_at", "updated_at", "start_date", "end_date", "issue_date", "expiration_date", "publication_date"):
            if key in item and isinstance(item[key], (datetime, date)):
                item[key] = item[key].isoformat()
        if table == "user_experiences":
            item["media"] = item.get("media") or []
        if table == "user_portfolio_projects":
            item["tech_stack"] = item.get("tech_stack") or []
            item["team"] = item.get("team") or []
        if table == "user_publications":
            item["authors"] = item.get("authors") or []
        items.append(item)
    return items


def _update_profile_me(token: Optional[str], fields: dict):
    # Ensure a profile exists (creates a minimal one for OAuth users)
    _, profile_id = _ensure_user_and_profile(token)
    supabase = get_service_client()
    # allow base fields first
    allowed = {
        "name", "phone", "bio", "linkedin", "github", "leetcode", "specializations", "projects", "semester", "regno",
        # Tunex-like extra fields:
        "headline", "location", "dob", "portfolio_url", "website", "twitter", "instagram", "medium",
        "verification_score", "technologies", "skills", "certifications", "languages", "interests",
        "project_info", "publications", "achievements", "experience",
    }
    fields = fields or {}
    experiences_payload = fields.pop("experiences", None)
    education_payload = fields.pop("education_entries", None)
    certifications_payload = fields.pop("certification_entries", None)
    projects_payload = fields.pop("portfolio_projects", None)
    publications_payload = fields.pop("publication_entries", None)

    payload = {k: v for k, v in fields.items() if k in allowed}

    # Resolve academic relations if provided
    college_name = fields.get("college_name")
    department_name = fields.get("department_name")
    batch_from = fields.get("batch_from")
    batch_to = fields.get("batch_to")

    if college_name:
        college_id = _resolve_college_id_by_name(college_name)
        payload["college_id"] = str(college_id)
        if department_name:
            department_id = _resolve_department_id(college_id, department_name)
            payload["department_id"] = str(department_id)
            if batch_from is not None and batch_to is not None:
                batch_id = _get_or_create_batch_id(college_id, department_id, int(batch_from), int(batch_to))
                payload["batch_id"] = str(batch_id)
                payload["batch_from"] = int(batch_from)
                payload["batch_to"] = int(batch_to)

    update_payload = _supabase_payload(payload)
    if update_payload:
        retry_payload = dict(update_payload)
        fallback_keys = {"college_id", "department_id", "batch_id", "batch_from", "batch_to"}
        for attempt in range(3):
            if not retry_payload:
                break
            try:
                upd = supabase.table("user_profiles").update(retry_payload).eq("id", profile_id).execute()
            except APIError as exc:
                err_code = getattr(exc, "code", None)
                err_message = getattr(exc, "message", None) or getattr(exc, "details", None) or str(exc)
                lower_msg = (err_message or "").lower()
                # If undefined column error, remove potential FK fields and retry.
                if err_code == "42703":
                    removed = False
                    for key in list(retry_payload.keys()):
                        if key in fallback_keys:
                            retry_payload.pop(key, None)
                            removed = True
                    if removed:
                        continue
                # If a DB trigger references a non-existent NEW.department_id (stale trigger), skip base update gracefully.
                if ("record \"new\" has no field" in lower_msg) or ("record new has no field" in lower_msg):
                    supabase_logger.warning("Skipping user_profiles update due to trigger missing field: %s", err_message)
                    retry_payload.clear()
                    break
                raise HTTPException(status_code=500, detail=f"Supabase error (update profile): {err_message}")

            err_msg = getattr(upd, "error", None)
            if err_msg:
                msg = str(err_msg)
                lowered = msg.lower()
                if ("42703" in lowered) or any(field in lowered for field in ("college_id", "department_id", "batch_id")):
                    removed = False
                    for key in list(retry_payload.keys()):
                        if key in fallback_keys:
                            retry_payload.pop(key, None)
                            removed = True
                    if removed:
                        continue
                # Handle stale trigger error pattern on successful call that returned an error payload
                if ("record \"new\" has no field" in lowered) or ("record new has no field" in lowered):
                    supabase_logger.warning("Skipping user_profiles update due to trigger missing field: %s", msg)
                    break
                raise HTTPException(status_code=500, detail=f"Supabase error (update profile): {msg}")
            break
    if experiences_payload is not None:
        prepared = _prepare_experience_rows(experiences_payload if isinstance(experiences_payload, list) else None)
        _sync_profile_collection(profile_id, "user_experiences", prepared)
    if education_payload is not None:
        prepared = _prepare_education_rows(education_payload if isinstance(education_payload, list) else None)
        _sync_profile_collection(profile_id, "user_education", prepared)
    if certifications_payload is not None:
        prepared = _prepare_certification_rows(certifications_payload if isinstance(certifications_payload, list) else None)
        _sync_profile_collection(profile_id, "user_certifications", prepared)
    if projects_payload is not None:
        prepared = _prepare_project_rows(projects_payload if isinstance(projects_payload, list) else None)
        _sync_profile_collection(profile_id, "user_portfolio_projects", prepared)
    if publications_payload is not None:
        prepared = _prepare_publication_rows(publications_payload if isinstance(publications_payload, list) else None)
        _sync_profile_collection(profile_id, "user_publications", prepared)
    return {"updated": True}


def _storage_get_client(svc_client):
    storage = getattr(svc_client, "storage", None)
    if callable(storage):
        storage = storage()
    if storage is None:
        raise HTTPException(status_code=500, detail="Storage client unavailable")
    return storage


def _storage_upload_bytes(svc_client, bucket: str, dest: str, content: bytes, content_type: Optional[str] = None) -> str:
    storage = _storage_get_client(svc_client)
    last_err = None
    result = None
    attempts = []
    # Attempt 1: modern signature
    try:
        attempts.append("upload(dest, bytes, opts dict upsert)")
        result = storage.from_(bucket).upload(
            dest,
            content,
            {
                "content-type": content_type or "application/octet-stream",
                "upsert": "true",
                "cache-control": "86400",
            },
        )
    except Exception as exc:
        last_err = exc
        result = None
    # Attempt 2: keyword args path/file
    if result is None:
        try:
            attempts.append("upload(path=dest, file=bytes)")
            result = storage.from_(bucket).upload(path=dest, file=content)
        except Exception as exc:
            last_err = exc
            result = None
    # Attempt 3: wrap bytes in BytesIO
    if result is None:
        import io as _io
        try:
            attempts.append("upload(path=dest, file=BytesIO)")
            result = storage.from_(bucket).upload(path=dest, file=_io.BytesIO(content))
        except Exception as exc:
            last_err = exc
            result = None
    # Attempt 4: raw simple call (legacy)
    if result is None:
        try:
            attempts.append("upload(dest, bytes)")
            result = storage.from_(bucket).upload(dest, content)
        except Exception as exc:
            last_err = exc
            result = None
    if result is None:
        # Fallback: attempt a remove then a plain upload (handles 409 duplicate when upsert unsupported)
        try:
            attempts.append("remove+reupload fallback")
            try:
                storage.from_(bucket).remove([dest])
            except Exception:
                pass
            result = storage.from_(bucket).upload(dest, content, {"content-type": content_type or "application/octet-stream"})
        except Exception as exc:
            last_err = exc
            supabase_logger.error("All upload attempts failed", extra={"bucket": bucket, "dest": dest, "attempts": attempts, "error": str(last_err)})
            raise HTTPException(status_code=500, detail=f"Upload failed: {last_err}")
    if result is None:
        supabase_logger.exception("Upload failed: %s", last_err)
        raise HTTPException(status_code=500, detail=f"Upload failed: {last_err}")
    if isinstance(result, dict) and result.get("error"):
        raise HTTPException(status_code=500, detail=f"Upload error: {result.get('error')}")
    if getattr(result, "error", None):
        raise HTTPException(status_code=500, detail=f"Upload error: {result.error}")
    return dest


def _storage_public_url(svc_client, bucket: str, path: str) -> str:
    base_url = os.getenv("SUPABASE_URL", "").rstrip("/")
    storage = _storage_get_client(svc_client)
    try:
        res = storage.from_(bucket).get_public_url(path)
        if isinstance(res, dict):
            cand = res.get("publicUrl") or res.get("publicURL") or (res.get("data") or {}).get("publicUrl")
            if cand:
                return cand
        elif isinstance(res, str):
            return res
        else:
            for attr in ("public_url", "publicUrl", "publicURL"):
                if hasattr(res, attr):
                    val = getattr(res, attr)
                    if isinstance(val, str):
                        return val
    except Exception:
        pass
    encoded = quote(path, safe="")
    return f"{base_url}/storage/v1/object/public/{bucket}/{encoded}"


def _read_upload_bytes(file_obj) -> bytes:
    try:
        file_obj.file.seek(0)
        return file_obj.file.read()
    except Exception:
        try:
            return file_obj.read()
        except Exception:
            return b""



def _upload_profile_asset(token: Optional[str], kind: str, file):
    from uuid import uuid4

    # Ensure a profile exists (creates a minimal one for OAuth users)
    _, profile_id = _ensure_user_and_profile(token)
    supabase = get_service_client()
    bucket = os.getenv("SUPABASE_BUCKET", "").strip()
    if not bucket:
        raise HTTPException(status_code=500, detail="Missing SUPABASE_BUCKET in environment")

    filename = file.filename or ("upload-" + uuid4().hex)
    ext = os.path.splitext(filename)[1].lower()
    media_kind = (kind or "").strip().lower()
    if media_kind == "image" and ext not in {".png", ".jpg", ".jpeg", ".webp"}:
        raise HTTPException(status_code=400, detail="Invalid image type")
    if media_kind == "resume" and ext not in {".pdf", ".doc", ".docx"}:
        raise HTTPException(status_code=400, detail="Invalid resume type")

    blob = _read_upload_bytes(file)
    if not blob:
        raise HTTPException(status_code=400, detail="Empty upload")

    safe_ext = ext if ext else (".png" if media_kind == "image" else ".pdf")
    dest_folder = f"profiles/{profile_id}"
    dest = f"{dest_folder}/{uuid4().hex}{safe_ext}"
    _storage_upload_bytes(supabase, bucket, dest, blob, file.content_type)
    public_url = _storage_public_url(supabase, bucket, dest)

    column = "profile_image_url" if media_kind == "image" else "resume_url"
    upd = supabase.table("user_profiles").update({column: public_url}).eq("id", profile_id).execute()
    if getattr(upd, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (update profile asset): {upd.error}")

    return {"kind": media_kind, "url": public_url, "path": dest}




def get_completed_topic_ids(token: Optional[str]):
    # Ensure a profile exists to scope progress correctly for new OAuth users
    _, profile_id = _ensure_user_and_profile(token)
    supabase = get_service_client()
    try:
        q = _supabase_retry(
            lambda: (
                supabase.table("user_topic_progress")
                .select("topic_id")
                .eq("user_profile_id", profile_id)
                .execute()
            )
        )
    except HTTPXRemoteProtocolError as exc:  # pragma: no cover - network dependent
        supabase_logger.warning("Progress lookup transient protocol error: %s", exc)
        raise HTTPException(status_code=503, detail="Upstream temporarily unavailable. Please retry.")
    if getattr(q, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (get progress): {q.error}")
    return {"completed_topic_ids": [row["topic_id"] for row in (q.data or [])]}


def toggle_topic_completion(token: Optional[str], topic_id: uuid.UUID, completed: bool):
    # Ensure a profile exists for new OAuth users
    _, profile_id = _ensure_user_and_profile(token)
    supabase = get_service_client()
    if completed:
        # Use upsert (idempotent) or gracefully ignore duplicate key errors.
        try:
            # Prefer upsert if available for idempotency
            upsert_method = getattr(supabase.table("user_topic_progress"), "upsert", None)
            if callable(upsert_method):
                resp = upsert_method({"user_profile_id": profile_id, "topic_id": str(topic_id)}).execute()
                if getattr(resp, "error", None):
                    raise HTTPException(status_code=500, detail=f"Supabase error (mark done): {resp.error}")
            else:
                resp = (
                    supabase.table("user_topic_progress")
                    .insert({"user_profile_id": profile_id, "topic_id": str(topic_id)})
                    .execute()
                )
                if getattr(resp, "error", None):
                    txt = str(resp.error).lower()
                    if "duplicate" not in txt and "unique" not in txt:
                        raise HTTPException(status_code=500, detail=f"Supabase error (mark done): {resp.error}")
        except Exception as exc:
            # Allow duplicate insert races silently
            msg = str(getattr(exc, "detail", exc))
            if not ("duplicate" in msg.lower() or "unique" in msg.lower()):
                raise
        return {"completed": True}
    else:
        resp = (
            supabase.table("user_topic_progress")
            .delete()
            .eq("user_profile_id", profile_id)
            .eq("topic_id", str(topic_id))
            .execute()
        )
        if getattr(resp, "error", None):
            raise HTTPException(status_code=500, detail=f"Supabase error (unmark done): {resp.error}")
        return {"completed": False}


def get_progress_summary(token: Optional[str]):
    _, profile_id = _require_user_and_profile(token)
    supabase = get_service_client()

    # Fetch per-course -> per-unit totals and completed counts
    # Get current user's batch/semester first to scope courses
    prof_q = (
        supabase.table("user_profiles").select("batch_id,semester").eq("id", profile_id).limit(1).execute()
    )
    if getattr(prof_q, "error", None) or not prof_q.data:
        raise HTTPException(status_code=500, detail=f"Supabase error (profile scope): {getattr(prof_q, 'error', None)}")
    batch_id = prof_q.data[0].get("batch_id")
    semester = prof_q.data[0].get("semester")

    courses_q = (
        supabase.table("syllabus_courses")
        .select("id,course_code,title,semester")
        .eq("batch_id", batch_id)
        .eq("semester", semester)
        .execute()
    )
    if getattr(courses_q, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (courses for progress): {courses_q.error}")

    # Completed set for quick lookup
    comp_q = (
        supabase.table("user_topic_progress")
        .select("topic_id")
        .eq("user_profile_id", profile_id)
        .execute()
    )
    if getattr(comp_q, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (completed topics): {comp_q.error}")
    completed_set = {row["topic_id"] for row in (comp_q.data or [])}

    summary = []
    for c in (courses_q.data or []):
        course_id = c["id"]
        units_q = (
            supabase.table("syllabus_units")
            .select("id,unit_title,order_in_course")
            .eq("course_id", course_id)
            .order("order_in_course")
            .execute()
        )
        if getattr(units_q, "error", None):
            raise HTTPException(status_code=500, detail=f"Supabase error (units for progress): {units_q.error}")
        unit_summaries = []
        course_total = 0
        course_done = 0
        for u in (units_q.data or []):
            topics_q = (
                supabase.table("syllabus_topics")
                .select("id")
                .eq("unit_id", u["id"])
                .order("order_in_unit")
                .execute()
            )
            if getattr(topics_q, "error", None):
                raise HTTPException(status_code=500, detail=f"Supabase error (topics for progress): {topics_q.error}")
            topic_ids = [t["id"] for t in (topics_q.data or [])]
            done = sum(1 for tid in topic_ids if tid in completed_set)
            total = len(topic_ids)
            unit_summaries.append({
                "unit_id": u["id"],
                "unit_title": u["unit_title"],
                "done": done,
                "total": total,
            })
            course_done += done
            course_total += total
        summary.append({
            "course_id": course_id,
            "course_code": c["course_code"],
            "title": c["title"],
            "done": course_done,
            "total": course_total,
            "units": unit_summaries,
        })

    return {"courses": summary}


# --- Projects & collaboration API ------------------------------------------


def _listify(value: Any) -> List[Any]:
    if isinstance(value, list):
        return value
    if value in (None, ""):
        return []
    return [value]


class ProjectBasics(BaseModel):
    title: str
    tagline: str
    domains: List[str] = Field(default_factory=list)
    description: str
    tech_stack: List[str] = Field(default_factory=list)


class ProjectStatus(BaseModel):
    status: str = ""
    start_date: Optional[str] = None
    end_date: Optional[str] = None
    milestones: List[str] = Field(default_factory=list)


class ProjectLinks(BaseModel):
    github: Optional[str] = None
    demo: Optional[str] = None
    video: Optional[str] = None
    docs: Optional[str] = None


class ProjectFunding(BaseModel):
    stage: Optional[str] = None
    budget_inr: Optional[int] = None
    use: Optional[str] = None


class ProjectTeam(BaseModel):
    members: List[str] = Field(default_factory=list)
    roles_hiring: List[str] = Field(default_factory=list)
    compensation: Optional[str] = None
    hours: Optional[str] = None
    role_desc: Optional[str] = None


class ProjectIn(BaseModel):
    basics: ProjectBasics
    status: ProjectStatus
    links: ProjectLinks
    funding: ProjectFunding
    team: ProjectTeam


class ProjectOut(ProjectIn):
    id: str
    user_id: str
    cover_url: Optional[str] = None
    gallery_urls: List[str] = Field(default_factory=list)
    created_at: Optional[str] = None
    updated_at: Optional[str] = None


class ProjectApplicationIn(BaseModel):
    message: Optional[str] = None


class ProjectApplicationOut(BaseModel):
    id: str
    project_id: str
    applicant_user_id: str
    message: Optional[str] = None
    status: str
    created_at: Optional[str] = None
    updated_at: Optional[str] = None


class ProjectApplicationUpdateIn(BaseModel):
    status: str = Field(..., pattern=r"^(pending|accepted|rejected)$")

class CollabMessageIn(BaseModel):
    content: str


class CollabMessageOut(BaseModel):
    id: str
    application_id: str
    sender_user_id: str
    content: str
    created_at: Optional[str] = None


class SkillTestStartIn(BaseModel):
    skill: str


class SkillTestStartOut(BaseModel):
    session_id: str
    skill: str
    questions: List[Dict[str, Any]]


class SkillAnswer(BaseModel):
    question_id: str
    response: str


class SkillTestSubmitIn(BaseModel):
    answers: List[SkillAnswer]


class SkillVerificationOut(BaseModel):
    skill: str
    best_score: Optional[float] = None
    attempts: int = 0
    status: Optional[str] = None
    updated_at: Optional[str] = None



def _project_payload_from_body(body: ProjectIn, user_id: str) -> dict:
    basics = body.basics
    status = body.status
    links = body.links
    funding = body.funding
    team = body.team

    def clean_list(items: Optional[List[str]]) -> List[str]:
        return [item.strip() for item in (items or []) if isinstance(item, str) and item.strip()]

    payload = {
        "user_id": user_id,
        "title": (basics.title or "").strip(),
        "tagline": (basics.tagline or "").strip(),
        "domains": clean_list(basics.domains),
        "description": (basics.description or "").strip(),
        "tech_stack": clean_list(basics.tech_stack),
        "proj_status": (status.status or "").strip(),
        "start_date": status.start_date or None,
        "end_date": status.end_date or None,
        "milestones": clean_list(status.milestones),
        "github": (links.github or None),
        "demo": (links.demo or None),
        "video": (links.video or None),
        "docs": (links.docs or None),
        "fund_stage": (funding.stage or None),
        "fund_budget_inr": int(funding.budget_inr) if funding.budget_inr is not None else None,
        "fund_use": (funding.use or None),
        "team_members": clean_list(team.members),
        "roles_hiring": clean_list(team.roles_hiring),
        "compensation": (team.compensation or None),
        "hours": (team.hours or None),
        "role_desc": (team.role_desc or None),
    }
    return payload


def _project_row_to_out(row: dict) -> dict:
    if not row:
        raise HTTPException(status_code=500, detail="Project payload missing")
    basics = ProjectBasics(
        title=row.get("title") or "",
        tagline=row.get("tagline") or "",
        domains=_listify(row.get("domains")),
        description=row.get("description") or "",
        tech_stack=_listify(row.get("tech_stack")),
    )
    status = ProjectStatus(
        status=row.get("proj_status") or "",
        start_date=row.get("start_date"),
        end_date=row.get("end_date"),
        milestones=_listify(row.get("milestones")),
    )
    links = ProjectLinks(
        github=row.get("github"),
        demo=row.get("demo"),
        video=row.get("video"),
        docs=row.get("docs"),
    )
    funding = ProjectFunding(
        stage=row.get("fund_stage"),
        budget_inr=row.get("fund_budget_inr"),
        use=row.get("fund_use"),
    )
    team = ProjectTeam(
        members=_listify(row.get("team_members")),
        roles_hiring=_listify(row.get("roles_hiring")),
        compensation=row.get("compensation"),
        hours=row.get("hours"),
        role_desc=row.get("role_desc"),
    )
    project = ProjectOut(
        basics=basics,
        status=status,
        links=links,
        funding=funding,
        team=team,
        id=str(row.get("id")),
        user_id=str(row.get("user_id")),
        cover_url=row.get("cover_url"),
        gallery_urls=_listify(row.get("gallery_urls")),
        created_at=row.get("created_at"),
        updated_at=row.get("updated_at"),
    )
    return project.dict()


def _application_row_to_out(row: dict) -> dict:
    app = ProjectApplicationOut(
        id=str(row.get("id")),
        project_id=str(row.get("project_id")),
        applicant_user_id=str(row.get("applicant_user_id")),
        message=row.get("message"),
        status=str(row.get("status") or "pending"),
        created_at=row.get("created_at"),
        updated_at=row.get("updated_at"),
    )
    return app.dict()


def _ensure_project_exists(project_id: str) -> dict:
    supabase = get_service_client()
    res = supabase.table(PROJECTS_TABLE).select("*").eq("id", project_id).single().execute()
    if getattr(res, "error", None):
        raise HTTPException(status_code=400, detail=f"Supabase error: {res.error}")
    data = getattr(res, "data", None)
    if not data:
        raise HTTPException(status_code=404, detail="Project not found")
    return data


def _ensure_owner(project_id: str, user_id: str) -> dict:
    project = _ensure_project_exists(project_id)
    if str(project.get("user_id")) != str(user_id):
        raise HTTPException(status_code=403, detail="Not your project")
    return project


def _normalize_skill(skill: str) -> str:
    return (skill or "").strip()


def _fallback_language_for_skill(skill: str) -> str:
    low = (skill or "").lower()
    if any(key in low for key in ("python", "pandas", "django")):
        return "python"
    if any(key in low for key in ("javascript", "react", "node")):
        return "javascript"
    if "java" in low:
        return "java"
    if "c++" in low or "cpp" in low:
        return "cpp"
    if "sql" in low:
        return "sql"
    return "python"


def _fallback_questions(skill: str) -> List[Dict[str, Any]]:
    language = _fallback_language_for_skill(skill)
    base = skill or "the skill"
    skill_lower = (skill or "skill").lower()
    keywords_map = {
        "python": ["def", "list", "dict"],
        "javascript": ["function", "const", "array"],
        "java": ["class", "public", "method"],
        "cpp": ["vector", "std", "loop"],
        "sql": ["select", "where", "join"],
    }
    keywords = keywords_map.get(language, [skill_lower, "project", "team"])
    qid_prefix = uuid.uuid4().hex
    return [
        {
            "id": f"{qid_prefix}-mc",
            "kind": "ceq",
            "prompt": f"Which option best captures a practical use-case of {base}?",
            "options": [
                "A. Documenting theory without implementation",
                "B. Building or iterating on a real project with measurable outcomes",
                "C. Collecting inspirational quotes",
                "D. Focusing only on certifications",
            ],
            "answer_key": {"correct_option": "b"},
        },
        {
            "id": f"{qid_prefix}-code",
            "kind": "coding",
            "language": language,
            "prompt": f"Write a short {language} snippet that demonstrates how you would track progress while learning {base}. Include a test or printed output.",
            "answer_key": {"keywords": keywords, "min_hits": max(2, len(keywords) - 1)},
        },
        {
            "id": f"{qid_prefix}-reflect",
            "kind": "coding",
            "language": language,
            "prompt": f"Describe in {language} (code or structured comments) how you would onboard a collaborator to your {base} project, mentioning tools, communication, and deliverables.",
            "answer_key": {"keywords": ["plan", "deliverable", "feedback", base.lower()], "min_hits": 2},
        },
    ]


def _public_questions(questions: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    out: List[Dict[str, Any]] = []
    for q in questions:
        out.append({k: v for k, v in q.items() if k != "answer_key"})
    return out


def _grade_skill_answers(questions: List[Dict[str, Any]], answers: Dict[str, str]) -> Dict[str, Any]:
    details: List[Dict[str, Any]] = []
    total = 0.0
    counted = 0
    for question in questions:
        qid = question.get("id")
        if not qid:
            continue
        kind = question.get("kind") or ""
        response = (answers.get(qid) or "").strip()
        info: Dict[str, Any] = {"question_id": qid, "kind": kind, "score": 0.0}
        if not response:
            info["feedback"] = "No answer provided."
            details.append(info)
            continue
        score = 0.0
        if kind == "ceq":
            expected = (question.get("answer_key") or {}).get("correct_option", "").strip().lower()
            if expected and response.lower().startswith(expected):
                score = 100.0
                info["feedback"] = "Correct option."
            else:
                info["feedback"] = f"Expected option {expected.upper() or '?'}"
        else:
            key = (question.get("answer_key") or {})
            keywords = key.get("keywords") if isinstance(key, dict) else None
            if keywords:
                text_lower = response.lower()
                hits = sum(1 for kw in keywords if isinstance(kw, str) and kw.lower() in text_lower)
                min_hits = key.get("min_hits") or len(keywords)
                score = min(100.0, (hits / max(1, min_hits)) * 100.0)
                info["matched_keywords"] = hits
                info["keywords"] = keywords
                info["feedback"] = f"Matched {hits} of {len(keywords)} keywords."
            else:
                score = 50.0
                info["feedback"] = "Heuristic score (no rubric)."
        score = max(0.0, min(100.0, score))
        info["score"] = round(score, 2)
        details.append(info)
        total += score
        counted += 1
    final_score = round(total / counted, 2) if counted else 0.0
    status = "verified" if final_score >= 70 else "needs_review"
    return {"score": final_score, "status": status, "details": details}


def _insert_skill_test_session(user_id: str, skill: str, questions: List[Dict[str, Any]]) -> str:
    supabase = get_service_client()
    res = supabase.table(SKILL_TESTS_TABLE).insert({
        "user_id": user_id,
        "skill": skill,
        "questions": questions,
        "status": "active",
    }).execute()
    data = getattr(res, "data", None)
    if isinstance(data, list) and data:
        return data[0].get("id")
    if isinstance(data, dict) and data.get("id"):
        return data.get("id")
    raise HTTPException(status_code=500, detail="Unable to create test session")


def _get_skill_test_session(session_id: str) -> Optional[dict]:
    supabase = get_service_client()
    res = supabase.table(SKILL_TESTS_TABLE).select("*").eq("id", session_id).single().execute()
    if getattr(res, "error", None):
        return None
    return getattr(res, "data", None)


def _update_skill_test_submission(session_id: str, result: Dict[str, Any]):
    supabase = get_service_client()
    supabase.table(SKILL_TESTS_TABLE).update({
        "status": "completed",
        "score": result.get("score"),
        "result": result,
        "submitted_at": datetime.utcnow().isoformat() + "Z",
    }).eq("id", session_id).execute()


def _upsert_skill_verification(user_id: str, skill: str, score: float):
    supabase = get_service_client()
    payload = {
        "user_id": user_id,
        "skill": skill,
        "best_score": score,
        "attempts": 1,
        "status": "verified" if score >= 70 else "needs_review",
    }
    try:
        supabase.table(SKILL_VERIFICATIONS_TABLE).upsert(payload, on_conflict="user_id,skill").execute()
    except Exception:
        existing = supabase.table(SKILL_VERIFICATIONS_TABLE).select("best_score,attempts").eq("user_id", user_id).eq("skill", skill).single().execute()
        row = getattr(existing, "data", None)
        if row:
            best = max(float(row.get("best_score") or 0.0), score)
            attempts = int(row.get("attempts") or 0) + 1
            supabase.table(SKILL_VERIFICATIONS_TABLE).update({
                "best_score": best,
                "attempts": attempts,
                "status": "verified" if best >= 70 else "needs_review",
            }).eq("user_id", user_id).eq("skill", skill).execute()
        else:
            supabase.table(SKILL_VERIFICATIONS_TABLE).insert(payload).execute()


def _recompute_profile_verification_score(user_id: str):
    supabase = get_service_client()
    res = supabase.table(SKILL_VERIFICATIONS_TABLE).select("best_score").eq("user_id", user_id).execute()
    if getattr(res, "error", None):
        return
    scores = [float(row.get("best_score") or 0.0) for row in (res.data or []) if row]
    if not scores:
        agg = 0
    else:
        top = sorted(scores, reverse=True)[:3]
        agg = round(sum(top) / len(top))
    try:
        supabase.table("user_profiles").update({"verification_score": agg}).eq("auth_user_id", user_id).execute()
    except Exception:
        pass



projects_router = APIRouter()


@projects_router.post("/api/projects", response_model=ProjectOut)
def create_project(body: ProjectIn, authorization: Optional[str] = Header(default=None)):
    token = _parse_bearer_token(authorization)
    user_id, _ = _require_user_and_profile(token)
    supabase = get_service_client()
    payload = _project_payload_from_body(body, user_id)
    res = supabase.table(PROJECTS_TABLE).insert(payload).execute()
    if getattr(res, "error", None):
        raise HTTPException(status_code=400, detail=f"Supabase error: {res.error}")
    data = getattr(res, "data", None)
    if isinstance(data, list) and data:
        return _project_row_to_out(data[0])
    if isinstance(data, dict):
        return _project_row_to_out(data)
    raise HTTPException(status_code=500, detail="Unexpected insert response")


@projects_router.get("/api/projects", response_model=List[ProjectOut])
def list_projects(limit: int = 50):
    supabase = get_service_client()
    res = (
        supabase.table(PROJECTS_TABLE)
        .select("*")
        .order("created_at", desc=True)
        .limit(max(1, min(200, limit)))
        .execute()
    )
    if getattr(res, "error", None):
        raise HTTPException(status_code=400, detail=f"Supabase error: {res.error}")
    rows = getattr(res, "data", []) or []
    return [_project_row_to_out(row) for row in rows]


@projects_router.get("/api/projects/{project_id}", response_model=ProjectOut)
def get_project(project_id: str):
    row = _ensure_project_exists(project_id)
    return _project_row_to_out(row)


@projects_router.post("/api/projects/{project_id}/upload")
async def upload_project_media(
    project_id: str,
    kind: str = Form(...),
    file: UploadFile = File(...),
    authorization: Optional[str] = Header(default=None),
):
    token = _parse_bearer_token(authorization)
    user_id, _ = _require_user_and_profile(token)
    if not PROJECTS_BUCKET:
        raise HTTPException(status_code=500, detail="Missing SUPABASE_PROJECTS_BUCKET or SUPABASE_BUCKET")

    project = _ensure_owner(project_id, user_id)
    media_kind = (kind or "").strip().lower()
    if media_kind not in PROJECT_MEDIA_EXTENSIONS:
        raise HTTPException(status_code=400, detail="Unsupported media kind")

    extension = os.path.splitext(file.filename or "")[1].lower()
    allowed = PROJECT_MEDIA_EXTENSIONS[media_kind]
    if extension and allowed and extension not in allowed:
        raise HTTPException(status_code=400, detail="File type not allowed")

    blob = await file.read()
    if not blob:
        raise HTTPException(status_code=400, detail="Empty upload")

    safe_ext = extension or (".png" if media_kind == "cover" else ".bin")
    storage_path = f"project_assets/{user_id}/{project_id}/{media_kind}_{int(time.time())}{safe_ext}"
    supabase = get_service_client()
    _storage_upload_bytes(supabase, PROJECTS_BUCKET, storage_path, blob, file.content_type)
    public_url = _storage_public_url(supabase, PROJECTS_BUCKET, storage_path)

    update: Dict[str, Any]
    if (media_kind == "cover"):
        update = {"cover_url": public_url}
    else:
        gallery = _listify(project.get("gallery_urls"))
        gallery.append(public_url)
        update = {"gallery_urls": gallery}

    supabase.table(PROJECTS_TABLE).update(update).eq("id", project_id).execute()
    refreshed = _ensure_project_exists(project_id)
    return {
        "project": _project_row_to_out(refreshed),
        "uploaded": {"kind": media_kind, "url": public_url, "path": storage_path},
    }


@projects_router.post("/api/projects/{project_id}/apply")
def apply_to_project(project_id: str, body: ProjectApplicationIn = None, authorization: Optional[str] = Header(default=None)):
    token = _parse_bearer_token(authorization)
    user_id, _ = _require_user_and_profile(token)
    project = _ensure_project_exists(project_id)
    if str(project.get("user_id")) == str(user_id):
        raise HTTPException(status_code=400, detail="You cannot apply to your own project")

    supabase = get_service_client()
    existing = (
        supabase.table(PROJECT_APPLICATIONS_TABLE)
        .select("*")
        .eq("project_id", project_id)
        .eq("applicant_user_id", user_id)
        .limit(1)
        .execute()
    )
    rows = getattr(existing, "data", []) or []
    if rows:
        return {"ok": True, "status": rows[0].get("status") or "pending"}

    payload = {
        "project_id": project_id,
        "applicant_user_id": user_id,
        "message": ((body.message if body else None) or None),
        "status": "pending",
    }
    res = supabase.table(PROJECT_APPLICATIONS_TABLE).insert(payload).execute()
    if getattr(res, "error", None):
        msg = str(res.error).lower()
        if "duplicate" in msg or "unique" in msg:
            return {"ok": True}
        raise HTTPException(status_code=400, detail=f"Supabase error: {res.error}")
    data = getattr(res, "data", None)
    if isinstance(data, list) and data:
        return {"ok": True, "status": data[0].get("status") or "pending"}
    return {"ok": True}


@projects_router.get("/api/projects/{project_id}/applications", response_model=List[ProjectApplicationOut])
def list_project_applications(project_id: str, authorization: Optional[str] = Header(default=None)):
    token = _parse_bearer_token(authorization)
    user_id, _ = _require_user_and_profile(token)
    _ensure_owner(project_id, user_id)
    supabase = get_service_client()
    res = (
        supabase.table(PROJECT_APPLICATIONS_TABLE)
        .select("*")
        .eq("project_id", project_id)
        .order("created_at", desc=True)
        .limit(1000)
        .execute()
    )
    if getattr(res, "error", None):
        raise HTTPException(status_code=400, detail=f"Supabase error: {res.error}")
    rows = getattr(res, "data", []) or []
    return [_application_row_to_out(row) for row in rows]


@projects_router.get("/api/projects/{project_id}/applications/me")
def get_my_project_application(project_id: str, authorization: Optional[str] = Header(default=None)):
    token = _parse_bearer_token(authorization)
    user_id, _ = _require_user_and_profile(token)
    supabase = get_service_client()
    res = (
        supabase.table(PROJECT_APPLICATIONS_TABLE)
        .select("*")
        .eq("project_id", project_id)
        .eq("applicant_user_id", user_id)
        .limit(1)
        .execute()
    )
    rows = getattr(res, "data", []) or []
    if not rows:
        return {"applied": False}
    return {"applied": True, "application": _application_row_to_out(rows[0])}


@projects_router.get("/api/projects/{project_id}/applications/{application_id}", response_model=ProjectApplicationOut)
def get_project_application(project_id: str, application_id: str, authorization: Optional[str] = Header(default=None)):
    token = _parse_bearer_token(authorization)
    user_id, _ = _require_user_and_profile(token)
    _ensure_owner(project_id, user_id)
    supabase = get_service_client()
    res = (
        supabase.table(PROJECT_APPLICATIONS_TABLE)
        .select("*")
        .eq("id", application_id)
        .eq("project_id", project_id)
        .single()
        .execute()
    )
    if getattr(res, "error", None):
        raise HTTPException(status_code=400, detail=f"Supabase error: {res.error}")
    data = getattr(res, "data", None)
    if not data:
        raise HTTPException(status_code=404, detail="Application not found")
    return _application_row_to_out(data)


@projects_router.patch("/api/projects/{project_id}/applications/{application_id}")
def update_project_application(project_id: str, application_id: str, body: ProjectApplicationUpdateIn, authorization: Optional[str] = Header(default=None)):
    token = _parse_bearer_token(authorization)
    user_id, _ = _require_user_and_profile(token)
    _ensure_owner(project_id, user_id)
    supabase = get_service_client()
    supabase.table(PROJECT_APPLICATIONS_TABLE).update({"status": body.status}).eq("id", application_id).eq("project_id", project_id).execute()
    res = (
        supabase.table(PROJECT_APPLICATIONS_TABLE)
        .select("*")
        .eq("id", application_id)
        .eq("project_id", project_id)
        .single()
        .execute()
    )
    if getattr(res, "error", None):
        raise HTTPException(status_code=400, detail=f"Supabase error: {res.error}")
    row = getattr(res, "data", None)
    if not row:
        raise HTTPException(status_code=404, detail="Application not found")
    return _application_row_to_out(row)


@projects_router.get("/api/applications/incoming")
def list_incoming_applications(authorization: Optional[str] = Header(default=None)):
    token = _parse_bearer_token(authorization)
    user_id, _ = _require_user_and_profile(token)
    supabase = get_service_client()
    projects_res = supabase.table(PROJECTS_TABLE).select("id,title,cover_url").eq("user_id", user_id).limit(1000).execute()
    if getattr(projects_res, "error", None):
        raise HTTPException(status_code=400, detail=f"Supabase error: {projects_res.error}")
    projects = getattr(projects_res, "data", []) or []
    project_ids = [p.get("id") for p in projects if p and p.get("id")]
    if not project_ids:
        return {"applications": [], "projects": []}
    apps_res = (
        supabase.table(PROJECT_APPLICATIONS_TABLE)
        .select("*")
        .in_("project_id", project_ids)
        .order("created_at", desc=True)
        .limit(2000)
        .execute()
    )
    if getattr(apps_res, "error", None):
        raise HTTPException(status_code=400, detail=f"Supabase error: {apps_res.error}")
    apps = getattr(apps_res, "data", []) or []
    return {
        "applications": [_application_row_to_out(row) for row in apps],
        "projects": [{"id": p.get("id"), "title": p.get("title"), "cover_url": p.get("cover_url")} for p in projects if p],
    }


@projects_router.get("/api/applications/mine")
def list_my_applications(authorization: Optional[str] = Header(default=None)):
    token = _parse_bearer_token(authorization)
    user_id, _ = _require_user_and_profile(token)
    supabase = get_service_client()
    apps_res = (
        supabase.table(PROJECT_APPLICATIONS_TABLE)
        .select("*")
        .eq("applicant_user_id", user_id)
        .order("created_at", desc=True)
        .limit(2000)
        .execute()
    )
    if getattr(apps_res, "error", None):
        raise HTTPException(status_code=400, detail=f"Supabase error: {apps_res.error}")
    apps = getattr(apps_res, "data", []) or []
    if not apps:
        return {"applications": [], "projects": []}
    project_ids = list({row.get("project_id") for row in apps if row and row.get("project_id")})
    projects = []
    if project_ids:
        proj_res = (
            supabase.table(PROJECTS_TABLE)
            .select("id,title,cover_url")
            .in_("id", project_ids)
            .limit(2000)
            .execute()
        )
        if getattr(proj_res, "data", None):
            projects = [{"id": r.get("id"), "title": r.get("title"), "cover_url": r.get("cover_url")} for r in proj_res.data if r]
    return {
        "applications": [_application_row_to_out(row) for row in apps],
        "projects": projects,
    }


@projects_router.get("/api/applications/{application_id}")
def get_application_by_id(application_id: str, authorization: Optional[str] = Header(default=None)):
    token = _parse_bearer_token(authorization)
    user_id, _ = _require_user_and_profile(token)
    supabase = get_service_client()
    res = supabase.table(PROJECT_APPLICATIONS_TABLE).select("*").eq("id", application_id).single().execute()
    if getattr(res, "error", None):
        raise HTTPException(status_code=400, detail=f"Supabase error: {res.error}")
    row = getattr(res, "data", None)
    if not row:
        raise HTTPException(status_code=404, detail="Application not found")
    project = _ensure_project_exists(str(row.get("project_id")))
    if str(row.get("applicant_user_id")) != str(user_id) and str(project.get("user_id")) != str(user_id):
        raise HTTPException(status_code=403, detail="Not permitted")
    return _application_row_to_out(row)


@projects_router.get("/api/collab/{application_id}/messages")
def list_collab_messages(application_id: str, authorization: Optional[str] = Header(default=None)):
    token = _parse_bearer_token(authorization)
    user_id, _ = _require_user_and_profile(token)
    supabase = get_service_client()
    app_res = supabase.table(PROJECT_APPLICATIONS_TABLE).select("*").eq("id", application_id).single().execute()
    if getattr(app_res, "error", None):
        raise HTTPException(status_code=400, detail=f"Supabase error: {app_res.error}")
    app_row = getattr(app_res, "data", None)
    if not app_row:
        raise HTTPException(status_code=404, detail="Application not found")
    project = _ensure_project_exists(str(app_row.get("project_id")))
    if str(app_row.get("status", "")).lower() != "accepted":
        raise HTTPException(status_code=400, detail="Collaboration opens after acceptance")
    if str(app_row.get("applicant_user_id")) != str(user_id) and str(project.get("user_id")) != str(user_id):
        raise HTTPException(status_code=403, detail="Not permitted")
    msgs_res = (
        supabase.table(PROJECT_COLLAB_TABLE)
        .select("*")
        .eq("application_id", application_id)
        .order("created_at")
        .limit(500)
        .execute()
    )
    if getattr(msgs_res, "error", None):
        raise HTTPException(status_code=400, detail=f"Supabase error: {msgs_res.error}")
    msgs = getattr(msgs_res, "data", []) or []
    return {"messages": msgs}


@projects_router.post("/api/collab/{application_id}/messages")
def send_collab_message(application_id: str, body: CollabMessageIn, authorization: Optional[str] = Header(default=None)):
    token = _parse_bearer_token(authorization)
    user_id, _ = _require_user_and_profile(token)
    content = (body.content or "").strip()
    if not content:
        raise HTTPException(status_code=400, detail="Message content required")
    supabase = get_service_client()
    app_res = supabase.table(PROJECT_APPLICATIONS_TABLE).select("*").eq("id", application_id).single().execute()
    if getattr(app_res, "error", None):
        raise HTTPException(status_code=400, detail=f"Supabase error: {app_res.error}")
    app_row = getattr(app_res, "data", None)
    if not app_row:
        raise HTTPException(status_code=404, detail="Application not found")
    project = _ensure_project_exists(str(app_row.get("project_id")))
    if str(app_row.get("status", "")).lower() != "accepted":
        raise HTTPException(status_code=400, detail="Collaboration opens after acceptance")
    if str(app_row.get("applicant_user_id")) != str(user_id) and str(project.get("user_id")) != str(user_id):
        raise HTTPException(status_code=403, detail="Not permitted")
    res = supabase.table(PROJECT_COLLAB_TABLE).insert({
        "application_id": application_id,
        "sender_user_id": user_id,
        "content": content,
    }).execute()
    if getattr(res, "error", None):
        raise HTTPException(status_code=400, detail=f"Supabase error: {res.error}")
    data = getattr(res, "data", None)
    row = data[0] if isinstance(data, list) and data else data
    return {"ok": True, "message": row}


@projects_router.get("/api/public/profiles/{user_id}")
def get_public_profile(user_id: str):
    supabase = get_service_client()
    res = (
        supabase.table("user_profiles")
        .select(
            "auth_user_id,name,profile_image_url,bio,headline,location,linkedin,github,leetcode,technologies,skills,certifications,languages,interests,project_info,publications,achievements,experience,verification_score"
        )
        .eq("auth_user_id", user_id)
        .limit(1)
        .execute()
    )
    if getattr(res, "error", None):
        raise HTTPException(status_code=400, detail=f"Supabase error: {res.error}")
    if not res.data:
        raise HTTPException(status_code=404, detail="Profile not found")
    row = dict(res.data[0])
    profile_id = row.get("id")
    if profile_id:
        row["experiences"] = _fetch_profile_related(
            profile_id,
            "user_experiences",
            [("order_index", False), ("start_date", True), ("created_at", False)],
        )
        row["education_entries"] = _fetch_profile_related(
            profile_id,
            "user_education",
            [("order_index", False), ("created_at", False)],
        )
        row["certification_entries"] = _fetch_profile_related(
            profile_id,
            "user_certifications",
            [("order_index", False), ("issue_date", True), ("created_at", False)],
        )
        row["portfolio_projects"] = _fetch_profile_related(
            profile_id,
            "user_portfolio_projects",
            [("order_index", False), ("start_date", True), ("created_at", False)],
        )
        row["publication_entries"] = _fetch_profile_related(
            profile_id,
            "user_publications",
            [("order_index", False), ("publication_date", True), ("created_at", False)],
        )
    row["user_id"] = row.pop("auth_user_id")
    return row


@projects_router.get("/api/public/skills/verifications/{user_id}", response_model=List[SkillVerificationOut])
def list_public_skill_verifications(user_id: str):
    supabase = get_service_client()
    res = (
        supabase.table(SKILL_VERIFICATIONS_TABLE)
        .select("skill,best_score,attempts,status,updated_at")
        .eq("user_id", user_id)
        .order("best_score", desc=True)
        .order("updated_at", desc=True)
        .execute()
    )
    if getattr(res, "error", None):
        raise HTTPException(status_code=400, detail=f"Supabase error: {res.error}")
    rows = getattr(res, "data", []) or []
    return [SkillVerificationOut(**{**row, "best_score": float(row.get("best_score") or 0.0)}).dict() for row in rows]


@projects_router.get("/api/skills/verifications", response_model=List[SkillVerificationOut])
def list_my_skill_verifications(authorization: Optional[str] = Header(default=None)):
    token = _parse_bearer_token(authorization)
    user_id, _ = _require_user_and_profile(token)
    supabase = get_service_client()
    res = (
        supabase.table(SKILL_VERIFICATIONS_TABLE)
        .select("skill,best_score,attempts,status,updated_at")
        .eq("user_id", user_id)
        .order("best_score", desc=True)
        .order("updated_at", desc=True)
        .execute()
    )
    if getattr(res, "error", None):
        raise HTTPException(status_code=400, detail=f"Supabase error: {res.error}")
    rows = getattr(res, "data", []) or []
    return [SkillVerificationOut(**{**row, "best_score": float(row.get("best_score") or 0.0)}).dict() for row in rows]


@projects_router.post("/api/skills/tests/start", response_model=SkillTestStartOut)
def start_skill_test(body: SkillTestStartIn, authorization: Optional[str] = Header(default=None)):
    token = _parse_bearer_token(authorization)
    user_id, _ = _require_user_and_profile(token)
    skill = _normalize_skill(body.skill)
    if not skill:
        raise HTTPException(status_code=400, detail="Skill is required")
    questions = _fallback_questions(skill)
    session_id = _insert_skill_test_session(user_id, skill, questions)
    public = _public_questions(questions)
    return {"session_id": session_id, "skill": skill, "questions": public}


@projects_router.post("/api/skills/tests/{session_id}/submit")
def submit_skill_test(session_id: str, body: SkillTestSubmitIn, authorization: Optional[str] = Header(default=None)):
    token = _parse_bearer_token(authorization)
    user_id, _ = _require_user_and_profile(token)
    session = _get_skill_test_session(session_id)
    if not session:
        raise HTTPException(status_code=404, detail="Session not found")
    if str(session.get("user_id")) != str(user_id):
        raise HTTPException(status_code=403, detail="Not your session")
    if (session.get("status") or "").lower() == "completed" and session.get("result"):
        return session.get("result")
    questions = session.get("questions") or []
    answers = {ans.question_id: (ans.response or "").strip() for ans in (body.answers or []) if ans.question_id}
    result = _grade_skill_answers(questions, answers)
    _update_skill_test_submission(session_id, result)
    score = float(result.get("score") or 0.0)
    skill_name = session.get("skill") or ""
    if skill_name:
        _upsert_skill_verification(user_id, skill_name, score)
        _recompute_profile_verification_score(user_id)
    return result


@projects_router.get("/api/profile")
def get_profile_compat(authorization: Optional[str] = Header(default=None)):
    token = _parse_bearer_token(authorization)
    return _get_profile_me(token)


@projects_router.post("/api/profile")
def update_profile_compat(payload: dict, authorization: Optional[str] = Header(default=None)):
    token = _parse_bearer_token(authorization)
    _update_profile_me(token, payload or {})
    return _get_profile_me(token)

# --- Academics API ---



academics_router = APIRouter()


@academics_router.post("/api/parse/syllabus-text", response_model=ParsedSyllabusOut, summary="Parse raw syllabus text into structured units/topics")
def parse_syllabus_text(payload: ParseSyllabusIn):
    return parse_syllabus(payload)

@academics_router.post("/api/syllabus/upload", response_model=SyllabusCourseOut, summary="Upload a syllabus PDF and parse + store units/topics")
async def upload_syllabus_pdf(
    request: Request,
    batch_id: uuid.UUID = Form(...),
    file: UploadFile = File(...),
    course_code: Optional[str] = Form(None),
    title: Optional[str] = Form(None),
    semester: Optional[int] = Form(None),
    prefer_naive: Optional[str] = Form(None),
):
    try:
        print(f"[UPLOAD] {request.method} {request.url.path} origin={request.headers.get('origin')} content-type={request.headers.get('content-type')}")
    except Exception:
        pass
    if not file or (file.content_type not in ("application/pdf", None) and not file.filename.lower().endswith(".pdf")):
        raise HTTPException(status_code=400, detail="Only PDF files are supported")

    blob = await file.read()
    if not blob:
        raise HTTPException(status_code=400, detail="Empty file")

    # Extract text off the event loop to avoid blocking concurrent requests
    raw_text = await run_in_threadpool(_pdf_bytes_to_text, blob)
    if not raw_text or len(raw_text) < 16:
        raise HTTPException(status_code=422, detail="Could not extract text from PDF")

    hints = {"course_code": course_code, "title": title}
    # Determine if client asked to skip AI parsing
    def _to_bool(val: Optional[str]) -> bool:
        if val is None:
            return False
        v = str(val).strip().lower()
        return v in {"1", "true", "yes", "y", "on"}

    # Always use the improved heuristic parser for stability
    parsed = _naive_extract_improved(raw_text)

    norm = _normalize_parsed_struct(parsed or {}, hints)
    filtered_units = _filter_lab_units(norm.units)
    if not filtered_units:
        fallback = _naive_extract_improved(raw_text)
        fallback_norm = _normalize_parsed_struct(fallback or {}, hints)
        filtered_units = _filter_lab_units(fallback_norm.units) or fallback_norm.units
        if filtered_units:
            norm = ParsedSyllabusOut(
                course_code=fallback_norm.course_code or norm.course_code,
                title=fallback_norm.title or norm.title,
                units=filtered_units,
            )
    else:
        norm = ParsedSyllabusOut(
            course_code=norm.course_code,
            title=norm.title,
            units=filtered_units,
        )

    if not norm.units:
        fallback_topics: List[TopicIn] = []
        for line in raw_text.splitlines():
            line = line.strip()
            if len(line) < 4:
                continue
            if _contains_lab(line):
                continue
            fallback_topics.append(TopicIn(topic=line))
            if len(fallback_topics) >= 12:
                break
        if fallback_topics:
            norm = ParsedSyllabusOut(
                course_code=norm.course_code,
                title=norm.title,
                units=[UnitIn(unit_title="Unit 1", topics=fallback_topics)],
            )

    sem_val: Optional[int] = None
    if semester is not None:
        sem_val = int(semester)
    else:
        try:
            sem_from_json = parsed.get("semester") if isinstance(parsed, dict) else None
        except Exception:
            sem_from_json = None
        if isinstance(sem_from_json, (int, float)):
            sem_val = int(sem_from_json)
        elif isinstance(sem_from_json, str) and sem_from_json.strip().isdigit():
            sem_val = int(sem_from_json.strip())
        if sem_val is None:
            m = re.search(r"sem(?:ester)?\s*[:\-]?\s*(\d{1,2})", raw_text, flags=re.IGNORECASE)
            if m:
                try:
                    sem_val = int(m.group(1))
                except Exception:
                    sem_val = None
    if not sem_val or sem_val < 1 or sem_val > 12:
        sem_val = 1
        try:
            print("[UPLOAD] semester fallback -> 1 (provide explicit semester to override)")
        except Exception:
            pass

    course_in = SyllabusCourseIn(
        batch_id=batch_id,
        semester=sem_val,
        course_code=(norm.course_code or "UNKNOWN").upper(),
        title=norm.title or "Untitled Course",
        units=norm.units or [],
    )
    course = await run_in_threadpool(upsert_syllabus_course, course_in)
    if not course.id:
        raise HTTPException(status_code=500, detail="Failed to create or resolve course id")
    units = await run_in_threadpool(sync_units_and_topics, course.id, norm.units or [])
    try:
        print(f"[UPLOAD] saved course id={course.id} code={course.course_code} title={course.title} units={len(units)}")
    except Exception:
        pass
    return SyllabusCourseOut(
        id=course.id,
        batch_id=course.batch_id,
        semester=course.semester,
        course_code=course.course_code,
        title=course.title,
        units=units,
    )

@academics_router.post("/api/syllabus/upload-bulk", summary="Upload a semester syllabus PDF containing multiple subjects; parse and store all")
async def upload_syllabus_pdf_bulk(
    request: Request,
    batch_id: uuid.UUID = Form(...),
    file: UploadFile = File(...),
    semester: Optional[int] = Form(None),
    prefer_naive: Optional[str] = Form(None),
):
    try:
        print(f"[UPLOAD-BULK] {request.method} {request.url.path} origin={request.headers.get('origin')} content-type={request.headers.get('content-type')}")
    except Exception:
        pass
    if not file or (file.content_type not in ("application/pdf", None) and not file.filename.lower().endswith(".pdf")):
        raise HTTPException(status_code=400, detail="Only PDF files are supported")

    blob = await file.read()
    if not blob:
        raise HTTPException(status_code=400, detail="Empty file")

    raw_text = await run_in_threadpool(_pdf_bytes_to_text, blob)
    if not raw_text or len(raw_text) < 16:
        raise HTTPException(status_code=422, detail="Could not extract text from PDF")

    def _to_bool(val: Optional[str]) -> bool:
        if val is None:
            return False
        v = str(val).strip().lower()
        return v in {"1", "true", "yes", "y", "on"}

    sections = _split_subject_sections(raw_text)
    results: List[SyllabusCourseOut] = []
    use_ai = not _to_bool(prefer_naive)  # Use AI by default unless prefer_naive is set
    sem_val = int(semester) if semester else 1
    
    # Helper function to process a subject from AI-parsed data
    async def save_subject_from_ai(subject_data: dict, sem: int) -> Optional[SyllabusCourseOut]:
        try:
            units_in: List[UnitIn] = []
            for unit in subject_data.get("units", []):
                topics_in = []
                for topic in unit.get("topics", []):
                    topic_name = topic.get("name") if isinstance(topic, dict) else str(topic)
                    if topic_name and topic_name.strip():
                        topics_in.append(TopicIn(topic=topic_name.strip()))
                if topics_in:
                    units_in.append(UnitIn(
                        unit_title=unit.get("name") or f"Unit {unit.get('unit_number', 1)}",
                        topics=topics_in
                    ))
            
            if not units_in:
                return None
            
            # Filter out lab units
            filtered_units = _filter_lab_units(units_in)
            if not filtered_units:
                filtered_units = units_in
            
            course_code = (subject_data.get("code") or "UNKNOWN").upper().strip()
            title = (subject_data.get("name") or "Untitled Course").strip()
            
            course_in = SyllabusCourseIn(
                batch_id=batch_id,
                semester=sem,
                course_code=course_code,
                title=title,
                units=filtered_units,
            )
            course = await run_in_threadpool(upsert_syllabus_course, course_in)
            units_saved = await run_in_threadpool(sync_units_and_topics, course.id, filtered_units)
            return SyllabusCourseOut(
                id=course.id,
                batch_id=course.batch_id,
                semester=course.semester,
                course_code=course.course_code,
                title=course.title,
                units=units_saved,
            )
        except Exception as e:
            print(f"[UPLOAD-BULK] Error saving subject: {e}")
            return None
    
    # Try AI parsing first for better accuracy with university syllabi
    if use_ai and GEMINI_API_KEY:
        try:
            print(f"[UPLOAD-BULK] Using AI (Gemini) for syllabus parsing...")
            ai_parsed = await run_in_threadpool(_gemini_parse, raw_text, {})
            
            if ai_parsed:
                # Check if AI returned multiple subjects
                if "subjects" in ai_parsed and ai_parsed["subjects"]:
                    print(f"[UPLOAD-BULK] AI found {len(ai_parsed['subjects'])} subjects")
                    for subj in ai_parsed["subjects"]:
                        result = await save_subject_from_ai(subj, sem_val)
                        if result:
                            results.append(result)
                elif ai_parsed.get("units"):
                    # Single subject format
                    subj_data = {
                        "code": ai_parsed.get("course_code"),
                        "name": ai_parsed.get("title"),
                        "units": ai_parsed.get("units", [])
                    }
                    # Convert units to new format if needed
                    converted_units = []
                    for u in ai_parsed.get("units", []):
                        topics = []
                        for t in u.get("topics", []):
                            topic_name = t.get("topic") if isinstance(t, dict) else str(t)
                            if topic_name:
                                topics.append({"name": topic_name})
                        converted_units.append({
                            "name": u.get("unit_title", "Unit"),
                            "unit_number": len(converted_units) + 1,
                            "topics": topics
                        })
                    subj_data["units"] = converted_units
                    result = await save_subject_from_ai(subj_data, sem_val)
                    if result:
                        results.append(result)
                
                if results:
                    print(f"[UPLOAD-BULK] AI parsing successful: saved {len(results)} courses")
                    return results
        except Exception as e:
            print(f"[UPLOAD-BULK] AI parsing failed, falling back to heuristic: {e}")
    
    # Fallback to heuristic parsing
    print(f"[UPLOAD-BULK] Using heuristic parsing...")
    sections = _split_subject_sections(raw_text)
    
    if not sections:
        # Fallback: treat as a single subject using the single-subject pipeline
        hints = {"course_code": None, "title": None}
        parsed = _naive_extract_improved(raw_text)
        norm = _normalize_parsed_struct(parsed or {}, hints)
        filtered_units = _filter_lab_units(norm.units)
        if not filtered_units:
            fallback_norm = _normalize_parsed_struct(_naive_extract_improved(raw_text), hints)
            filtered_units = _filter_lab_units(fallback_norm.units) or fallback_norm.units
            if filtered_units:
                norm = ParsedSyllabusOut(
                    course_code=fallback_norm.course_code or norm.course_code,
                    title=fallback_norm.title or norm.title,
                    units=filtered_units,
                )
        course_in = SyllabusCourseIn(
            batch_id=batch_id,
            semester=sem_val,
            course_code=(norm.course_code or "UNKNOWN").upper(),
            title=norm.title or "Untitled Course",
            units=norm.units or [],
        )
        course = await run_in_threadpool(upsert_syllabus_course, course_in)
        units_saved = await run_in_threadpool(sync_units_and_topics, course.id, norm.units or [])
        results.append(
            SyllabusCourseOut(
                id=course.id,
                batch_id=course.batch_id,
                semester=course.semester,
                course_code=course.course_code,
                title=course.title,
                units=units_saved,
            )
        )
        return results

    # Multi-section path
    for sec in sections:
        sec_text = sec.get("text") or ""
        hints = {"course_code": sec.get("code"), "title": sec.get("title")}
        parsed = _naive_extract_improved(sec_text)
        norm = _normalize_parsed_struct(parsed or {}, hints)
        filtered_units = _filter_lab_units(norm.units)
        if not filtered_units:
            fallback_norm = _normalize_parsed_struct(_naive_extract_improved(sec_text), hints)
            filtered_units = _filter_lab_units(fallback_norm.units) or fallback_norm.units
            if filtered_units:
                norm = ParsedSyllabusOut(
                    course_code=fallback_norm.course_code or norm.course_code,
                    title=fallback_norm.title or norm.title,
                    units=filtered_units,
                )
        course_in = SyllabusCourseIn(
            batch_id=batch_id,
            semester=sem_val,
            course_code=(norm.course_code or "UNKNOWN").upper(),
            title=norm.title or "Untitled Course",
            units=norm.units or [],
        )
        course = await run_in_threadpool(upsert_syllabus_course, course_in)
        units_saved = await run_in_threadpool(sync_units_and_topics, course.id, norm.units or [])
        results.append(
            SyllabusCourseOut(
                id=course.id,
                batch_id=course.batch_id,
                semester=course.semester,
                course_code=course.course_code,
                title=course.title,
                units=units_saved,
            )
        )
    try:
        print(f"[UPLOAD-BULK] saved {len(results)} courses for batch={batch_id} semester={sem_val}")
    except Exception:
        pass
    return results

@academics_router.get("/api/syllabus/upload", summary="Info: how to use the syllabus upload endpoint")
def upload_syllabus_info():
    return {
        "ok": True,
        "message": "Use POST multipart/form-data to /api/syllabus/upload with fields: file (PDF), batch_id (UUID), optional course_code, title, semester (1-12)",
    }

@academics_router.options("/api/syllabus/upload")
def upload_syllabus_options():
    # Explicit OPTIONS handler to help with certain proxies while debugging
    return Response(status_code=200)


# ---------------- Admin: List Users -----------------

def _is_admin_user(user_id: Optional[str], email: Optional[str]) -> bool:
    """Basic admin gating: match against comma-separated ADMIN_USER_IDS or ADMIN_EMAILS env vars.

    Falls back to allowing emails ending with domains in ADMIN_EMAIL_DOMAINS (comma-separated) if provided.
    """
    if not user_id and not email:
        return False
    ids = {s.strip() for s in os.getenv("ADMIN_USER_IDS", "").split(",") if s.strip()}
    if user_id and user_id in ids:
        return True
    emails = {s.strip().lower() for s in os.getenv("ADMIN_EMAILS", "").split(",") if s.strip()}
    if email and email.lower() in emails:
        return True
    domains = {s.strip().lower() for s in os.getenv("ADMIN_EMAIL_DOMAINS", "").split(",") if s.strip()}
    if email and domains:
        try:
            domain = email.split("@",1)[1].lower()
            if domain in domains:
                return True
        except Exception:
            pass
    # Database role check (admin_roles table) if we have a service client and user id
    try:
        if user_id:
            supabase = get_service_client()
            if supabase:
                resp = supabase.table("admin_roles").select("role").eq("auth_user_id", user_id).limit(1).execute()
                data = getattr(resp, "data", []) or []
                if data and (data[0].get("role") == "admin"):
                    return True
    except Exception:
        # Silent fail: do not block auth if table missing or permission issue
        pass
    return False


@academics_router.get("/api/admin/users", summary="Admin: list user profiles")
def list_admin_users(
    authorization: Optional[str] = Header(default=None),
    limit: int = Query(default=500, ge=1, le=2000),
    q: Optional[str] = Query(default=None, max_length=120),
    role: Optional[str] = Query(default=None, max_length=32),
    department: Optional[str] = Query(default=None, max_length=120),
    section: Optional[str] = Query(default=None, max_length=32),
    batch_range: Optional[str] = Query(default=None, max_length=32),
    semester: Optional[int] = Query(default=None, ge=1, le=12),
    min_streak: Optional[int] = Query(default=None, ge=0, le=3650),
    last_login_days: Optional[int] = Query(default=None, ge=1, le=3650),
):
    token = _parse_bearer_token(authorization)
    if not token:
        raise HTTPException(status_code=401, detail="Missing bearer token")
    anon_client = get_anon_client()
    if not anon_client:
        raise HTTPException(status_code=500, detail="Auth disabled (no anon client)")
    try:
        auth_user = anon_client.auth.get_user(token)
    except Exception:
        raise HTTPException(status_code=401, detail="Invalid token")
    user_obj = getattr(auth_user, "user", None) or (auth_user.get("user") if isinstance(auth_user, dict) else None)
    user_id = None
    email = None
    if user_obj:
        user_id = getattr(user_obj, "id", None) or (user_obj.get("id") if isinstance(user_obj, dict) else None)
        email = getattr(user_obj, "email", None) or (user_obj.get("email") if isinstance(user_obj, dict) else None)
    if not _is_admin_user(user_id, email):
        raise HTTPException(status_code=403, detail="Not an admin user")

    supabase = get_service_client()
    # Core fields from user_profiles. Department/batch are sourced from user_education.
    base_cols = [
        "id","auth_user_id","email","name","gender","phone","semester","regno",
        "profile_image_url","verification_score","updated_at","created_at","linkedin","github",
        "leetcode","skills","technologies","specializations"
    ]
    try:
        query = supabase.table("user_profiles").select(",".join(base_cols))
        if q:
            qv = (q or "").strip()
            if qv:
                # Search by name/email/regno (case-insensitive)
                pattern = f"%{qv}%"
                query = query.or_(
                    f"name.ilike.{pattern},email.ilike.{pattern},regno.ilike.{pattern}"
                )
        res = query.order("updated_at", desc=True).limit(limit).execute()
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Supabase error (list users) exec: {str(e)}")
    err = getattr(res, "error", None)
    if err:
        raise HTTPException(status_code=500, detail=f"Supabase error (list users): {err}")
    rows: List[dict] = getattr(res, "data", []) or []

    # Load roles for all involved auth_user_ids in one query
    auth_ids = [r.get("auth_user_id") for r in rows if r.get("auth_user_id")]
    uniq_ids = list({i for i in auth_ids if i})

    # Fetch primary education per profile from user_education.
    # NOTE: user_education links to user_profiles via user_profile_id (not auth_user_id).
    edu_map: Dict[str, dict] = {}
    profile_ids = [r.get("id") for r in rows if r.get("id")]
    uniq_profile_ids = list({pid for pid in profile_ids if pid})
    if uniq_profile_ids:
        # Try with order_index if present, else fallback.
        edu_select_try = [
            "user_profile_id,department_id,batch_id,college_id,degree_id,section,current_semester,regno,order_index,created_at",
            "user_profile_id,department_id,batch_id,college_id,degree_id,section,current_semester,regno,created_at",
            "user_profile_id,department_id,batch_id,college_id,degree_id,current_semester,regno,order_index,created_at",
            "user_profile_id,department_id,batch_id,college_id,degree_id,current_semester,regno,created_at",
            "user_profile_id,department_id,batch_id,college_id,section,current_semester,regno,order_index,created_at",
            "user_profile_id,department_id,batch_id,college_id,section,current_semester,regno,created_at",
            "user_profile_id,department_id,batch_id,college_id,current_semester,regno,order_index,created_at",
            "user_profile_id,department_id,batch_id,college_id,current_semester,regno,created_at",
            # Fallback if user_education has no college_id
            "user_profile_id,department_id,batch_id,section,current_semester,regno,order_index,created_at",
            "user_profile_id,department_id,batch_id,section,current_semester,regno,created_at",
            "user_profile_id,department_id,batch_id,current_semester,regno,order_index,created_at",
            "user_profile_id,department_id,batch_id,current_semester,regno,created_at",
        ]
        edu_rows: List[dict] = []
        for sel in edu_select_try:
            try:
                eres = supabase.table("user_education").select(sel).in_("user_profile_id", uniq_profile_ids).execute()
            except Exception as e:
                msg = str(e)
                if "order_index" in sel and "order_index" in msg:
                    continue
                if "degree_id" in sel and "degree_id" in msg:
                    continue
                # If user_education doesn't exist or query fails, just skip education enrichment.
                edu_rows = []
                break
            if getattr(eres, "error", None):
                edu_rows = []
                break
            edu_rows = getattr(eres, "data", []) or []
            break

        def _edu_rank(ed: dict) -> Tuple[int, float]:
            oi_raw = ed.get("order_index")
            try:
                oi = int(oi_raw) if oi_raw is not None else 9999
            except Exception:
                oi = 9999
            ts = 0.0
            try:
                if ed.get("created_at"):
                    ts = datetime.fromisoformat(str(ed.get("created_at")).replace("Z", "+00:00")).timestamp()
            except Exception:
                ts = 0.0
            # Prefer lower order_index; for ties prefer latest created_at.
            return (oi, -ts)

        for ed in edu_rows:
            pid = ed.get("user_profile_id")
            if not pid:
                continue
            prev = edu_map.get(pid)
            if not prev or _edu_rank(ed) < _edu_rank(prev):
                edu_map[pid] = ed

    # Preload department + batch + college + degree info to enrich output
    dept_ids = {ed.get("department_id") for ed in edu_map.values() if ed.get("department_id")}
    batch_ids = {ed.get("batch_id") for ed in edu_map.values() if ed.get("batch_id")}
    college_ids = {ed.get("college_id") for ed in edu_map.values() if ed.get("college_id")}
    degree_ids = {ed.get("degree_id") for ed in edu_map.values() if ed.get("degree_id")}
    dept_map: Dict[str, dict] = {}
    batch_map: Dict[str, dict] = {}
    college_map: Dict[str, dict] = {}
    degree_map: Dict[str, dict] = {}
    role_map: Dict[str, str] = {}

    def _chunked(values: List[str], size: int = 150) -> List[List[str]]:
        return [values[i : i + size] for i in range(0, len(values), size)]

    def _as_id_list(values_set) -> List[str]:
        out_ids: List[str] = []
        for v in (values_set or []):
            if v is None:
                continue
            s = str(v).strip()
            if s:
                out_ids.append(s)
        return list(dict.fromkeys(out_ids))

    def _safe_in_select(table: str, cols: str, key: str, ids_list: List[str], size: int = 150) -> List[dict]:
        if not ids_list:
            return []
        out_rows: List[dict] = []
        for chunk in _chunked(ids_list, size=size):
            try:
                res = supabase.table(table).select(cols).in_(key, chunk).execute()
                if getattr(res, "error", None):
                    continue
                out_rows.extend(getattr(res, "data", None) or [])
            except Exception:
                # Do not fail entire /api/admin/users just because enrichment failed
                continue
        return out_rows

    # These queries can create very large URLs when the IN list is big; batch them.
    try:
        dept_rows = _safe_in_select("departments", "id,name", "id", _as_id_list(dept_ids))
        for d in dept_rows:
            if isinstance(d, dict) and d.get("id"):
                dept_map[d.get("id")] = d
    except Exception:
        pass

    try:
        batch_rows = _safe_in_select("batches", "id,from_year,to_year", "id", _as_id_list(batch_ids))
        for b in batch_rows:
            if isinstance(b, dict) and b.get("id"):
                batch_map[b.get("id")] = b
    except Exception:
        pass

    try:
        college_rows = _safe_in_select("colleges", "id,name", "id", _as_id_list(college_ids))
        for c in college_rows:
            if isinstance(c, dict) and c.get("id"):
                college_map[c.get("id")] = c
    except Exception:
        pass

    try:
        degree_rows = _safe_in_select("degrees", "id,name", "id", _as_id_list(degree_ids))
        for g in degree_rows:
            if isinstance(g, dict) and g.get("id"):
                degree_map[g.get("id")] = g
    except Exception:
        pass

    try:
        role_rows = _safe_in_select("admin_roles", "auth_user_id,role", "auth_user_id", _as_id_list(uniq_ids), size=200)
        for rr in role_rows:
            if not isinstance(rr, dict):
                continue
            rid = rr.get("auth_user_id")
            if rid:
                role_map[str(rid)] = (rr.get("role") or "student")
    except Exception:
        pass

    # Load streaks for all profiles in one query (table name may vary across deployments).
    # NOTE: academicas.html uses /api/streak which reads from notex_streak.
    streak_current_map: Dict[str, int] = {}  # profile_id -> current_streak
    streak_longest_map: Dict[str, int] = {}  # profile_id -> longest_streak
    streak_last_activity_map: Dict[str, Any] = {}  # profile_id -> last_activity_date
    if uniq_profile_ids:
        for streak_table in ("notex_streak", "user_streaks"):
            try:
                sres = supabase.table(streak_table).select(
                    "user_profile_id,current_streak,longest_streak,last_activity_date"
                ).in_(
                    "user_profile_id", uniq_profile_ids
                ).execute()
            except Exception:
                continue
            if getattr(sres, "error", None):
                continue
            for s in (getattr(sres, "data", None) or []):
                pid = s.get("user_profile_id")
                if pid:
                    try:
                        streak_current_map[pid] = int(s.get("current_streak") or 0)
                    except Exception:
                        streak_current_map[pid] = 0
                    try:
                        streak_longest_map[pid] = int(s.get("longest_streak") or 0)
                    except Exception:
                        streak_longest_map[pid] = 0
                    if s.get("last_activity_date") is not None:
                        streak_last_activity_map[pid] = s.get("last_activity_date")
            # If we got any rows, stop trying fallbacks.
            if streak_current_map:
                break

    # Load last_seen_at per auth user from user_sessions (aggregate in Python).
    last_seen_map: Dict[str, str] = {}  # auth_user_id -> ISO timestamp
    if uniq_ids:
        try:
            # Cap rows to avoid unbounded reads on very large session tables.
            sres = (
                supabase.table("user_sessions")
                .select("user_id,last_seen_at")
                .in_("user_id", uniq_ids)
                .order("last_seen_at", desc=True)
                .limit(50000)
                .execute()
            )
            if not getattr(sres, "error", None):
                for row in (getattr(sres, "data", None) or []):
                    uid = row.get("user_id")
                    ts = row.get("last_seen_at")
                    if uid and ts and uid not in last_seen_map:
                        # Because results are ordered desc, first seen is the latest.
                        last_seen_map[uid] = ts
        except Exception:
            pass

    out: List[dict] = []
    for r in rows:
        pid = r.get("id")
        edu = edu_map.get(pid, {}) if pid else {}
        dept = dept_map.get(edu.get("department_id")) if edu.get("department_id") else {}
        batch = batch_map.get(edu.get("batch_id")) if edu.get("batch_id") else {}
        college = college_map.get(edu.get("college_id")) if edu.get("college_id") else {}
        degree = degree_map.get(edu.get("degree_id")) if edu.get("degree_id") else {}
        semester_val = r.get("semester") if r.get("semester") is not None else edu.get("current_semester")
        regno_val = r.get("regno") if r.get("regno") else edu.get("regno")
        user_id_val = r.get("auth_user_id")
        current_streak_val = streak_current_map.get(pid, 0) if pid else 0
        longest_streak_val = streak_longest_map.get(pid, 0) if pid else 0
        last_seen_val = last_seen_map.get(user_id_val) if user_id_val else None
        batch_range_val = (f"{batch.get('from_year')}-{batch.get('to_year')}" if batch and batch.get('from_year') and batch.get('to_year') else None)

        out.append({
            "profile_id": r.get("id"),
            "user_id": user_id_val,
            "name": r.get("name"),
            "email": r.get("email"),
            "role": role_map.get(r.get("auth_user_id"), "student"),
            "semester": semester_val,
            "regno": regno_val,
            "college": college.get("name") if college else None,
            "degree": degree.get("name") if degree else None,
            "department": dept.get("name") if dept else None,
            "section": edu.get("section") if edu else None,
            "batch_from": batch.get("from_year") if batch else None,
            "batch_to": batch.get("to_year") if batch else None,
            "batch_range": batch_range_val,
            # Back-compat (users.html initially used streak_current)
            "streak_current": current_streak_val,
            # Align naming with /api/streak response (academicas.html)
            "current_streak": current_streak_val,
            "longest_streak": longest_streak_val,
            "last_activity_date": streak_last_activity_map.get(pid) if pid else None,
            "last_seen_at": last_seen_val,
            "profile_image_url": r.get("profile_image_url"),
            "verification_score": r.get("verification_score"),
            "linkedin": r.get("linkedin"),
            "github": r.get("github"),
            "leetcode": r.get("leetcode"),
            "skills": r.get("skills") or [],
            "technologies": r.get("technologies") or [],
            "specializations": r.get("specializations") or [],
            "updated_at": r.get("updated_at"),
            "created_at": r.get("created_at"),
        })

    # Server-side filters (for real-time filter UI)
    def _norm(s: Any) -> str:
        return (str(s).strip().lower() if s is not None else "")

    filtered = out
    if role:
        want = _norm(role)
        filtered = [u for u in filtered if _norm(u.get("role")) == want]
    if department:
        want = _norm(department)
        filtered = [u for u in filtered if _norm(u.get("department")) == want]
    if section:
        want = _norm(section)
        filtered = [u for u in filtered if _norm(u.get("section")) == want]
    if batch_range:
        want = _norm(batch_range)
        filtered = [u for u in filtered if _norm(u.get("batch_range")) == want]
    if semester is not None:
        filtered = [u for u in filtered if str(u.get("semester") or "") == str(semester)]
    if min_streak is not None:
        filtered = [u for u in filtered if int(u.get("current_streak") or 0) >= int(min_streak)]
    if last_login_days is not None:
        cutoff = datetime.utcnow() - timedelta(days=int(last_login_days))

        def _is_recent(u: dict) -> bool:
            ts = u.get("last_seen_at")
            if not ts:
                return False
            try:
                dt = datetime.fromisoformat(str(ts).replace("Z", "+00:00"))
                # Compare as naive UTC-ish (safe even if dt is tz-aware)
                if dt.tzinfo is not None:
                    dt = dt.replace(tzinfo=None)
                return dt >= cutoff
            except Exception:
                return False

        filtered = [u for u in filtered if _is_recent(u)]

    return {"users": filtered, "count": len(filtered)}


@academics_router.get("/api/admin/self-check", summary="Admin: verify current token admin status")
def admin_self_check(authorization: Optional[str] = Header(default=None)):
    token = _parse_bearer_token(authorization)
    if not token:
        raise HTTPException(status_code=401, detail="Missing bearer token")
    anon_client = get_anon_client()
    if not anon_client:
        raise HTTPException(status_code=500, detail="Auth disabled (no anon client)")
    try:
        auth_user = anon_client.auth.get_user(token)
    except Exception:
        raise HTTPException(status_code=401, detail="Invalid token")
    user_obj = getattr(auth_user, "user", None) or (auth_user.get("user") if isinstance(auth_user, dict) else None)
    user_id = getattr(user_obj, "id", None) if user_obj else None
    email = getattr(user_obj, "email", None) if user_obj else None
    is_admin = _is_admin_user(user_id, email)
    if not is_admin:
        raise HTTPException(status_code=403, detail="Not an admin user")
    return {"ok": True, "user_id": user_id, "email": email, "admin": True}


class RoleUpdateIn(BaseModel):
    role: str


class AdminAcademicUpdateIn(BaseModel):
    # Optional FK ids (preferred when known)
    college_id: Optional[str] = None
    degree_id: Optional[str] = None
    department_id: Optional[str] = None
    batch_id: Optional[str] = None

    college_name: Optional[str] = None
    degree_name: Optional[str] = None
    department_name: Optional[str] = None
    batch_from: Optional[int] = Field(default=None, ge=1900, le=2100)
    batch_to: Optional[int] = Field(default=None, ge=1900, le=2100)
    batch_range: Optional[str] = None  # e.g. "2022-2026"
    section: Optional[str] = None
    semester: Optional[int] = Field(default=None, ge=1, le=12)
    regno: Optional[str] = None

    @validator(
        "college_id",
        "degree_id",
        "department_id",
        "batch_id",
        "college_name",
        "degree_name",
        "department_name",
        "batch_range",
        "section",
        "regno",
        pre=True,
    )
    def _trim_admin_academic(cls, v: Any):  # noqa: N805
        return _strip_or_none(v)


def _get_auth_user(authorization: Optional[str]) -> Tuple[Optional[str], Optional[str]]:
    token = _parse_bearer_token(authorization)
    if not token:
        raise HTTPException(status_code=401, detail="Missing bearer token")
    anon_client = get_anon_client()
    if not anon_client:
        raise HTTPException(status_code=500, detail="Auth disabled (no anon client)")
    try:
        auth_user = anon_client.auth.get_user(token)
    except Exception:
        raise HTTPException(status_code=401, detail="Invalid token")
    user_obj = getattr(auth_user, "user", None) or (auth_user.get("user") if isinstance(auth_user, dict) else None)
    if not user_obj:
        raise HTTPException(status_code=401, detail="Invalid auth context")
    user_id = getattr(user_obj, "id", None) or (user_obj.get("id") if isinstance(user_obj, dict) else None)
    email = getattr(user_obj, "email", None) or (user_obj.get("email") if isinstance(user_obj, dict) else None)
    return user_id, email


def _require_admin(authorization: Optional[str]):
    uid, em = _get_auth_user(authorization)
    if not _is_admin_user(uid, em):
        raise HTTPException(status_code=403, detail="Not an admin user")
    return uid, em


def _is_admin_or_employee(user_id: Optional[str], email: Optional[str]) -> bool:
    """Check if user is admin or employee role."""
    # First check if they are admin
    if _is_admin_user(user_id, email):
        return True
    # Then check for employee role in database
    try:
        if user_id:
            supabase = get_service_client()
            if supabase:
                resp = supabase.table("admin_roles").select("role").eq("auth_user_id", user_id).limit(1).execute()
                data = getattr(resp, "data", []) or []
                if data and (data[0].get("role") == "employee"):
                    return True
    except Exception:
        pass
    return False


def _require_admin_or_employee(authorization: Optional[str]):
    """Require user to be admin or employee role."""
    uid, em = _get_auth_user(authorization)
    if not _is_admin_or_employee(uid, em):
        raise HTTPException(status_code=403, detail="Not an admin or employee user")
    return uid, em


def _count_admins(supabase) -> int:
    try:
        resp = supabase.table("admin_roles").select("role", count='exact').eq("role", "admin").execute()
        # Some supabase libs embed count differently; attempt both
        if hasattr(resp, 'count') and resp.count is not None:
            return resp.count
        data = getattr(resp, 'data', []) or []
        return len([r for r in data if r.get('role') == 'admin'])
    except Exception:
        return 0


VALID_ROLES = {"admin", "teacher", "hod", "student", "moderator", "employee"}


def _admin_pick_primary_education_row(supabase, profile_id: str) -> Optional[dict]:
    try:
        res = (
            supabase.table("user_education")
            .select("id,order_index,created_at")
            .eq("user_profile_id", profile_id)
            .limit(50)
            .execute()
        )
    except Exception:
        return None
    if getattr(res, "error", None) or not getattr(res, "data", None):
        return None
    rows = [r for r in (res.data or []) if isinstance(r, dict) and r.get("id")]
    if not rows:
        return None

    def _rank(ed: dict) -> Tuple[int, float]:
        oi_raw = ed.get("order_index")
        try:
            oi = int(oi_raw) if oi_raw is not None else 9999
        except Exception:
            oi = 9999
        ts = 0.0
        try:
            if ed.get("created_at"):
                ts = datetime.fromisoformat(str(ed.get("created_at")).replace("Z", "+00:00")).timestamp()
        except Exception:
            ts = 0.0
        return (oi, -ts)

    rows.sort(key=_rank)
    return rows[0]


def _admin_safe_update_user_education(supabase, edu_id: str, updates: Dict[str, Any]):
    """Best-effort update: retries by dropping unknown columns if schema differs."""
    if not updates:
        return
    retry_payload = dict(updates)
    for _attempt in range(3):
        try:
            upd = supabase.table("user_education").update(retry_payload).eq("id", edu_id).execute()
        except APIError as exc:
            err_message = getattr(exc, "message", None) or getattr(exc, "details", None) or str(exc)
            lower_msg = (err_message or "").lower()
            # Undefined column (Postgres 42703)
            if getattr(exc, "code", None) == "42703" or "42703" in lower_msg or "column" in lower_msg and "does not exist" in lower_msg:
                # Drop likely FK columns first
                removed = False
                for k in ("college_id", "degree_id", "department_id", "batch_id"):
                    if k in retry_payload:
                        retry_payload.pop(k, None)
                        removed = True
                if removed:
                    continue
            raise HTTPException(status_code=500, detail=f"Supabase error (update education): {err_message}")
        if getattr(upd, "error", None):
            msg = str(upd.error)
            lowered = msg.lower()
            if ("42703" in lowered) or ("does not exist" in lowered and "column" in lowered):
                removed = False
                for k in ("college_id", "degree_id", "department_id", "batch_id"):
                    if k in retry_payload:
                        retry_payload.pop(k, None)
                        removed = True
                if removed:
                    continue
            raise HTTPException(status_code=500, detail=f"Supabase error (update education): {msg}")
        break


@academics_router.post("/api/admin/users/{auth_user_id}/academic", summary="Admin: update user's academic/education details")
def admin_update_user_academic(
    auth_user_id: str,
    payload: AdminAcademicUpdateIn,
    authorization: Optional[str] = Header(default=None),
):
    _require_admin(authorization)
    supabase = get_service_client()

    # Resolve profile id
    prof_q = (
        supabase.table("user_profiles")
        .select("id")
        .eq("auth_user_id", auth_user_id)
        .limit(1)
        .execute()
    )
    if getattr(prof_q, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (get profile): {prof_q.error}")
    if not prof_q.data:
        raise HTTPException(status_code=404, detail="User profile not found")
    profile_id = prof_q.data[0].get("id")
    if not profile_id:
        raise HTTPException(status_code=404, detail="User profile not found")

    data = payload.dict(exclude_unset=True)
    # Update basic user_profiles fields when present
    prof_updates: Dict[str, Any] = {}
    if data.get("semester") is not None:
        prof_updates["semester"] = data.get("semester")
    if data.get("regno") is not None:
        prof_updates["regno"] = data.get("regno")
    if prof_updates:
        upd = supabase.table("user_profiles").update(_supabase_payload(prof_updates)).eq("id", profile_id).execute()
        if getattr(upd, "error", None):
            raise HTTPException(status_code=500, detail=f"Supabase error (update profile academic): {upd.error}")

    # Prepare a single education row (for FK resolution) without wiping other rows
    school = data.get("college_name")
    college_id = data.get("college_id")
    degree_name = data.get("degree_name")
    degree_id = data.get("degree_id")
    dept = data.get("department_name")
    department_id = data.get("department_id")
    batch_id = data.get("batch_id")
    section = data.get("section")
    regno = data.get("regno")
    current_semester = data.get("semester")

    if not school and college_id:
        try:
            cres = supabase.table("colleges").select("name").eq("id", college_id).limit(1).execute()
            if not getattr(cres, "error", None) and (cres.data or []):
                school = cres.data[0].get("name")
        except Exception:
            pass

    batch_range = data.get("batch_range")
    if not batch_range:
        bf = data.get("batch_from")
        bt = data.get("batch_to")
        if bf is not None and bt is not None:
            batch_range = f"{int(bf)}-{int(bt)}"

    edu_in = {
        "school": school or "",
        "degree": degree_name,
        "department": dept,
        "batch_range": batch_range,
        "section": section,
        "regno": regno,
        "current_semester": current_semester,
        "grade": None,
        "activities": None,
        "description": None,
        "college_id": college_id,
        "degree_id": degree_id,
        "department_id": department_id,
        "batch_id": batch_id,
    }

    prepared_list = _prepare_education_rows([edu_in])
    if not prepared_list:
        # If admin didn't provide college_name, we can't build education entry; still allow profile update.
        return {"ok": True, "updated": True, "auth_user_id": auth_user_id, "profile_id": profile_id, "education_updated": False}

    prepared = dict(prepared_list[0])
    # Remove fields not in user_education table payload
    prepared.pop("id", None)
    prepared.pop("updated_at", None)
    # Ensure we don't set null-like empty strings
    edu_updates = {k: v for k, v in prepared.items() if v is not None}
    edu_updates["updated_at"] = datetime.utcnow().isoformat()

    # Update primary education row, or insert if missing
    primary = _admin_pick_primary_education_row(supabase, profile_id)
    if primary and primary.get("id"):
        _admin_safe_update_user_education(supabase, primary["id"], edu_updates)
        edu_row_id = primary["id"]
    else:
        insert_payload = {**edu_updates, "user_profile_id": profile_id}
        if "order_index" not in insert_payload:
            insert_payload["order_index"] = 0
        ins = supabase.table("user_education").insert(insert_payload).execute()
        if getattr(ins, "error", None):
            raise HTTPException(status_code=500, detail=f"Supabase error (insert education): {ins.error}")
        # Try refetch
        ref = _admin_pick_primary_education_row(supabase, profile_id)
        edu_row_id = (ref.get("id") if ref else None)

    return {
        "ok": True,
        "updated": True,
        "auth_user_id": auth_user_id,
        "profile_id": profile_id,
        "education_updated": True,
        "user_education_id": edu_row_id,
    }


@academics_router.post("/api/admin/users/{auth_user_id}/role", summary="Admin: update a user's role")
def update_user_role(auth_user_id: str, payload: RoleUpdateIn, authorization: Optional[str] = Header(default=None)):
    _require_admin(authorization)
    desired = payload.role.lower().strip()
    if desired not in VALID_ROLES:
        raise HTTPException(status_code=400, detail=f"Invalid role '{desired}'")
    supabase = get_service_client()
    if desired == 'admin':
        # Upsert row
        res = supabase.table('admin_roles').upsert({"auth_user_id": auth_user_id, "role": "admin"}).execute()
        if getattr(res, 'error', None):
            raise HTTPException(status_code=500, detail=f"Role upsert failed: {res.error}")
    else:
        # If demoting from admin ensure not last admin
        # Check if currently admin
        existing = supabase.table('admin_roles').select('role').eq('auth_user_id', auth_user_id).limit(1).execute()
        is_admin_now = False
        if not getattr(existing, 'error', None):
            rows = getattr(existing, 'data', []) or []
            is_admin_now = bool(rows and rows[0].get('role') == 'admin')
        if is_admin_now:
            admin_count = _count_admins(supabase)
            if admin_count <= 1:
                raise HTTPException(status_code=400, detail="Cannot demote the last admin")
            del_res = supabase.table('admin_roles').delete().eq('auth_user_id', auth_user_id).execute()
            if getattr(del_res, 'error', None):
                raise HTTPException(status_code=500, detail=f"Role demote failed: {del_res.error}")
        # For non-admin roles we can store or remove row (choose store for non-student roles)
        if desired in {"teacher", "hod", "moderator", "employee"}:
            up_res = supabase.table('admin_roles').upsert({"auth_user_id": auth_user_id, "role": desired}).execute()
            if getattr(up_res, 'error', None):
                raise HTTPException(status_code=500, detail=f"Role update failed: {up_res.error}")
        elif desired == 'student':
            # Remove row if not needed
            supabase.table('admin_roles').delete().eq('auth_user_id', auth_user_id).execute()
    return {"ok": True, "auth_user_id": auth_user_id, "role": desired}


@academics_router.delete("/api/admin/users/{auth_user_id}", summary="Admin: delete a user (profile + role)")
def delete_user(auth_user_id: str, authorization: Optional[str] = Header(default=None)):
    _require_admin(authorization)
    supabase = get_service_client()
    # Prevent deleting last admin if target is sole admin
    existing_role = supabase.table('admin_roles').select('role').eq('auth_user_id', auth_user_id).limit(1).execute()
    target_is_admin = False
    if not getattr(existing_role, 'error', None):
        rows = getattr(existing_role, 'data', []) or []
        target_is_admin = bool(rows and rows[0].get('role') == 'admin')
    if target_is_admin:
        admin_count = _count_admins(supabase)
        if admin_count <= 1:
            raise HTTPException(status_code=400, detail="Cannot delete the last admin")
    # Delete profile first (soft cascade pattern) then role row
    prof_del = supabase.table('user_profiles').delete().eq('auth_user_id', auth_user_id).execute()
    if getattr(prof_del, 'error', None):
        raise HTTPException(status_code=500, detail=f"Profile delete failed: {prof_del.error}")
    supabase.table('admin_roles').delete().eq('auth_user_id', auth_user_id).execute()
    # NOTE: We are NOT deleting from auth.users here (would require service role elevated call). Document manual removal if needed.
    return {"ok": True, "deleted_auth_user_id": auth_user_id}

# ================= Teacher Feature Backend =====================

teacher_router = APIRouter()


class TeacherSignupIn(BaseModel):
    name: str
    email: str
    password: str
    college: Optional[str] = None
    department: Optional[str] = None
    subjects: Optional[List[str]] = None

    @validator("subjects", pre=True, always=True)
    def _clean_subjects(cls, v):
        if not v:
            return []
        out = []
        for s in v:
            if not s:
                continue
            s2 = str(s).strip()
            if s2 and s2 not in out:
                out.append(s2[:64])
        return out


class TeacherApproveIn(BaseModel):
    status: str = Field(..., pattern=r"^(approved|rejected)$")
    notes: Optional[str] = None


class TeacherMessageIn(BaseModel):
    content: str = Field(..., min_length=1, max_length=4000)


# ---------------- WebSocket Chat Manager (Teacher) -----------------
class TeacherChatManager:
    """In-memory tracking of active teacher chat WebSocket connections.

    Structure: {connection_id: {user_id: websocket}}
    For multi-process / multi-instance deployments, replace with shared pub/sub.
    """
    def __init__(self):
        self.active: dict[str, dict[str, WebSocket]] = {}

    async def connect(self, connection_id: str, user_id: str, websocket: WebSocket):
        await websocket.accept()
        self.active.setdefault(connection_id, {})[user_id] = websocket

    def disconnect(self, connection_id: str, user_id: str):
        try:
            if connection_id in self.active and user_id in self.active[connection_id]:
                del self.active[connection_id][user_id]
                if not self.active[connection_id]:
                    del self.active[connection_id]
        except Exception:
            pass

    async def broadcast(self, connection_id: str, payload: dict):
        conns = self.active.get(connection_id, {})
        stale = []
        for uid, ws in conns.items():
            try:
                await ws.send_json(payload)
            except Exception:
                stale.append(uid)
        for uid in stale:
            self.disconnect(connection_id, uid)


chat_manager = TeacherChatManager()


def _ensure_teacher_role(auth_user_id: str):
    supabase = get_service_client()
    # Upsert teacher role if not exists
    row = supabase.table("admin_roles").select("role").eq("auth_user_id", auth_user_id).limit(1).execute()
    if getattr(row, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (get role): {row.error}")
    data = row.data or []
    if data:
        role = data[0].get("role")
        if role != "teacher":
            # do not override admin; if admin keep dual capability
            if role in {"admin","moderator"}:
                return
            upd = supabase.table("admin_roles").update({"role": "teacher"}).eq("auth_user_id", auth_user_id).execute()
            if getattr(upd, "error", None):
                raise HTTPException(status_code=500, detail=f"Supabase error (promote teacher): {upd.error}")
    else:
        ins = supabase.table("admin_roles").insert({"auth_user_id": auth_user_id, "role": "teacher"}).execute()
        if getattr(ins, "error", None):
            raise HTTPException(status_code=500, detail=f"Supabase error (insert teacher role): {ins.error}")


def _require_teacher(authorization: Optional[str]):
    uid, _ = _get_auth_user(authorization)
    supabase = get_service_client()
    role_q = supabase.table("admin_roles").select("role").eq("auth_user_id", uid).limit(1).execute()
    if getattr(role_q, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (role check): {role_q.error}")
    data = role_q.data or []
    role = data[0].get("role") if data else "student"
    if role not in {"teacher", "hod", "admin"}:  # admins + hod also allowed
        raise HTTPException(status_code=403, detail="Teacher role required")
    return uid


def _require_hod(authorization: Optional[str]) -> str:
    """Require a user to be HOD (or admin).

    This uses `public.admin_roles.role` and expects HOD users to have `role='hod'`.
    """
    uid, _ = _get_auth_user(authorization)
    supabase = get_service_client()
    role_q = supabase.table("admin_roles").select("role").eq("auth_user_id", uid).limit(1).execute()
    if getattr(role_q, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (role check): {role_q.error}")
    data = role_q.data or []
    role = data[0].get("role") if data else "student"
    if role not in {"hod", "admin"}:
        raise HTTPException(status_code=403, detail="HOD role required")
    return uid


def _hod_scope_departments(supabase: Client, hod_user_id: str) -> Tuple[Optional[str], List[str]]:
    """Return (college_id, department_ids) for the HOD.

    Priority:
      1) `public.hod_departments` rows
      2) fallback: `public.teacher_profiles.department_id`
      3) fallback: `public.teacher_applications.department_id`
    """
    # 1) explicit mapping table (supports multiple departments)
    dept_ids: List[str] = []
    try:
        q = supabase.table("hod_departments").select("department_id").eq("hod_user_id", hod_user_id).limit(200).execute()
        if not getattr(q, "error", None) and q.data:
            dept_ids = [r.get("department_id") for r in (q.data or []) if r.get("department_id")]
    except Exception:
        dept_ids = []

    college_id: Optional[str] = None
    # Resolve college/department from teacher_profiles or application
    prof = supabase.table("teacher_profiles").select("college_id,department_id").eq("auth_user_id", hod_user_id).limit(1).execute()
    if not getattr(prof, "error", None) and prof.data:
        college_id = prof.data[0].get("college_id") or None
        if not dept_ids and prof.data[0].get("department_id"):
            dept_ids = [prof.data[0].get("department_id")]
    if not college_id or not dept_ids:
        app = supabase.table("teacher_applications").select("college_id,department_id").eq("auth_user_id", hod_user_id).limit(1).execute()
        if not getattr(app, "error", None) and app.data:
            college_id = college_id or (app.data[0].get("college_id") or None)
            if not dept_ids and app.data[0].get("department_id"):
                dept_ids = [app.data[0].get("department_id")]

    # De-dupe, preserve order
    seen: Set[str] = set()
    unique: List[str] = []
    for d in dept_ids:
        if not d or d in seen:
            continue
        seen.add(d)
        unique.append(d)
    return college_id, unique


def _build_hod_ai_agent() -> AssistantAgent:
    system = """You are an assistant for a Head of Department (HOD).

Rules:
- Be practical and concise.
- Use only the provided data; do not invent numbers.
- If data is missing, explicitly say what is missing.
- Output Markdown with short headings and bullets.
"""
    return AssistantAgent(
        "paperx_hod_agent",
        model_client=gemini_model_client,
        system_message=system,
    )


def _hod_ai_generate_markdown(user_prompt: str) -> str:
    """Generate markdown text for HOD pages using configured LLM.

    Falls back to a deterministic template if LLM isn't configured.
    """
    try:
        if not (os.getenv("GEMINI_API_KEY") or os.getenv("OPENAI_API_KEY") or os.getenv("OPENROUTER_API_KEY")):
            raise RuntimeError("No AI API key configured")
        agent = _build_hod_ai_agent()
        result = _run_assistant_blocking(agent, user_prompt)
        content = result.messages[-1].content
        return str(content or "").strip() or "(empty AI response)"
    except Exception as e:
        # Safe fallback: still return something useful
        return "\n".join([
            "# HOD Assistant (Fallback)",
            "", 
            "AI provider is not configured or failed.",
            f"Error: {str(e)}",
            "", 
            "## Suggested Next Actions",
            "- Review pending teacher applications in your department",
            "- Check unassigned/overloaded classes",
            "- Schedule a short staff sync (15–20 mins)",
        ])


@teacher_router.post("/api/teacher/signup", summary="Teacher signup with ID card images (multipart)")
async def teacher_signup(
    email: str = Form(...),
    password: str = Form(...),
    confirm_password: str = Form(...),
    name: str = Form(...),
    college: Optional[str] = Form(None),
    department: Optional[str] = Form(None),
    subjects: Optional[str] = Form(None),  # JSON array or comma list
    id_card_front: UploadFile = File(...),
    id_card_back: UploadFile = File(...),
):
    if password != confirm_password:
        raise HTTPException(status_code=400, detail="Passwords do not match")
    # basic file validation
    allowed = {"image/png","image/jpeg","image/jpg","image/webp"}
    if id_card_front.content_type not in allowed or id_card_back.content_type not in allowed:
        raise HTTPException(status_code=400, detail="ID card images must be png/jpg/webp")
    anon_client = get_anon_client()
    if not anon_client:
        raise HTTPException(status_code=500, detail="Auth disabled")
    try:
        auth_res = anon_client.auth.sign_up({"email": email, "password": password})
        auth_user_id = _get_user_id_from_auth_response(auth_res)
        if not auth_user_id:
            raise HTTPException(status_code=400, detail="Failed to create auth user")
        access_token = _extract_access_token(auth_res)
        supabase = get_service_client()
        college_id = None
        department_id = None
        if college:
            try:
                college_id = str(_resolve_college_id_by_name(college))
            except Exception:
                college_id = None
        if department and college_id:
            try:
                department_id = str(_resolve_department_id(uuid.UUID(college_id), department.upper()))
            except Exception:
                department_id = None
        # subjects parse
        subj_list: List[str] = []
        if subjects:
            try:
                if subjects.strip().startswith("["):
                    subj_list = [s[:64] for s in json.loads(subjects) if isinstance(s, str)]
                else:
                    subj_list = [s.strip()[:64] for s in subjects.split(',') if s.strip()]
            except Exception:
                subj_list = []
        # Read image bytes and validate size
        front_bytes = await id_card_front.read()
        back_bytes = await id_card_back.read()
        if len(front_bytes) > 5*1024*1024 or len(back_bytes) > 5*1024*1024:
            raise HTTPException(status_code=400, detail="Image too large (max 5MB)")
        # Upload images to Supabase 'teacher' bucket
        front_ext = Path(id_card_front.filename or "front").suffix.lower() or ".jpg"
        back_ext = Path(id_card_back.filename or "back").suffix.lower() or ".jpg"
        front_name = f"{auth_user_id}_front{front_ext}"
        back_name = f"{auth_user_id}_back{back_ext}"
        front_content_type = id_card_front.content_type or "image/jpeg"
        back_content_type = id_card_back.content_type or "image/jpeg"
        # Upload to 'teacher' bucket and get public URLs
        _storage_upload_bytes(supabase, "teacher", front_name, front_bytes, front_content_type)
        _storage_upload_bytes(supabase, "teacher", back_name, back_bytes, back_content_type)
        front_url = _storage_public_url(supabase, "teacher", front_name)
        back_url = _storage_public_url(supabase, "teacher", back_name)
        ins = supabase.table("teacher_applications").insert({
            "auth_user_id": auth_user_id,
            "email": email,
            "name": name,
            "college_id": college_id,
            "department_id": department_id,
            "subjects": subj_list,
            "id_card_front_path": front_url,
            "id_card_back_path": back_url,
            "status": "pending"
        }).execute()
        if getattr(ins, "error", None):
            raise HTTPException(status_code=500, detail=f"Supabase error (teacher application insert): {ins.error}")
        return {"message": "Teacher application submitted", "access_token": access_token, "user_id": auth_user_id}
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Unexpected signup error: {e}")


@teacher_router.post("/api/hod/signup", summary="HOD signup (creates teacher + HOD role applications)")
async def hod_signup(
    email: str = Form(...),
    password: str = Form(...),
    confirm_password: str = Form(...),
    name: str = Form(...),
    college: Optional[str] = Form(None),
    department: Optional[str] = Form(None),
    motivation: Optional[str] = Form(None),
    id_card_front: UploadFile = File(...),
    id_card_back: UploadFile = File(...),
):
    """HOD-only signup flow.

    Creates:
      - public.teacher_applications (pending)
      - public.hod_role_applications (pending)

    Admin later reviews hod_role_applications and assigns role='hod'.
    """
    if password != confirm_password:
        raise HTTPException(status_code=400, detail="Passwords do not match")
    allowed = {"image/png", "image/jpeg", "image/jpg", "image/webp"}
    if id_card_front.content_type not in allowed or id_card_back.content_type not in allowed:
        raise HTTPException(status_code=400, detail="ID card images must be png/jpg/webp")
    anon_client = get_anon_client()
    if not anon_client:
        raise HTTPException(status_code=500, detail="Auth disabled")
    try:
        auth_res = anon_client.auth.sign_up({"email": email, "password": password})
        auth_user_id = _get_user_id_from_auth_response(auth_res)
        if not auth_user_id:
            raise HTTPException(status_code=400, detail="Failed to create auth user")
        access_token = _extract_access_token(auth_res)
        supabase = get_service_client()

        college_id = None
        department_id = None
        if college:
            try:
                college_id = str(_resolve_college_id_by_name(college))
            except Exception:
                college_id = None
        if department and college_id:
            try:
                department_id = str(_resolve_department_id(uuid.UUID(college_id), department.upper()))
            except Exception:
                department_id = None

        front_bytes = await id_card_front.read()
        back_bytes = await id_card_back.read()
        if len(front_bytes) > 5 * 1024 * 1024 or len(back_bytes) > 5 * 1024 * 1024:
            raise HTTPException(status_code=400, detail="Image too large (max 5MB)")

        front_ext = Path(id_card_front.filename or "front").suffix.lower() or ".jpg"
        back_ext = Path(id_card_back.filename or "back").suffix.lower() or ".jpg"
        front_name = f"{auth_user_id}_front{front_ext}"
        back_name = f"{auth_user_id}_back{back_ext}"
        front_content_type = id_card_front.content_type or "image/jpeg"
        back_content_type = id_card_back.content_type or "image/jpeg"
        _storage_upload_bytes(supabase, "teacher", front_name, front_bytes, front_content_type)
        _storage_upload_bytes(supabase, "teacher", back_name, back_bytes, back_content_type)
        front_url = _storage_public_url(supabase, "teacher", front_name)
        back_url = _storage_public_url(supabase, "teacher", back_name)

        # Always create a teacher application too (so HOD is also a teacher).
        ins_teacher = supabase.table("teacher_applications").insert({
            "auth_user_id": auth_user_id,
            "email": email,
            "name": name,
            "college_id": college_id,
            "department_id": department_id,
            "subjects": [],
            "id_card_front_path": front_url,
            "id_card_back_path": back_url,
            "status": "pending",
        }).execute()
        if getattr(ins_teacher, "error", None):
            raise HTTPException(status_code=500, detail=f"Supabase error (teacher application insert): {ins_teacher.error}")

        ins_hod = supabase.table("hod_role_applications").insert({
            "auth_user_id": auth_user_id,
            "email": email,
            "name": name,
            "college_id": college_id,
            "department_id": department_id,
            "motivation": motivation,
            "id_card_front_path": front_url,
            "id_card_back_path": back_url,
            "status": "pending",
        }).execute()
        if getattr(ins_hod, "error", None):
            raise HTTPException(status_code=500, detail=f"Supabase error (hod application insert): {ins_hod.error}")

        return {"message": "HOD application submitted", "access_token": access_token, "user_id": auth_user_id}
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Unexpected signup error: {e}")


class HodRoleApproveIn(BaseModel):
    status: str = Field(..., description="approved|rejected")
    notes: Optional[str] = None


@teacher_router.get("/api/admin/hod-applications", summary="Admin: list HOD role applications")
def list_hod_role_applications(status: Optional[str] = Query(default=None), authorization: Optional[str] = Header(default=None)):
    _require_admin(authorization)
    supabase = get_service_client()
    query = supabase.table("hod_role_applications").select("*").order("created_at", desc=True)
    if status:
        query = query.eq("status", status)
    res = query.execute()
    if getattr(res, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (list hod apps): {res.error}")
    return {"applications": res.data or []}


@teacher_router.post("/api/admin/hod-applications/{application_id}/review", summary="Admin: approve or reject a HOD role application")
def review_hod_role_application(application_id: str, payload: HodRoleApproveIn, authorization: Optional[str] = Header(default=None)):
    admin_uid, _ = _require_admin(authorization)
    status = (payload.status or "").strip().lower()
    if status not in {"approved", "rejected"}:
        raise HTTPException(status_code=400, detail="Invalid status (approved|rejected)")
    supabase = get_service_client()

    app_q = supabase.table("hod_role_applications").select("auth_user_id,status,department_id").eq("id", application_id).limit(1).execute()
    if getattr(app_q, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (fetch hod app): {app_q.error}")
    if not app_q.data:
        raise HTTPException(status_code=404, detail="HOD application not found")
    row = app_q.data[0]
    target_uid = row.get("auth_user_id")
    department_id = row.get("department_id")

    upd = supabase.table("hod_role_applications").update({
        "status": status,
        "notes": payload.notes,
        "reviewed_by": admin_uid,
        "reviewed_at": datetime.utcnow().isoformat(),
    }).eq("id", application_id).execute()
    if getattr(upd, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (update hod app): {upd.error}")

    if status == "approved" and target_uid:
        # Ensure teacher application is approved and profile is synced.
        try:
            tapp = supabase.table("teacher_applications").select("id,status").eq("auth_user_id", target_uid).limit(1).execute()
            if not getattr(tapp, "error", None) and tapp.data:
                if tapp.data[0].get("status") != "approved":
                    supabase.table("teacher_applications").update({
                        "status": "approved",
                        "notes": (payload.notes or ""),
                        "reviewed_by": admin_uid,
                        "reviewed_at": datetime.utcnow().isoformat(),
                    }).eq("id", tapp.data[0].get("id")).execute()
        except Exception:
            pass

        try:
            _ensure_teacher_role(target_uid)
        except Exception:
            pass
        try:
            _sync_teacher_profile_from_application(supabase, target_uid)
        except Exception:
            pass

        # Assign HOD role.
        try:
            supabase.table("admin_roles").upsert({"auth_user_id": target_uid, "role": "hod"}).execute()
        except Exception:
            pass

        # Map HOD to their department.
        if department_id:
            try:
                existing = supabase.table("hod_departments").select("id").eq("hod_user_id", target_uid).eq("department_id", department_id).limit(1).execute()
                if not getattr(existing, "error", None) and not (existing.data or []):
                    supabase.table("hod_departments").insert({"hod_user_id": target_uid, "department_id": department_id}).execute()
            except Exception:
                pass

    return {"ok": True, "application_id": application_id, "status": status}


@teacher_router.get("/api/teacher/applications", summary="Admin/Employee: list teacher applications")
def list_teacher_applications(status: Optional[str] = Query(default=None), authorization: Optional[str] = Header(default=None)):
    _require_admin_or_employee(authorization)
    supabase = get_service_client()
    query = supabase.table("teacher_applications").select("*").order("created_at", desc=True)
    if status:
        query = query.eq("status", status)
    res = query.execute()
    if getattr(res, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (list teacher apps): {res.error}")
    return {"applications": res.data or []}


@teacher_router.post("/api/teacher/applications/{application_id}/review", summary="Admin/Employee: approve or reject a teacher application")
def review_teacher_application(application_id: str, payload: TeacherApproveIn, authorization: Optional[str] = Header(default=None)):
    admin_uid, _ = _require_admin_or_employee(authorization)
    supabase = get_service_client()
    debug = bool(os.getenv("TEACHER_PROFILE_DEBUG"))
    if debug:
        try:
            print(f"[TPROF_DEBUG] Review start application_id={application_id} target_status={payload.status} admin={admin_uid}")
        except Exception:
            pass
    app_q = supabase.table("teacher_applications").select("auth_user_id,status").eq("id", application_id).limit(1).execute()
    if getattr(app_q, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (fetch app): {app_q.error}")
    if not app_q.data:
        raise HTTPException(status_code=404, detail="Application not found")
    row = app_q.data[0]
    if debug:
        try:
            print(f"[TPROF_DEBUG] Existing application status={row.get('status')} auth_user_id={row.get('auth_user_id')}")
        except Exception:
            pass
    if row.get("status") != "pending":
        # allow re-review? Only if moving from rejected to approved maybe
        pass
    upd = supabase.table("teacher_applications").update({
        "status": payload.status,
        "notes": payload.notes,
        "reviewed_by": admin_uid,
        "reviewed_at": datetime.utcnow().isoformat()
    }).eq("id", application_id).execute()
    if getattr(upd, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (update app): {upd.error}")
    sync_result: Dict[str, Any] = {"profile_sync": "skipped"}
    if payload.status == "approved":
        _ensure_teacher_role(row.get("auth_user_id"))
        try:
            sync_result = _sync_teacher_profile_from_application(supabase, row.get("auth_user_id")) or {"profile_sync": "no-op"}
            if debug:
                try:
                    print(f"[TPROF_DEBUG] Sync result: {sync_result}")
                except Exception:
                    pass
        except Exception as e:
            # Non-fatal: surface minimal info
            sync_result = {"profile_sync": "error", "error": str(e)}
            try:
                supabase_logger.exception(f"Teacher profile sync failed on approval: {e}")
            except Exception:
                pass
    return {"ok": True, "application_id": application_id, "status": payload.status, **sync_result}


@teacher_router.get("/api/teacher/me/status", summary="Teacher applicant status (self)")
def teacher_me_status(authorization: Optional[str] = Header(default=None)):
    uid, _ = _get_auth_user(authorization)
    supabase = get_service_client()
    app_q = supabase.table("teacher_applications").select("status").eq("auth_user_id", uid).limit(1).execute()
    if getattr(app_q, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (get teacher status): {app_q.error}")
    status = app_q.data[0]["status"] if app_q.data else None
    role_q = supabase.table("admin_roles").select("role").eq("auth_user_id", uid).limit(1).execute()
    current_role = None
    if not getattr(role_q, "error", None) and role_q.data:
        current_role = role_q.data[0].get("role")
    return {"status": status, "role": current_role}


@teacher_router.get("/api/teachers", summary="List approved teachers")
def list_teachers(limit: int = Query(default=100, ge=1, le=500)):
    supabase = get_service_client()
    # join applications + roles + profile (if exists)
    apps = supabase.table("teacher_applications").select("auth_user_id,name,email,college_id,department_id").eq("status", "approved").order("created_at", desc=True).limit(limit).execute()
    if getattr(apps, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (list teachers): {apps.error}")
    rows = apps.data or []
    # Attach role sanity & optional college/department names
    college_ids = {r.get("college_id") for r in rows if r.get("college_id")}
    dept_ids = {r.get("department_id") for r in rows if r.get("department_id")}
    college_map = {}
    dept_map = {}
    if college_ids:
        def _fetch_colleges():
            return supabase.table("colleges").select("id,name").in_("id", list(college_ids)).execute()
        try:
            csel = _supabase_retry(_fetch_colleges)
            if not getattr(csel, "error", None):
                for c in csel.data or []:
                    college_map[c.get("id")] = c
        except Exception as e:  # fallback: proceed without college names
            supabase_logger.warning("College lookup failed after retries: %s", e)
    if dept_ids:
        def _fetch_departments():
            return supabase.table("departments").select("id,name").in_("id", list(dept_ids)).execute()
        try:
            dsel = _supabase_retry(_fetch_departments)
            if not getattr(dsel, "error", None):
                for d in dsel.data or []:
                    dept_map[d.get("id")] = d
        except Exception as e:  # fallback: proceed without department names
            supabase_logger.warning("Department lookup failed after retries: %s", e)
    out = []
    for r in rows:
        out.append({
            "auth_user_id": r.get("auth_user_id"),
            "name": r.get("name"),
            "email": r.get("email"),
            "college": college_map.get(r.get("college_id"), {}).get("name"),
            "department": dept_map.get(r.get("department_id"), {}).get("name"),
        })
    return {"teachers": out, "count": len(out)}


def _canonical_pair(a: str, b: str) -> Tuple[str, str]:
    return (a, b) if a < b else (b, a)


@teacher_router.post("/api/teacher/connect/{other_user_id}", summary="Create or fetch teacher connection")
def teacher_connect(other_user_id: str, authorization: Optional[str] = Header(default=None)):
    uid = _require_teacher(authorization)
    if other_user_id == uid:
        raise HTTPException(status_code=400, detail="Cannot connect to self")
    supabase = get_service_client()
    a, b = _canonical_pair(uid, other_user_id)
    q = supabase.table("teacher_connections").select("id").eq("teacher_a", a).eq("teacher_b", b).limit(1).execute()
    if getattr(q, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (fetch connection): {q.error}")
    if q.data:
        return {"connection_id": q.data[0]["id"], "existing": True}
    ins = supabase.table("teacher_connections").insert({"teacher_a": a, "teacher_b": b}).execute()
    if getattr(ins, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (create connection): {ins.error}")
    cid = ins.data[0]["id"] if ins.data else None
    return {"connection_id": cid, "existing": False}


@teacher_router.get("/api/teacher/connections", summary="List my teacher connections")
def list_my_connections(authorization: Optional[str] = Header(default=None)):
    uid = _require_teacher(authorization)
    supabase = get_service_client()
    # union pattern via OR filter not supported; fetch both sides
    # Apply lightweight retry for transient httpx/httpcore protocol disconnects
    rows_a: List[Dict[str, Any]] = []
    rows_b: List[Dict[str, Any]] = []
    try:
        a_res = _supabase_retry(
            lambda: (
                supabase
                .table("teacher_connections")
                .select("id,teacher_a,teacher_b,created_at")
                .eq("teacher_a", uid)
                .execute()
            )
        )
        if getattr(a_res, "error", None):
            raise HTTPException(status_code=500, detail=f"Supabase error (conn A): {a_res.error}")
        rows_a = a_res.data or []
    except HTTPXRemoteProtocolError as exc:  # pragma: no cover - network timing dependent
        logging.getLogger("teachers").warning(
            "list_my_connections transient protocol error on A-side: %s", exc
        )
        rows_a = []
    try:
        b_res = _supabase_retry(
            lambda: (
                supabase
                .table("teacher_connections")
                .select("id,teacher_a,teacher_b,created_at")
                .eq("teacher_b", uid)
                .execute()
            )
        )
        if getattr(b_res, "error", None):
            raise HTTPException(status_code=500, detail=f"Supabase error (conn B): {b_res.error}")
        rows_b = b_res.data or []
    except HTTPXRemoteProtocolError as exc:  # pragma: no cover - network timing dependent
        logging.getLogger("teachers").warning(
            "list_my_connections transient protocol error on B-side: %s", exc
        )
        rows_b = []
    rows = rows_a + rows_b
    partner_ids = []
    for r in rows:
        partner_ids.append(r.get("teacher_b") if r.get("teacher_a") == uid else r.get("teacher_a"))
    # enrich partner basic info from teacher_applications (fallback to user_profiles)
    partner_ids = [p for p in partner_ids if p]
    uniq = list({p for p in partner_ids})
    partner_map = {}
    if uniq:
        try:
            tapp = _supabase_retry(
                lambda: (
                    supabase
                    .table("teacher_applications")
                    .select("auth_user_id,name")
                    .in_("auth_user_id", uniq)
                    .execute()
                )
            )
            if not getattr(tapp, "error", None):
                for t in tapp.data or []:
                    partner_map[t.get("auth_user_id")] = t
        except HTTPXRemoteProtocolError as exc:  # pragma: no cover - network timing dependent
            logging.getLogger("teachers").warning(
                "list_my_connections transient protocol error on partner lookup: %s", exc
            )
    out = []
    for r in rows:
        partner = r.get("teacher_b") if r.get("teacher_a") == uid else r.get("teacher_a")
        out.append({
            "connection_id": r.get("id"),
            "partner_user_id": partner,
            "partner_name": partner_map.get(partner, {}).get("name"),
            "created_at": r.get("created_at"),
        })
    return {"connections": out, "count": len(out)}


@teacher_router.post("/api/teacher/connections/{connection_id}/messages", summary="Send message on a connection")
def send_message(connection_id: str, payload: TeacherMessageIn, authorization: Optional[str] = Header(default=None)):
    uid = _require_teacher(authorization)
    supabase = get_service_client()
    # verify membership
    c = supabase.table("teacher_connections").select("teacher_a,teacher_b").eq("id", connection_id).limit(1).execute()
    if getattr(c, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (get conn): {c.error}")
    if not c.data:
        raise HTTPException(status_code=404, detail="Connection not found")
    row = c.data[0]
    if uid not in {row.get("teacher_a"), row.get("teacher_b")}:
        raise HTTPException(status_code=403, detail="Not a participant")
    ins = supabase.table("teacher_messages").insert({
        "connection_id": connection_id,
        "sender_user_id": uid,
        "content": payload.content
    }).execute()
    if getattr(ins, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (insert msg): {ins.error}")
    return {"ok": True, "message_id": ins.data[0]["id"] if ins.data else None}


@teacher_router.get("/api/teacher/connections/{connection_id}/messages", summary="List messages in a connection")
def list_messages(connection_id: str, since: Optional[str] = Query(default=None), limit: int = Query(default=200, ge=1, le=500), authorization: Optional[str] = Header(default=None)):
    uid = _require_teacher(authorization)
    supabase = get_service_client()
    c = supabase.table("teacher_connections").select("teacher_a,teacher_b").eq("id", connection_id).limit(1).execute()
    if getattr(c, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (get conn): {c.error}")
    if not c.data:
        raise HTTPException(status_code=404, detail="Connection not found")
    row = c.data[0]
    if uid not in {row.get("teacher_a"), row.get("teacher_b")}:
        raise HTTPException(status_code=403, detail="Not a participant")
    q = supabase.table("teacher_messages").select("id,sender_user_id,content,created_at").eq("connection_id", connection_id).order("created_at")
    if since:
        q = q.gt("created_at", since)
    res = q.limit(limit).execute()
    if getattr(res, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (list msgs): {res.error}")
    return {"messages": res.data or [], "count": len(res.data or [])}


@teacher_router.get("/api/teacher/notes/upload-meta", summary="Dynamic academic dropdown metadata for teacher notes upload")
def teacher_notes_meta(authorization: Optional[str] = Header(default=None)):
    _require_teacher(authorization)  # role gating only
    supabase = get_service_client()

    def _safe_select(table: str, columns: str, order: Optional[str] = None):
        try:
            q = supabase.table(table).select(columns)
            if order:
                q = q.order(order)
            res = q.execute()
            if getattr(res, "error", None):
                # Best-effort: return empty list on transient failures
                try:
                    supabase_logger.warning(f"Teacher meta query error: {table}: {res.error}")
                except Exception:
                    pass
                return []
            return res.data or []
        except Exception as e:  # network/protocol errors
            try:
                supabase_logger.exception(f"Teacher meta exception on {table}")
            except Exception:
                pass
            return []

    colleges = _safe_select("colleges", "id,name", order="name")
    degrees = _safe_select("degrees", "id,name,college_id")
    departments = _safe_select("departments", "id,name,college_id,degree_id")
    batches = _safe_select("batches", "id,department_id,from_year,to_year")

    if not (colleges or degrees or departments or batches):
        raise HTTPException(status_code=500, detail="Supabase unavailable for teacher meta")

    return {
        "colleges": colleges,
        "degrees": degrees,
        "departments": departments,
        "batches": batches,
    }


# Integrate simple reuse of existing marketplace notes for teacher uploads: teacher uses existing /api/marketplace/notes routes.
# Extra filtering endpoint for teacher's own notes.
@teacher_router.get("/api/teacher/notes/mine", summary="List notes uploaded by the current teacher (marketplace integration)")
def teacher_my_notes(authorization: Optional[str] = Header(default=None)):
    uid = _require_teacher(authorization)
    supabase = get_service_client()
    res = (
        supabase.table("marketplace_notes")
        .select("id,title,description,subject,subject_id,semester,created_at,updated_at,price_cents,unit,exam_type,categories,original_filename")
        .eq("owner_user_id", uid)
        .order("created_at", desc=True)
        .limit(200)
        .execute()
    )
    if getattr(res, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (my teacher notes): {res.error}")
    return {"notes": res.data or []}

# ---------------- Teacher Profile & Classes Aggregation ----------------

def _fetch_teacher_core(supabase: Client, teacher_user_id: str) -> Dict[str, Any]:
    """Fetch teacher core info from teacher_applications + admin_roles + user_profiles + teacher_profiles."""
    core: Dict[str, Any] = {"auth_user_id": teacher_user_id}
    # Extended teacher profile (now primary source for identity & academic linkage)
    tprof = supabase.table("teacher_profiles").select("name,email,college_id,department_id,headline,bio,specialization,years_experience,qualification,availability,social,profile_image_url").eq("auth_user_id", teacher_user_id).limit(1).execute()
    if not getattr(tprof, "error", None) and tprof.data:
        rowp = tprof.data[0]
        # Identity / academic fields preferred from teacher_profiles first
        for k in ["name","email","college_id","department_id"]:
            if rowp.get(k):
                core[k] = rowp.get(k)
        core.update({k: rowp.get(k) for k in ["headline","bio","specialization","years_experience","qualification","availability","social"] if k in rowp})
        if rowp.get("profile_image_url"):
            core["avatar_url"] = rowp.get("profile_image_url")
    # Application (fallback / supplemental for subjects & status & missing identity)
    app = supabase.table("teacher_applications").select("name,email,college_id,department_id,subjects,status").eq("auth_user_id", teacher_user_id).limit(1).execute()
    if getattr(app, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (teacher application): {app.error}")
    if app.data:
        arow = app.data[0]
        # subjects and status always sourced from application
        for k in ["subjects","status"]:
            if arow.get(k) is not None:
                core[k] = arow.get(k)
        # Only fill identity if still missing
        for k in ["name","email","college_id","department_id"]:
            if not core.get(k) and arow.get(k):
                core[k] = arow.get(k)
    # Role
    role_res = supabase.table("admin_roles").select("role").eq("auth_user_id", teacher_user_id).limit(1).execute()
    if not getattr(role_res, "error", None) and role_res.data:
        core["role"] = role_res.data[0].get("role")
    # User profile (legacy avatar/headline/bio fallback)
    prof = supabase.table("user_profiles").select("name,profile_image_url,headline,bio,semester,batch_from,batch_to").eq("auth_user_id", teacher_user_id).limit(1).execute()
    if not getattr(prof, "error", None) and prof.data:
        row = prof.data[0]
        if not core.get("name") and row.get("name"):
            core["name"] = row.get("name")
        if not core.get("avatar_url") and row.get("profile_image_url"):
            core["avatar_url"] = row.get("profile_image_url")
        if not core.get("headline") and row.get("headline"):
            core["headline"] = row.get("headline")
        if not core.get("bio") and row.get("bio"):
            core["bio"] = row.get("bio")
    return core

def _sync_teacher_profile_from_application(supabase: Client, auth_user_id: Optional[str]) -> Optional[Dict[str, Any]]:
    """Idempotently upsert identity & academic linkage fields from approved teacher_application into teacher_profiles.

    Returns a dict with keys: profile_sync (insert|update|no-op|error), and maybe details.
    Silent no-op if application missing or not approved.
    """
    debug = bool(os.getenv("TEACHER_PROFILE_DEBUG"))
    if not auth_user_id:
        if debug:
            try: print("[TPROF_DEBUG] No auth_user_id passed to sync")
            except Exception: pass
        return {"profile_sync": "no-op", "reason": "missing auth_user_id"}
    app = supabase.table("teacher_applications").select("name,email,college_id,department_id,status").eq("auth_user_id", auth_user_id).limit(1).execute()
    if getattr(app, "error", None) or not app.data:
        if debug:
            try: print(f"[TPROF_DEBUG] Application missing or error error={getattr(app,'error',None)}")
            except Exception: pass
        return {"profile_sync": "no-op", "reason": "application missing"}
    row = app.data[0]
    if row.get("status") != "approved":
        if debug:
            try: print(f"[TPROF_DEBUG] Application not approved status={row.get('status')}")
            except Exception: pass
        return {"profile_sync": "no-op", "reason": "not approved"}
    payload = _supabase_payload({
        "auth_user_id": str(auth_user_id) if auth_user_id else None,
        "name": row.get("name"),
        "email": row.get("email"),
        "college_id": row.get("college_id"),
        "department_id": row.get("department_id"),
    })
    existing = supabase.table("teacher_profiles").select("auth_user_id").eq("auth_user_id", auth_user_id).limit(1).execute()
    if getattr(existing, "error", None):
        if debug:
            try: print(f"[TPROF_DEBUG] Existing profile lookup error={existing.error}")
            except Exception: pass
        return {"profile_sync": "error", "error": str(existing.error)}
    if existing.data:
        upd_payload = {k: v for k, v in payload.items() if k != "auth_user_id"}
        upd = supabase.table("teacher_profiles").update(upd_payload).eq("auth_user_id", auth_user_id).execute()
        if getattr(upd, "error", None):
            err_txt = str(upd.error)
            if debug:
                try: print(f"[TPROF_DEBUG] Update error err={err_txt}")
                except Exception: pass
            return {"profile_sync": "error", "error": err_txt}
        if debug:
            try: print("[TPROF_DEBUG] Profile updated")
            except Exception: pass
        return {"profile_sync": "update"}
    ins = supabase.table("teacher_profiles").insert(payload).execute()
    if getattr(ins, "error", None):
        err_txt = str(ins.error)
        if debug:
            try: print(f"[TPROF_DEBUG] Insert error err={err_txt}")
            except Exception: pass
        # Common cause: migration not applied (new columns absent)
        return {"profile_sync": "error", "error": err_txt, "hint": "Run ALTER TABLE statements from db.sql migration note"}
    if debug:
        try: print("[TPROF_DEBUG] Profile inserted")
        except Exception: pass
    return {"profile_sync": "insert"}

@teacher_router.post("/api/admin/teacher/profile/resync/{user_id}", summary="Admin: force resync teacher profile from application")
def admin_resync_teacher_profile(user_id: str, authorization: Optional[str] = Header(default=None)):
    _require_admin(authorization)
    supabase = get_service_client()
    result = _sync_teacher_profile_from_application(supabase, user_id)
    return {"ok": True, **(result or {"profile_sync": "no-op"})}

@teacher_router.get("/api/admin/teacher-profiles/{user_id}", summary="Admin: fetch raw teacher_profiles row")
def admin_get_teacher_profile_row(user_id: str, authorization: Optional[str] = Header(default=None)):
    _require_admin(authorization)
    supabase = get_service_client()
    res = supabase.table("teacher_profiles").select("*").eq("auth_user_id", user_id).limit(1).execute()
    if getattr(res, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (get teacher profile raw): {res.error}")
    return {"row": (res.data or [None])[0]}

@teacher_router.post("/api/admin/teacher-profiles/backfill", summary="Admin: backfill all approved teacher applications into teacher_profiles")
def admin_backfill_teacher_profiles(authorization: Optional[str] = Header(default=None)):
    _require_admin(authorization)
    supabase = get_service_client()
    # Fetch all approved applications
    apps = supabase.table("teacher_applications").select("auth_user_id,name,email,college_id,department_id,status").eq("status","approved").execute()
    if getattr(apps, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (fetch approved apps): {apps.error}")
    rows = apps.data or []
    inserted = 0; updated = 0; errors = []
    for r in rows:
        uid = r.get("auth_user_id")
        if not uid:
            continue
        payload = _supabase_payload({
            "auth_user_id": str(uid) if uid else None,
            "name": r.get("name"),
            "email": r.get("email"),
            "college_id": r.get("college_id"),
            "department_id": r.get("department_id"),
        })
        existing = supabase.table("teacher_profiles").select("auth_user_id").eq("auth_user_id", uid).limit(1).execute()
        if getattr(existing, "error", None):
            errors.append({"user": uid, "error": str(existing.error)})
            continue
        if existing.data:
            upd_payload = {k: v for k, v in payload.items() if k != "auth_user_id"}
            upd = supabase.table("teacher_profiles").update(upd_payload).eq("auth_user_id", uid).execute()
            if getattr(upd, "error", None):
                errors.append({"user": uid, "error": str(upd.error)})
            else:
                updated += 1
        else:
            ins = supabase.table("teacher_profiles").insert(payload).execute()
            if getattr(ins, "error", None):
                errors.append({"user": uid, "error": str(ins.error)})
            else:
                inserted += 1
    return {"ok": True, "inserted": inserted, "updated": updated, "errors": errors, "total_approved": len(rows)}

@teacher_router.get("/api/admin/teacher-profiles/diagnostics", summary="Admin: diagnostics counts for teacher profiles vs applications")
def admin_teacher_profiles_diagnostics(authorization: Optional[str] = Header(default=None)):
    _require_admin(authorization)
    supabase = get_service_client()
    apps = supabase.table("teacher_applications").select("auth_user_id,status").execute()
    if getattr(apps, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (apps diag): {apps.error}")
    profs = supabase.table("teacher_profiles").select("auth_user_id").execute()
    if getattr(profs, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (profiles diag): {profs.error}")
    app_rows = apps.data or []
    prof_rows = profs.data or []
    approved_set = {r.get("auth_user_id") for r in app_rows if r.get("status") == "approved" and r.get("auth_user_id")}
    profile_set = {r.get("auth_user_id") for r in prof_rows if r.get("auth_user_id")}
    missing = sorted(list(approved_set - profile_set))
    extra = sorted(list(profile_set - approved_set))
    return {"approved_count": len(approved_set), "profile_count": len(profile_set), "missing_profiles_for_approved": missing, "profiles_without_approved_app": extra}

def _enrich_academics(supabase: Client, core: Dict[str, Any]):
    college_id = core.get("college_id")
    dept_id = core.get("department_id")
    if college_id:
        c = supabase.table("colleges").select("name").eq("id", college_id).limit(1).execute()
        if not getattr(c, "error", None) and c.data:
            core["college_name"] = c.data[0].get("name")
    if dept_id:
        d = supabase.table("departments").select("name,degree_id").eq("id", dept_id).limit(1).execute()
        degree_id = None
        if not getattr(d, "error", None) and d.data:
            core["department_name"] = d.data[0].get("name")
            degree_id = d.data[0].get("degree_id")
        if degree_id:
            deg = supabase.table("degrees").select("name").eq("id", degree_id).limit(1).execute()
            if not getattr(deg, "error", None) and deg.data:
                core["degree_name"] = deg.data[0].get("name")

def _fetch_teacher_classes(supabase: Client, teacher_user_id: str) -> List[Dict[str, Any]]:
    cls = supabase.table("teacher_classes").select("id,batch_id,semester,subject,subject_id,section,degree_id,department_id,college_id,created_at").eq("teacher_user_id", teacher_user_id).order("created_at", desc=True).limit(500).execute()
    if getattr(cls, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (teacher classes): {cls.error}")
    classes = cls.data or []
    # Collect IDs for enrichment
    batch_ids = {c.get("batch_id") for c in classes if c.get("batch_id")}
    degree_ids = {c.get("degree_id") for c in classes if c.get("degree_id")}
    dept_ids = {c.get("department_id") for c in classes if c.get("department_id")}
    college_ids = {c.get("college_id") for c in classes if c.get("college_id")}
    batch_map: Dict[str, Any] = {}
    def _sel(table, ids, cols):
        if not ids:
            return {}
        res = supabase.table(table).select(cols).in_("id", list(ids)).execute()
        out = {}
        if not getattr(res, "error", None):
            for r in res.data or []:
                out[r.get("id")] = r
        return out
    batch_map = _sel("batches", batch_ids, "id,from_year,to_year")
    degree_map = _sel("degrees", degree_ids, "id,name")
    dept_map = _sel("departments", dept_ids, "id,name")
    college_map = _sel("colleges", college_ids, "id,name")
    for c in classes:
        bid = c.get("batch_id"); b = batch_map.get(bid)
        if b:
            c["batch_range"] = f"{b.get('from_year')}-{b.get('to_year')}" if b.get('from_year') and b.get('to_year') else None
        if c.get("degree_id"):
            c["degree_name"] = degree_map.get(c.get("degree_id"), {}).get("name")
        if c.get("department_id"):
            c["department_name"] = dept_map.get(c.get("department_id"), {}).get("name")
        if c.get("college_id"):
            c["college_name"] = college_map.get(c.get("college_id"), {}).get("name")
        # Label for UI
        sem = c.get("semester")
        subj = c.get("subject")
        c["label"] = f"Sem {sem}: {subj}" if sem and subj else (subj or "Class")
    return classes

def _group_classes(classes: List[Dict[str, Any]]) -> Dict[str, List[Dict[str, Any]]]:
    groups: Dict[str, List[Dict[str, Any]]] = {}
    for c in classes:
        sem = c.get("semester") or 0
        key = f"Semester {sem}" if sem else "Unassigned"
        groups.setdefault(key, []).append(c)
    # Sort within groups by subject
    for g in groups.values():
        g.sort(key=lambda x: (x.get("subject") or "").lower())
    # Order groups numerically
    ordered = dict(sorted(groups.items(), key=lambda kv: int(re.sub(r"[^0-9]", "", kv[0]) or 0)))
    return ordered

@teacher_router.get("/api/teacher/profile/{user_id}", summary="Get teacher profile (public)")
async def get_teacher_profile(
    user_id: str,
    authorization: Optional[str] = Header(default=None),
    strict_q: Optional[str] = Query(default=None, alias="strict"),
    x_strict: Optional[str] = Header(default=None, alias="X-TeacherProfile-Strict"),
):
    """Optimized variant that:
    - Resolves 'me' and then performs concurrent Supabase reads
    - Honors `strict` flag (query or header) to avoid legacy fallbacks for faster response
    - Uses estimated count for notes to reduce DB cost
    """
    supabase = get_service_client()

    # Resolve 'me'
    if user_id == "me":
        try:
            uid, _ = _get_auth_user(authorization)
            user_id = uid
        except HTTPException:
            raise HTTPException(status_code=401, detail="Authentication required for 'me'")

    def _is_truthy(val: Optional[str]) -> bool:
        if val is None:
            return False
        v = str(val).strip().lower()
        return v in {"1", "true", "yes", "on"}

    strict = _is_truthy(strict_q) or _is_truthy(x_strict)

    # --- Concurrent fetches: core, classes, and notes count ---
    def _retry_blocking(fn, *a, retries=3, base_delay=0.15, **kw):
        last_exc = None
        for attempt in range(retries):
            try:
                return fn(*a, **kw)
            except RETRYABLE_EXCEPTIONS as e:  # type: ignore
                last_exc = e
                time.sleep(base_delay * (attempt + 1))
                continue
        if last_exc:
            supabase_logger.warning("teacher_profile core fetch retry exhausted: %s", last_exc)
        return fn(*a, **kw)  # final attempt, let exception propagate

    async def fetch_core():
        # Parallelize core sources: teacher_profiles, teacher_applications, admin_roles, user_profiles (if not strict)
        loop = asyncio.get_running_loop()
        tprof_fut = loop.run_in_executor(None, lambda: _retry_blocking(lambda: supabase.table("teacher_profiles").select(
            "name,email,college_id,department_id,headline,bio,specialization,years_experience,qualification,availability,social,profile_image_url"
        ).eq("auth_user_id", user_id).limit(1).execute()))
        tapp_fut = loop.run_in_executor(None, lambda: _retry_blocking(lambda: supabase.table("teacher_applications").select(
            "name,email,college_id,department_id,subjects,status"
        ).eq("auth_user_id", user_id).limit(1).execute()))
        role_fut = loop.run_in_executor(None, lambda: _retry_blocking(lambda: supabase.table("admin_roles").select("role").eq("auth_user_id", user_id).limit(1).execute()))
        prof_fut = None
        if not strict:
            prof_fut = loop.run_in_executor(None, lambda: _retry_blocking(lambda: supabase.table("user_profiles").select(
                "name,profile_image_url,headline,bio,semester,batch_from,batch_to"
            ).eq("auth_user_id", user_id).limit(1).execute()))

        tprof, tapp, role_res, prof = await asyncio.gather(
            tprof_fut, tapp_fut, role_fut, prof_fut if prof_fut is not None else asyncio.sleep(0, result=None)
        )

        if getattr(tapp, "error", None):
            raise HTTPException(status_code=500, detail=f"Supabase error (teacher application): {tapp.error}")

        core: Dict[str, Any] = {"auth_user_id": user_id}
        if not getattr(tprof, "error", None) and getattr(tprof, "data", None):
            rowp = tprof.data[0]
            for k in ["name", "email", "college_id", "department_id"]:
                if rowp.get(k):
                    core[k] = rowp.get(k)
            core.update({k: rowp.get(k) for k in ["headline", "bio", "specialization", "years_experience", "qualification", "availability", "social"] if k in rowp})
            if rowp.get("profile_image_url"):
                core["avatar_url"] = rowp.get("profile_image_url")

        if getattr(tapp, "data", None):
            arow = tapp.data[0]
            for k in ["subjects", "status"]:
                if arow.get(k) is not None:
                    core[k] = arow.get(k)
            for k in ["name", "email", "college_id", "department_id"]:
                if not core.get(k) and arow.get(k):
                    core[k] = arow.get(k)

        if not getattr(role_res, "error", None) and getattr(role_res, "data", None):
            core["role"] = role_res.data[0].get("role")

        if (not strict) and prof is not None and (not getattr(prof, "error", None)) and getattr(prof, "data", None):
            row = prof.data[0]
            if not core.get("name") and row.get("name"):
                core["name"] = row.get("name")
            if not core.get("avatar_url") and row.get("profile_image_url"):
                core["avatar_url"] = row.get("profile_image_url")
            if not core.get("headline") and row.get("headline"):
                core["headline"] = row.get("headline")
            if not core.get("bio") and row.get("bio"):
                core["bio"] = row.get("bio")

        return core

    async def fetch_classes():
        # Fetch classes, then enrich lookups concurrently
        loop = asyncio.get_running_loop()
        cls = await loop.run_in_executor(None, lambda: _retry_blocking(lambda: supabase.table("teacher_classes").select(
            "id,batch_id,semester,subject,subject_id,section,degree_id,department_id,college_id,created_at"
        ).eq("teacher_user_id", user_id).order("created_at", desc=True).limit(500).execute()))
        if getattr(cls, "error", None):
            raise HTTPException(status_code=500, detail=f"Supabase error (teacher classes): {cls.error}")
        classes = cls.data or []
        batch_ids = {c.get("batch_id") for c in classes if c.get("batch_id")}
        degree_ids = {c.get("degree_id") for c in classes if c.get("degree_id")}
        dept_ids = {c.get("department_id") for c in classes if c.get("department_id")}
        college_ids = {c.get("college_id") for c in classes if c.get("college_id")}

        def _sel(table, ids, cols):
            if not ids:
                return {}
            def _do():
                return supabase.table(table).select(cols).in_("id", list(ids)).execute()
            res = _retry_blocking(_do)
            out = {}
            if not getattr(res, "error", None):
                for r in res.data or []:
                    out[r.get("id")] = r
            return out

        # Run the 4 lookups concurrently
        batch_fut = asyncio.to_thread(_sel, "batches", batch_ids, "id,from_year,to_year")
        degree_fut = asyncio.to_thread(_sel, "degrees", degree_ids, "id,name")
        dept_fut = asyncio.to_thread(_sel, "departments", dept_ids, "id,name")
        college_fut = asyncio.to_thread(_sel, "colleges", college_ids, "id,name")
        
        # Also fetch student counts in parallel
        async def _fetch_student_counts():
            # Gather unique combinations to query
            keys = set()
            for c in classes:
                bid = str(c.get("batch_id")) if c.get("batch_id") else None
                sec = str(c.get("section")) if c.get("section") else None
                sem = int(c.get("semester")) if c.get("semester") else None
                if bid and sec and sem:
                    keys.add((bid, sec, sem))
            
            if not keys:
                return {}
            
            # We can't easily do a precise "IN" query for tuples in Supabase/PostgREST without a stored procedure 
            # or complex OR filtering which might hit URL length limits.
            # Instead, we'll fetch students filter by the set of batch_ids (which is usually small)
            # and then aggregate in memory. This is generally safe as a teacher doesn't have thousands of classes.
            relevant_batch_ids = {k[0] for k in keys}
            if not relevant_batch_ids:
                return {}
            
            try:
                # Fetch minimal fields for counting
                loop = asyncio.get_running_loop()
                res = await loop.run_in_executor(None, lambda: supabase.table("user_education")
                    .select("batch_id,section,current_semester")
                    .in_("batch_id", list(relevant_batch_ids))
                    .execute())
                
                if getattr(res, "error", None):
                    return {}
                
                rows = res.data or []
                counts = {}
                for r in rows:
                    b = str(r.get("batch_id"))
                    s = str(r.get("section"))
                    sm = int(r.get("current_semester") or 0)
                    k = f"{b}|{s}|{sm}"
                    counts[k] = counts.get(k, 0) + 1
                return counts
            except Exception:
                return {}

        batch_map, degree_map, dept_map, college_map, student_counts = await asyncio.gather(
            batch_fut, degree_fut, dept_fut, college_fut, _fetch_student_counts()
        )

        for c in classes:
            bid = c.get("batch_id"); b = batch_map.get(bid)
            if b:
                c["batch_range"] = f"{b.get('from_year')}-{b.get('to_year')}" if b.get('from_year') and b.get('to_year') else None
            if c.get("degree_id"):
                c["degree_name"] = degree_map.get(c.get("degree_id"), {}).get("name")
            if c.get("department_id"):
                c["department_name"] = dept_map.get(c.get("department_id"), {}).get("name")
            if c.get("college_id"):
                c["college_name"] = college_map.get(c.get("college_id"), {}).get("name")
            
            # Map student count
            bid_str = str(c.get("batch_id")) if c.get("batch_id") else None
            sec_str = str(c.get("section")) if c.get("section") else None
            sem_int = int(c.get("semester")) if c.get("semester") else None
            if bid_str and sec_str and sem_int:
               c["student_count"] = student_counts.get(f"{bid_str}|{sec_str}|{sem_int}", 0)
            else:
               c["student_count"] = 0

            sem = c.get("semester"); subj = c.get("subject")
            c["label"] = f"Sem {sem}: {subj}" if sem and subj else (subj or "Class")
        return classes

    async def enrich_academics(core: Dict[str, Any]):
        # Resolve college/department names; do department & college in parallel, then maybe degree
        loop = asyncio.get_running_loop()
        college_id = core.get("college_id")
        dept_id = core.get("department_id")
        if not college_id and not dept_id:
            return
        college_fut = loop.run_in_executor(None, lambda: _retry_blocking(lambda: supabase.table("colleges").select("name").eq("id", college_id).limit(1).execute())) if college_id else asyncio.sleep(0, result=None)
        dept_fut = loop.run_in_executor(None, lambda: _retry_blocking(lambda: supabase.table("departments").select("name,degree_id").eq("id", dept_id).limit(1).execute())) if dept_id else asyncio.sleep(0, result=None)
        college_res, dept_res = await asyncio.gather(college_fut, dept_fut)
        degree_id = None
        if college_res and not getattr(college_res, "error", None) and getattr(college_res, "data", None):
            core["college_name"] = college_res.data[0].get("name")
        if dept_res and not getattr(dept_res, "error", None) and getattr(dept_res, "data", None):
            core["department_name"] = dept_res.data[0].get("name")
            degree_id = dept_res.data[0].get("degree_id")
        if degree_id:
            deg = await loop.run_in_executor(None, lambda: _retry_blocking(lambda: supabase.table("degrees").select("name").eq("id", degree_id).limit(1).execute()))
            if not getattr(deg, "error", None) and getattr(deg, "data", None):
                core["degree_name"] = deg.data[0].get("name")

    async def fetch_notes_count():
        loop = asyncio.get_running_loop()
        try:
            # estimated is much cheaper than exact for counts
            nres = await loop.run_in_executor(None, lambda: supabase.table("marketplace_notes").select("id", count="estimated").eq("owner_user_id", user_id).execute())
            if not getattr(nres, "error", None):
                return getattr(nres, "count", 0) or 0
        except Exception:
            return 0
        return 0

    core, classes, notes_count = await asyncio.gather(fetch_core(), fetch_classes(), fetch_notes_count())

    if core.get("role") not in {"teacher", "admin"} and core.get("status") != "approved":
        raise HTTPException(status_code=404, detail="Teacher not found")

    # Enrich academics after core is fetched
    await enrich_academics(core)

    subjects = sorted({c.get("subject") for c in classes if c.get("subject")})
    grouped = _group_classes(classes)
    stats = {
        "notes_count": notes_count,
        "classes_count": len(classes),
        "subjects_count": len(subjects),
    }
    return {"teacher": core, "classes": classes, "grouped_classes": grouped, "subjects": subjects, "stats": stats}

class TeacherProfileUpsertIn(BaseModel):
    headline: Optional[str] = None
    bio: Optional[str] = None
    specialization: Optional[List[str]] = None
    years_experience: Optional[int] = Field(None, ge=0, le=80)
    qualification: Optional[str] = None
    availability: Optional[Dict[str, Any]] = None
    social: Optional[Dict[str, Any]] = None
    # Allow editing identity/academic linkage if needed (optional; admin may validate externally)
    name: Optional[str] = None
    email: Optional[str] = None
    college_id: Optional[uuid.UUID] = None
    department_id: Optional[uuid.UUID] = None
    profile_image_url: Optional[str] = None  # normally set via upload endpoint

# ================= Teacher Classes CRUD Models ==================
class TeacherClassIn(BaseModel):
    subject: str = Field(..., min_length=1, max_length=255)
    semester: Optional[int] = Field(None, ge=1, le=12)
    batch_id: Optional[uuid.UUID] = None
    section: Optional[str] = Field(None, max_length=32)
    college_id: Optional[uuid.UUID] = None
    degree_id: Optional[uuid.UUID] = None
    department_id: Optional[uuid.UUID] = None
    subject_id: Optional[uuid.UUID] = None  # link to syllabus_courses.id
    notes: Optional[str] = None

class TeacherClassUpdate(BaseModel):
    subject: Optional[str] = Field(None, min_length=1, max_length=255)
    semester: Optional[int] = Field(None, ge=1, le=12)
    batch_id: Optional[uuid.UUID] = None
    section: Optional[str] = Field(None, max_length=32)
    college_id: Optional[uuid.UUID] = None
    degree_id: Optional[uuid.UUID] = None
    department_id: Optional[uuid.UUID] = None
    subject_id: Optional[uuid.UUID] = None
    notes: Optional[str] = None

class TeacherClassOut(BaseModel):
    id: uuid.UUID
    teacher_user_id: uuid.UUID
    subject: str
    semester: Optional[int] = None
    batch_id: Optional[uuid.UUID] = None
    section: Optional[str] = None
    college_id: Optional[uuid.UUID] = None
    degree_id: Optional[uuid.UUID] = None
    department_id: Optional[uuid.UUID] = None
    subject_id: Optional[uuid.UUID] = None
    notes: Optional[str] = None
    created_at: Optional[datetime] = None
    updated_at: Optional[datetime] = None


class TeacherClassStudent(BaseModel):
    profile_id: uuid.UUID
    user_education_id: uuid.UUID
    user_auth_id: Optional[uuid.UUID] = None
    name: Optional[str] = None
    email: Optional[str] = None
    regno: Optional[str] = None
    phone: Optional[str] = None
    section: Optional[str] = None
    batch_id: Optional[uuid.UUID] = None
    batch_label: Optional[str] = None
    current_semester: Optional[int] = None
    avatar_url: Optional[str] = None
    completed_topics: int = 0
    total_topics: int = 0
    progress_pct: Optional[float] = None


class TeacherClassStudentsResponse(BaseModel):
    class_info: TeacherClassOut
    students: List[TeacherClassStudent]
    total: int
    applied_filters: Dict[str, Any] = Field(default_factory=dict)

def _map_teacher_class_row(row: Dict[str, Any]) -> TeacherClassOut:
    return TeacherClassOut(
        id=uuid.UUID(row["id"]),
        teacher_user_id=uuid.UUID(row["teacher_user_id"]),
        subject=row.get("subject") or "",
        semester=row.get("semester"),
        batch_id=uuid.UUID(row["batch_id"]) if row.get("batch_id") else None,
        section=row.get("section"),
        college_id=uuid.UUID(row["college_id"]) if row.get("college_id") else None,
        degree_id=uuid.UUID(row["degree_id"]) if row.get("degree_id") else None,
        department_id=uuid.UUID(row["department_id"]) if row.get("department_id") else None,
        subject_id=uuid.UUID(row["subject_id"]) if row.get("subject_id") else None,
        notes=row.get("notes"),
        created_at=row.get("created_at"),
        updated_at=row.get("updated_at"),
    )

@teacher_router.put("/api/teacher/profile/me", summary="Upsert my extended teacher profile")
def upsert_teacher_profile(payload: TeacherProfileUpsertIn, authorization: Optional[str] = Header(default=None)):
    uid = _require_teacher(authorization)
    supabase = get_service_client()
    row = _supabase_payload(payload.dict(exclude_unset=True))
    if not row:
        return {"ok": True, "updated": False}
    row["auth_user_id"] = str(uid)
    row = _supabase_payload(row)
    # Try update first
    existing = supabase.table("teacher_profiles").select("auth_user_id").eq("auth_user_id", uid).limit(1).execute()
    if getattr(existing, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (get teacher profile): {existing.error}")
    if existing.data:
        upd = supabase.table("teacher_profiles").update(row).eq("auth_user_id", uid).execute()
        if getattr(upd, "error", None):
            raise HTTPException(status_code=500, detail=f"Supabase error (update teacher profile): {upd.error}")
    else:
        ins = supabase.table("teacher_profiles").insert(row).execute()
        if getattr(ins, "error", None):
            raise HTTPException(status_code=500, detail=f"Supabase error (insert teacher profile): {ins.error}")
    return {"ok": True, "updated": True}

@teacher_router.post("/api/teacher/profile/avatar", summary="Upload/replace teacher profile avatar (stores public URL in teacher_profiles)")
async def upload_teacher_avatar(file: UploadFile = File(...), authorization: Optional[str] = Header(default=None)):
    """Store teacher avatar in a Supabase Storage bucket and persist the public URL.

    Env vars consulted (first found wins):
      SUPABASE_TEACHER_AVATARS_BUCKET | SUPABASE_AVATARS_BUCKET | SUPABASE_PUBLIC_BUCKET | (fallback) 'teacher-avatars'
    Object key pattern: teacher_avatars/<auth_user_id><ext>
    """
    uid = _require_teacher(authorization)
    supabase = get_service_client()
    debug = bool(os.getenv("TEACHER_PROFILE_DEBUG"))
    filename = file.filename or "avatar.jpg"
    ext = (Path(filename).suffix or ".jpg").lower()
    allowed = {".jpg", ".jpeg", ".png", ".webp", ".gif"}
    if ext not in allowed:
        raise HTTPException(status_code=400, detail="Unsupported image type")
    data = await file.read()
    if len(data) > 3 * 1024 * 1024:
        raise HTTPException(status_code=400, detail="Image too large (max 3MB)")
    mime_map = {".jpg": "image/jpeg", ".jpeg": "image/jpeg", ".png": "image/png", ".webp": "image/webp", ".gif": "image/gif"}
    content_type = mime_map.get(ext, "application/octet-stream")
    bucket = (
        os.getenv("SUPABASE_TEACHER_AVATARS_BUCKET")
        or os.getenv("SUPABASE_AVATARS_BUCKET")
        or os.getenv("SUPABASE_BUCKET")  # generic project bucket key you provided
        or os.getenv("SUPABASE_PUBLIC_BUCKET")
        or "teacher-avatars"
    )
    object_path = f"teacher_avatars/{uid}{ext}"
    try:
        # Ensure bucket exists (service role key has permission)
        try:
            buckets = supabase.storage.list_buckets()
            names = {b.get('name') for b in (buckets or []) if isinstance(b, dict)}
            if bucket not in names:
                if debug:
                    try: print(f"[TPROF_DEBUG] Creating missing bucket {bucket}")
                    except Exception: pass
                supabase.storage.create_bucket(bucket, public=True)
        except Exception as be:
            if debug:
                try: print(f"[TPROF_DEBUG] Bucket check/create error={be}")
                except Exception: pass
            # Continue; upload may still work if race condition
        storage = supabase.storage.from_(bucket)
        # Supabase python client expects header values as str; use 'true' not True
        file_opts = {
            "content-type": content_type,
            "upsert": "true",  # critical: must be string
            "cache-control": "86400",
        }
        upload_res = storage.upload(object_path, data, file_opts)
        if debug:
            try: print(f"[TPROF_DEBUG] Avatar upload result bucket={bucket} path={object_path} res={upload_res}")
            except Exception: pass
    except Exception as e:
        msg = str(e)
        if debug:
            try: print(f"[TPROF_DEBUG] Avatar upload exception msg={msg}")
            except Exception: pass
        if "Header value" in msg and "bool" in msg:
            msg += " (probable cause: boolean value in file options; fixed to string but please retry)"
        raise HTTPException(status_code=500, detail=f"Failed to upload avatar: {msg}")
    # Obtain public URL
    try:
        pub = storage.get_public_url(object_path)
        # supabase-py returns dict with publicUrl key
        if isinstance(pub, dict):
            public_url = pub.get("publicUrl") or pub.get("public_url") or pub.get("data") or ""
        else:
            public_url = str(pub)
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to get public URL: {e}")
    if not public_url:
        raise HTTPException(status_code=500, detail="Public URL empty after upload")
    # Update / upsert teacher_profiles
    existing = supabase.table("teacher_profiles").select("auth_user_id").eq("auth_user_id", uid).limit(1).execute()
    if getattr(existing, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (get teacher profile for avatar): {existing.error}")
    payload = {"profile_image_url": public_url}
    if existing.data:
        upd = supabase.table("teacher_profiles").update(payload).eq("auth_user_id", uid).execute()
        if getattr(upd, "error", None):
            raise HTTPException(status_code=500, detail=f"Supabase error (update avatar): {upd.error}")
    else:
        payload["auth_user_id"] = uid
        ins = supabase.table("teacher_profiles").insert(payload).execute()
        if getattr(ins, "error", None):
            raise HTTPException(status_code=500, detail=f"Supabase error (insert avatar profile): {ins.error}")
    return {"ok": True, "profile_image_url": public_url, "avatar_url": public_url, "bucket": bucket, "path": object_path}

@teacher_router.post("/api/teacher/profile/me/avatar", summary="Upload/replace my teacher profile avatar (alt path)")
async def upload_teacher_avatar_alt(file: UploadFile = File(...), authorization: Optional[str] = Header(default=None)):
    # Reuse logic by calling original function
    return await upload_teacher_avatar(file=file, authorization=authorization)

# ================= Teacher Academics Options (college/departments/batches) ==================
@teacher_router.get("/api/teacher/academics/mine", summary="Return teacher's academic linkage and available departments + batches")
def get_teacher_academics_mine(authorization: Optional[str] = Header(default=None)):
    uid = _require_teacher(authorization)
    supabase = get_service_client()
    # Resolve college / department primarily from teacher_profiles then fallback application
    prof = supabase.table("teacher_profiles").select("college_id,department_id").eq("auth_user_id", uid).limit(1).execute()
    college_id = department_id = None
    if not getattr(prof, "error", None) and prof.data:
        row = prof.data[0]
        college_id = row.get("college_id") or None
        department_id = row.get("department_id") or None
    if not college_id or not department_id:
        app = supabase.table("teacher_applications").select("college_id,department_id").eq("auth_user_id", uid).limit(1).execute()
        if not getattr(app, "error", None) and app.data:
            arow = app.data[0]
            college_id = college_id or arow.get("college_id") or None
            department_id = department_id or arow.get("department_id") or None
    departments: List[Dict[str, Any]] = []
    batches: List[Dict[str, Any]] = []
    degrees: List[Dict[str, Any]] = []
    degree_id: Optional[str] = None
    if college_id:
        # Departments for college
        dres = supabase.table("departments").select("id,name,degree_id").eq("college_id", str(college_id)).order("name").execute()
        if getattr(dres, "error", None):
            raise HTTPException(status_code=500, detail=f"Supabase error (departments): {dres.error}")
        departments = dres.data or []
        # Degrees for college (only those referenced by departments for efficiency)
        deg_ids = sorted({d.get("degree_id") for d in departments if d.get("degree_id")})
        if deg_ids:
            deg_res = supabase.table("degrees").select("id,name").in_("id", deg_ids).execute()
            if not getattr(deg_res, "error", None):
                degrees = deg_res.data or []
        # Determine teacher's degree id via their department row
        if department_id:
            for d in departments:
                if d.get("id") == department_id:
                    degree_id = d.get("degree_id")
                    break
        # Batches (filtered later by department client-side)
        bres = supabase.table("batches").select("id,department_id,from_year,to_year").eq("college_id", str(college_id)).order("from_year").execute()
        if getattr(bres, "error", None):
            raise HTTPException(status_code=500, detail=f"Supabase error (batches): {bres.error}")
        raw_batches = bres.data or []
        for b in raw_batches:
            fy = b.get("from_year"); ty = b.get("to_year")
            b["label"] = f"{fy}-{ty}" if fy and ty else "Batch"
            batches.append(b)
    return {
        "college_id": college_id,
        "department_id": department_id,
        "degree_id": degree_id,
        "departments": departments,
        "batches": batches,
        "degrees": degrees,
    }


# ================= HOD Portal (Department-level) ==================

class HodClassReassignIn(BaseModel):
    new_teacher_user_id: uuid.UUID


class HodClassAssignIn(BaseModel):
    department_id: uuid.UUID
    batch_id: uuid.UUID
    semester: int = Field(ge=1, le=12)
    section: Optional[str] = None
    subject_id: uuid.UUID
    subject: str
    new_teacher_user_id: uuid.UUID


class HodStaffRemoveIn(BaseModel):
    teacher_user_id: uuid.UUID
    department_id: Optional[uuid.UUID] = None


class HodApplicationReviewIn(BaseModel):
    recommendation: Literal["approve", "reject", "review"] = "review"
    note: Optional[str] = None


class HodAiAgendaIn(BaseModel):
    focus: Optional[str] = None
    days: int = Field(default=14, ge=1, le=180)


def _enrich_departments(supabase: Client, department_ids: List[str]) -> List[Dict[str, Any]]:
    if not department_ids:
        return []
    d = supabase.table("departments").select("id,name,college_id,degree_id").in_("id", department_ids).order("name").execute()
    if getattr(d, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (departments lookup): {d.error}")
    return d.data or []


@teacher_router.get("/api/hod/me", summary="HOD: current scope + departments")
def hod_me(authorization: Optional[str] = Header(default=None)):
    hod_user_id = _require_hod(authorization)
    supabase = get_service_client()
    college_id, dept_ids = _hod_scope_departments(supabase, hod_user_id)
    if not dept_ids:
        raise HTTPException(status_code=404, detail="No department scope configured for this HOD")

    # Core profile
    core: Dict[str, Any] = {"auth_user_id": hod_user_id, "role": "hod"}
    tprof = supabase.table("teacher_profiles").select("name,email,college_id,department_id,profile_image_url").eq("auth_user_id", hod_user_id).limit(1).execute()
    if not getattr(tprof, "error", None) and tprof.data:
        row = tprof.data[0]
        for k in ["name", "email", "college_id", "department_id"]:
            if row.get(k) is not None:
                core[k] = row.get(k)
        if row.get("profile_image_url"):
            core["avatar_url"] = row.get("profile_image_url")

    # Role readback (admin can be HOD too)
    role_q = supabase.table("admin_roles").select("role").eq("auth_user_id", hod_user_id).limit(1).execute()
    if not getattr(role_q, "error", None) and role_q.data:
        core["role"] = role_q.data[0].get("role") or core.get("role")

    departments = _enrich_departments(supabase, dept_ids)

    college = None
    if college_id:
        cres = supabase.table("colleges").select("id,name,logo_url").eq("id", str(college_id)).limit(1).execute()
        if not getattr(cres, "error", None) and cres.data:
            college = cres.data[0]

    return {
        "hod": core,
        "scope": {"college_id": college_id, "department_ids": dept_ids},
        "departments": departments,
        "college": college,
    }


@teacher_router.get("/api/hod/classes", summary="HOD: list classes in my departments")
def hod_list_classes(
    authorization: Optional[str] = Header(default=None),
    department_id: Optional[str] = Query(default=None),
    semester: Optional[int] = Query(default=None, ge=1, le=12),
    batch_id: Optional[str] = Query(default=None),
    q: Optional[str] = Query(default=None, description="Search in subject/section"),
    current_only: bool = Query(default=False, description="If true, restrict to current batch+semester pairs from hod_batch_management"),
):
    hod_user_id = _require_hod(authorization)
    supabase = get_service_client()
    _, dept_ids = _hod_scope_departments(supabase, hod_user_id)
    if not dept_ids:
        raise HTTPException(status_code=404, detail="No departments configured for this HOD")
    if department_id:
        if department_id not in dept_ids:
            raise HTTPException(status_code=403, detail="Department out of scope")
        dept_ids = [department_id]

    allowed_pairs: Optional[set[str]] = None
    allowed_batch_ids: Optional[List[str]] = None
    allowed_semesters: Optional[List[int]] = None

    if current_only:
        mg = (
            supabase.table("hod_batch_management")
            .select(
                "department_id,first_year_batch_id,first_year_sem,second_year_batch_id,second_year_sem,third_year_batch_id,third_year_sem,final_year_batch_id,final_year_sem"
            )
            .in_("department_id", dept_ids)
            .execute()
        )
        if getattr(mg, "error", None):
            raise HTTPException(status_code=500, detail=f"Supabase error (hod batch management): {mg.error}")

        rows = mg.data or []
        pairs: set[str] = set()
        bset: set[str] = set()
        sset: set[int] = set()

        def _add(bid: Any, semv: Any) -> None:
            if not bid or not semv:
                return
            try:
                sem_i = int(semv)
            except Exception:
                return
            bid_s = str(bid)
            pairs.add(f"{bid_s}|{sem_i}")
            bset.add(bid_s)
            sset.add(sem_i)

        for r in rows:
            _add(r.get("first_year_batch_id"), r.get("first_year_sem"))
            _add(r.get("second_year_batch_id"), r.get("second_year_sem"))
            _add(r.get("third_year_batch_id"), r.get("third_year_sem"))
            _add(r.get("final_year_batch_id"), r.get("final_year_sem"))

        if not pairs:
            return {"total": 0, "classes": [], "meta": {"current_only": True, "has_mapping": False}}

        allowed_pairs = pairs
        allowed_batch_ids = sorted(bset)
        allowed_semesters = sorted(sset)

        # If caller asked for a specific batch/semester, validate it's still within the allowed set.
        if batch_id and batch_id not in allowed_batch_ids:
            return {"total": 0, "classes": [], "meta": {"current_only": True, "has_mapping": True}}
        if semester is not None and semester not in allowed_semesters:
            return {"total": 0, "classes": [], "meta": {"current_only": True, "has_mapping": True}}

    query = supabase.table("teacher_classes").select(
        "id,teacher_user_id,batch_id,semester,subject,subject_id,section,department_id"
    ).in_("department_id", dept_ids)

    if current_only and allowed_batch_ids and allowed_semesters:
        query = query.in_("batch_id", allowed_batch_ids).in_("semester", allowed_semesters)
    if semester is not None:
        query = query.eq("semester", semester)
    if batch_id:
        query = query.eq("batch_id", batch_id)
    # NOTE: postgrest ilike is supported via .ilike, but we keep safe fallback by client-side filtering
    res = query.order("id", desc=True).limit(1200).execute()
    if getattr(res, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (hod classes): {res.error}")
    classes = res.data or []

    if current_only and allowed_pairs is not None:
        filtered = []
        for c in classes:
            bid = c.get("batch_id")
            semv = c.get("semester")
            if not bid or not semv:
                continue
            try:
                sem_i = int(semv)
            except Exception:
                continue
            if f"{str(bid)}|{sem_i}" in allowed_pairs:
                filtered.append(c)
        classes = filtered

    # client-side filter for q (fast enough for <=1000)
    if q:
        needle = q.strip().lower()
        if needle:
            def _hit(row: Dict[str, Any]) -> bool:
                return needle in str(row.get("subject") or "").lower() or needle in str(row.get("section") or "").lower()
            classes = [c for c in classes if _hit(c)]

    # Students count (fast): count user_education entries matching batch + section + current_semester
    students_count_map: Dict[str, int] = {}
    try:
        rel_batch_ids = sorted({str(c.get("batch_id")) for c in classes if c.get("batch_id")})
        rel_sems = sorted({int(c.get("semester")) for c in classes if c.get("semester")})
        if rel_batch_ids and rel_sems:
            edu = (
                supabase.table("user_education")
                .select("batch_id,section,current_semester")
                .in_("department_id", dept_ids)
                .in_("batch_id", rel_batch_ids)
                .in_("current_semester", rel_sems)
                .limit(20000)
                .execute()
            )
            if not getattr(edu, "error", None):
                for r in edu.data or []:
                    bid = r.get("batch_id")
                    sec = r.get("section")
                    semv = r.get("current_semester")
                    if not bid or not sec or not semv:
                        continue
                    try:
                        sem_i = int(semv)
                    except Exception:
                        continue
                    k = f"{str(bid)}|{str(sec)}|{sem_i}"
                    students_count_map[k] = students_count_map.get(k, 0) + 1
    except Exception:
        students_count_map = {}

    # Ensure we display all subjects for each section/semester, even when no teacher is assigned.
    # We do this by reading syllabus_courses (per batch + semester) and synthesizing virtual rows.
    if current_only and allowed_batch_ids and allowed_semesters:
        try:
            # Map: (batch_id, semester) -> list of courses
            sc = (
                supabase.table("syllabus_courses")
                .select("id,batch_id,semester,course_code,title,type")
                .in_("batch_id", allowed_batch_ids)
                .in_("semester", allowed_semesters)
                .limit(5000)
                .execute()
            )
            if not getattr(sc, "error", None):
                courses_by_pair: Dict[str, List[Dict[str, Any]]] = {}
                courses_by_code: Dict[str, Dict[str, Any]] = {}
                courses_by_title: Dict[str, Dict[str, Any]] = {}
                valid_course_ids: Dict[str, set[str]] = {}

                def _norm_title(val: Any) -> str:
                    s = str(val or "").strip().lower()
                    s = re.sub(r"\s+", " ", s)
                    s = re.sub(r"[^a-z0-9 ]+", "", s)
                    return s.strip()

                def _extract_code(val: Any) -> Optional[str]:
                    s = str(val or "").strip().upper()
                    # Match code at start: AMHS405, AMPC401A, etc.
                    m = re.match(r"^([A-Z]{2,}\d{2,}[A-Z]?)\b", s)
                    return m.group(1) if m else None

                for cr in sc.data or []:
                    bid = cr.get("batch_id")
                    semv = cr.get("semester")
                    if not bid or not semv:
                        continue
                    try:
                        sem_i = int(semv)
                    except Exception:
                        continue
                    key = f"{str(bid)}|{sem_i}"
                    courses_by_pair.setdefault(key, []).append(cr)
                    if cr.get("id"):
                        valid_course_ids.setdefault(key, set()).add(str(cr.get("id")))

                    code = (cr.get("course_code") or "").strip().upper()
                    if code:
                        courses_by_code[f"{str(bid)}|{sem_i}|{code}"] = cr
                    title_norm = _norm_title(cr.get("title"))
                    if title_norm:
                        courses_by_title[f"{str(bid)}|{sem_i}|{title_norm}"] = cr

                # Backfill missing subject_id on existing teacher_classes rows.
                # This avoids creating a virtual Unassigned row when the class is actually assigned,
                # but was saved without subject_id (legacy data) or with formatting-only subject text.
                for c in classes:
                    bid = c.get("batch_id")
                    semv = c.get("semester")
                    if not bid or not semv:
                        continue
                    try:
                        sem_i = int(semv)
                    except Exception:
                        continue

                    pair_key = f"{str(bid)}|{sem_i}"
                    sid = c.get("subject_id")
                    # Only attempt remap when missing OR clearly invalid for this batch+semester syllabus set.
                    if sid and str(sid) in (valid_course_ids.get(pair_key) or set()):
                        continue

                    subj_text = c.get("subject")
                    code = _extract_code(subj_text)
                    match = None
                    if code:
                        match = courses_by_code.get(f"{str(bid)}|{sem_i}|{code}")

                    if not match:
                        # Try title-based match, stripping any leading code/dash.
                        raw = str(subj_text or "")
                        if "—" in raw:
                            raw = raw.split("—", 1)[1]
                        elif "-" in raw:
                            raw = raw.split("-", 1)[1]
                        title_norm = _norm_title(raw)
                        if title_norm:
                            match = courses_by_title.get(f"{str(bid)}|{sem_i}|{title_norm}")

                    if match and match.get("id"):
                        c["subject_id"] = str(match.get("id"))
                        # Prefer the canonical display form with course code if available.
                        cc = (match.get("course_code") or "").strip()
                        tt = match.get("title") or c.get("subject")
                        if cc and tt:
                            c["subject"] = f"{cc} — {tt}"

                # Existing: (batch_id, semester, section) -> set(course_id)
                existing_course_ids: Dict[str, set[str]] = {}
                section_keys: set[str] = set()
                for c in classes:
                    bid = c.get("batch_id")
                    semv = c.get("semester")
                    sec = c.get("section")
                    if not bid or not semv or not sec:
                        continue
                    try:
                        sem_i = int(semv)
                    except Exception:
                        continue
                    sk = f"{str(bid)}|{sem_i}|{str(sec)}"
                    section_keys.add(sk)
                    sid = c.get("subject_id")
                    if sid:
                        existing_course_ids.setdefault(sk, set()).add(str(sid))

                virtual_rows: List[Dict[str, Any]] = []
                for sk in sorted(section_keys):
                    bid, sem_s, sec = sk.split("|", 2)
                    try:
                        sem_i = int(sem_s)
                    except Exception:
                        continue
                    course_list = courses_by_pair.get(f"{bid}|{sem_i}") or []
                    if not course_list:
                        continue
                    have = existing_course_ids.get(sk, set())
                    for cr in course_list:
                        cid = cr.get("id")
                        if not cid:
                            continue
                        cid_s = str(cid)
                        if cid_s in have:
                            continue
                        title = cr.get("title") or "Subject"
                        code = cr.get("course_code")
                        subj = f"{code} — {title}" if code else str(title)
                        virtual_id = f"virtual:{cid_s}:{bid}:{sem_i}:{sec}"
                        dept_guess = None
                        # Best-effort: infer department from existing rows of same section key
                        for c in classes:
                            if str(c.get("batch_id")) == bid and int(c.get("semester") or 0) == sem_i and str(c.get("section")) == sec:
                                dept_guess = c.get("department_id")
                                break
                        row = {
                            "id": virtual_id,
                            "virtual": True,
                            "teacher_user_id": None,
                            "batch_id": bid,
                            "semester": sem_i,
                            "subject": subj,
                            "subject_id": cid_s,
                            "section": sec,
                            "department_id": dept_guess,
                        }
                        virtual_rows.append(row)

                if virtual_rows:
                    classes.extend(virtual_rows)
        except Exception:
            pass

    teacher_ids = sorted({c.get("teacher_user_id") for c in classes if c.get("teacher_user_id")})
    batch_ids = sorted({c.get("batch_id") for c in classes if c.get("batch_id")})
    dept_ids2 = sorted({c.get("department_id") for c in classes if c.get("department_id")})

    teacher_map: Dict[str, Dict[str, Any]] = {}
    if teacher_ids:
        t = supabase.table("teacher_profiles").select("auth_user_id,name,email,profile_image_url,department_id").in_("auth_user_id", teacher_ids).execute()
        if not getattr(t, "error", None):
            for r in t.data or []:
                teacher_map[str(r.get("auth_user_id"))] = r

    batch_map: Dict[str, Dict[str, Any]] = {}
    if batch_ids:
        b = supabase.table("batches").select("id,from_year,to_year,department_id").in_("id", batch_ids).execute()
        if not getattr(b, "error", None):
            for r in b.data or []:
                batch_map[str(r.get("id"))] = r

    dept_map: Dict[str, Dict[str, Any]] = {}
    if dept_ids2:
        d = supabase.table("departments").select("id,name").in_("id", dept_ids2).execute()
        if not getattr(d, "error", None):
            for r in d.data or []:
                dept_map[str(r.get("id"))] = r

    # attach derived fields
    for c in classes:
        # students count
        try:
            bid = c.get("batch_id")
            sec = c.get("section")
            semv = c.get("semester")
            if bid and sec and semv:
                sem_i = int(semv)
                c["students_count"] = students_count_map.get(f"{str(bid)}|{str(sec)}|{sem_i}", 0)
        except Exception:
            pass

        tid = str(c.get("teacher_user_id")) if c.get("teacher_user_id") else None
        if tid and tid in teacher_map:
            tp = teacher_map[tid]
            c["teacher_name"] = tp.get("name")
            c["teacher_email"] = tp.get("email")
            c["teacher_avatar_url"] = tp.get("profile_image_url")
        bid = str(c.get("batch_id")) if c.get("batch_id") else None
        if bid and bid in batch_map:
            br = batch_map[bid]
            fy, ty = br.get("from_year"), br.get("to_year")
            c["batch_label"] = f"{fy}-{ty}" if fy and ty else None
        did = str(c.get("department_id")) if c.get("department_id") else None
        if did and did in dept_map:
            c["department_name"] = dept_map[did].get("name")
        sem = c.get("semester")
        subj = c.get("subject")
        c["label"] = f"Sem {sem}: {subj}" if sem and subj else (subj or "Class")

    meta: Dict[str, Any] = {}
    if current_only:
        meta = {"current_only": True, "has_mapping": True}
    return {"total": len(classes), "classes": classes, "meta": meta}


@teacher_router.get("/api/hod/batches", summary="HOD: list batches in my departments")
def hod_list_batches(
    authorization: Optional[str] = Header(default=None),
    department_id: Optional[str] = Query(default=None),
):
    hod_user_id = _require_hod(authorization)
    supabase = get_service_client()
    _, dept_ids = _hod_scope_departments(supabase, hod_user_id)
    if not dept_ids:
        raise HTTPException(status_code=404, detail="No departments configured for this HOD")
    if department_id:
        if department_id not in dept_ids:
            raise HTTPException(status_code=403, detail="Department out of scope")
        dept_ids = [department_id]

    res = (
        supabase.table("batches")
        .select("id,college_id,department_id,from_year,to_year,created_at")
        .in_("department_id", dept_ids)
        .order("from_year", desc=True)
        .limit(2000)
        .execute()
    )
    if getattr(res, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (hod batches): {res.error}")
    batches = res.data or []

    # Attach department names for UI.
    dept_map: Dict[str, str] = {}
    try:
        d = supabase.table("departments").select("id,name").in_("id", list({str(b.get("department_id")) for b in batches if b.get("department_id")}) or ["00000000-0000-0000-0000-000000000000"]).execute()
        if not getattr(d, "error", None):
            for r in d.data or []:
                dept_map[str(r.get("id"))] = r.get("name")
    except Exception:
        pass

    for b in batches:
        fy, ty = b.get("from_year"), b.get("to_year")
        b["label"] = f"{fy}-{ty}" if fy and ty else None
        did = b.get("department_id")
        if did:
            b["department_name"] = dept_map.get(str(did))

    return {"total": len(batches), "batches": batches}


class HodBatchManagementIn(BaseModel):
    department_id: str
    first_year_batch_id: Optional[str] = None
    first_year_sem: Optional[int] = None
    second_year_batch_id: Optional[str] = None
    second_year_sem: Optional[int] = None
    third_year_batch_id: Optional[str] = None
    third_year_sem: Optional[int] = None
    final_year_batch_id: Optional[str] = None
    final_year_sem: Optional[int] = None


@teacher_router.get("/api/hod/batch-management", summary="HOD: get batch management selections")
def hod_get_batch_management(
    authorization: Optional[str] = Header(default=None),
    department_id: Optional[str] = Query(default=None),
):
    hod_user_id = _require_hod(authorization)
    supabase = get_service_client()
    _, dept_ids = _hod_scope_departments(supabase, hod_user_id)
    if not dept_ids:
        raise HTTPException(status_code=404, detail="No departments configured for this HOD")
    if department_id:
        if department_id not in dept_ids:
            raise HTTPException(status_code=403, detail="Department out of scope")
        dept_ids = [department_id]

    q = (
        supabase.table("hod_batch_management")
        .select(
            "department_id,first_year_batch_id,first_year_sem,second_year_batch_id,second_year_sem,third_year_batch_id,third_year_sem,final_year_batch_id,final_year_sem,updated_at"
        )
        .in_("department_id", dept_ids)
        .execute()
    )
    if getattr(q, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (get batch management): {q.error}")
    rows = q.data or []
    # Return as map for easy UI.
    by_dept: Dict[str, Any] = {}
    for r in rows:
        by_dept[str(r.get("department_id"))] = r
    return {"departments": dept_ids, "management": by_dept}


@teacher_router.post("/api/hod/batch-management", summary="HOD: update batch management selections")
def hod_update_batch_management(payload: HodBatchManagementIn, authorization: Optional[str] = Header(default=None)):
    hod_user_id = _require_hod(authorization)
    supabase = get_service_client()
    _, dept_ids = _hod_scope_departments(supabase, hod_user_id)
    if not dept_ids:
        raise HTTPException(status_code=404, detail="No departments configured for this HOD")
    department_id = str(payload.department_id or "").strip()
    if not department_id:
        raise HTTPException(status_code=400, detail="department_id is required")
    if department_id not in dept_ids:
        raise HTTPException(status_code=403, detail="Department out of scope")

    def _validate_sem(year_label: str, sem: Optional[int], allowed) -> None:
        if sem is None:
            return
        if sem not in allowed:
            raise HTTPException(status_code=400, detail=f"Invalid {year_label} semester")

    _validate_sem("second_year_sem", payload.second_year_sem, {3, 4})
    _validate_sem("third_year_sem", payload.third_year_sem, {5, 6})
    _validate_sem("final_year_sem", payload.final_year_sem, {7, 8})

    if payload.second_year_sem is not None and not payload.second_year_batch_id:
        raise HTTPException(status_code=400, detail="Select second year batch")
    if payload.third_year_sem is not None and not payload.third_year_batch_id:
        raise HTTPException(status_code=400, detail="Select third year batch")
    if payload.final_year_sem is not None and not payload.final_year_batch_id:
        raise HTTPException(status_code=400, detail="Select final year batch")

    # NOTE: First-year is managed by coordinators (not HOD). Never update or validate it here.
    chosen = [payload.second_year_batch_id, payload.third_year_batch_id, payload.final_year_batch_id]
    chosen = [c for c in chosen if c]
    if chosen:
        # Validate all chosen batches belong to this department.
        bq = supabase.table("batches").select("id,department_id").in_("id", chosen).execute()
        if getattr(bq, "error", None):
            raise HTTPException(status_code=500, detail=f"Supabase error (validate batches): {bq.error}")
        found = {str(r.get("id")): str(r.get("department_id")) for r in (bq.data or [])}
        for bid in chosen:
            if str(bid) not in found:
                raise HTTPException(status_code=400, detail=f"Invalid batch_id: {bid}")
            if found[str(bid)] != department_id:
                raise HTTPException(status_code=400, detail=f"Batch {bid} does not belong to department")

    # Preserve any existing first-year mapping/semester.
    # (Older clients might send null/empty and we must not clear coordinator-managed values.)
    upsert_payload: Dict[str, Any] = {
        "department_id": department_id,
        "second_year_batch_id": payload.second_year_batch_id,
        "second_year_sem": payload.second_year_sem,
        "third_year_batch_id": payload.third_year_batch_id,
        "third_year_sem": payload.third_year_sem,
        "final_year_batch_id": payload.final_year_batch_id,
        "final_year_sem": payload.final_year_sem,
        "updated_by": hod_user_id,
    }

    up = supabase.table("hod_batch_management").upsert(upsert_payload, on_conflict="department_id").execute()
    if getattr(up, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (update batch management): {up.error}")
    return {"ok": True, "department_id": department_id}


@teacher_router.get("/api/hod/staff", summary="HOD: list staff (teachers) in my departments")
def hod_list_staff(
    authorization: Optional[str] = Header(default=None),
    q: Optional[str] = Query(default=None, description="Search by name/email"),
    department_id: Optional[str] = Query(default=None, description="Filter to a single department (must be in HOD scope)"),
):
    hod_user_id = _require_hod(authorization)
    supabase = get_service_client()
    _, dept_ids = _hod_scope_departments(supabase, hod_user_id)
    if not dept_ids:
        raise HTTPException(status_code=404, detail="No departments configured for this HOD")

    if department_id:
        dep = str(department_id).strip()
        if dep not in dept_ids:
            raise HTTPException(status_code=403, detail="Department out of scope")
        dept_ids = [dep]

    needle = (q or "").strip()
    needle_l = needle.lower()

    tq = supabase.table("teacher_profiles").select(
        "auth_user_id,name,email,department_id,college_id,profile_image_url,headline,updated_at"
    ).in_("department_id", dept_ids)
    if needle:
        # Best-effort server-side search (name/email)
        # Supabase: OR across columns
        escaped = needle.replace(",", " ").strip()
        tq = tq.or_(f"name.ilike.%{escaped}%,email.ilike.%{escaped}%")
    staff_res = tq.order("updated_at", desc=True).limit(500).execute()
    if getattr(staff_res, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (staff list): {staff_res.error}")
    staff = staff_res.data or []

    # Add quick workload estimate (class count)
    teacher_ids = [str(r.get("auth_user_id")) for r in staff if r.get("auth_user_id")]
    class_counts: Dict[str, int] = {tid: 0 for tid in teacher_ids}
    if teacher_ids:
        # Supabase doesn't give per-user counts in one query; do a lightweight scan instead (bounded)
        cls_rows = supabase.table("teacher_classes").select("teacher_user_id").in_("teacher_user_id", teacher_ids).limit(5000).execute()
        if not getattr(cls_rows, "error", None):
            for row in cls_rows.data or []:
                tid = str(row.get("teacher_user_id") or "")
                if tid in class_counts:
                    class_counts[tid] += 1

    # Fallback client-side search (in case OR/ilike isn't supported by client/table)
    if needle_l:
        def _hit(row: Dict[str, Any]) -> bool:
            return needle_l in str(row.get("name") or "").lower() or needle_l in str(row.get("email") or "").lower()
        staff = [r for r in staff if _hit(r)]

    for r in staff:
        tid = str(r.get("auth_user_id") or "")
        r["classes_count"] = class_counts.get(tid, 0)

    return {"total": len(staff), "staff": staff}


@teacher_router.post("/api/hod/staff/remove", summary="HOD: remove a staff member from a department and unassign their classes")
def hod_remove_staff(payload: HodStaffRemoveIn, authorization: Optional[str] = Header(default=None)):
    hod_user_id = _require_hod(authorization)
    if str(payload.teacher_user_id) == str(hod_user_id):
        raise HTTPException(status_code=400, detail="You cannot remove yourself")

    supabase = get_service_client()
    _, dept_ids = _hod_scope_departments(supabase, hod_user_id)
    if not dept_ids:
        raise HTTPException(status_code=404, detail="No departments configured for this HOD")

    # Resolve staff department
    prof = supabase.table("teacher_profiles").select("auth_user_id,department_id,college_id,name,email").eq(
        "auth_user_id", str(payload.teacher_user_id)
    ).limit(1).execute()
    if getattr(prof, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (staff lookup): {prof.error}")
    if not prof.data:
        raise HTTPException(status_code=404, detail="Staff not found")

    current_dept_id = prof.data[0].get("department_id")
    target_dept_id = str(payload.department_id) if payload.department_id else (str(current_dept_id) if current_dept_id else None)
    if not target_dept_id:
        raise HTTPException(status_code=400, detail="Staff has no department")
    if target_dept_id not in dept_ids:
        raise HTTPException(status_code=403, detail="Staff out of scope")

    # Count classes to be unassigned (bounded scan)
    cls_rows = supabase.table("teacher_classes").select("id").eq("teacher_user_id", str(payload.teacher_user_id)).eq(
        "department_id", target_dept_id
    ).limit(5000).execute()
    if getattr(cls_rows, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (classes scan): {cls_rows.error}")
    classes_count = len(cls_rows.data or [])

    # 1) Remove staff from the department
    upd_prof = supabase.table("teacher_profiles").update({"department_id": None}).eq(
        "auth_user_id", str(payload.teacher_user_id)
    ).eq("department_id", target_dept_id).execute()
    if getattr(upd_prof, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (remove staff from dept): {upd_prof.error}")

    # 2) Unassign their classes (teacher_user_id -> NULL)
    upd_cls = supabase.table("teacher_classes").update({"teacher_user_id": None}).eq(
        "teacher_user_id", str(payload.teacher_user_id)
    ).eq("department_id", target_dept_id).execute()
    if getattr(upd_cls, "error", None):
        err_txt = str(upd_cls.error)
        hint = "Run packages/sql/hod_unassign_teacher_classes.sql to allow unassigned classes" if ("not-null" in err_txt.lower() or "null value" in err_txt.lower()) else None
        if hint:
            raise HTTPException(status_code=500, detail=f"Supabase error (unassign classes): {err_txt}. Hint: {hint}")
        raise HTTPException(status_code=500, detail=f"Supabase error (unassign classes): {upd_cls.error}")

    return {
        "ok": True,
        "teacher_user_id": str(payload.teacher_user_id),
        "department_id": target_dept_id,
        "classes_unassigned": classes_count,
    }


@teacher_router.post("/api/hod/classes/{class_id}/reassign", summary="HOD: reassign a class to another teacher")
def hod_reassign_class(class_id: uuid.UUID, payload: HodClassReassignIn, authorization: Optional[str] = Header(default=None)):
    hod_user_id = _require_hod(authorization)
    supabase = get_service_client()
    _, dept_ids = _hod_scope_departments(supabase, hod_user_id)
    if not dept_ids:
        raise HTTPException(status_code=404, detail="No departments configured for this HOD")

    cls = supabase.table("teacher_classes").select("id,department_id").eq("id", str(class_id)).limit(1).execute()
    if getattr(cls, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (class lookup): {cls.error}")
    if not cls.data:
        raise HTTPException(status_code=404, detail="Class not found")
    dept_id = cls.data[0].get("department_id")
    if dept_id not in dept_ids:
        raise HTTPException(status_code=403, detail="Class out of scope")

    # Ensure the target teacher belongs to the same department (best-effort)
    t = supabase.table("teacher_profiles").select("auth_user_id,department_id").eq("auth_user_id", str(payload.new_teacher_user_id)).limit(1).execute()
    if not getattr(t, "error", None) and t.data:
        if t.data[0].get("department_id") and t.data[0].get("department_id") != dept_id:
            raise HTTPException(status_code=400, detail="Target teacher belongs to a different department")

    upd = supabase.table("teacher_classes").update({"teacher_user_id": str(payload.new_teacher_user_id)}).eq("id", str(class_id)).execute()
    if getattr(upd, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (reassign): {upd.error}")
    return {"ok": True, "class_id": str(class_id), "new_teacher_user_id": str(payload.new_teacher_user_id)}


@teacher_router.get("/api/hod/applications", summary="HOD: list teacher applications in my departments")
def hod_list_teacher_applications(
    authorization: Optional[str] = Header(default=None),
    status: Optional[str] = Query(default=None, description="pending|approved|rejected"),
):
    hod_user_id = _require_hod(authorization)
    supabase = get_service_client()
    _, dept_ids = _hod_scope_departments(supabase, hod_user_id)
    if not dept_ids:
        raise HTTPException(status_code=404, detail="No departments configured for this HOD")

    query = supabase.table("teacher_applications").select(
        "id,auth_user_id,email,name,college_id,department_id,subjects,status,notes,reviewed_by,reviewed_at,created_at,updated_at"
    ).in_("department_id", dept_ids)
    if status:
        query = query.eq("status", status)
    apps_res = query.order("created_at", desc=True).limit(500).execute()
    if getattr(apps_res, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (applications): {apps_res.error}")
    apps = apps_res.data or []

    # Attach HOD review notes if table exists
    try:
        app_ids = [a.get("id") for a in apps if a.get("id")]
        review_map: Dict[str, Dict[str, Any]] = {}
        if app_ids:
            rres = supabase.table("hod_application_reviews").select("application_id,hod_user_id,recommendation,note,updated_at").eq("hod_user_id", hod_user_id).in_("application_id", app_ids).execute()
            if not getattr(rres, "error", None):
                for r in rres.data or []:
                    review_map[str(r.get("application_id"))] = r
        for a in apps:
            rid = review_map.get(str(a.get("id")))
            if rid:
                a["hod_review"] = {
                    "recommendation": rid.get("recommendation"),
                    "note": rid.get("note"),
                    "updated_at": rid.get("updated_at"),
                }
    except Exception:
        pass

    return {"total": len(apps), "applications": apps}


@teacher_router.post("/api/hod/applications/{application_id}/review", summary="HOD: save recommendation note for an application")
def hod_review_application(application_id: uuid.UUID, payload: HodApplicationReviewIn, authorization: Optional[str] = Header(default=None)):
    hod_user_id = _require_hod(authorization)
    supabase = get_service_client()
    _, dept_ids = _hod_scope_departments(supabase, hod_user_id)
    if not dept_ids:
        raise HTTPException(status_code=404, detail="No departments configured for this HOD")

    app_res = supabase.table("teacher_applications").select("id,department_id").eq("id", str(application_id)).limit(1).execute()
    if getattr(app_res, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (application lookup): {app_res.error}")
    if not app_res.data:
        raise HTTPException(status_code=404, detail="Application not found")
    if app_res.data[0].get("department_id") not in dept_ids:
        raise HTTPException(status_code=403, detail="Application out of scope")

    row = {
        "application_id": str(application_id),
        "hod_user_id": hod_user_id,
        "recommendation": payload.recommendation,
        "note": payload.note,
    }
    try:
        up = supabase.table("hod_application_reviews").upsert(row).execute()
        if getattr(up, "error", None):
            raise HTTPException(status_code=500, detail=f"Supabase error (save review): {up.error}")
    except APIError as e:
        raise HTTPException(status_code=500, detail=f"Supabase error (save review): {e}")

@teacher_router.post("/api/hod/classes/assign", summary="HOD: assign a staff to an unassigned/virtual subject")
def hod_assign_class(payload: HodClassAssignIn, authorization: Optional[str] = Header(default=None)):
    hod_user_id = _require_hod(authorization)
    supabase = get_service_client()
    _, dept_ids = _hod_scope_departments(supabase, hod_user_id)
    if not dept_ids:
        raise HTTPException(status_code=404, detail="No departments configured for this HOD")

    dep_id = str(payload.department_id)
    if dep_id not in dept_ids:
        raise HTTPException(status_code=403, detail="Department out of scope")

    # Validate batch belongs to department
    bq = supabase.table("batches").select("id,department_id").eq("id", str(payload.batch_id)).limit(1).execute()
    if getattr(bq, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (batch lookup): {bq.error}")
    if not bq.data:
        raise HTTPException(status_code=400, detail="Invalid batch")
    if str(bq.data[0].get("department_id")) != dep_id:
        raise HTTPException(status_code=400, detail="Batch does not belong to department")

    # Validate syllabus course matches batch+semester
    cq = supabase.table("syllabus_courses").select("id,batch_id,semester,course_code,title").eq("id", str(payload.subject_id)).limit(1).execute()
    if getattr(cq, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (course lookup): {cq.error}")
    if not cq.data:
        raise HTTPException(status_code=400, detail="Invalid subject")
    if str(cq.data[0].get("batch_id")) != str(payload.batch_id) or int(cq.data[0].get("semester") or 0) != int(payload.semester):
        raise HTTPException(status_code=400, detail="Subject does not match batch/semester")

    # Ensure the target teacher belongs to the same department (best-effort)
    t = supabase.table("teacher_profiles").select("auth_user_id,department_id").eq("auth_user_id", str(payload.new_teacher_user_id)).limit(1).execute()
    if getattr(t, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (teacher lookup): {t.error}")
    if not t.data:
        raise HTTPException(status_code=400, detail="Target staff not found")
    if t.data[0].get("department_id") and str(t.data[0].get("department_id")) != dep_id:
        raise HTTPException(status_code=400, detail="Target staff belongs to a different department")

    # Department meta for insert
    d = supabase.table("departments").select("id,college_id,degree_id").eq("id", dep_id).limit(1).execute()
    if getattr(d, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (department lookup): {d.error}")
    if not d.data:
        raise HTTPException(status_code=400, detail="Invalid department")
    college_id = d.data[0].get("college_id")
    degree_id = d.data[0].get("degree_id")

    sec = (payload.section or "").strip() or None

    # Find existing matching class row
    q = supabase.table("teacher_classes").select("id,teacher_user_id").eq("department_id", dep_id).eq("batch_id", str(payload.batch_id)).eq(
        "semester", int(payload.semester)
    ).eq("subject_id", str(payload.subject_id))
    if sec is None:
        q = q.is_("section", "null")
    else:
        q = q.eq("section", sec)
    existing = q.limit(1).execute()
    if getattr(existing, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (class lookup): {existing.error}")

    if existing.data:
        row = existing.data[0]
        if row.get("teacher_user_id"):
            raise HTTPException(status_code=409, detail="Class already assigned; use reassign")
        upd = supabase.table("teacher_classes").update({"teacher_user_id": str(payload.new_teacher_user_id)}).eq("id", str(row.get("id"))).execute()
        if getattr(upd, "error", None):
            raise HTTPException(status_code=500, detail=f"Supabase error (assign class): {upd.error}")
        return {"ok": True, "class_id": str(row.get("id")), "assigned": True, "created": False}

    ins_payload: Dict[str, Any] = {
        "teacher_user_id": str(payload.new_teacher_user_id),
        "batch_id": str(payload.batch_id),
        "semester": int(payload.semester),
        "subject": str(payload.subject).strip() or f"{cq.data[0].get('course_code')} — {cq.data[0].get('title')}",
        "section": sec,
        "degree_id": degree_id,
        "department_id": dep_id,
        "college_id": college_id,
        "subject_id": str(payload.subject_id),
    }
    ins = supabase.table("teacher_classes").insert(ins_payload).execute()
    if getattr(ins, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (create class): {ins.error}")
    new_id = (ins.data or [{}])[0].get("id")
    return {"ok": True, "class_id": str(new_id) if new_id else None, "assigned": True, "created": True}
    return {"ok": True, "application_id": str(application_id), "saved": True}


@teacher_router.post("/api/hod/ai/meeting-agenda", summary="HOD AI: generate meeting agenda for the department")
def hod_ai_meeting_agenda(payload: HodAiAgendaIn, authorization: Optional[str] = Header(default=None)):
    hod_user_id = _require_hod(authorization)
    supabase = get_service_client()
    college_id, dept_ids = _hod_scope_departments(supabase, hod_user_id)
    if not dept_ids:
        raise HTTPException(status_code=404, detail="No departments configured for this HOD")

    # Gather lightweight stats
    classes = supabase.table("teacher_classes").select("id,teacher_user_id,semester,subject,department_id", count="estimated").in_("department_id", dept_ids).execute()
    classes_count = getattr(classes, "count", None) or (len(classes.data or []) if getattr(classes, "data", None) else 0)
    apps = supabase.table("teacher_applications").select("id,status", count="estimated").in_("department_id", dept_ids).execute()
    pending_apps = 0
    if getattr(apps, "data", None):
        pending_apps = len([a for a in (apps.data or []) if a.get("status") == "pending"])
    staff = supabase.table("teacher_profiles").select("auth_user_id", count="estimated").in_("department_id", dept_ids).execute()
    staff_count = getattr(staff, "count", None) or (len(staff.data or []) if getattr(staff, "data", None) else 0)

    dept_rows = _enrich_departments(supabase, dept_ids)
    dept_names = [d.get("name") for d in dept_rows if d.get("name")]

    prompt = "\n".join([
        "Generate a staff meeting agenda for the HOD.",
        "", 
        f"Department(s): {', '.join(dept_names) if dept_names else '(unknown)'}",
        f"College ID: {college_id or '(unknown)'}",
        f"Window (days): {payload.days}",
        f"Classes count: {classes_count}",
        f"Staff count: {staff_count}",
        f"Pending teacher applications: {pending_apps}",
        "", 
        "Constraints:",
        "- Keep it to 30–40 minutes total",
        "- Include: quick metrics, curriculum/syllabus, student performance signals, blockers, action items",
        f"Extra focus from HOD: {payload.focus or '(none)'}",
    ])
    md = _hod_ai_generate_markdown(prompt)
    return {"ok": True, "markdown": md}


@teacher_router.post("/api/hod/ai/risk-flags", summary="HOD AI: generate risk flags and recommendations")
def hod_ai_risk_flags(payload: HodAiAgendaIn, authorization: Optional[str] = Header(default=None)):
    hod_user_id = _require_hod(authorization)
    supabase = get_service_client()
    _, dept_ids = _hod_scope_departments(supabase, hod_user_id)
    if not dept_ids:
        raise HTTPException(status_code=404, detail="No departments configured for this HOD")

    cls_res = supabase.table("teacher_classes").select("id,teacher_user_id,semester,subject,section,department_id").in_("department_id", dept_ids).limit(2000).execute()
    if getattr(cls_res, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (classes): {cls_res.error}")
    classes = cls_res.data or []
    workload: Dict[str, int] = {}
    for c in classes:
        tid = str(c.get("teacher_user_id") or "")
        if not tid:
            continue
        workload[tid] = workload.get(tid, 0) + 1
    top_load = sorted(workload.items(), key=lambda kv: kv[1], reverse=True)[:10]

    apps_res = supabase.table("teacher_applications").select("id,status").in_("department_id", dept_ids).limit(1000).execute()
    pending_apps = len([a for a in (apps_res.data or []) if a.get("status") == "pending"]) if not getattr(apps_res, "error", None) else 0

    dept_rows = _enrich_departments(supabase, dept_ids)
    dept_names = [d.get("name") for d in dept_rows if d.get("name")]

    prompt = "\n".join([
        "Identify risks for the department(s) and propose mitigations.",
        "", 
        f"Departments: {', '.join(dept_names) if dept_names else '(unknown)'}",
        f"Classes: {len(classes)}",
        f"Pending teacher applications: {pending_apps}",
        "", 
        "Top teacher workloads (teacher_id -> classes_count):",
        *(f"- {tid}: {cnt}" for tid, cnt in top_load),
        "", 
        "Output format:",
        "- ## Risks (bullets)",
        "- ## Recommendations (bullets)",
        "- ## Quick Wins (next 7 days)",
        f"Extra focus from HOD: {payload.focus or '(none)'}",
    ])
    md = _hod_ai_generate_markdown(prompt)
    return {"ok": True, "markdown": md}
# ================= Debug Helpers (can be removed in production) ==================
@teacher_router.get("/api/debug/teacher-routes", summary="Debug: list registered teacher routes")
def debug_list_teacher_routes():
    from fastapi.routing import APIRoute
    routes = []
    for r in teacher_router.routes:  # only teacher_router scope
        if isinstance(r, APIRoute):
            routes.append({
                "path": r.path,
                "methods": sorted(list(r.methods - {"HEAD"})),
                "name": r.name,
            })
    return {"routes": routes}

@teacher_router.get("/api/debug/teacher-avatar-route", summary="Debug: confirm avatar route present")
def debug_avatar_route_presence():
    from fastapi.routing import APIRoute
    present = False
    methods: List[str] = []
    for r in teacher_router.routes:
        if isinstance(r, APIRoute) and r.path == "/api/teacher/profile/avatar":
            present = True
            methods = sorted(list(r.methods - {"HEAD"}))
            break
    return {"avatar_route_present": present, "methods": methods}

# ================= Teacher Classes CRUD Endpoints ==================
@teacher_router.get("/api/teacher/classes/mine", response_model=List[TeacherClassOut], summary="List my teacher classes")
def list_my_teacher_classes(authorization: Optional[str] = Header(default=None)):
    uid = _require_teacher(authorization)
    supabase = get_service_client()
    # Apply lightweight retry for transient protocol disconnections from httpx/httpcore
    try:
        res = _supabase_retry(
            lambda: (
                supabase
                .table("teacher_classes")
                .select("*")
                .eq("teacher_user_id", uid)
                .order("updated_at", desc=True)
                .execute()
            )
        )
    except HTTPXRemoteProtocolError as exc:  # pragma: no cover - network timing dependent
        logging.getLogger("teachers").warning(
            "list_my_teacher_classes transient protocol error, returning empty list: %s", exc
        )
        return []
    if getattr(res, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (list classes): {res.error}")
    rows = res.data or []
    return [_map_teacher_class_row(r) for r in rows]


@teacher_router.get("/api/teacher/classes/{class_id}", response_model=TeacherClassOut, summary="Get a teacher class")
def get_teacher_class(class_id: uuid.UUID, authorization: Optional[str] = Header(default=None)):
    uid = _require_teacher(authorization)
    supabase = get_service_client()
    res = supabase.table("teacher_classes").select("*").eq("id", str(class_id)).limit(1).execute()
    if getattr(res, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (get class): {res.error}")
    if not res.data:
        raise HTTPException(status_code=404, detail="Class not found")
    row = res.data[0]
    if row.get("teacher_user_id") != uid:
        raise HTTPException(status_code=403, detail="Cannot view another teacher's class")
    return _map_teacher_class_row(row)

@teacher_router.post("/api/teacher/classes", response_model=TeacherClassOut, summary="Create a teacher class")
def create_teacher_class(payload: TeacherClassIn, authorization: Optional[str] = Header(default=None)):
    uid = _require_teacher(authorization)
    supabase = get_service_client()
    row = _supabase_payload(payload.dict(exclude_unset=True))
    row["teacher_user_id"] = str(uid)
    row = _supabase_payload(row)
    ins = supabase.table("teacher_classes").insert(row).execute()
    if getattr(ins, "error", None):
        err_txt = str(ins.error)
        if "duplicate key value" in err_txt or "unique" in err_txt.lower():
            raise HTTPException(status_code=409, detail="Class already exists for subject + batch + semester + section")
        raise HTTPException(status_code=500, detail=f"Supabase error (insert class): {ins.error}")
    created = (ins.data or [])[0]
    return _map_teacher_class_row(created)

@teacher_router.put("/api/teacher/classes/{class_id}", response_model=TeacherClassOut, summary="Update a teacher class")
def update_teacher_class(class_id: uuid.UUID, payload: TeacherClassUpdate, authorization: Optional[str] = Header(default=None)):
    uid = _require_teacher(authorization)
    supabase = get_service_client()
    # Ensure ownership
    existing = supabase.table("teacher_classes").select("id,teacher_user_id").eq("id", str(class_id)).limit(1).execute()
    if getattr(existing, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (get class): {existing.error}")
    if not existing.data:
        raise HTTPException(status_code=404, detail="Class not found")
    if existing.data[0].get("teacher_user_id") != uid:
        raise HTTPException(status_code=403, detail="Cannot modify another teacher's class")
    updates = _supabase_payload(payload.dict(exclude_unset=True))
    if not updates:
        row_res = supabase.table("teacher_classes").select("*").eq("id", str(class_id)).limit(1).execute()
        if getattr(row_res, "error", None):
            raise HTTPException(status_code=500, detail=f"Supabase error (reload class): {row_res.error}")
        return _map_teacher_class_row(row_res.data[0])
    upd = supabase.table("teacher_classes").update(updates).eq("id", str(class_id)).execute()
    if getattr(upd, "error", None):
        err_txt = str(upd.error)
        if "duplicate key value" in err_txt or "unique" in err_txt.lower():
            raise HTTPException(status_code=409, detail="Another class with same keys exists")
        raise HTTPException(status_code=500, detail=f"Supabase error (update class): {upd.error}")
    row_res = supabase.table("teacher_classes").select("*").eq("id", str(class_id)).limit(1).execute()
    if getattr(row_res, "error", None) or not row_res.data:
        raise HTTPException(status_code=500, detail="Failed to reload updated class")
    return _map_teacher_class_row(row_res.data[0])

@teacher_router.delete("/api/teacher/classes/{class_id}", summary="Delete a teacher class")
def delete_teacher_class(class_id: uuid.UUID, authorization: Optional[str] = Header(default=None)):
    uid = _require_teacher(authorization)
    supabase = get_service_client()
    existing = supabase.table("teacher_classes").select("id,teacher_user_id").eq("id", str(class_id)).limit(1).execute()
    if getattr(existing, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (get class): {existing.error}")
    if not existing.data:
        raise HTTPException(status_code=404, detail="Class not found")
    if existing.data[0].get("teacher_user_id") != uid:
        raise HTTPException(status_code=403, detail="Cannot delete another teacher's class")
    del_res = supabase.table("teacher_classes").delete().eq("id", str(class_id)).execute()
    if getattr(del_res, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (delete class): {del_res.error}")
    return {"ok": True, "deleted": True, "id": str(class_id)}


@teacher_router.get(
    "/api/teacher/classes/{class_id}/students",
    response_model=TeacherClassStudentsResponse,
    summary="List students for a teacher class",
)
def list_teacher_class_students(class_id: uuid.UUID, authorization: Optional[str] = Header(default=None)):
    uid = _require_teacher(authorization)
    supabase = get_service_client()
    class_res = _supabase_retry(
        lambda: supabase.table("teacher_classes").select("*").eq("id", str(class_id)).limit(1).execute()
    )
    if getattr(class_res, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (get class): {class_res.error}")
    if not class_res.data:
        raise HTTPException(status_code=404, detail="Class not found")
    class_row = class_res.data[0]
    if class_row.get("teacher_user_id") != uid:
        raise HTTPException(status_code=403, detail="Cannot view another teacher's class")

    course_topic_ids: List[str] = []
    total_course_topics = 0
    subject_id_value = class_row.get("subject_id")
    if subject_id_value:
        units_res = _supabase_retry(
            lambda: supabase
            .table("syllabus_units")
            .select("id")
            .eq("course_id", str(subject_id_value))
            .execute()
        )
        if getattr(units_res, "error", None):
            raise HTTPException(status_code=500, detail=f"Supabase error (units for course): {units_res.error}")
        unit_ids = [row.get("id") for row in (units_res.data or []) if row.get("id")]
        if unit_ids:
            topics_res = _supabase_retry(
                lambda: supabase
                .table("syllabus_topics")
                .select("id")
                .in_("unit_id", unit_ids)
                .execute()
            )
            if getattr(topics_res, "error", None):
                raise HTTPException(status_code=500, detail=f"Supabase error (topics for course): {topics_res.error}")
            course_topic_ids = [row.get("id") for row in (topics_res.data or []) if row.get("id")]
            total_course_topics = len(course_topic_ids)

    applied_filters: Dict[str, Any] = {}
    def _track_filter(key: str, value: Any):
        if value is None:
            return
        if isinstance(value, str) and value == "":
            return
        if isinstance(value, (list, tuple, set, dict)) and not value:
            return
        applied_filters[key] = value

    edu_query = supabase.table("user_education").select(
        "id,user_profile_id,batch_id,section,current_semester,regno,degree_id,department_id,college_id"
    )
    filter_count = 0
    for column, value in (
        ("batch_id", class_row.get("batch_id")),
        ("section", class_row.get("section")),
        ("current_semester", class_row.get("semester")),
        ("degree_id", class_row.get("degree_id")),
        ("department_id", class_row.get("department_id")),
        ("college_id", class_row.get("college_id")),
    ):
        if value not in {None, ""}:
            eq_value = str(value) if column.endswith("_id") or column == "batch_id" else value
            edu_query = edu_query.eq(column, eq_value)
            filter_count += 1
            _track_filter(column, value)

    if filter_count == 0:
        return TeacherClassStudentsResponse(
            class_info=_map_teacher_class_row(class_row),
            students=[],
            total=0,
            applied_filters={"missing_filters": True},
        )

    edu_query = edu_query.order("updated_at", desc=True).limit(500)
    edu_res = _supabase_retry(lambda: edu_query.execute())
    if getattr(edu_res, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (student list): {edu_res.error}")
    edu_rows = edu_res.data or []
    profile_ids = {row.get("user_profile_id") for row in edu_rows if row.get("user_profile_id")}

    profiles: Dict[str, Dict[str, Any]] = {}
    if profile_ids:
        prof_res = _supabase_retry(
            lambda: supabase
            .table("user_profiles")
            .select("id,auth_user_id,name,email,regno,phone,semester,profile_image_url")
            .in_("id", list(profile_ids))
            .execute()
        )
        if getattr(prof_res, "error", None):
            raise HTTPException(status_code=500, detail=f"Supabase error (profiles): {prof_res.error}")
        for row in prof_res.data or []:
            profiles[row.get("id")] = row

    batch_ids = {row.get("batch_id") for row in edu_rows if row.get("batch_id")}
    if class_row.get("batch_id"):
        batch_ids.add(class_row.get("batch_id"))
    batch_labels: Dict[str, str] = {}
    if batch_ids:
        batch_res = _supabase_retry(
            lambda: supabase
            .table("batches")
            .select("id,from_year,to_year")
            .in_("id", list(batch_ids))
            .execute()
        )
        if getattr(batch_res, "error", None):
            raise HTTPException(status_code=500, detail=f"Supabase error (batches): {batch_res.error}")
        for row in batch_res.data or []:
            fid = row.get("id")
            if not fid:
                continue
            fy = row.get("from_year")
            ty = row.get("to_year")
            if fy and ty:
                batch_labels[fid] = f"{fy}-{ty}"

    progress_counts: Dict[str, int] = {}
    if profile_ids and course_topic_ids:
        prog_res = _supabase_retry(
            lambda: supabase
            .table("user_topic_progress")
            .select("user_profile_id,topic_id")
            .in_("user_profile_id", list(profile_ids))
            .in_("topic_id", course_topic_ids)
            .execute()
        )
        if getattr(prog_res, "error", None):
            raise HTTPException(status_code=500, detail=f"Supabase error (topic progress): {prog_res.error}")
        for row in prog_res.data or []:
            pid = row.get("user_profile_id")
            if pid:
                progress_counts[pid] = progress_counts.get(pid, 0) + 1

    students: List[TeacherClassStudent] = []
    for edu in edu_rows:
        prof_id = edu.get("user_profile_id")
        prof = profiles.get(prof_id)
        if not prof_id or not prof:
            continue
        completed = progress_counts.get(prof_id, 0)
        total_topics = total_course_topics
        progress_pct = None
        if total_topics > 0:
            progress_pct = round((completed / total_topics) * 100, 1)
        try:
            entry = TeacherClassStudent(
                profile_id=uuid.UUID(prof_id),
                user_education_id=uuid.UUID(edu["id"]),
                user_auth_id=uuid.UUID(prof["auth_user_id"]) if prof.get("auth_user_id") else None,
                name=prof.get("name"),
                email=prof.get("email"),
                regno=(prof.get("regno") or edu.get("regno")),
                phone=prof.get("phone"),
                section=edu.get("section"),
                batch_id=uuid.UUID(edu["batch_id"]) if edu.get("batch_id") else None,
                batch_label=batch_labels.get(edu.get("batch_id")),
                current_semester=edu.get("current_semester") or prof.get("semester"),
                avatar_url=prof.get("profile_image_url"),
                completed_topics=completed,
                total_topics=total_topics,
                progress_pct=progress_pct,
            )
        except (KeyError, ValueError):
            continue
        students.append(entry)

    students.sort(key=lambda s: ((s.name or "").lower(), s.regno or ""))
    if class_row.get("batch_id") and class_row.get("batch_id") in batch_labels:
        applied_filters["batch_label"] = batch_labels[class_row.get("batch_id")]

    return TeacherClassStudentsResponse(
        class_info=_map_teacher_class_row(class_row),
        students=students,
        total=len(students),
        applied_filters=applied_filters,
    )


def _asset_debug(msg: str, **extra):  # lightweight conditional debug
    if os.getenv("ASSET_DEBUG"):
        try:
            print(f"[ASSET_DEBUG] {msg} " + (" ".join(f"{k}={v}" for k,v in extra.items())))
        except Exception:
            pass


def _upload_college_logo(college_id: uuid.UUID, file: UploadFile):
    supabase = get_service_client()
    # Resolve primary bucket with fallbacks
    bucket = (
        os.getenv("SUPABASE_BUCKET", "").strip()
        or os.getenv("SUPABASE_ASSETS_BUCKET", "").strip()
        or os.getenv("SUPABASE_PROJECTS_BUCKET", "").strip()
    )
    if not bucket:
        raise HTTPException(status_code=500, detail="Missing SUPABASE_BUCKET (and fallbacks) in environment")
    filename = file.filename or "logo.png"
    ext = os.path.splitext(filename)[1].lower()
    if ext not in {".png", ".jpg", ".jpeg", ".webp", ".gif", ".svg"}:
        raise HTTPException(status_code=400, detail="Unsupported logo image type")
    blob = _read_upload_bytes(file)
    if not blob:
        raise HTTPException(status_code=400, detail="Empty upload")
    # Path pattern: colleges/{college_id}/logo{rand}.ext (keep history by random hash)
    from uuid import uuid4
    dest = f"colleges/{college_id}/logo-{uuid4().hex}{ext}"
    try:
        supabase_logger.info(
            "Uploading college logo", extra={
                "college_id": str(college_id),
                "bucket": bucket,
                "dest": dest,
                "size": len(blob),
                "content_type": file.content_type,
            }
        )
        _asset_debug("start_logo_upload", college_id=college_id, bucket=bucket, dest=dest, size=len(blob), ct=file.content_type)
        _storage_upload_bytes(supabase, bucket, dest, blob, file.content_type or "image/png")
    except HTTPException:
        _asset_debug("logo_upload_http_exception", college_id=college_id)
        raise
    except Exception as exc:
        supabase_logger.exception("College logo upload unexpected failure")
        _asset_debug("logo_upload_unexpected_failure", college_id=college_id, error=exc)
        raise HTTPException(status_code=500, detail=f"Unexpected upload failure: {exc}")
    public_url = _storage_public_url(supabase, bucket, dest)
    _asset_debug("logo_public_url", college_id=college_id, url=public_url)
    upd = supabase.table("colleges").update({"logo_url": public_url}).eq("id", str(college_id)).execute()
    if getattr(upd, "error", None):
        _asset_debug("logo_db_update_failed", college_id=college_id, error=upd.error)
        raise HTTPException(status_code=500, detail=f"Supabase error (update college logo): {upd.error}")
    _asset_debug("logo_db_update_success", college_id=college_id)
    return {"college_id": str(college_id), "logo_url": public_url, "path": dest}


@academics_router.post("/api/colleges/{college_id}/logo", summary="Upload/replace college logo")
def upload_college_logo(college_id: uuid.UUID, file: UploadFile = File(...)):
    # Existence check (fast fail)
    supabase = get_service_client()
    _asset_debug("endpoint_invoked", college_id=college_id, filename=file.filename, ct=file.content_type)
    exists = supabase.table("colleges").select("id").eq("id", str(college_id)).limit(1).execute()
    if getattr(exists, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (find college): {exists.error}")
    if not exists.data:
        raise HTTPException(status_code=404, detail="College not found")
    return _upload_college_logo(college_id, file)


@academics_router.get("/api/colleges/{college_id}/logo/debug", summary="Debug: list stored logo objects for a college")
def debug_list_college_logos(college_id: uuid.UUID):  # pragma: no cover - debug utility
    supabase = get_service_client()
    bucket = (
        os.getenv("SUPABASE_BUCKET", "").strip()
        or os.getenv("SUPABASE_ASSETS_BUCKET", "").strip()
        or os.getenv("SUPABASE_PROJECTS_BUCKET", "").strip()
    )
    if not bucket:
        raise HTTPException(status_code=500, detail="Missing SUPABASE_BUCKET (and fallbacks) in environment")
    storage = _storage_get_client(supabase)
    prefix = f"colleges/{college_id}".rstrip("/")
    try:
        # Some SDK versions: list(path=prefix, ...) ; others: from_(bucket).list(path=prefix)
        try:
            objs = storage.from_(bucket).list(prefix)
        except TypeError:
            objs = storage.from_(bucket).list(path=prefix)
    except Exception as exc:
        supabase_logger.exception("List objects failed")
        raise HTTPException(status_code=500, detail=f"List failed: {exc}")
    out = []
    if isinstance(objs, list):
        for o in objs:
            if not isinstance(o, dict):
                continue
            name = o.get("name") or o.get("Key")
            if not name:
                continue
            full_path = f"{prefix}/{name}" if not name.startswith(prefix) else name
            out.append({
                "name": name,
                "path": full_path,
                "size": o.get("metadata", {}).get("size") if isinstance(o.get("metadata"), dict) else o.get("size"),
                "last_modified": o.get("updated_at") or o.get("LastModified") or o.get("last_modified"),
                "public_url": _storage_public_url(supabase, bucket, full_path),
            })
    return {"bucket": bucket, "prefix": prefix, "objects": out}


@academics_router.post("/api/colleges", response_model=CollegeFullOut, summary="Create or update a college with departments & batches")
def create_college(payload: CollegeCreateIn):
    college_id = upsert_college(payload.college_name)
    sync_degree_hierarchy(college_id, payload.degrees or [])
    data = get_college_full(college_id)
    return CollegeFullOut(**data)


@academics_router.post(
    "/api/colleges/simple",
    response_model=CollegeFullOut,
    summary="Create a college by name with no academic structure",
)
def create_college_simple(payload: CollegeNameOnlyIn):
    college_id = upsert_college(payload.name)
    data = get_college_full(college_id)
    return CollegeFullOut(**data)


@academics_router.get("/api/colleges", response_model=List[College], summary="List all colleges (id & name)")
def list_colleges():
    supabase = get_service_client()
    # Apply lightweight retry for transient RemoteProtocolError or connection resets
    import time, logging
    from httpx import RemoteProtocolError
    attempts = 0
    last_exc = None
    backoffs = [0.0, 0.15, 0.35, 0.75]
    while attempts < len(backoffs):
        try:
            res = supabase.table("colleges").select("id,name,logo_url").order("name").execute()
            # Some SDK versions expose .error
            if getattr(res, "error", None):
                raise RuntimeError(f"Supabase error: {res.error}")
            return [
                {"id": row["id"], "name": row["name"], "logo_url": row.get("logo_url")}
                for row in (res.data or [])
                if isinstance(row, dict) and row.get("id") and row.get("name")
            ]
        except (RemoteProtocolError, ConnectionError) as exc:  # transient network layer
            last_exc = exc
            wait = backoffs[attempts]
            logging.getLogger("academics").warning(
                "list_colleges transient error attempt %s/%s: %s (backing off %.2fs)",
                attempts + 1,
                len(backoffs),
                exc,
                wait,
            )
            time.sleep(wait)
            attempts += 1
            continue
        except Exception as exc:  # non-transient
            logging.getLogger("academics").exception("list_colleges failed unrecoverably")
            raise HTTPException(status_code=500, detail=f"Failed to list colleges: {exc}")
    # Fallback after retries exhausted
    logging.getLogger("academics").error("list_colleges exhausted retries: %s", last_exc)
    # Graceful empty list so UI can still render (client can retry separately)
    return []

@academics_router.get(
    "/api/colleges/{college_id}/degrees",
    response_model=List[DegreeOut],
    summary="List degrees for a college (with departments & batches)",
)
def list_degrees_for_college(college_id: uuid.UUID):
    """Return the degrees for a college.

    Reuses the existing get_college_full aggregation logic so each DegreeOut
    includes its departments (with batches) consistent with other responses.
    This complements the existing POST /api/colleges/{college_id}/degrees which creates a degree.
    """
    data = get_college_full(college_id)
    degrees: List[DegreeOut] = data["degrees"]
    return degrees


@academics_router.post(
    "/api/colleges/{college_id}/degrees",
    response_model=DegreeOut,
    summary="Create a degree for a college",
)
def create_degree_simple(college_id: uuid.UUID, payload: DegreeSimpleCreateIn):
    supabase = get_service_client()

    existing = (
        supabase.table("degrees")
        .select("id,name,level,duration_years")
        .eq("college_id", str(college_id))
        .eq("name", payload.name)
        .limit(1)
        .execute()
    )
    if getattr(existing, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (find degree): {existing.error}")

    if existing.data:
        degree_row = existing.data[0]
    else:
        insert_payload: Dict[str, Any] = {
            "college_id": str(college_id),
            "name": payload.name,
        }
        if payload.level is not None:
            insert_payload["level"] = payload.level
        if payload.duration_years is not None:
            insert_payload["duration_years"] = payload.duration_years

        ins = supabase.table("degrees").insert(insert_payload).execute()
        if getattr(ins, "error", None):
            raise HTTPException(status_code=500, detail=f"Supabase error (insert degree): {ins.error}")
        if ins.data:
            degree_row = ins.data[0]
        else:
            refetch = (
                supabase.table("degrees")
                .select("id,name,level,duration_years")
                .eq("college_id", str(college_id))
                .eq("name", payload.name)
                .limit(1)
                .execute()
            )
            if getattr(refetch, "error", None) or not refetch.data:
                raise HTTPException(status_code=500, detail="Failed to retrieve created degree.")
            degree_row = refetch.data[0]

    college_snapshot = get_college_full(college_id)
    degree_id = uuid.UUID(degree_row["id"])
    for item in college_snapshot["degrees"]:
        if item.id == degree_id:
            return item

    raise HTTPException(status_code=404, detail="Created degree not found in college snapshot.")


@academics_router.put(
    "/api/degrees/{degree_id}",
    response_model=DegreeOut,
    summary="Update degree metadata",
)
def update_degree(degree_id: uuid.UUID, payload: DegreeSimpleCreateIn):
    supabase = get_service_client()

    existing = (
        supabase.table("degrees")
        .select("id,college_id")
        .eq("id", str(degree_id))
        .limit(1)
        .execute()
    )
    if getattr(existing, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (find degree): {existing.error}")
    if not existing.data:
        raise HTTPException(status_code=404, detail="Degree not found")

    degree_row = existing.data[0]
    college_id_str = degree_row.get("college_id")
    if not college_id_str:
        raise HTTPException(status_code=400, detail="Degree missing college reference")

    updates: Dict[str, Any] = {
        "name": payload.name,
        "level": payload.level,
        "duration_years": payload.duration_years,
    }

    upd = (
        supabase.table("degrees")
        .update(updates)
        .eq("id", str(degree_id))
        .execute()
    )
    if getattr(upd, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (update degree): {upd.error}")

    college_snapshot = get_college_full(uuid.UUID(college_id_str))
    for item in college_snapshot["degrees"]:
        if item.id == degree_id:
            return item

    raise HTTPException(status_code=404, detail="Updated degree not found in college snapshot.")


def _check_department_delete_blockers(supabase: Client, department_id: str) -> List[str]:
    """Return list of human-readable blockers for deleting a department id."""
    blocking_sources: List[str] = []
    dependency_checks = [
        ("marketplace_notes", "marketplace notes", "department_id"),
        ("teacher_applications", "teacher applications", "department_id"),
        ("teacher_classes", "teacher classes", "department_id"),
        ("teacher_profiles", "teacher profiles", "department_id"),
        ("user_education", "user education records", "department_id"),
    ]
    for table_name, label, column in dependency_checks:
        check = (
            supabase.table(table_name)
            .select(column)
            .eq(column, str(department_id))
            .limit(1)
            .execute()
        )
        if getattr(check, "error", None):
            raise HTTPException(
                status_code=500,
                detail=f"Supabase error (check {label} for department): {check.error}",
            )
        if check.data:
            blocking_sources.append(label)
    return blocking_sources


@academics_router.delete(
    "/api/degrees/{degree_id}",
    summary="Delete a degree, its departments, batches, and related syllabus data",
)
def delete_degree(degree_id: uuid.UUID, force: bool = False):
    supabase = get_service_client()

    deg_res = (
        supabase.table("degrees")
        .select("id,college_id,name")
        .eq("id", str(degree_id))
        .limit(1)
        .execute()
    )
    if getattr(deg_res, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (find degree): {deg_res.error}")
    if not deg_res.data:
        raise HTTPException(status_code=404, detail="Degree not found")

    deg_row = deg_res.data[0]
    college_id_str = deg_row.get("college_id")
    if not college_id_str:
        raise HTTPException(status_code=400, detail="Degree missing college reference")

    # Block deletion if other products depend on the degree directly
    direct_blockers: List[str] = []
    
    # If force is True, we will attempt to delete or unlink these records instead of blocking
    if not force:
        for table_name, label, column in (
            ("teacher_classes", "teacher classes", "degree_id"),
            ("user_education", "user education records", "degree_id"),
        ):
            check = supabase.table(table_name).select(column).eq(column, str(degree_id)).limit(1).execute()
            if getattr(check, "error", None):
                raise HTTPException(status_code=500, detail=f"Supabase error (check {label} for degree): {check.error}")
            if check.data:
                direct_blockers.append(label)

        if direct_blockers:
            formatted = ", ".join(direct_blockers)
            raise HTTPException(
                status_code=409,
                detail=f"Cannot delete degree because related data exists: {formatted}. Remove or reassign those records first.",
            )
    else:
        # Force delete: clean up direct dependencies
        # 1. User Education - delete records linked to this degree
        _ = supabase.table("user_education").delete().eq("degree_id", str(degree_id)).execute()
        # 2. Teacher Classes - delete classes linked to this degree
        _ = supabase.table("teacher_classes").delete().eq("degree_id", str(degree_id)).execute()

    dept_res = (
        supabase.table("departments")
        .select("id,name")
        .eq("degree_id", str(degree_id))
        .execute()
    )
    if getattr(dept_res, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (list departments for degree): {dept_res.error}")

    # Check blockers per department before any deletion (unless forced)
    if not force:
        per_dept_blockers: Dict[str, List[str]] = {}
        for dept_row in dept_res.data or []:
            dept_id = dept_row.get("id")
            if not dept_id:
                continue
            blockers = _check_department_delete_blockers(supabase, dept_id)
            if blockers:
                per_dept_blockers[dept_row.get("name") or dept_id] = blockers

        if per_dept_blockers:
            # Keep message short but actionable
            names = ", ".join(list(per_dept_blockers.keys())[:5])
            suffix = "" if len(per_dept_blockers) <= 5 else f" (+{len(per_dept_blockers) - 5} more)"
            raise HTTPException(
                status_code=409,
                detail=f"Cannot delete degree because related data exists under departments: {names}{suffix}. Remove or reassign those records first.",
            )

    stats = {"departments": 0, "batches": 0, "courses": 0}
    for dept_row in dept_res.data or []:
        dept_id_str = dept_row.get("id")
        if not dept_id_str:
            continue
        
        if force:
            # Force cleanup for department dependencies
            # 1. Marketplace Notes
            _ = supabase.table("marketplace_notes").delete().eq("department_id", dept_id_str).execute()
            # 2. Teacher Applications
            _ = supabase.table("teacher_applications").delete().eq("department_id", dept_id_str).execute()
            # 3. Teacher Classes (by department)
            _ = supabase.table("teacher_classes").delete().eq("department_id", dept_id_str).execute()
            # 4. User Education (by department)
            _ = supabase.table("user_education").delete().eq("department_id", dept_id_str).execute()
            # 5. Teacher Profiles - Update to NULL (don't delete user profile)
            _ = supabase.table("teacher_profiles").update({"department_id": None}).eq("department_id", dept_id_str).execute()

        dept_uuid = uuid.UUID(dept_id_str)
        batch_stats = _cascade_delete_department_batches(supabase, dept_uuid)
        stats["batches"] += int(batch_stats.get("batches", 0))
        stats["courses"] += int(batch_stats.get("courses", 0))

        del_dept = supabase.table("departments").delete().eq("id", dept_id_str).execute()
        if getattr(del_dept, "error", None):
            raise HTTPException(status_code=500, detail=f"Supabase error (delete department): {del_dept.error}")
        stats["departments"] += 1

    del_deg = supabase.table("degrees").delete().eq("id", str(degree_id)).execute()
    if getattr(del_deg, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (delete degree): {del_deg.error}")

    return {
        "ok": True,
        "deleted_degree_id": str(degree_id),
        "deleted_departments": stats["departments"],
        "deleted_batches": stats["batches"],
        "deleted_courses": stats["courses"],
    }


@academics_router.post(
    "/api/degrees/{degree_id}/departments",
    response_model=DepartmentWithBatchesOut,
    summary="Create a department for a degree",
)
def create_department_simple(degree_id: uuid.UUID, payload: DepartmentSimpleCreateIn):
    supabase = get_service_client()

    degree_res = (
        supabase.table("degrees")
        .select("id,college_id")
        .eq("id", str(degree_id))
        .limit(1)
        .execute()
    )
    if getattr(degree_res, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (find degree): {degree_res.error}")
    if not degree_res.data:
        raise HTTPException(status_code=404, detail="Degree not found")

    degree_row = degree_res.data[0]
    college_id_str = degree_row.get("college_id")
    if not college_id_str:
        raise HTTPException(status_code=400, detail="Degree missing college reference")
    college_id = uuid.UUID(college_id_str)

    department_id: Optional[uuid.UUID] = None
    existing = (
        supabase.table("departments")
        .select("id")
        .eq("degree_id", str(degree_id))
        .eq("name", payload.name)
        .limit(1)
        .execute()
    )
    if getattr(existing, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (find department): {existing.error}")

    if existing.data:
        department_id = uuid.UUID(existing.data[0]["id"])
    else:
        insert_payload: Dict[str, Any] = {
            "college_id": str(college_id),
            "degree_id": str(degree_id),
            "name": payload.name,
        }
        ins = supabase.table("departments").insert(insert_payload).execute()
        if getattr(ins, "error", None):
            error_text = str(ins.error)
            if "duplicate key value" in error_text:
                raise HTTPException(
                    status_code=409,
                    detail="A department with this name already exists for this degree.",
                )
            raise HTTPException(status_code=500, detail=f"Supabase error (insert department): {ins.error}")
        if ins.data:
            department_id = uuid.UUID(ins.data[0]["id"])
        else:
            refetch = (
                supabase.table("departments")
                .select("id")
                .eq("degree_id", str(degree_id))
                .eq("name", payload.name)
                .limit(1)
                .execute()
            )
            if getattr(refetch, "error", None) or not refetch.data:
                raise HTTPException(status_code=500, detail="Failed to retrieve created department.")
            department_id = uuid.UUID(refetch.data[0]["id"])

    if department_id is None:
        raise HTTPException(status_code=500, detail="Unable to determine department identifier after upsert.")

    _ensure_department_batches(college_id, department_id, payload.batches)

    college_snapshot = get_college_full(college_id)
    return _locate_department_from_snapshot(college_snapshot, degree_id, department_id)


@academics_router.put(
    "/api/departments/{department_id}",
    response_model=DepartmentWithBatchesOut,
    summary="Update department metadata",
)
def update_department_simple(department_id: uuid.UUID, payload: DepartmentSimpleUpdateIn):
    supabase = get_service_client()

    dept_res = (
        supabase.table("departments")
        .select("id,degree_id,college_id")
        .eq("id", str(department_id))
        .limit(1)
        .execute()
    )
    if getattr(dept_res, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (find department): {dept_res.error}")
    if not dept_res.data:
        raise HTTPException(status_code=404, detail="Department not found")

    dept_row = dept_res.data[0]
    college_id_str = dept_row.get("college_id")
    degree_id_str = dept_row.get("degree_id")
    if not college_id_str or not degree_id_str:
        raise HTTPException(status_code=400, detail="Department missing degree or college reference")

    dup_res = (
        supabase.table("departments")
        .select("id")
        .eq("degree_id", degree_id_str)
        .eq("name", payload.name)
        .neq("id", str(department_id))
        .limit(1)
        .execute()
    )
    if getattr(dup_res, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (check duplicate department): {dup_res.error}")
    if dup_res.data:
        raise HTTPException(status_code=409, detail="Another department with this name already exists for the degree.")

    upd = (
        supabase.table("departments")
        .update({"name": payload.name})
        .eq("id", str(department_id))
        .execute()
    )
    if getattr(upd, "error", None):
        error_text = str(upd.error)
        if "duplicate key value" in error_text:
            raise HTTPException(
                status_code=409,
                detail="A department with this name already exists for the college. Please choose a different name.",
            )
        raise HTTPException(status_code=500, detail=f"Supabase error (update department): {upd.error}")

    college_id = uuid.UUID(college_id_str)
    degree_uuid = uuid.UUID(degree_id_str)
    _ensure_department_batches(college_id, department_id, payload.batches)

    college_snapshot = get_college_full(college_id)
    return _locate_department_from_snapshot(college_snapshot, degree_uuid, department_id)


def _cascade_delete_department_batches(supabase: Client, department_id: uuid.UUID) -> Dict[str, int]:
    """Delete all batches (and their subjects) owned by a department."""
    stats = {"batches": 0, "courses": 0}
    batch_res = (
        supabase.table("batches").select("id").eq("department_id", str(department_id)).execute()
    )
    if getattr(batch_res, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (list batches for department): {batch_res.error}")

    for batch_row in batch_res.data or []:
        batch_id_str = batch_row.get("id")
        if not batch_id_str:
            continue

        course_res = (
            supabase.table("syllabus_courses").select("id").eq("batch_id", batch_id_str).execute()
        )
        if getattr(course_res, "error", None):
            raise HTTPException(status_code=500, detail=f"Supabase error (list courses for batch): {course_res.error}")

        for course_row in course_res.data or []:
            course_id_str = course_row.get("id")
            if not course_id_str:
                continue
            delete_course_cascade(uuid.UUID(course_id_str))
            stats["courses"] += 1

        batch_del = supabase.table("batches").delete().eq("id", batch_id_str).execute()
        if getattr(batch_del, "error", None):
            raise HTTPException(status_code=500, detail=f"Supabase error (delete batch): {batch_del.error}")
        stats["batches"] += 1

    return stats


@academics_router.delete(
    "/api/departments/{department_id}",
    summary="Delete a department, its batches, and related syllabus data",
)
def delete_department(department_id: uuid.UUID):
    supabase = get_service_client()

    dept_res = (
        supabase.table("departments")
        .select("id,college_id,degree_id,name")
        .eq("id", str(department_id))
        .limit(1)
        .execute()
    )
    if getattr(dept_res, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (find department): {dept_res.error}")
    if not dept_res.data:
        raise HTTPException(status_code=404, detail="Department not found")

    blocking_sources: List[str] = []
    dependency_checks = [
        ("marketplace_notes", "marketplace notes"),
        ("teacher_applications", "teacher applications"),
        ("teacher_classes", "teacher classes"),
        ("teacher_profiles", "teacher profiles"),
        ("user_education", "user education records"),
    ]
    for table_name, label in dependency_checks:
        check = (
            supabase.table(table_name)
            .select("department_id")
            .eq("department_id", str(department_id))
            .limit(1)
            .execute()
        )
        if getattr(check, "error", None):
            raise HTTPException(
                status_code=500,
                detail=f"Supabase error (check {label} for department): {check.error}",
            )
        if check.data:
            blocking_sources.append(label)

    if blocking_sources:
        formatted = ", ".join(blocking_sources)
        raise HTTPException(
            status_code=409,
            detail=f"Cannot delete department because related data exists: {formatted}. Remove or reassign those records first.",
        )

    batch_stats = _cascade_delete_department_batches(supabase, department_id)

    del_res = supabase.table("departments").delete().eq("id", str(department_id)).execute()
    if getattr(del_res, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (delete department): {del_res.error}")

    return {
        "ok": True,
        "deleted_department_id": str(department_id),
        "deleted_batches": batch_stats["batches"],
        "deleted_courses": batch_stats["courses"],
    }


@academics_router.put(
    "/api/colleges/{college_id}",
    response_model=College,
    summary="Rename a college",
)
def rename_college(college_id: uuid.UUID, payload: CollegeNameOnlyIn):
    supabase = get_service_client()
    upd = (
        supabase.table("colleges")
        .update({"name": payload.name})
        .eq("id", str(college_id))
        .execute()
    )
    if getattr(upd, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (rename college): {upd.error}")

    ref = (
        supabase.table("colleges")
        .select("id,name")
        .eq("id", str(college_id))
        .limit(1)
        .execute()
    )
    if getattr(ref, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (fetch college): {ref.error}")
    if not ref.data:
        raise HTTPException(status_code=404, detail="College not found")
    row = ref.data[0]
    return College(id=uuid.UUID(row["id"]), name=row["name"])


@academics_router.delete(
    "/api/colleges/{college_id}",
    summary="Delete a college, its degrees/departments/batches, and related syllabus data",
)
def delete_college(college_id: uuid.UUID, force: bool = Query(default=False)):
    supabase = get_service_client()

    col_res = (
        supabase.table("colleges")
        .select("id,name")
        .eq("id", str(college_id))
        .limit(1)
        .execute()
    )
    if getattr(col_res, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (find college): {col_res.error}")
    if not col_res.data:
        raise HTTPException(status_code=404, detail="College not found")

    def _chunked(values: List[str], size: int = 150) -> List[List[str]]:
        return [values[i : i + size] for i in range(0, len(values), size)]

    def _as_id_list(values) -> List[str]:
        out_ids: List[str] = []
        for v in (values or []):
            if v is None:
                continue
            s = str(v).strip()
            if s:
                out_ids.append(s)
        return list(dict.fromkeys(out_ids))

    def _try_delete_in(table: str, key: str, ids_list: List[str]) -> int:
        if not ids_list:
            return 0
        deleted = 0
        for chunk in _chunked(_as_id_list(ids_list)):
            res = supabase.table(table).delete().in_(key, chunk).execute()
            if getattr(res, "error", None):
                raise HTTPException(status_code=500, detail=f"Supabase error (delete {table}): {res.error}")
            deleted += len(getattr(res, "data", None) or [])
        return deleted

    def _try_update_in(table: str, key: str, ids_list: List[str], updates: Dict[str, Any]) -> int:
        if not ids_list:
            return 0
        updated = 0
        for chunk in _chunked(_as_id_list(ids_list)):
            res = supabase.table(table).update(updates).in_(key, chunk).execute()
            if getattr(res, "error", None):
                raise HTTPException(status_code=500, detail=f"Supabase error (update {table}): {res.error}")
            updated += len(getattr(res, "data", None) or [])
        return updated

    if not force:
        # Block deletion if other products depend on the college directly
        direct_blockers: List[str] = []
        for table_name, label, column in (
            ("teacher_applications", "teacher applications", "college_id"),
            ("teacher_profiles", "teacher profiles", "college_id"),
            ("teacher_classes", "teacher classes", "college_id"),
            ("user_education", "user education records", "college_id"),
            ("user_profiles", "user profiles", "college_id"),
            ("marketplace_notes", "marketplace notes", "college_id"),
            ("hod_role_applications", "hod role applications", "college_id"),
        ):
            check = supabase.table(table_name).select(column).eq(column, str(college_id)).limit(1).execute()
            if getattr(check, "error", None):
                raise HTTPException(status_code=500, detail=f"Supabase error (check {label} for college): {check.error}")
            if check.data:
                direct_blockers.append(label)

        if direct_blockers:
            formatted = ", ".join(direct_blockers)
            raise HTTPException(
                status_code=409,
                detail=f"Cannot delete college because related data exists: {formatted}. Remove or reassign those records first.",
            )

    deg_res = (
        supabase.table("degrees")
        .select("id")
        .eq("college_id", str(college_id))
        .execute()
    )
    if getattr(deg_res, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (list degrees for college): {deg_res.error}")

    # When force=true, proactively detach/delete dependent rows that would block FK constraints.
    force_stats: Dict[str, int] = {}
    if force:
        # Identify hierarchy ids first.
        deg_ids = [r.get("id") for r in (deg_res.data or []) if isinstance(r, dict) and r.get("id")]
        dept_q = supabase.table("departments").select("id").eq("college_id", str(college_id)).execute()
        if getattr(dept_q, "error", None):
            raise HTTPException(status_code=500, detail=f"Supabase error (list departments for college): {dept_q.error}")
        dept_ids = [r.get("id") for r in (dept_q.data or []) if isinstance(r, dict) and r.get("id")]

        batch_q = supabase.table("batches").select("id").eq("college_id", str(college_id)).execute()
        if getattr(batch_q, "error", None):
            raise HTTPException(status_code=500, detail=f"Supabase error (list batches for college): {batch_q.error}")
        batch_ids = [r.get("id") for r in (batch_q.data or []) if isinstance(r, dict) and r.get("id")]

        # Marketplace notes (delete reviews/purchases first to satisfy FKs)
        note_q = supabase.table("marketplace_notes").select("id").eq("college_id", str(college_id)).execute()
        if getattr(note_q, "error", None):
            raise HTTPException(status_code=500, detail=f"Supabase error (list marketplace notes for college): {note_q.error}")
        note_ids = [r.get("id") for r in (note_q.data or []) if isinstance(r, dict) and r.get("id")]
        if note_ids:
            force_stats["marketplace_reviews_deleted"] = _try_delete_in("marketplace_reviews", "note_id", note_ids)
            force_stats["marketplace_purchases_deleted"] = _try_delete_in("marketplace_purchases", "note_id", note_ids)
            force_stats["marketplace_notes_deleted"] = _try_delete_in("marketplace_notes", "id", note_ids)

        # Teacher applications and related HOD role applications (safe to delete)
        del_apps = supabase.table("teacher_applications").delete().eq("college_id", str(college_id)).execute()
        if getattr(del_apps, "error", None):
            raise HTTPException(status_code=500, detail=f"Supabase error (delete teacher applications for college): {del_apps.error}")
        force_stats["teacher_applications_deleted"] = len(getattr(del_apps, "data", None) or [])
        if dept_ids:
            force_stats["teacher_applications_deleted"] += _try_delete_in("teacher_applications", "department_id", dept_ids)

        hod_role_del = supabase.table("hod_role_applications").delete().eq("college_id", str(college_id)).execute()
        if getattr(hod_role_del, "error", None):
            raise HTTPException(status_code=500, detail=f"Supabase error (delete hod role applications for college): {hod_role_del.error}")
        force_stats["hod_role_applications_deleted"] = len(getattr(hod_role_del, "data", None) or [])
        if dept_ids:
            force_stats["hod_role_applications_deleted"] += _try_delete_in("hod_role_applications", "department_id", dept_ids)

        # Teacher classes: delete (these are college-scoped operational rows)
        del_cls = supabase.table("teacher_classes").delete().eq("college_id", str(college_id)).execute()
        if getattr(del_cls, "error", None):
            raise HTTPException(status_code=500, detail=f"Supabase error (delete teacher classes for college): {del_cls.error}")
        force_stats["teacher_classes_deleted"] = len(getattr(del_cls, "data", None) or [])
        force_stats["teacher_classes_deleted"] += _try_delete_in("teacher_classes", "degree_id", deg_ids)
        force_stats["teacher_classes_deleted"] += _try_delete_in("teacher_classes", "department_id", dept_ids)
        force_stats["teacher_classes_deleted"] += _try_delete_in("teacher_classes", "batch_id", batch_ids)

        # Teacher profiles: detach (avoid deleting user identity)
        upd_tp = supabase.table("teacher_profiles").update({"college_id": None, "department_id": None}).eq("college_id", str(college_id)).execute()
        if getattr(upd_tp, "error", None):
            raise HTTPException(status_code=500, detail=f"Supabase error (detach teacher profiles): {upd_tp.error}")
        force_stats["teacher_profiles_detached"] = len(getattr(upd_tp, "data", None) or [])
        if dept_ids:
            force_stats["teacher_profiles_detached"] += _try_update_in("teacher_profiles", "department_id", dept_ids, {"department_id": None, "college_id": None})

        # User education: delete records (aligns with delete_degree behavior)
        del_ue = supabase.table("user_education").delete().eq("college_id", str(college_id)).execute()
        if getattr(del_ue, "error", None):
            raise HTTPException(status_code=500, detail=f"Supabase error (delete user education): {del_ue.error}")
        force_stats["user_education_deleted"] = len(getattr(del_ue, "data", None) or [])

        upd_up = supabase.table("user_profiles").update({"college_id": None}).eq("college_id", str(college_id)).execute()
        if getattr(upd_up, "error", None):
            raise HTTPException(status_code=500, detail=f"Supabase error (detach user profiles college): {upd_up.error}")
        force_stats["user_profiles_detached"] = len(getattr(upd_up, "data", None) or [])
        force_stats["user_profiles_detached"] += _try_update_in("user_profiles", "department_id", dept_ids, {"department_id": None})
        force_stats["user_profiles_detached"] += _try_update_in("user_profiles", "batch_id", batch_ids, {"batch_id": None})

        upd_ux = supabase.table("user_experiences").update({"batch_id": None}).in_("batch_id", _as_id_list(batch_ids) or ["00000000-0000-0000-0000-000000000000"]).execute()
        if getattr(upd_ux, "error", None):
            raise HTTPException(status_code=500, detail=f"Supabase error (detach user experiences batch): {upd_ux.error}")
        force_stats["user_experiences_detached"] = len(getattr(upd_ux, "data", None) or [])

        # HOD batch management references batches without ON DELETE SET NULL -> detach.
        if dept_ids:
            hb_updates = {
                "first_year_batch_id": None,
                "second_year_batch_id": None,
                "third_year_batch_id": None,
                "final_year_batch_id": None,
            }
            force_stats["hod_batch_management_detached"] = _try_update_in("hod_batch_management", "department_id", dept_ids, hb_updates)

    stats = {"degrees": 0, "departments": 0, "batches": 0, "courses": 0}
    for deg_row in deg_res.data or []:
        deg_id_str = deg_row.get("id")
        if not deg_id_str:
            continue

        dept_res = (
            supabase.table("departments")
            .select("id")
            .eq("degree_id", deg_id_str)
            .execute()
        )
        if getattr(dept_res, "error", None):
            raise HTTPException(status_code=500, detail=f"Supabase error (list departments for degree): {dept_res.error}")

        if not force:
            # Ensure no department blockers exist (defensive; should already be implied by direct blockers)
            for drow in dept_res.data or []:
                did = drow.get("id")
                if not did:
                    continue
                blockers = _check_department_delete_blockers(supabase, did)
                if blockers:
                    formatted = ", ".join(blockers)
                    raise HTTPException(
                        status_code=409,
                        detail=f"Cannot delete college because related data exists under a department: {formatted}. Remove or reassign those records first.",
                    )

        for drow in dept_res.data or []:
            did = drow.get("id")
            if not did:
                continue
            dept_uuid = uuid.UUID(did)
            batch_stats = _cascade_delete_department_batches(supabase, dept_uuid)
            stats["batches"] += int(batch_stats.get("batches", 0))
            stats["courses"] += int(batch_stats.get("courses", 0))
            del_dept = supabase.table("departments").delete().eq("id", did).execute()
            if getattr(del_dept, "error", None):
                raise HTTPException(status_code=500, detail=f"Supabase error (delete department): {del_dept.error}")
            stats["departments"] += 1

        del_deg = supabase.table("degrees").delete().eq("id", deg_id_str).execute()
        if getattr(del_deg, "error", None):
            raise HTTPException(status_code=500, detail=f"Supabase error (delete degree): {del_deg.error}")
        stats["degrees"] += 1

    del_col = supabase.table("colleges").delete().eq("id", str(college_id)).execute()
    if getattr(del_col, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (delete college): {del_col.error}")

    return {
        "ok": True,
        "forced": bool(force),
        "force_cleanup": force_stats,
        "deleted_college_id": str(college_id),
        "deleted_degrees": stats["degrees"],
        "deleted_departments": stats["departments"],
        "deleted_batches": stats["batches"],
        "deleted_courses": stats["courses"],
    }


@academics_router.get("/api/colleges/{college_id}", response_model=CollegeFullOut, summary="Get a college with departments & batches")
def get_college(college_id: uuid.UUID):
    data = get_college_full(college_id)
    return CollegeFullOut(**data)


@academics_router.get("/api/colleges/{college_id}/departments", response_model=List[str], summary="List departments for a college")
def list_departments_for_college(college_id: uuid.UUID):
    supabase = get_service_client()
    res = (
        supabase.table("departments").select("name").eq("college_id", str(college_id)).order("name").execute()
    )
    if getattr(res, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (list departments): {res.error}")
    return [row["name"] for row in (res.data or [])]


@academics_router.get(
    "/api/colleges/{college_id}/departments/full",
    response_model=List[DepartmentOut],
    summary="List departments (id & name) for a college",
)
def list_departments_full(college_id: uuid.UUID):
    supabase = get_service_client()
    res = (
        supabase.table("departments").select("id,name").eq("college_id", str(college_id)).order("name").execute()
    )
    if getattr(res, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (list departments full): {res.error}")
    return [{"id": row["id"], "name": row["name"]} for row in (res.data or [])]


@academics_router.get(
    "/api/colleges/{college_id}/departments/{dept_name}/batches",
    response_model=List[BatchIn],
    summary="List batches for a department within a college",
)
def list_batches_for_department(college_id: uuid.UUID, dept_name: str):
    supabase = get_service_client()
    dept_q = (
        supabase.table("departments")
        .select("id")
        .eq("college_id", str(college_id))
        .eq("name", dept_name.upper())
        .limit(1)
        .execute()
    )
    if getattr(dept_q, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (find department): {dept_q.error}")
    if not dept_q.data:
        raise HTTPException(status_code=404, detail="Department not found for college")
    dept_id = dept_q.data[0]["id"]

    b = (
        supabase.table("batches")
        .select("from_year,to_year")
        .eq("college_id", str(college_id))
        .eq("department_id", dept_id)
        .order("from_year")
        .execute()
    )
    if getattr(b, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (get dept batches): {b.error}")
    return [BatchIn(**{"from": row["from_year"], "to": row["to_year"]}) for row in (b.data or [])]


@academics_router.get(
    "/api/colleges/{college_id}/departments/{dept_name}/batches/full",
    response_model=List[BatchWithIdOut],
    summary="List batches (with id) for a department within a college",
)
def list_batches_for_department_with_ids(college_id: uuid.UUID, dept_name: str):
    supabase = get_service_client()
    dept_q = (
        supabase.table("departments")
        .select("id")
        .eq("college_id", str(college_id))
        .eq("name", dept_name.upper())
        .limit(1)
        .execute()
    )
    if getattr(dept_q, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (find department): {dept_q.error}")
    if not dept_q.data:
        raise HTTPException(status_code=404, detail="Department not found for college")
    dept_id = dept_q.data[0]["id"]

    b = (
        supabase.table("batches")
        .select("id,from_year,to_year")
        .eq("college_id", str(college_id))
        .eq("department_id", dept_id)
        .order("from_year")
        .execute()
    )
    if getattr(b, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (get dept batches full): {b.error}")
    return [
        {"id": row["id"], "from_year": row["from_year"], "to_year": row["to_year"]}
        for row in (b.data or [])
    ]


@academics_router.get(
    "/api/departments/{department_id}/batches/full",
    response_model=List[BatchWithIdOut],
    summary="List batches (with id) for a department by id",
)
def list_batches_for_department_with_ids_by_id(department_id: uuid.UUID):
    supabase = get_service_client()
    dept_res = (
        supabase.table("departments")
        .select("id,college_id")
        .eq("id", str(department_id))
        .limit(1)
        .execute()
    )
    if getattr(dept_res, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (find department by id): {dept_res.error}")
    if not dept_res.data:
        raise HTTPException(status_code=404, detail="Department not found")

    college_id_value = dept_res.data[0].get("college_id")

    query = (
        supabase.table("batches")
        .select("id,from_year,to_year")
        .eq("department_id", str(department_id))
        .order("from_year")
    )
    if college_id_value:
        query = query.eq("college_id", str(college_id_value))

    b = query.execute()
    if getattr(b, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (get dept batches full by id): {b.error}")
    return [
        {"id": row["id"], "from_year": row["from_year"], "to_year": row["to_year"]}
        for row in (b.data or [])
    ]


@academics_router.post(
    "/api/departments/{department_id}/batches",
    response_model=BatchWithIdOut,
    summary="Create a batch for a department",
)
def create_batch_for_department(department_id: uuid.UUID, payload: BatchIn):
    supabase = get_service_client()

    dept_res = (
        supabase.table("departments")
        .select("id,college_id")
        .eq("id", str(department_id))
        .limit(1)
        .execute()
    )
    if getattr(dept_res, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (find department): {dept_res.error}")
    if not dept_res.data:
        raise HTTPException(status_code=404, detail="Department not found")

    dept_row = dept_res.data[0]
    college_id_str = dept_row.get("college_id")
    if not college_id_str:
        raise HTTPException(status_code=400, detail="Department missing college reference")

    dup_res = (
        supabase.table("batches")
        .select("id")
        .eq("department_id", str(department_id))
        .eq("from_year", payload.from_year)
        .eq("to_year", payload.to_year)
        .limit(1)
        .execute()
    )
    if getattr(dup_res, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (check duplicate batch): {dup_res.error}")
    if dup_res.data:
        raise HTTPException(status_code=409, detail="This batch already exists for the department.")

    insert_payload = {
        "college_id": college_id_str,
        "department_id": str(department_id),
        "from_year": payload.from_year,
        "to_year": payload.to_year,
    }
    ins = supabase.table("batches").insert(insert_payload).execute()
    if getattr(ins, "error", None):
        error_text = str(ins.error)
        if "duplicate key value" in error_text:
            raise HTTPException(status_code=409, detail="This batch already exists for the department.")
        raise HTTPException(status_code=500, detail=f"Supabase error (insert batch): {ins.error}")

    if not ins.data:
        refetch = (
            supabase.table("batches")
            .select("id,from_year,to_year")
            .eq("department_id", str(department_id))
            .eq("from_year", payload.from_year)
            .eq("to_year", payload.to_year)
            .limit(1)
            .execute()
        )
        if getattr(refetch, "error", None) or not refetch.data:
            raise HTTPException(status_code=500, detail="Failed to retrieve created batch.")
        batch_row = refetch.data[0]
    else:
        batch_row = ins.data[0]

    return BatchWithIdOut(
        id=uuid.UUID(batch_row["id"]),
        from_year=int(batch_row.get("from_year", payload.from_year)),
        to_year=int(batch_row.get("to_year", payload.to_year)),
    )


@academics_router.put(
    "/api/batches/{batch_id}",
    response_model=BatchWithIdOut,
    summary="Update batch years",
)
def update_batch(batch_id: uuid.UUID, payload: BatchIn):
    supabase = get_service_client()

    batch_res = (
        supabase.table("batches")
        .select("id,college_id,department_id")
        .eq("id", str(batch_id))
        .limit(1)
        .execute()
    )
    if getattr(batch_res, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (find batch): {batch_res.error}")
    if not batch_res.data:
        raise HTTPException(status_code=404, detail="Batch not found")

    batch_row = batch_res.data[0]
    department_id_str = batch_row.get("department_id")
    if not department_id_str:
        raise HTTPException(status_code=400, detail="Batch missing department reference")

    dup_res = (
        supabase.table("batches")
        .select("id")
        .eq("department_id", department_id_str)
        .eq("from_year", payload.from_year)
        .eq("to_year", payload.to_year)
        .neq("id", str(batch_id))
        .limit(1)
        .execute()
    )
    if getattr(dup_res, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (check duplicate batch): {dup_res.error}")
    if dup_res.data:
        raise HTTPException(status_code=409, detail="Another batch with this year range already exists for the department.")

    upd = (
        supabase.table("batches")
        .update({"from_year": payload.from_year, "to_year": payload.to_year})
        .eq("id", str(batch_id))
        .execute()
    )
    if getattr(upd, "error", None):
        error_text = str(upd.error)
        if "duplicate key value" in error_text:
            raise HTTPException(status_code=409, detail="Another batch with this year range already exists for the department.")
        raise HTTPException(status_code=500, detail=f"Supabase error (update batch): {upd.error}")

    return BatchWithIdOut(id=batch_id, from_year=payload.from_year, to_year=payload.to_year)


@academics_router.delete(
    "/api/batches/{batch_id}",
    summary="Delete a batch and related syllabus data",
)
def delete_batch(batch_id: uuid.UUID):
    supabase = get_service_client()

    batch_res = (
        supabase.table("batches")
        .select("id")
        .eq("id", str(batch_id))
        .limit(1)
        .execute()
    )
    if getattr(batch_res, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (find batch): {batch_res.error}")
    if not batch_res.data:
        raise HTTPException(status_code=404, detail="Batch not found")

    deleted_courses = 0
    course_res = (
        supabase.table("syllabus_courses")
        .select("id")
        .eq("batch_id", str(batch_id))
        .execute()
    )
    if getattr(course_res, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (list courses for batch): {course_res.error}")

    for course_row in course_res.data or []:
        course_id_str = course_row.get("id")
        if not course_id_str:
            continue
        delete_course_cascade(uuid.UUID(course_id_str))
        deleted_courses += 1

    del_res = supabase.table("batches").delete().eq("id", str(batch_id)).execute()
    if getattr(del_res, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (delete batch): {del_res.error}")

    return {
        "ok": True,
        "deleted_batch_id": str(batch_id),
        "deleted_courses": deleted_courses,
    }


@academics_router.post(
    "/api/batches/{batch_id}/courses",
    response_model=SyllabusCourseSummaryOut,
    summary="Create a subject for a batch",
)
def create_course_for_batch(batch_id: uuid.UUID, payload: SyllabusCourseSimpleCreateIn):
    supabase = get_service_client()

    batch_res = (
        supabase.table("batches")
        .select("id")
        .eq("id", str(batch_id))
        .limit(1)
        .execute()
    )
    if getattr(batch_res, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (find batch): {batch_res.error}")
    if not batch_res.data:
        raise HTTPException(status_code=404, detail="Batch not found")

    dup_res = (
        supabase.table("syllabus_courses")
        .select("id")
        .eq("batch_id", str(batch_id))
        .eq("semester", payload.semester)
        .eq("course_code", payload.course_code)
        .limit(1)
        .execute()
    )
    if getattr(dup_res, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (check duplicate course): {dup_res.error}")
    if dup_res.data:
        raise HTTPException(
            status_code=409,
            detail="A subject with this course code already exists for the semester in this batch.",
        )

    insert_payload = {
        "batch_id": str(batch_id),
        "semester": payload.semester,
        "course_code": payload.course_code,
        "title": payload.title,
        "type": getattr(payload, "type", None) or "practical",
    }
    ins = supabase.table("syllabus_courses").insert(insert_payload).execute()
    if getattr(ins, "error", None):
        error_text = str(ins.error)
        if "duplicate key value" in error_text:
            raise HTTPException(
                status_code=409,
                detail="A subject with this course code already exists for the semester in this batch.",
            )
        raise HTTPException(status_code=500, detail=f"Supabase error (insert course): {ins.error}")

    if ins.data:
        row = ins.data[0]
    else:
        refetch = (
            supabase.table("syllabus_courses")
            .select("id,semester,course_code,title,type")
            .eq("batch_id", str(batch_id))
            .eq("semester", payload.semester)
            .eq("course_code", payload.course_code)
            .limit(1)
            .execute()
        )
        if getattr(refetch, "error", None) or not refetch.data:
            raise HTTPException(status_code=500, detail="Failed to retrieve created subject.")
        row = refetch.data[0]

    return SyllabusCourseSummaryOut(
        id=uuid.UUID(row["id"]),
        batch_id=batch_id,
        semester=int(row.get("semester", payload.semester)),
        course_code=row.get("course_code", payload.course_code),
        title=row.get("title", payload.title),
        type=row.get("type", getattr(payload, "type", None)),
    )


@academics_router.post("/api/batches/resolve", response_model=BatchWithIdOut, summary="Resolve or create a batch id for a college + dept + year range")
def resolve_or_create_batch(payload: BatchResolveIn):
    return resolve_or_create_batch(payload.college_id, payload.dept_name, payload.from_year, payload.to_year)


@academics_router.post("/signup")
def signup(user: UserAuth):
    return signup_user(user)


@academics_router.post("/login")
def login(user: UserAuth):
    return login_user(user)


class RefreshTokenRequest(BaseModel):
    refresh_token: str


@academics_router.post("/refresh", summary="Refresh access token using refresh token")
def refresh_token(payload: RefreshTokenRequest):
    """Refresh the access token using a valid refresh token.
    
    This endpoint allows clients to obtain a new access token without
    requiring the user to re-enter credentials. Used for persistent sessions.
    """
    anon_client = get_anon_client()
    if not anon_client:
        raise HTTPException(status_code=500, detail="Server missing SUPABASE_ANON_KEY")
    if not payload.refresh_token:
        raise HTTPException(status_code=400, detail="refresh_token is required")
    try:
        # Use Supabase's refresh_session method
        res = anon_client.auth.refresh_session(payload.refresh_token)
        session = getattr(res, "session", None)
        access_token = getattr(session, "access_token", None) if session else None
        refresh_token = getattr(session, "refresh_token", None) if session else None
        expires_in = getattr(session, "expires_in", 3600) if session else 3600
        if not access_token:
            raise HTTPException(status_code=401, detail="Token refresh failed: invalid or expired refresh token")
        return {
            "message": "Token refreshed successfully",
            "access_token": access_token,
            "refresh_token": refresh_token,
            "expires_in": expires_in,
        }
    except HTTPException:
        raise
    except Exception as e:
        supabase_logger.exception("Token refresh failed")
        raise HTTPException(status_code=401, detail="Token refresh failed: invalid or expired refresh token")


@academics_router.get("/api/public/supabase", summary="Public Supabase client config")
def public_supabase_config():
    base_url = os.getenv("SUPABASE_URL", "").rstrip("/")
    anon = os.getenv("SUPABASE_ANON_KEY", "")
    if not base_url or not anon:
        raise HTTPException(status_code=500, detail="Missing SUPABASE_URL or SUPABASE_ANON_KEY")
    return {"url": base_url, "anonKey": anon}


@academics_router.get("/api/public/academic-meta", summary="Public academic hierarchy for signup")
def public_academic_meta():
    """Return colleges, degrees, departments (minimal fields) without requiring auth.

    Used by teacher signup (pre-auth). Batches omitted for brevity.
    """
    supabase = get_service_client()
    colleges = supabase.table("colleges").select("id,name").order("name").execute()
    if getattr(colleges, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (colleges): {colleges.error}")
    degrees = supabase.table("degrees").select("id,name,college_id").execute()
    departments = supabase.table("departments").select("id,name,college_id,degree_id").execute()
    return {
        "colleges": getattr(colleges, "data", []) or [],
        "degrees": getattr(degrees, "data", []) or [],
        "departments": getattr(departments, "data", []) or [],
    }


@academics_router.post("/api/signup/full")
def signup_full(payload: SignupFullIn):
    return signup_full_user(payload)


def _parse_bearer_token(authorization: Optional[str]) -> Optional[str]:
    if not authorization:
        return None
    parts = authorization.split()
    if len(parts) == 2 and parts[0].lower() == "bearer":
        return parts[1]
    return None


@academics_router.get("/api/me")
def get_me(authorization: Optional[str] = Header(default=None)):
    token = _parse_bearer_token(authorization)
    return get_current_user_profile(token)


@academics_router.post("/api/syllabus/courses", response_model=SyllabusCourseOut, summary="Upsert syllabus course with units & topics")
def api_upsert_syllabus_course(payload: SyllabusCourseIn):
    return upsert_syllabus_course(payload)


@academics_router.put(
    "/api/syllabus/courses/{course_id}",
    response_model=SyllabusCourseSummaryOut,
    summary="Update subject metadata",
)
def update_course_metadata(course_id: uuid.UUID, payload: SyllabusCourseSimpleUpdateIn):
    supabase = get_service_client()

    existing = (
        supabase.table("syllabus_courses")
        .select("id,batch_id")
        .eq("id", str(course_id))
        .limit(1)
        .execute()
    )
    if getattr(existing, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (find course): {existing.error}")
    if not existing.data:
        raise HTTPException(status_code=404, detail="Subject not found")

    row = existing.data[0]
    batch_id_str = row.get("batch_id")
    if not batch_id_str:
        raise HTTPException(status_code=400, detail="Subject missing batch reference")

    dup_res = (
        supabase.table("syllabus_courses")
        .select("id")
        .eq("batch_id", batch_id_str)
        .eq("semester", payload.semester)
        .eq("course_code", payload.course_code)
        .neq("id", str(course_id))
        .limit(1)
        .execute()
    )
    if getattr(dup_res, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (check duplicate course): {dup_res.error}")
    if dup_res.data:
        raise HTTPException(
            status_code=409,
            detail="Another subject with this course code already exists for the semester in this batch.",
        )

    updates = {
        "semester": payload.semester,
        "course_code": payload.course_code,
        "title": payload.title,
        "type": getattr(payload, "type", None) or "practical",
    }
    upd = (
        supabase.table("syllabus_courses")
        .update(updates)
        .eq("id", str(course_id))
        .execute()
    )
    if getattr(upd, "error", None):
        error_text = str(upd.error)
        if "duplicate key value" in error_text:
            raise HTTPException(
                status_code=409,
                detail="Another subject with this course code already exists for the semester in this batch.",
            )
        raise HTTPException(status_code=500, detail=f"Supabase error (update course): {upd.error}")

    return SyllabusCourseSummaryOut(
        id=course_id,
        batch_id=uuid.UUID(batch_id_str),
        semester=payload.semester,
        course_code=payload.course_code,
        title=payload.title,
        type=updates.get("type"),
    )

# Alias route to support existing UI paths
@academics_router.put(
    "/api/courses/{course_id}",
    response_model=SyllabusCourseSummaryOut,
    summary="Update subject metadata (alias)",
)
def update_course_metadata_alias(course_id: uuid.UUID, payload: SyllabusCourseSimpleUpdateIn):
    return update_course_metadata(course_id, payload)


@academics_router.delete(
    "/api/courses/{course_id}",
    summary="Delete a subject and all units & topics",
)
def delete_course_cascade(course_id: uuid.UUID):
    supabase = get_service_client()

    # Verify course exists
    course_q = (
        supabase.table("syllabus_courses")
        .select("id")
        .eq("id", str(course_id))
        .limit(1)
        .execute()
    )
    if getattr(course_q, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (find course): {course_q.error}")
    if not course_q.data:
        raise HTTPException(status_code=404, detail="Subject not found")

    # Collect unit ids
    units_q = (
        supabase.table("syllabus_units")
        .select("id")
        .eq("course_id", str(course_id))
        .execute()
    )
    if getattr(units_q, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (list units): {units_q.error}")
    unit_ids = [u.get("id") for u in (units_q.data or []) if u.get("id")]

    # For each unit: delete progress for its topics, then delete topics
    total_deleted_topics = 0
    for uid in unit_ids:
        # topics under this unit
        t_q = (
            supabase.table("syllabus_topics")
            .select("id")
            .eq("unit_id", uid)
            .execute()
        )
        if getattr(t_q, "error", None):
            raise HTTPException(status_code=500, detail=f"Supabase error (list topics): {t_q.error}")
        t_ids = [t.get("id") for t in (t_q.data or []) if t.get("id")]

        # delete progress in chunks to avoid IN payload issues
        if t_ids:
            for i in range(0, len(t_ids), 100):
                chunk = t_ids[i:i+100]
                prog_del = (
                    supabase.table("user_topic_progress")
                    .delete()
                    .in_("topic_id", chunk)
                    .execute()
                )
                if getattr(prog_del, "error", None):
                    raise HTTPException(status_code=500, detail=f"Supabase error (delete progress): {prog_del.error}")

            # delete topics for this unit
            topics_del = (
                supabase.table("syllabus_topics")
                .delete()
                .eq("unit_id", uid)
                .execute()
            )
            if getattr(topics_del, "error", None):
                raise HTTPException(status_code=500, detail=f"Supabase error (delete topics): {topics_del.error}")
            total_deleted_topics += len(t_ids)

    # Delete units by course_id
    if unit_ids:
        units_del = (
            supabase.table("syllabus_units")
            .delete()
            .eq("course_id", str(course_id))
            .execute()
        )
        if getattr(units_del, "error", None):
            raise HTTPException(status_code=500, detail=f"Supabase error (delete units): {units_del.error}")

    # Delete course
    course_del = (
        supabase.table("syllabus_courses")
        .delete()
        .eq("id", str(course_id))
        .execute()
    )
    if getattr(course_del, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (delete course): {course_del.error}")

    return {"ok": True, "deleted_units": len(unit_ids), "deleted_topics": total_deleted_topics}


@academics_router.get(
    "/api/syllabus/units/{unit_id}/topics",
    response_model=List[TopicOut],
    summary="List topics for a unit (direct from syllabus_topics)",
)
def api_list_topics_for_unit(unit_id: uuid.UUID):
    supabase = get_service_client()
    res = (
        supabase.table("syllabus_topics")
        .select("id,topic,order_in_unit,image_url,video_url,ppt_url,lab_url")
        .eq("unit_id", str(unit_id))
        .order("order_in_unit")
        .execute()
    )
    if getattr(res, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (get topics for unit): {res.error}")

    return [
        TopicOut(
            id=uuid.UUID(row["id"]),
            topic=row.get("topic"),
            order_in_unit=int(row.get("order_in_unit", 0)),
            image_url=row.get("image_url"),
            video_url=row.get("video_url"),
            ppt_url=row.get("ppt_url"),
            lab_url=row.get("lab_url"),
        )
        for row in (res.data or [])
    ]


@academics_router.get(
    "/api/syllabus/topics/by-title",
    response_model=List[TopicOut],
    summary="Find syllabus topics by exact title (with URLs)",
)
def api_find_topics_by_title(topic: str = Query(..., min_length=1, max_length=512)):
    """Lookup topics in syllabus_topics by exact topic title.

    This is used by the notes generator to attach a recommended video
    from the structured syllabus (video_url) ahead of generic search
    results.
    """
    clean = (topic or "").strip()
    if not clean:
        return []

    supabase = get_service_client()
    res = (
        supabase.table("syllabus_topics")
        .select("id,topic,order_in_unit,image_url,video_url,ppt_url,lab_url,unit_id")
        .eq("topic", clean)
        .order("order_in_unit")
        .limit(5)
        .execute()
    )
    if getattr(res, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (find topic by title): {res.error}")

    unit_ids = [row.get("unit_id") for row in (res.data or []) if row.get("unit_id")]
    course_by_unit: Dict[str, str] = {}
    course_types: Dict[str, str] = {}
    if unit_ids:
        units_res = (
            supabase.table("syllabus_units")
            .select("id,course_id")
            .in_("id", list({uid for uid in unit_ids}))
            .execute()
        )
        if getattr(units_res, "error", None):
            raise HTTPException(status_code=500, detail=f"Supabase error (lookup units for topics): {units_res.error}")
        course_ids = [row.get("course_id") for row in (units_res.data or []) if row.get("course_id")]
        for row in (units_res.data or []):
            uid = row.get("id")
            cid = row.get("course_id")
            if uid and cid:
                course_by_unit[str(uid)] = str(cid)
        if course_ids:
            courses_res = (
                supabase.table("syllabus_courses")
                .select("id,type")
                .in_("id", list({cid for cid in course_ids}))
                .execute()
            )
            if getattr(courses_res, "error", None):
                raise HTTPException(status_code=500, detail=f"Supabase error (lookup course types): {courses_res.error}")
            for crow in (courses_res.data or []):
                if crow.get("id"):
                    course_types[str(crow["id"])] = crow.get("type")

    out: List[TopicOut] = []
    for row in (res.data or []):
        uid_raw = row.get("unit_id")
        uid_key = str(uid_raw) if uid_raw else None
        cid_raw = course_by_unit.get(uid_key) if uid_key else None
        out.append(
            TopicOut(
                id=uuid.UUID(row["id"]),
                topic=row.get("topic"),
                order_in_unit=int(row.get("order_in_unit", 0)),
                image_url=row.get("image_url"),
                video_url=row.get("video_url"),
                ppt_url=row.get("ppt_url"),
                lab_url=row.get("lab_url"),
                unit_id=uuid.UUID(uid_key) if uid_key else None,
                course_id=uuid.UUID(cid_raw) if cid_raw else None,
                course_type=course_types.get(str(cid_raw)) if cid_raw else None,
            )
        )
    return out


@academics_router.get(
    "/api/batches/{batch_id}/courses",
    response_model=List[SyllabusCourseSummaryOut],
    summary="List syllabus courses (subjects) for a batch",
)
def api_list_courses_for_batch(
    batch_id: uuid.UUID,
    semester: Optional[int] = Query(default=None, ge=1, le=12),
):
    supabase = get_service_client()
    query = (
        supabase.table("syllabus_courses")
        .select("id,batch_id,semester,course_code,title,type")
        .eq("batch_id", str(batch_id))
    )
    if semester is not None:
        query = query.eq("semester", semester)
    res = query.order("course_code").execute()
    if getattr(res, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (list courses): {res.error}")
    return [
        SyllabusCourseSummaryOut(
            id=uuid.UUID(row["id"]),
            batch_id=uuid.UUID(row["batch_id"]),
            semester=int(row["semester"]),
            course_code=row.get("course_code"),
            title=row.get("title"),
            type=row.get("type"),
        )
        for row in (res.data or [])
    ]


@academics_router.get(
    "/api/syllabus/courses/{course_id}",
    response_model=SyllabusCourseOut,
    summary="Get syllabus course with units & topics",
)
def api_get_syllabus_course(course_id: uuid.UUID):
    return load_course_with_units(course_id)


@academics_router.post(
    "/api/syllabus/courses/{course_id}/units",
    response_model=UnitOut,
    summary="Create a unit (with topics) for a syllabus course",
)
def create_unit_for_course(course_id: uuid.UUID, payload: UnitTopicsIn):
    supabase = get_service_client()

    course_res = (
        supabase.table("syllabus_courses")
        .select("id")
        .eq("id", str(course_id))
        .limit(1)
        .execute()
    )
    if getattr(course_res, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (find course): {course_res.error}")
    if not course_res.data:
        raise HTTPException(status_code=404, detail="Course not found")

    dup_res = (
        supabase.table("syllabus_units")
        .select("id")
        .eq("course_id", str(course_id))
        .eq("unit_title", payload.unit_title)
        .limit(1)
        .execute()
    )
    if getattr(dup_res, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (check duplicate unit): {dup_res.error}")
    if dup_res.data:
        raise HTTPException(status_code=409, detail="A unit with this title already exists for this course.")

    units_res = (
        supabase.table("syllabus_units")
        .select("id")
        .eq("course_id", str(course_id))
        .execute()
    )
    if getattr(units_res, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (list units): {units_res.error}")
    next_order = len(units_res.data or [])

    ins = (
        supabase.table("syllabus_units")
        .insert(
            {
                "course_id": str(course_id),
                "unit_title": payload.unit_title,
                "order_in_course": next_order,
            }
        )
        .execute()
    )
    if getattr(ins, "error", None):
        error_text = str(ins.error)
        if "duplicate key value" in error_text:
            raise HTTPException(status_code=409, detail="A unit with this title already exists for this course.")
        raise HTTPException(status_code=500, detail=f"Supabase error (insert unit): {ins.error}")

    if ins.data:
        unit_id_str = ins.data[0].get("id")
    else:
        refetch = (
            supabase.table("syllabus_units")
            .select("id")
            .eq("course_id", str(course_id))
            .eq("unit_title", payload.unit_title)
            .limit(1)
            .execute()
        )
        if getattr(refetch, "error", None) or not refetch.data:
            raise HTTPException(status_code=500, detail="Failed to retrieve created unit.")
        unit_id_str = refetch.data[0].get("id")

    if not unit_id_str:
        raise HTTPException(status_code=500, detail="Unit identifier missing after creation.")
    unit_id = uuid.UUID(unit_id_str)

    topic_rows = [
        {
            "unit_id": str(unit_id),
            "topic": topic.topic,
            "order_in_unit": index,
        }
        for index, topic in enumerate(payload.topics or [])
    ]
    if topic_rows:
        tins = supabase.table("syllabus_topics").insert(topic_rows).execute()
        if getattr(tins, "error", None):
            raise HTTPException(status_code=500, detail=f"Supabase error (insert topics): {tins.error}")

    return load_unit_with_topics(unit_id)


@academics_router.put(
    "/api/syllabus/units/{unit_id}",
    response_model=UnitOut,
    summary="Update a syllabus unit title and topics",
)
def update_unit_topics(unit_id: uuid.UUID, payload: UnitTopicsIn):
    supabase = get_service_client()

    unit_res = (
        supabase.table("syllabus_units")
        .select("id,course_id")
        .eq("id", str(unit_id))
        .limit(1)
        .execute()
    )
    if getattr(unit_res, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (find unit): {unit_res.error}")
    if not unit_res.data:
        raise HTTPException(status_code=404, detail="Unit not found")

    course_id_str = unit_res.data[0].get("course_id")
    if not course_id_str:
        raise HTTPException(status_code=400, detail="Unit missing course reference")

    dup_res = (
        supabase.table("syllabus_units")
        .select("id")
        .eq("course_id", course_id_str)
        .eq("unit_title", payload.unit_title)
        .neq("id", str(unit_id))
        .limit(1)
        .execute()
    )
    if getattr(dup_res, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (check duplicate unit): {dup_res.error}")
    if dup_res.data:
        raise HTTPException(status_code=409, detail="Another unit with this title already exists for this course.")

    upd = (
        supabase.table("syllabus_units")
        .update({"unit_title": payload.unit_title})
        .eq("id", str(unit_id))
        .execute()
    )
    if getattr(upd, "error", None):
        error_text = str(upd.error)
        if "duplicate key value" in error_text:
            raise HTTPException(status_code=409, detail="Another unit with this title already exists for this course.")
        raise HTTPException(status_code=500, detail=f"Supabase error (update unit): {upd.error}")

    # --- Update topics without wiping per-topic URL fields ---
    # Strategy:
    # 1) If client sends topic IDs, upsert by ID and delete removed IDs.
    # 2) If no IDs are sent (legacy clients), fall back to positional update to
    #    preserve URLs as much as possible.

    existing_q = (
        supabase.table("syllabus_topics")
        .select("id,order_in_unit")
        .eq("unit_id", str(unit_id))
        .order("order_in_unit")
        .execute()
    )
    if getattr(existing_q, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (list existing topics): {existing_q.error}")
    existing_rows = existing_q.data or []
    existing_ids: List[str] = [r.get("id") for r in existing_rows if r.get("id")]
    existing_id_set: Set[str] = set(existing_ids)

    incoming_topics = payload.topics or []
    incoming_ids: List[str] = [str(t.id) for t in incoming_topics if getattr(t, "id", None)]
    incoming_id_set: Set[str] = set(incoming_ids)
    any_ids_provided = bool(incoming_id_set)

    def _delete_topics_and_progress(topic_ids: List[str]) -> None:
        if not topic_ids:
            return
        for i in range(0, len(topic_ids), 100):
            chunk = topic_ids[i : i + 100]
            prog_del = (
                supabase.table("user_topic_progress")
                .delete()
                .in_("topic_id", chunk)
                .execute()
            )
            if getattr(prog_del, "error", None):
                raise HTTPException(status_code=500, detail=f"Supabase error (delete progress): {prog_del.error}")

        t_del = supabase.table("syllabus_topics").delete().in_("id", topic_ids).execute()
        if getattr(t_del, "error", None):
            raise HTTPException(status_code=500, detail=f"Supabase error (delete topics): {t_del.error}")

    if any_ids_provided:
        # Update or insert each topic row
        for index, topic in enumerate(incoming_topics):
            tid = getattr(topic, "id", None)
            if tid is not None and str(tid) in existing_id_set:
                upd_t = (
                    supabase.table("syllabus_topics")
                    .update({"topic": topic.topic, "order_in_unit": index})
                    .eq("id", str(tid))
                    .eq("unit_id", str(unit_id))
                    .execute()
                )
                if getattr(upd_t, "error", None):
                    raise HTTPException(status_code=500, detail=f"Supabase error (update topic): {upd_t.error}")
            else:
                ins_t = (
                    supabase.table("syllabus_topics")
                    .insert({"unit_id": str(unit_id), "topic": topic.topic, "order_in_unit": index})
                    .execute()
                )
                if getattr(ins_t, "error", None):
                    raise HTTPException(status_code=500, detail=f"Supabase error (insert topic): {ins_t.error}")

        # Delete topics that were removed from the unit
        removed = sorted(existing_id_set - incoming_id_set)
        _delete_topics_and_progress(removed)
    else:
        # Legacy client path: positional update to avoid wiping URL fields.
        existing_pos_ids = [r.get("id") for r in existing_rows if r.get("id")]
        keep_count = min(len(existing_pos_ids), len(incoming_topics))
        for index in range(keep_count):
            tid = existing_pos_ids[index]
            topic = incoming_topics[index]
            upd_t = (
                supabase.table("syllabus_topics")
                .update({"topic": topic.topic, "order_in_unit": index})
                .eq("id", str(tid))
                .eq("unit_id", str(unit_id))
                .execute()
            )
            if getattr(upd_t, "error", None):
                raise HTTPException(status_code=500, detail=f"Supabase error (update topic): {upd_t.error}")

        for index in range(keep_count, len(incoming_topics)):
            topic = incoming_topics[index]
            ins_t = (
                supabase.table("syllabus_topics")
                .insert({"unit_id": str(unit_id), "topic": topic.topic, "order_in_unit": index})
                .execute()
            )
            if getattr(ins_t, "error", None):
                raise HTTPException(status_code=500, detail=f"Supabase error (insert topic): {ins_t.error}")

        removed = existing_pos_ids[keep_count:]
        _delete_topics_and_progress([str(tid) for tid in removed if tid])

    return load_unit_with_topics(unit_id)


@academics_router.delete(
    "/api/syllabus/units/{unit_id}",
    summary="Delete a syllabus unit and all its topics",
)
def delete_unit_cascade(unit_id: uuid.UUID):
    """Delete a single unit and cascade-delete its topics and related user progress."""
    supabase = get_service_client()

    # Verify unit exists
    unit_q = (
        supabase.table("syllabus_units")
        .select("id")
        .eq("id", str(unit_id))
        .limit(1)
        .execute()
    )
    if getattr(unit_q, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (find unit): {unit_q.error}")
    if not unit_q.data:
        raise HTTPException(status_code=404, detail="Unit not found")

    # Gather topic ids under this unit
    topics_q = (
        supabase.table("syllabus_topics")
        .select("id")
        .eq("unit_id", str(unit_id))
        .execute()
    )
    if getattr(topics_q, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (list topics): {topics_q.error}")
    topic_ids = [t.get("id") for t in (topics_q.data or []) if t.get("id")]

    # Delete progress rows referencing these topics, in chunks
    if topic_ids:
        for i in range(0, len(topic_ids), 100):
            chunk = topic_ids[i:i+100]
            prog_del = (
                supabase.table("user_topic_progress")
                .delete()
                .in_("topic_id", chunk)
                .execute()
            )
            if getattr(prog_del, "error", None):
                raise HTTPException(status_code=500, detail=f"Supabase error (delete progress): {prog_del.error}")

        # Delete the topics
        t_del = (
            supabase.table("syllabus_topics")
            .delete()
            .eq("unit_id", str(unit_id))
            .execute()
        )
        if getattr(t_del, "error", None):
            raise HTTPException(status_code=500, detail=f"Supabase error (delete topics): {t_del.error}")

    # Finally delete the unit
    u_del = (
        supabase.table("syllabus_units")
        .delete()
        .eq("id", str(unit_id))
        .execute()
    )
    if getattr(u_del, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (delete unit): {u_del.error}")


@academics_router.put(
    "/api/syllabus/topics/{topic_id}/image",
    summary="Set or update image URL for a syllabus topic",
)
def set_topic_image_url(topic_id: uuid.UUID, payload: Dict[str, Any]):
    """Update the image_url for a single topic.

    Expects JSON body: {"image_url": "https://..."}.
    Pass null/empty string to clear via the dedicated DELETE endpoint instead.
    """

    supabase = get_service_client()

    if not isinstance(payload, dict):
        raise HTTPException(status_code=400, detail="Invalid payload")

    raw_url = payload.get("image_url")
    image_url: Optional[str]
    if raw_url is None:
        image_url = None
    else:
        if not isinstance(raw_url, str):
            raise HTTPException(status_code=400, detail="image_url must be a string or null")
        trimmed = raw_url.strip()
        image_url = trimmed or None

    # Ensure topic exists first
    topic_q = (
        supabase.table("syllabus_topics")
        .select("id")
        .eq("id", str(topic_id))
        .limit(1)
        .execute()
    )
    if getattr(topic_q, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (find topic): {topic_q.error}")
    if not topic_q.data:
        raise HTTPException(status_code=404, detail="Topic not found")

    upd = (
        supabase.table("syllabus_topics")
        .update({"image_url": image_url})
        .eq("id", str(topic_id))
        .execute()
    )
    if getattr(upd, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (update topic image): {upd.error}")

    return {"id": str(topic_id), "image_url": image_url}


@academics_router.delete(
    "/api/syllabus/topics/{topic_id}/image",
    summary="Clear image URL for a syllabus topic",
)
def clear_topic_image_url(topic_id: uuid.UUID):
    """Clear (set to null) the image_url for a single topic."""

    supabase = get_service_client()

    topic_q = (
        supabase.table("syllabus_topics")
        .select("id")
        .eq("id", str(topic_id))
        .limit(1)
        .execute()
    )
    if getattr(topic_q, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (find topic): {topic_q.error}")
    if not topic_q.data:
        raise HTTPException(status_code=404, detail="Topic not found")

    upd = (
        supabase.table("syllabus_topics")
        .update({"image_url": None})
        .eq("id", str(topic_id))
        .execute()
    )
    if getattr(upd, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (clear topic image): {upd.error}")

    return {"id": str(topic_id), "image_url": None}


def _update_topic_url_field(topic_id: uuid.UUID, field: str, value: Optional[str]) -> Dict[str, Any]:
    """Internal helper to validate topic existence and update a single URL field."""

    supabase = get_service_client()

    topic_q = (
        supabase.table("syllabus_topics")
        .select("id")
        .eq("id", str(topic_id))
        .limit(1)
        .execute()
    )
    if getattr(topic_q, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (find topic): {topic_q.error}")
    if not topic_q.data:
        raise HTTPException(status_code=404, detail="Topic not found")

    upd = (
        supabase.table("syllabus_topics")
        .update({field: value})
        .eq("id", str(topic_id))
        .execute()
    )
    if getattr(upd, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (update topic {field}): {upd.error}")

    return {"id": str(topic_id), field: value}


@academics_router.put(
    "/api/syllabus/topics/{topic_id}/video",
    summary="Set or update video URL for a syllabus topic",
)
def set_topic_video_url(topic_id: uuid.UUID, payload: Dict[str, Any]):
    """Update the video_url for a single topic.

    Expects JSON body: {"video_url": "https://..."}.
    Use the DELETE endpoint to clear.
    """

    if not isinstance(payload, dict):
        raise HTTPException(status_code=400, detail="Invalid payload")

    raw_url = payload.get("video_url")
    if raw_url is None:
        video_url = None
    else:
        if not isinstance(raw_url, str):
            raise HTTPException(status_code=400, detail="video_url must be a string or null")
        video_url = raw_url.strip() or None

    return _update_topic_url_field(topic_id, "video_url", video_url)


@academics_router.delete(
    "/api/syllabus/topics/{topic_id}/video",
    summary="Clear video URL for a syllabus topic",
)
def clear_topic_video_url(topic_id: uuid.UUID):
    """Clear (set to null) the video_url for a single topic."""

    return _update_topic_url_field(topic_id, "video_url", None)


@academics_router.put(
    "/api/syllabus/topics/{topic_id}/ppt",
    summary="Set or update PPT URL for a syllabus topic",
)
def set_topic_ppt_url(topic_id: uuid.UUID, payload: Dict[str, Any]):
    """Update the ppt_url for a single topic.

    Expects JSON body: {"ppt_url": "https://..."}.
    Use the DELETE endpoint to clear.
    """

    if not isinstance(payload, dict):
        raise HTTPException(status_code=400, detail="Invalid payload")

    raw_url = payload.get("ppt_url")
    if raw_url is None:
        ppt_url = None
    else:
        if not isinstance(raw_url, str):
            raise HTTPException(status_code=400, detail="ppt_url must be a string or null")
        ppt_url = raw_url.strip() or None

    return _update_topic_url_field(topic_id, "ppt_url", ppt_url)


@academics_router.delete(
    "/api/syllabus/topics/{topic_id}/ppt",
    summary="Clear PPT URL for a syllabus topic",
)
def clear_topic_ppt_url(topic_id: uuid.UUID):
    """Clear (set to null) the ppt_url for a single topic."""

    return _update_topic_url_field(topic_id, "ppt_url", None)


@academics_router.put(
    "/api/syllabus/topics/{topic_id}/lab",
    summary="Set or update Lab URL for a syllabus topic",
)
def set_topic_lab_url(topic_id: uuid.UUID, payload: Dict[str, Any]):
    """Update the lab_url for a single topic.

    Expects JSON body: {"lab_url": "https://..."}.
    Use the DELETE endpoint to clear.
    """

    if not isinstance(payload, dict):
        raise HTTPException(status_code=400, detail="Invalid payload")

    raw_url = payload.get("lab_url")
    if raw_url is None:
        lab_url = None
    else:
        if not isinstance(raw_url, str):
            raise HTTPException(status_code=400, detail="lab_url must be a string or null")
        lab_url = raw_url.strip() or None

    return _update_topic_url_field(topic_id, "lab_url", lab_url)


@academics_router.delete(
    "/api/syllabus/topics/{topic_id}/lab",
    summary="Clear Lab URL for a syllabus topic",
)
def clear_topic_lab_url(topic_id: uuid.UUID):
    """Clear (set to null) the lab_url for a single topic."""

    return _update_topic_url_field(topic_id, "lab_url", None)


# ---------- Topic Ratings ----------

class TopicRatingIn(BaseModel):
    rating: int = Field(..., ge=1, le=3, description="Rating value from 1 to 3 stars")


class TopicRatingOut(BaseModel):
    topic_id: uuid.UUID
    rating: int
    updated_at: Optional[datetime] = None


@academics_router.put(
    "/api/syllabus/topics/{topic_id}/rating",
    response_model=TopicRatingOut,
    summary="Set or update rating for a syllabus topic (1-3 stars)",
)
def set_topic_rating(
    topic_id: uuid.UUID,
    payload: TopicRatingIn,
    authorization: Optional[str] = Header(default=None),
):
    """Upsert a rating for a single topic by the current teacher.

    Rating must be between 1 and 3 (inclusive).
    Requires authentication via Bearer token.
    """
    token = _parse_bearer_token(authorization)
    if not token:
        raise HTTPException(status_code=401, detail="Authentication required")

    # Get user ID from token
    supabase = get_service_client()
    try:
        user_res = supabase.auth.get_user(token)
        user = getattr(user_res, "user", None)
        if not user or not getattr(user, "id", None):
            raise HTTPException(status_code=401, detail="Invalid or expired token")
        teacher_user_id = str(user.id)
    except Exception as e:
        raise HTTPException(status_code=401, detail="Invalid or expired token")

    # Verify topic exists
    topic_q = (
        supabase.table("syllabus_topics")
        .select("id")
        .eq("id", str(topic_id))
        .limit(1)
        .execute()
    )
    if getattr(topic_q, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (find topic): {topic_q.error}")
    if not topic_q.data:
        raise HTTPException(status_code=404, detail="Topic not found")

    # Upsert rating (insert or update based on unique constraint)
    now_iso = datetime.now(timezone.utc).isoformat()
    
    # Check if rating exists
    existing = (
        supabase.table("topic_ratings")
        .select("id")
        .eq("topic_id", str(topic_id))
        .eq("teacher_user_id", teacher_user_id)
        .limit(1)
        .execute()
    )
    
    if existing.data:
        # Update existing rating
        upd = (
            supabase.table("topic_ratings")
            .update({"rating": payload.rating, "updated_at": now_iso})
            .eq("topic_id", str(topic_id))
            .eq("teacher_user_id", teacher_user_id)
            .execute()
        )
        if getattr(upd, "error", None):
            raise HTTPException(status_code=500, detail=f"Supabase error (update rating): {upd.error}")
    else:
        # Insert new rating
        ins = (
            supabase.table("topic_ratings")
            .insert({
                "topic_id": str(topic_id),
                "teacher_user_id": teacher_user_id,
                "rating": payload.rating,
                "created_at": now_iso,
                "updated_at": now_iso,
            })
            .execute()
        )
        if getattr(ins, "error", None):
            raise HTTPException(status_code=500, detail=f"Supabase error (insert rating): {ins.error}")

    return TopicRatingOut(
        topic_id=topic_id,
        rating=payload.rating,
        updated_at=datetime.now(timezone.utc),
    )


@academics_router.get(
    "/api/syllabus/topics/{topic_id}/rating",
    response_model=Optional[TopicRatingOut],
    summary="Get current user's rating for a syllabus topic",
)
def get_topic_rating(
    topic_id: uuid.UUID,
    authorization: Optional[str] = Header(default=None),
):
    """Get the current teacher's rating for a single topic.

    Returns null if no rating exists.
    Requires authentication via Bearer token.
    """
    token = _parse_bearer_token(authorization)
    if not token:
        raise HTTPException(status_code=401, detail="Authentication required")

    # Get user ID from token
    supabase = get_service_client()
    try:
        user_res = supabase.auth.get_user(token)
        user = getattr(user_res, "user", None)
        if not user or not getattr(user, "id", None):
            raise HTTPException(status_code=401, detail="Invalid or expired token")
        teacher_user_id = str(user.id)
    except Exception as e:
        raise HTTPException(status_code=401, detail="Invalid or expired token")

    # Get rating
    rating_q = (
        supabase.table("topic_ratings")
        .select("topic_id,rating,updated_at")
        .eq("topic_id", str(topic_id))
        .eq("teacher_user_id", teacher_user_id)
        .limit(1)
        .execute()
    )
    if getattr(rating_q, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (get rating): {rating_q.error}")

    if not rating_q.data:
        return None

    row = rating_q.data[0]
    return TopicRatingOut(
        topic_id=uuid.UUID(row["topic_id"]),
        rating=int(row["rating"]),
        updated_at=row.get("updated_at"),
    )


@academics_router.get(
    "/api/syllabus/courses/{course_id}/ratings",
    summary="Get all topic ratings for a course by current user",
)
def get_course_topic_ratings(
    course_id: uuid.UUID,
    authorization: Optional[str] = Header(default=None),
):
    """Get all topic ratings for a course by the current teacher.

    Returns a dict mapping topic_id to rating value.
    Requires authentication via Bearer token.
    """
    token = _parse_bearer_token(authorization)
    if not token:
        return {"ratings": {}}

    # Get user ID from token
    supabase = get_service_client()
    try:
        user_res = supabase.auth.get_user(token)
        user = getattr(user_res, "user", None)
        if not user or not getattr(user, "id", None):
            return {"ratings": {}}
        teacher_user_id = str(user.id)
    except Exception:
        return {"ratings": {}}

    # Get all unit IDs for this course
    units_q = (
        supabase.table("syllabus_units")
        .select("id")
        .eq("course_id", str(course_id))
        .execute()
    )
    if getattr(units_q, "error", None) or not units_q.data:
        return {"ratings": {}}

    unit_ids = [u["id"] for u in units_q.data]

    # Get all topic IDs for these units
    topics_q = (
        supabase.table("syllabus_topics")
        .select("id")
        .in_("unit_id", unit_ids)
        .execute()
    )
    if getattr(topics_q, "error", None) or not topics_q.data:
        return {"ratings": {}}

    topic_ids = [t["id"] for t in topics_q.data]

    # Get all ratings for these topics by this user
    ratings_q = (
        supabase.table("topic_ratings")
        .select("topic_id,rating")
        .eq("teacher_user_id", teacher_user_id)
        .in_("topic_id", topic_ids)
        .execute()
    )
    if getattr(ratings_q, "error", None):
        return {"ratings": {}}

    ratings = {r["topic_id"]: r["rating"] for r in (ratings_q.data or [])}
    return {"ratings": ratings}


@academics_router.post(
    "/api/syllabus/topics/ratings/batch",
    summary="Get ratings for multiple topics (public, for student view)",
)
def get_topic_ratings_batch(
    topic_ids: List[uuid.UUID] = Body(..., embed=True),
):
    """Get ratings for a batch of topic IDs.

    Returns a dict mapping topic_id to rating value.
    Public endpoint - no authentication required.
    Returns the first/only rating for each topic (since one staff per topic typically).
    """
    if not topic_ids:
        return {"ratings": {}}

    supabase = get_service_client()
    
    # Convert UUIDs to strings for query
    topic_id_strs = [str(tid) for tid in topic_ids]
    
    # Get all ratings for these topics
    ratings_q = (
        supabase.table("topic_ratings")
        .select("topic_id,rating")
        .in_("topic_id", topic_id_strs)
        .execute()
    )
    if getattr(ratings_q, "error", None):
        return {"ratings": {}}

    # Return first rating found for each topic
    ratings = {}
    for r in (ratings_q.data or []):
        tid = r["topic_id"]
        if tid not in ratings:
            ratings[tid] = r["rating"]
    
    return {"ratings": ratings}


# ---------- Progress tracking ----------

class TopicToggleIn(BaseModel):
    topic_id: uuid.UUID
    completed: bool


@academics_router.get("/api/progress/topics", summary="Get completed topic ids for current user")
def get_completed_topics(authorization: Optional[str] = Header(default=None)):
    token = _parse_bearer_token(authorization)
    return get_completed_topic_ids(token)


@academics_router.post("/api/progress/toggle", summary="Mark/unmark a topic as completed for current user")
def toggle_topic(payload: TopicToggleIn, authorization: Optional[str] = Header(default=None)):
    token = _parse_bearer_token(authorization)
    return toggle_topic_completion(token, payload.topic_id, payload.completed)


@academics_router.get("/api/progress/summary", summary="Progress summary per course and unit for current user")
def progress_summary(authorization: Optional[str] = Header(default=None)):
    token = _parse_bearer_token(authorization)
    return get_progress_summary(token)


# ---------- Wishlist API ----------


class WishlistToggleIn(BaseModel):
    topic_id: uuid.UUID


@academics_router.get("/api/wishlist", summary="Get user's wishlisted topics")
def get_wishlist(authorization: Optional[str] = Header(default=None)):
    token = _parse_bearer_token(authorization)
    _, profile_id = _ensure_user_and_profile(token)
    supabase = get_service_client()
    try:
        q = supabase.table("user_topic_wishlist").select(
            "id,topic_id,created_at,syllabus_topics(id,topic,unit_id,syllabus_units(id,unit_title,course_id,syllabus_courses(id,course_code,title)))"
        ).eq("user_profile_id", profile_id).order("created_at", desc=True).execute()
    except Exception as exc:
        raise HTTPException(status_code=500, detail=f"Error fetching wishlist: {exc}")
    if getattr(q, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (wishlist): {q.error}")
    items = []
    for row in (q.data or []):
        topic_data = row.get("syllabus_topics") or {}
        unit_data = topic_data.get("syllabus_units") or {}
        course_data = unit_data.get("syllabus_courses") or {}
        items.append({
            "id": row.get("id"),
            "topic_id": row.get("topic_id"),
            "topic_name": topic_data.get("topic"),
            "unit_title": unit_data.get("unit_title"),
            "course_code": course_data.get("course_code"),
            "course_title": course_data.get("title"),
            "created_at": row.get("created_at"),
        })
    return {"wishlist": items}


@academics_router.post("/api/wishlist/toggle", summary="Add/remove topic from wishlist")
def toggle_wishlist(payload: WishlistToggleIn, authorization: Optional[str] = Header(default=None)):
    token = _parse_bearer_token(authorization)
    _, profile_id = _ensure_user_and_profile(token)
    supabase = get_service_client()
    topic_id_str = str(payload.topic_id)
    # Check if already in wishlist
    existing = supabase.table("user_topic_wishlist").select("id").eq(
        "user_profile_id", profile_id
    ).eq("topic_id", topic_id_str).limit(1).execute()
    if existing.data:
        # Remove from wishlist
        supabase.table("user_topic_wishlist").delete().eq("id", existing.data[0]["id"]).execute()
        return {"wishlisted": False, "message": "Removed from wishlist"}
    else:
        # Add to wishlist
        try:
            supabase.table("user_topic_wishlist").insert({
                "user_profile_id": profile_id,
                "topic_id": topic_id_str
            }).execute()
            return {"wishlisted": True, "message": "Added to wishlist"}
        except Exception as exc:
            msg = str(exc).lower()
            if "duplicate" in msg or "unique" in msg:
                return {"wishlisted": True, "message": "Already in wishlist"}
            raise HTTPException(status_code=500, detail=f"Error adding to wishlist: {exc}")


@academics_router.delete("/api/wishlist/{topic_id}", summary="Remove topic from wishlist")
def remove_from_wishlist(topic_id: uuid.UUID, authorization: Optional[str] = Header(default=None)):
    token = _parse_bearer_token(authorization)
    _, profile_id = _ensure_user_and_profile(token)
    supabase = get_service_client()
    supabase.table("user_topic_wishlist").delete().eq(
        "user_profile_id", profile_id
    ).eq("topic_id", str(topic_id)).execute()
    return {"message": "Removed from wishlist"}


@academics_router.get("/api/wishlist/check/{topic_id}", summary="Check if topic is wishlisted")
def check_wishlist(topic_id: uuid.UUID, authorization: Optional[str] = Header(default=None)):
    token = _parse_bearer_token(authorization)
    _, profile_id = _ensure_user_and_profile(token)
    supabase = get_service_client()
    existing = supabase.table("user_topic_wishlist").select("id").eq(
        "user_profile_id", profile_id
    ).eq("topic_id", str(topic_id)).limit(1).execute()
    return {"wishlisted": bool(existing.data)}


# ---------- History API ----------


class HistoryRecordIn(BaseModel):
    topic_id: uuid.UUID
    topic_name: str


@academics_router.get("/api/history", summary="Get user's recently viewed topics")
def get_history(
    limit: int = Query(default=50, le=200),
    authorization: Optional[str] = Header(default=None)
):
    token = _parse_bearer_token(authorization)
    _, profile_id = _ensure_user_and_profile(token)
    supabase = get_service_client()
    try:
        q = supabase.table("user_topic_history").select(
            "id,topic_id,topic_name,viewed_at"
        ).eq("user_profile_id", profile_id).order("viewed_at", desc=True).limit(limit).execute()
    except Exception as exc:
        raise HTTPException(status_code=500, detail=f"Error fetching history: {exc}")
    if getattr(q, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (history): {q.error}")
    return {"history": q.data or []}


@academics_router.post("/api/history/record", summary="Record a topic view")
def record_history(payload: HistoryRecordIn, authorization: Optional[str] = Header(default=None)):
    token = _parse_bearer_token(authorization)
    _, profile_id = _ensure_user_and_profile(token)
    supabase = get_service_client()
    topic_id_str = str(payload.topic_id)
    topic_name = (payload.topic_name or "").strip()[:500]  # Limit topic name length
    try:
        supabase.table("user_topic_history").insert({
            "user_profile_id": profile_id,
            "topic_id": topic_id_str,
            "topic_name": topic_name
        }).execute()
        return {"recorded": True}
    except Exception as exc:
        # Log but don't fail - history is non-critical
        supabase_logger.warning("Failed to record history: %s", exc)
        return {"recorded": False}


@academics_router.delete("/api/history/clear", summary="Clear user's history")
def clear_history(authorization: Optional[str] = Header(default=None)):
    token = _parse_bearer_token(authorization)
    _, profile_id = _ensure_user_and_profile(token)
    supabase = get_service_client()
    supabase.table("user_topic_history").delete().eq("user_profile_id", profile_id).execute()
    return {"message": "History cleared"}


# ---------- Profile update & uploads ----------


def _strip_or_none(value: Any) -> Optional[str]:
    if value is None:
        return None
    if isinstance(value, str):
        trimmed = value.strip()
        return trimmed or None
    return str(value)


def _date_or_none(value: Any) -> Optional[str]:
    if value is None or value == "":
        return None
    if isinstance(value, datetime):
        return value.date().isoformat()
    if isinstance(value, date):
        return value.isoformat()
    if isinstance(value, str):
        try:
            return date.fromisoformat(value.strip()).isoformat()
        except ValueError:
            return None
    return None


class ProfileMediaItemIn(BaseModel):
    kind: Optional[str] = None
    url: Optional[str] = None
    title: Optional[str] = None

    @validator("kind", "url", "title", pre=True)
    def _normalize(cls, v: Any):  # noqa: N805
        return _strip_or_none(v)


class ExperienceIn(BaseModel):
    id: Optional[str] = None
    title: str
    employment_type: Optional[str] = None
    company: Optional[str] = None
    company_logo_url: Optional[str] = None
    location: Optional[str] = None
    location_type: Optional[str] = None
    start_date: date
    end_date: Optional[date] = None
    is_current: Optional[bool] = False
    description: Optional[str] = None
    media: Optional[List[ProfileMediaItemIn]] = None

    @validator("title", "employment_type", "company", "company_logo_url", "location", "location_type", "description", pre=True)
    def _trim_str(cls, v: Any):  # noqa: N805
        return _strip_or_none(v)


class EducationIn(BaseModel):
    """Revised education model: replaces field_of_study + start/end dates with department, batch_range, regno, current_semester."""

    id: Optional[str] = None
    school: str
    degree: Optional[str] = None
    department: Optional[str] = None
    batch_range: Optional[str] = None  # e.g. "2022-2026"
    section: Optional[str] = None
    regno: Optional[str] = None
    current_semester: Optional[int] = None
    grade: Optional[str] = None
    activities: Optional[str] = None
    description: Optional[str] = None

    @validator(
        "school",
        "degree",
        "department",
        "batch_range",
        "section",
        "regno",
        "grade",
        "activities",
        "description",
        pre=True,
    )
    def _trim(cls, v: Any):  # noqa: N805
        return _strip_or_none(v)


class CertificationIn(BaseModel):
    id: Optional[str] = None
    name: str
    issuing_org: Optional[str] = None
    issue_date: Optional[date] = None
    expiration_date: Optional[date] = None
    does_not_expire: Optional[bool] = False
    credential_id: Optional[str] = None
    credential_url: Optional[str] = None
    description: Optional[str] = None

    @validator("name", "issuing_org", "credential_id", "credential_url", "description", pre=True)
    def _trim(cls, v: Any):  # noqa: N805
        return _strip_or_none(v)


class PortfolioProjectIn(BaseModel):
    id: Optional[str] = None
    name: str
    associated_experience_id: Optional[str] = None
    associated_education_id: Optional[str] = None
    start_date: Optional[date] = None
    end_date: Optional[date] = None
    url: Optional[str] = None
    description: Optional[str] = None
    tech_stack: Optional[List[str]] = None
    team: Optional[List[Dict[str, Any]]] = None

    @validator("name", "associated_experience_id", "associated_education_id", "url", "description", pre=True)
    def _trim(cls, v: Any):  # noqa: N805
        return _strip_or_none(v)


class PublicationIn(BaseModel):
    id: Optional[str] = None
    title: str
    publisher: Optional[str] = None
    publication_date: Optional[date] = None
    authors: Optional[List[str]] = None
    url: Optional[str] = None
    abstract: Optional[str] = None

    @validator("title", "publisher", "url", "abstract", pre=True)
    def _trim(cls, v: Any):  # noqa: N805
        return _strip_or_none(v)

    @validator("authors", pre=True)
    def _normalize_authors(cls, v: Any):  # noqa: N805
        if v is None:
            return None
        if isinstance(v, str):
            return [part.strip() for part in v.split(",") if part.strip()]
        if isinstance(v, list):
            cleaned = []
            for item in v:
                cleaned_item = _strip_or_none(item)
                if cleaned_item:
                    cleaned.append(cleaned_item)
            return cleaned
        return None


class ProfileUpdateIn(BaseModel):
    name: Optional[str] = None
    phone: Optional[str] = None
    bio: Optional[str] = None
    linkedin: Optional[str] = None
    github: Optional[str] = None
    leetcode: Optional[str] = None
    specializations: Optional[list[str]] = None
    projects: Optional[list[dict]] = None
    experiences: Optional[List[ExperienceIn]] = None
    education_entries: Optional[List[EducationIn]] = None
    certification_entries: Optional[List[CertificationIn]] = None
    portfolio_projects: Optional[List[PortfolioProjectIn]] = None
    publication_entries: Optional[List[PublicationIn]] = None
    # Extended profile fields (UI sends these too)
    headline: Optional[str] = None
    location: Optional[str] = None
    dob: Optional[str] = None  # YYYY-MM-DD
    portfolio_url: Optional[str] = None
    website: Optional[str] = None
    twitter: Optional[str] = None
    instagram: Optional[str] = None
    medium: Optional[str] = None
    technologies: Optional[str] = None
    skills: Optional[str] = None
    certifications: Optional[str] = None
    languages: Optional[str] = None
    interests: Optional[str] = None
    achievements: Optional[str] = None
    experience: Optional[str] = None
    publications: Optional[str] = None
    project_info: Optional[str] = None
    # Academic/identity fields
    semester: Optional[int] = None
    regno: Optional[str] = None
    college_name: Optional[str] = None
    department_name: Optional[str] = None
    batch_from: Optional[int] = None
    batch_to: Optional[int] = None


@academics_router.get("/api/profile/me", summary="Get current user's extended profile")
def get_profile_me(authorization: Optional[str] = Header(default=None)):
    token = _parse_bearer_token(authorization)
    return _get_profile_me(token)


@academics_router.put("/api/profile/me", summary="Update current user's profile fields")
def update_profile_me(payload: ProfileUpdateIn, authorization: Optional[str] = Header(default=None)):
    token = _parse_bearer_token(authorization)
    return _update_profile_me(token, payload.dict(exclude_unset=True))


@academics_router.post("/api/profile/upload", summary="Upload profile image or resume and save URL")
def upload_profile_asset(
    kind: str = Form(..., pattern=r"^(image|resume)$"),  # use pattern instead of regex
    file: UploadFile = File(...),
    authorization: Optional[str] = Header(default=None),
):
    token = _parse_bearer_token(authorization)
    return _upload_profile_asset(token, kind, file)

# --- Notes API ---




notes_router = APIRouter()

# ---- Degree-specific Allowed Domains API ----

class DegreeDomainsIn(BaseModel):
    degree_label: str = Field(..., min_length=1, max_length=128)
    domains: List[str] = Field(default_factory=list, description="List of hostnames like 'geeksforgeeks.org'")


class DegreeDomainsOut(BaseModel):
    degree_key: str
    degree_label: str
    domains: List[str]


def db_replace_allowed_domains_for_degree(degree_label: str, domains: List[str]) -> DegreeDomainsOut:
    key = _normalize_degree_key(degree_label)
    if not key:
        raise HTTPException(status_code=400, detail="Invalid degree label")
    # normalize domains (host only, lowercase)
    cleaned: List[str] = []
    for d in domains:
        d = (d or "").strip().lower()
        if not d:
            continue
        # If a URL was pasted, extract host
        try:
            if "://" in d:
                d = urlparse(d).netloc or d
        except Exception:
            pass
        d = d.strip()
        d = d.lstrip("*")  # don't allow wildcards
        if d and d not in cleaned:
            cleaned.append(d)

    supabase = get_service_client()
    now = datetime.utcnow().isoformat()
    # Upsert rows
    rows = [{
        "degree_key": key,
        "degree_label": degree_label,
        "domain": dom,
        "enabled": True,
        "updated_at": now,
    } for dom in cleaned]
    try:
        if rows:
            supabase.table(DEGREE_ALLOWED_DOMAINS_TABLE).upsert(rows, on_conflict="degree_key,domain").execute()
        # Remove extras not in the new list
        if cleaned:
            supabase.table(DEGREE_ALLOWED_DOMAINS_TABLE).delete().eq("degree_key", key).notin_("domain", cleaned).execute()
        else:
            # If empty list provided, delete all for degree
            supabase.table(DEGREE_ALLOWED_DOMAINS_TABLE).delete().eq("degree_key", key).execute()
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"DB upsert failed: {e}")
    return DegreeDomainsOut(degree_key=key, degree_label=degree_label, domains=cleaned)


def db_delete_degree_domain(degree: str, domain: str) -> dict:
    key = _normalize_degree_key(degree)
    if not key or not domain:
        raise HTTPException(status_code=400, detail="Invalid parameters")
    supabase = get_service_client()
    try:
        supabase.table(DEGREE_ALLOWED_DOMAINS_TABLE).delete().eq("degree_key", key).eq("domain", domain.lower()).execute()
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Delete failed: {e}")
    return {"ok": True}


def db_list_degree_configs() -> List[DegreeDomainsOut]:
    supabase = get_service_client()
    try:
        res = supabase.table(DEGREE_ALLOWED_DOMAINS_TABLE).select("degree_key,degree_label,domain,enabled").eq("enabled", True).execute()
        rows = getattr(res, 'data', []) or []
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"DB fetch failed: {e}")
    agg: Dict[str, Dict[str, Any]] = {}
    for r in rows:
        key = (r.get("degree_key") or "").strip()
        lbl = (r.get("degree_label") or "").strip() or key
        dom = (r.get("domain") or "").strip().lower()
        if not key or not dom:
            continue
        if key not in agg:
            agg[key] = {"degree_key": key, "degree_label": lbl, "domains": []}
        if dom not in agg[key]["domains"]:
            agg[key]["domains"].append(dom)
    out: List[DegreeDomainsOut] = []
    for v in agg.values():
        v["domains"].sort()
        out.append(DegreeDomainsOut(**v))
    # sort by label
    out.sort(key=lambda x: x.degree_label.lower())
    return out


@notes_router.get("/api/notes/allowed-domains", summary="Get allowed domains for a degree", response_model=DegreeDomainsOut)
def api_get_allowed_domains(degree: str = Query(..., min_length=1)):
    key = _normalize_degree_key(degree)
    domains = db_get_allowed_domains_for_degree(degree)
    return DegreeDomainsOut(degree_key=key or "", degree_label=degree, domains=domains or [])


@notes_router.post("/api/notes/allowed-domains", summary="Replace allowed domains for a degree", response_model=DegreeDomainsOut)
def api_set_allowed_domains(payload: DegreeDomainsIn):
    return db_replace_allowed_domains_for_degree(payload.degree_label, payload.domains or [])


@notes_router.delete("/api/notes/allowed-domains", summary="Delete a single domain from a degree")
def api_delete_domain(degree: str = Query(..., min_length=1), domain: str = Query(..., min_length=1)):
    return db_delete_degree_domain(degree, domain)


@notes_router.get("/api/notes/degrees", summary="List degrees with configured domains", response_model=List[DegreeDomainsOut])
def api_list_degrees():
    return db_list_degree_configs()


# ---- Notes Feedback (Notes Generator) ----

NOTES_FEEDBACK_TABLE = os.getenv("NOTES_FEEDBACK_TABLE", "notes_feedback")
VALID_NOTES_FEEDBACK_STATUS = {"new", "reviewing", "resolved", "ignored"}


def _require_feedback_staff(authorization: Optional[str]) -> Tuple[str, Optional[str]]:
    """Allow admin OR employee to access feedback inbox endpoints."""
    uid, email = _get_auth_user(authorization)
    if _is_admin_user(uid, email):
        return str(uid), email
    # Employee role in admin_roles table
    try:
        supabase = get_service_client()
        r = supabase.table("admin_roles").select("role").eq("auth_user_id", uid).limit(1).execute()
        row = (getattr(r, 'data', []) or [{}])[0]
        if (row.get("role") or "").strip().lower() == "employee":
            return str(uid), email
    except Exception:
        pass
    raise HTTPException(status_code=403, detail="Not authorized")


class NotesFeedbackIn(BaseModel):
    category: str = Field(..., min_length=1, max_length=80)
    message: str = Field(..., min_length=1, max_length=2000)

    quick_tags: List[str] = Field(default_factory=list)
    rating: Optional[int] = Field(default=None, ge=1, le=5)

    note_id: Optional[str] = Field(default=None, description="ai_notes UUID (if available)")
    note_variant: Optional[str] = Field(default=None, description="detailed|cheatsheet|simple")
    note_title: Optional[str] = Field(default=None, max_length=256)
    topic: Optional[str] = Field(default=None, max_length=256)

    page_path: Optional[str] = Field(default=None, max_length=256)
    page_url: Optional[str] = Field(default=None, max_length=1200)
    selected_text: Optional[str] = Field(default=None, max_length=1200)

    meta: Dict[str, Any] = Field(default_factory=dict)

    @validator("quick_tags", pre=True)
    def _clean_quick_tags(cls, v: Any):  # noqa: N805
        if v is None:
            return []
        if isinstance(v, str):
            v = [p.strip() for p in v.split(",")]
        if not isinstance(v, (list, tuple)):
            return []
        cleaned: List[str] = []
        for raw in v:
            s = str(raw or "").strip()
            if not s:
                continue
            if len(s) > 40:
                s = s[:40]
            if s not in cleaned:
                cleaned.append(s)
            if len(cleaned) >= 12:
                break
        return cleaned


class NotesFeedbackAdminUpdateIn(BaseModel):
    status: Optional[str] = Field(default=None)
    admin_notes: Optional[str] = Field(default=None, max_length=2000)

    @validator("status")
    def _validate_status(cls, v: Optional[str]):  # noqa: N805
        if v is None:
            return None
        s = str(v).strip().lower()
        if s not in VALID_NOTES_FEEDBACK_STATUS:
            raise ValueError("Invalid status")
        return s


@notes_router.post("/api/notes/feedback", summary="Submit feedback about generated notes")
def submit_notes_feedback(payload: NotesFeedbackIn, request: Request, authorization: Optional[str] = Header(default=None)):
    user_id, email = _get_auth_user(authorization)
    supabase = get_service_client()

    note_uuid: Optional[str] = None
    raw_note_id = (payload.note_id or "").strip()
    if raw_note_id:
        try:
            note_uuid = str(uuid.UUID(raw_note_id))
        except Exception:
            note_uuid = None

    now = _now_iso()
    ip = None
    try:
        if request.client:
            ip = request.client.host
    except Exception:
        ip = None

    user_agent = (request.headers.get("user-agent") or "")[:400]

    row: Dict[str, Any] = {
        "created_at": now,
        "updated_at": now,
        "status": "new",
        "user_id": str(user_id),
        "user_email": (email or "")[:320] if email else None,

        "note_id": note_uuid,
        "note_variant": (payload.note_variant or "")[:32] if payload.note_variant else None,
        "note_title": (payload.note_title or "")[:256] if payload.note_title else None,
        "topic": (payload.topic or "")[:256] if payload.topic else None,

        "page_path": (payload.page_path or "")[:256] if payload.page_path else None,
        "page_url": (payload.page_url or "")[:1200] if payload.page_url else None,

        "category": (payload.category or "")[:80],
        "quick_tags": payload.quick_tags or [],
        "rating": payload.rating,
        "message": payload.message,
        "selected_text": payload.selected_text,

        "ip": ip,
        "user_agent": user_agent,
        "meta": payload.meta or {},
    }
    if raw_note_id and not note_uuid:
        # Preserve raw ID if UI sent a non-UUID identifier
        try:
            row["meta"] = {**(row.get("meta") or {}), "note_id_raw": raw_note_id}
        except Exception:
            pass

    try:
        res = supabase.table(NOTES_FEEDBACK_TABLE).insert(_supabase_payload(row)).execute()
        if getattr(res, "error", None):
            raise Exception(res.error)
        inserted = (getattr(res, "data", []) or [{}])[0]
        return {"ok": True, "feedback": inserted}
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to store feedback: {e}")


@notes_router.get("/api/admin/notes-feedback", summary="Admin: list notes feedback")
def admin_list_notes_feedback(
    status: Optional[str] = Query(default=None),
    q: Optional[str] = Query(default=None),
    limit: int = Query(default=1000, ge=1, le=5000),
    authorization: Optional[str] = Header(default=None),
):
    _require_feedback_staff(authorization)
    supabase = get_service_client()
    cols = "id,created_at,updated_at,status,user_id,user_email,note_id,note_variant,note_title,topic,page_path,page_url,category,quick_tags,rating,message,selected_text,ip,user_agent,meta,admin_notes,resolved_at"
    try:
        builder = supabase.table(NOTES_FEEDBACK_TABLE).select(cols).order("created_at", desc=True).limit(limit)
        if status:
            builder = builder.eq("status", str(status).strip().lower())
        res = builder.execute()
        data = (getattr(res, "data", []) or [])
        if q:
            qq = (q or "").strip().lower()
            if qq:
                def _match(r: Dict[str, Any]) -> bool:
                    blob = " ".join(
                        str(r.get(k, "") or "")
                        for k in ("user_email", "user_id", "topic", "note_title", "note_variant", "category", "message", "page_path")
                    ).lower()
                    return qq in blob
                data = [r for r in data if isinstance(r, dict) and _match(r)]
        return {"feedback": data}
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to list feedback: {e}")


@notes_router.get("/api/admin/notes-feedback/{feedback_id}", summary="Admin: get a single feedback item")
def admin_get_notes_feedback(feedback_id: str, authorization: Optional[str] = Header(default=None)):
    _require_feedback_staff(authorization)
    supabase = get_service_client()
    try:
        res = supabase.table(NOTES_FEEDBACK_TABLE).select("*").eq("id", feedback_id).limit(1).execute()
        row = (getattr(res, "data", []) or [{}])[0]
        if not row:
            raise HTTPException(status_code=404, detail="Feedback not found")
        return row
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to fetch feedback: {e}")


@notes_router.patch("/api/admin/notes-feedback/{feedback_id}", summary="Admin: update feedback status/notes")
def admin_update_notes_feedback(
    feedback_id: str,
    payload: NotesFeedbackAdminUpdateIn,
    authorization: Optional[str] = Header(default=None),
):
    _require_feedback_staff(authorization)
    supabase = get_service_client()
    updates: Dict[str, Any] = {"updated_at": _now_iso()}
    if payload.status is not None:
        updates["status"] = payload.status
        if payload.status == "resolved":
            updates["resolved_at"] = _now_iso()
    if payload.admin_notes is not None:
        updates["admin_notes"] = payload.admin_notes
    if len(updates) == 1:
        return admin_get_notes_feedback(feedback_id, authorization)
    try:
        upd = supabase.table(NOTES_FEEDBACK_TABLE).update(_supabase_payload(updates)).eq("id", feedback_id).execute()
        if getattr(upd, "error", None):
            raise Exception(upd.error)
        return admin_get_notes_feedback(feedback_id, authorization)
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to update feedback: {e}")

# --- Print API ---
print_router = APIRouter()

# --------- Print/Orders domain models ---------

class ShopCapabilities(BaseModel):
    color: bool = True
    duplex: bool = True
    sizes: List[str] = Field(default_factory=lambda: ["A4"])  # e.g., ["A4","A3","Letter"]
    bindings: List[str] = Field(default_factory=lambda: ["none", "staple", "spiral"])
    gsm: List[int] = Field(default_factory=lambda: [70, 80, 100])


class PrintShopIn(BaseModel):
    name: str
    phone: Optional[str] = None
    email: Optional[str] = None
    address: Optional[str] = None
    lat: Optional[float] = None
    lng: Optional[float] = None
    hours: Optional[dict] = None
    capabilities: Optional[ShopCapabilities] = None


class PriceTier(BaseModel):
    min: Optional[int] = Field(default=None, ge=0)
    upto: Optional[int] = Field(default=None, ge=1)
    per_page: float = Field(description="Price per page for this tier")


class ShopPricing(BaseModel):
    bw_single: List[PriceTier] = Field(default_factory=list)
    bw_duplex: List[PriceTier] = Field(default_factory=list)
    color_single: List[PriceTier] = Field(default_factory=list)
    color_duplex: List[PriceTier] = Field(default_factory=list)


class UpdateShopIn(BaseModel):
    name: Optional[str] = None
    phone: Optional[str] = None
    email: Optional[str] = None
    address: Optional[str] = None
    lat: Optional[float] = None
    lng: Optional[float] = None
    hours: Optional[dict] = None
    capabilities: Optional[ShopCapabilities] = None
    is_open: Optional[bool] = None
    paused: Optional[bool] = None
    price_hint: Optional[str] = None
    pricing: Optional[ShopPricing] = None


@print_router.post("/api/shop/logo", summary="Upload or replace shop logo (public URL stored in print_shops.logo_url)")
async def upload_shop_logo(file: UploadFile = File(...), authorization: Optional[str] = Header(default=None)):
    uid, _ = _get_auth_user(authorization)
    supabase = get_service_client()
    # Resolve shop owned by user
    shop_q = supabase.table(PRINT_SHOPS_TABLE).select("id").eq("owner_user_id", uid).limit(1).execute()
    shop = (getattr(shop_q, 'data', []) or [{}])[0]
    if not shop:
        raise HTTPException(status_code=404, detail="No shop found for user")
    shop_id = shop.get("id")

    # Validate file
    filename = file.filename or "logo.png"
    ext = (Path(filename).suffix or ".png").lower()
    allowed = {".jpg", ".jpeg", ".png", ".webp", ".gif", ".svg"}
    if ext not in allowed:
        raise HTTPException(status_code=400, detail="Unsupported image type")
    blob = await file.read()
    if len(blob) > 4 * 1024 * 1024:
        raise HTTPException(status_code=400, detail="Image too large (max 4MB)")
    mime_map = {".jpg": "image/jpeg", ".jpeg": "image/jpeg", ".png": "image/png", ".webp": "image/webp", ".gif": "image/gif", ".svg": "image/svg+xml"}
    content_type = mime_map.get(ext, "application/octet-stream")

    # Resolve bucket
    bucket = (
        os.getenv("SUPABASE_SHOP_ASSETS_BUCKET", "").strip()
        or os.getenv("SUPABASE_ASSETS_BUCKET", "").strip()
        or os.getenv("SUPABASE_BUCKET", "").strip()
        or os.getenv("SUPABASE_PUBLIC_BUCKET", "").strip()
        or "paperx-assets"
    )
    # Key path (use upsert to replace same path for caching simplicity)
    dest = f"shops/{shop_id}/logo{ext}"
    try:
        _storage_upload_bytes(supabase, bucket, dest, blob, content_type)
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Upload failed: {e}")
    public_url = _storage_public_url(supabase, bucket, dest)
    if not public_url:
        raise HTTPException(status_code=500, detail="Failed to resolve public URL")
    # Persist to shop row (logo_url text column expected)
    try:
        upd = supabase.table(PRINT_SHOPS_TABLE).update({"logo_url": public_url, "updated_at": _now_iso()}).eq("id", shop_id).execute()
        if getattr(upd, 'error', None):
            # If column missing, expose clear message
            msg = str(upd.error)
            raise HTTPException(status_code=500, detail=f"DB update failed (did you add logo_url column?): {msg}")
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"DB update error: {e}")
    return {"ok": True, "url": public_url, "bucket": bucket, "path": dest}


class PrintSettings(BaseModel):
    copies: int = Field(default=1, ge=1, le=50)
    color_mode: str = Field(default="auto", description="auto|bw|color")
    duplex: str = Field(default="off", description="off|long|short")
    n_up: int = Field(default=1, description="1|2|4|6|9")
    paper_size: str = Field(default="A4")
    paper_gsm: Optional[int] = Field(default=None)
    finishing: str = Field(default="none")
    page_range: str = Field(default="all")
    scale: str = Field(default="fit")
    collate: bool = Field(default=True)
    orientation: Literal["vertical", "horizontal"] = Field(default="vertical")
    notes_to_shop: Optional[str] = None


class CreateJobIn(BaseModel):
    shop_id: str
    settings: PrintSettings
    marketplace_note_id: Optional[str] = None
    estimated_pages: Optional[int] = None
    estimated_price: Optional[float] = None
    file_size: Optional[int] = None
    contact_name: Optional[str] = None
    contact_phone: Optional[str] = None
    pickup_window: Optional[str] = None


def _random_otp() -> str:
    return f"{random.randint(0, 999999):06d}"


def _now_iso() -> str:
    return datetime.utcnow().isoformat()


# --------- Print/Orders endpoints ---------

@print_router.get("/api/print/shops", summary="List print shops with simple filters")
def list_print_shops(
    q: Optional[str] = Query(default=None),
    open_now: Optional[bool] = Query(default=None),
    color: Optional[bool] = Query(default=None),
    size: Optional[str] = Query(default=None, description="A4|A3|Letter"),
    binding: Optional[str] = Query(default=None),
    sort: Optional[str] = Query(default="nearest"),
    limit: int = Query(default=50, ge=1, le=200),
):
    supabase = get_service_client()
    query = supabase.table(PRINT_SHOPS_TABLE).select("*")
    try:
        res = query.execute()
        data = getattr(res, 'data', []) or []
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to fetch shops: {e}")

    def match_filters(shop: dict) -> bool:
        caps = (shop.get("capabilities") or {})
        if isinstance(caps, str):
            try:
                caps = json.loads(caps)
            except Exception:
                caps = {}
        if q:
            s = (shop.get("name") or '') + ' ' + (shop.get("address") or '')
            if q.lower() not in s.lower():
                return False
        if open_now is True and not shop.get("is_open", False):
            return False
        if color is True and not caps.get("color", False):
            return False
        if size and size not in (caps.get("sizes") or []):
            return False
        if binding and binding not in (caps.get("bindings") or []):
            return False
        return True

    filtered = [s for s in data if match_filters(s)]
    # very rough sort: rating desc for "fastest" else by name
    if sort == "fastest":
        filtered.sort(key=lambda s: (-(s.get("rating") or 0), s.get("name") or ""))
    else:
        filtered.sort(key=lambda s: s.get("name") or "")
    return {"shops": filtered[:limit]}


@print_router.post("/api/print/jobs", summary="Create a print job (no payment)")
def create_print_job(payload: CreateJobIn, authorization: Optional[str] = Header(default=None)):
    try:
        user_id, _ = _get_auth_user(authorization)
    except HTTPException:
        # allow unauth for demo; attribute to null user
        user_id = None
    supabase = get_service_client()
    job_id = str(uuid.uuid4())
    otp = _random_otp()
    now = _now_iso()
    row = {
        "id": job_id,
        "user_id": user_id,
        "shop_id": payload.shop_id,
        "status": "submitted",
        "otp": otp,
        "settings": _supabase_payload(payload.settings.dict()),
        "estimated_pages": payload.estimated_pages,
        "estimated_price": payload.estimated_price,
        "file_size": payload.file_size,
        "marketplace_note_id": payload.marketplace_note_id,
        "pickup_window": payload.pickup_window,
        "contact_name": payload.contact_name,
        "contact_phone": payload.contact_phone,
        "created_at": now,
        "updated_at": now,
    }
    try:
        ins = supabase.table(PRINT_JOBS_TABLE).insert(row).execute()
        if getattr(ins, 'error', None):
            raise Exception(ins.error)
        supabase.table(PRINT_JOB_EVENTS_TABLE).insert({
            "job_id": job_id, "status": "submitted", "note": "Job submitted",
            "created_at": now
        }).execute()
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to create job: {e}")
    return {"ok": True, "job_id": job_id, "otp": otp}


@print_router.get("/api/orders", summary="List my print jobs")
def list_my_orders(status: Optional[str] = Query(default=None), authorization: Optional[str] = Header(default=None)):
    try:
        user_id, _ = _get_auth_user(authorization)
    except HTTPException:
        user_id = None
    supabase = get_service_client()
    try:
        q = supabase.table(PRINT_JOBS_TABLE).select("*")
        if user_id:
            q = q.eq("user_id", user_id)
        if status:
            q = q.eq("status", status)
        res = q.order("created_at", desc=True).execute()
        data = getattr(res, 'data', []) or []
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to list orders: {e}")
    return {"jobs": data}


@print_router.get("/api/orders/{job_id}", summary="Get job details")
def get_order(job_id: str, authorization: Optional[str] = Header(default=None)):
    supabase = get_service_client()
    try:
        job = supabase.table(PRINT_JOBS_TABLE).select("*").eq("id", job_id).limit(1).execute()
        job_row = (getattr(job, 'data', []) or [{}])[0]
        ev = supabase.table(PRINT_JOB_EVENTS_TABLE).select("*").eq("job_id", job_id).order("created_at").execute()
        events = getattr(ev, 'data', []) or []
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed: {e}")
    if not job_row:
        raise HTTPException(status_code=404, detail="Not found")
    return {"job": job_row, "events": events}


@print_router.post("/api/orders/{job_id}/cancel", summary="Cancel a job if not accepted")
def cancel_order(job_id: str, authorization: Optional[str] = Header(default=None)):
    user_id, _ = _get_auth_user(authorization)
    supabase = get_service_client()
    try:
        job = supabase.table(PRINT_JOBS_TABLE).select("id,user_id,status").eq("id", job_id).limit(1).execute()
        row = (getattr(job, 'data', []) or [{}])[0]
    except Exception:
        row = {}
    if not row:
        raise HTTPException(status_code=404, detail="Not found")
    if row.get("user_id") and user_id and row.get("user_id") != user_id:
        raise HTTPException(status_code=403, detail="Forbidden")
    if row.get("status") not in {"submitted"}:
        raise HTTPException(status_code=400, detail="Cannot cancel now")
    now = _now_iso()
    supabase.table(PRINT_JOBS_TABLE).update({"status": "cancelled", "updated_at": now}).eq("id", job_id).execute()
    supabase.table(PRINT_JOB_EVENTS_TABLE).insert({"job_id": job_id, "status": "cancelled", "note": "Cancelled by student", "created_at": now}).execute()
    return {"ok": True}


@print_router.post("/api/orders/{job_id}/resend-otp", summary="Regenerate OTP")
def resend_otp(job_id: str, authorization: Optional[str] = Header(default=None)):
    user_id, _ = _get_auth_user(authorization)
    supabase = get_service_client()
    otp = _random_otp()
    now = _now_iso()
    supabase.table(PRINT_JOBS_TABLE).update({"otp": otp, "updated_at": now}).eq("id", job_id).execute()
    supabase.table(PRINT_JOB_EVENTS_TABLE).insert({"job_id": job_id, "status": "otp", "note": "OTP regenerated", "created_at": now}).execute()
    return {"ok": True, "otp": otp}


# ---- Shop owner endpoints (require token matching owner_user_id) ----

def _require_shop_owner(shop_id: str, authorization: Optional[str]) -> str:
    uid, _ = _get_auth_user(authorization)
    supabase = get_service_client()
    res = supabase.table(PRINT_SHOPS_TABLE).select("id,owner_user_id").eq("id", shop_id).limit(1).execute()
    row = (getattr(res, 'data', []) or [{}])[0]
    if not row:
        raise HTTPException(status_code=404, detail="Shop not found")
    if row.get("owner_user_id") != uid:
        raise HTTPException(status_code=403, detail="Not your shop")
    return uid


@print_router.post("/api/shop/signup", summary="Create a shop owned by current user")
def shop_signup(payload: PrintShopIn, authorization: Optional[str] = Header(default=None)):
    uid, _ = _get_auth_user(authorization)
    supabase = get_service_client()
    shop_id = str(uuid.uuid4())
    row = _supabase_payload({
        "id": shop_id,
        "owner_user_id": uid,
        "name": payload.name,
        "phone": payload.phone,
        "email": payload.email,
        "address": payload.address,
        "lat": payload.lat,
        "lng": payload.lng,
        "hours": payload.hours or {},
        "capabilities": payload.capabilities.dict() if payload.capabilities else {},
        "is_open": True,
        "paused": False,
        "rating": 0,
        "created_at": _now_iso(),
        "updated_at": _now_iso(),
    })
    ins = supabase.table(PRINT_SHOPS_TABLE).insert(row).execute()
    if getattr(ins, 'error', None):
        raise HTTPException(status_code=500, detail=f"Failed: {ins.error}")
    return {"ok": True, "shop_id": shop_id}


@print_router.get("/api/shop/me", summary="Get my shop profile")
def shop_me(authorization: Optional[str] = Header(default=None)):
    uid, _ = _get_auth_user(authorization)
    supabase = get_service_client()
    res = supabase.table(PRINT_SHOPS_TABLE).select("*").eq("owner_user_id", uid).limit(1).execute()
    row = (getattr(res, 'data', []) or [{}])[0]
    if not row:
        raise HTTPException(status_code=404, detail="No shop found for user")
    return row


@print_router.patch("/api/shop/me", summary="Update my shop profile")
def shop_me_update(payload: UpdateShopIn, authorization: Optional[str] = Header(default=None)):
    uid, _ = _get_auth_user(authorization)
    supabase = get_service_client()
    # Resolve my shop id
    res = supabase.table(PRINT_SHOPS_TABLE).select("id").eq("owner_user_id", uid).limit(1).execute()
    row = (getattr(res, 'data', []) or [{}])[0]
    if not row:
        raise HTTPException(status_code=404, detail="No shop found for user")
    shop_id = row.get("id")
    # Build update dict
    to_update: Dict[str, Any] = {}
    if payload.name is not None:
        to_update["name"] = payload.name
    if payload.phone is not None:
        to_update["phone"] = payload.phone
    if payload.email is not None:
        to_update["email"] = payload.email
    if payload.address is not None:
        to_update["address"] = payload.address
    if payload.lat is not None:
        to_update["lat"] = payload.lat
    if payload.lng is not None:
        to_update["lng"] = payload.lng
    if payload.hours is not None:
        to_update["hours"] = payload.hours
    if payload.capabilities is not None:
        to_update["capabilities"] = payload.capabilities.dict()
    if payload.is_open is not None:
        to_update["is_open"] = payload.is_open
    if payload.paused is not None:
        to_update["paused"] = payload.paused
    if payload.price_hint is not None:
        to_update["price_hint"] = payload.price_hint
    if payload.pricing is not None:
        to_update["pricing"] = payload.pricing.dict()
    to_update["updated_at"] = _now_iso()
    if not to_update:
        return {"ok": True, "updated": 0}
    upd = supabase.table(PRINT_SHOPS_TABLE).update(_supabase_payload(to_update)).eq("id", shop_id).execute()
    if getattr(upd, 'error', None):
        raise HTTPException(status_code=500, detail=f"Failed to update: {upd.error}")
    return {"ok": True}


# ---- Admin endpoints: roles + shops listing ----

@print_router.get("/api/admin/roles/me", summary="Return current user's admin role")
def admin_role_me(authorization: Optional[str] = Header(default=None)):
    token = _bearer_token_from_header(authorization)
    uid = _require_auth_user_id(token)
    supabase = get_service_client()
    try:
        r = supabase.table("admin_roles").select("role,permissions").eq("auth_user_id", uid).limit(1).execute()
        row = (getattr(r, 'data', []) or [{}])[0]
        role = (row.get("role") or "student")
        perms = row.get("permissions") or {}
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to fetch role: {e}")
    return {"role": role, "permissions": perms}


@print_router.get("/api/admin/print/shops", summary="Admin: list print shops")
def admin_list_shops(q: Optional[str] = Query(default=None), limit: int = Query(default=1000, ge=1, le=5000), authorization: Optional[str] = Header(default=None)):
    _require_admin(authorization)
    supabase = get_service_client()
    cols = "id,name,phone,email,address,is_open,paused,rating,logo_url,owner_user_id,updated_at,created_at"
    try:
        builder = supabase.table(PRINT_SHOPS_TABLE).select(cols).order("updated_at", desc=True).limit(limit)
        # Conservative: apply search client-side if ilike is unavailable
        res = builder.execute()
        data = (getattr(res, 'data', []) or [])
        if q:
            qq = (q or '').strip().lower()
            def _match(row: Dict[str, Any]) -> bool:
                s = " ".join(str(row.get(k, "")) for k in ("name","email","phone","address")).lower()
                return qq in s
            data = [r for r in data if _match(r)]
        return {"shops": data}
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to list shops: {e}")


@print_router.get("/api/admin/print/shops/{shop_id}/jobs", summary="Admin: list jobs for a shop")
def admin_shop_jobs(shop_id: str, status: Optional[str] = Query(default=None), limit: int = Query(default=500, ge=1, le=5000), authorization: Optional[str] = Header(default=None)):
    _require_admin(authorization)
    supabase = get_service_client()
    try:
        q = supabase.table(PRINT_JOBS_TABLE).select("*").eq("shop_id", shop_id)
        if status:
            q = q.eq("status", status)
        res = q.order("created_at", desc=True).limit(limit).execute()
        return {"jobs": getattr(res, 'data', []) or []}
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to list shop jobs: {e}")


class AdminSettleOut(BaseModel):
    settled_count: int
    settled_amount: float


@print_router.post("/api/admin/print/shops/{shop_id}/settle", summary="Admin: settle all completed jobs -> settled", response_model=AdminSettleOut)
def admin_settle_shop(shop_id: str, authorization: Optional[str] = Header(default=None)):
    _require_admin(authorization)
    supabase = get_service_client()
    try:
        # Fetch all completed (not already settled) jobs for this shop
        res = supabase.table(PRINT_JOBS_TABLE).select("id,estimated_price,status").eq("shop_id", shop_id).in_("status", ["completed"]).execute()
        rows = (getattr(res, 'data', []) or [])
        if not rows:
            return AdminSettleOut(settled_count=0, settled_amount=0.0)
        now = _now_iso()
        ids = [r["id"] for r in rows]
        total = 0.0
        for r in rows:
            try:
                amt = float(r.get("estimated_price") or 0)
            except Exception:
                amt = 0.0
            total += amt
        # Bulk update in batches (Supabase might limit IN size)
        batch_size = 200
        for i in range(0, len(ids), batch_size):
            chunk = ids[i:i+batch_size]
            supabase.table(PRINT_JOBS_TABLE).update({"status": "settled", "updated_at": now}).in_("id", chunk).execute()
            # Insert events for audit
            ev = [{"job_id": jid, "status": "settled", "note": "Settled by admin", "created_at": now} for jid in chunk]
            if ev:
                supabase.table(PRINT_JOB_EVENTS_TABLE).insert(ev).execute()
        return AdminSettleOut(settled_count=len(ids), settled_amount=round(total, 2))
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to settle: {e}")


@print_router.post("/api/admin/print/shops/{shop_id}/jobs/{job_id}/close", summary="Admin: close a job for a shop (closed by admin)")
def admin_close_job(shop_id: str, job_id: str, authorization: Optional[str] = Header(default=None)):
    """
    Mark a specific job as closed by admin. Adds an audit event.
    """
    _require_admin(authorization)
    supabase = get_service_client()
    try:
        # Verify job belongs to shop
        jr = supabase.table(PRINT_JOBS_TABLE).select("id,shop_id,status").eq("id", job_id).eq("shop_id", shop_id).limit(1).execute()
        job = (getattr(jr, 'data', []) or [{}])
        if not job or not job[0].get("id"):
            raise HTTPException(status_code=404, detail="Job not found")
        # If already settled, disallow closing
        cur_status = (job[0].get("status") or "").lower()
        if cur_status in ("settled",):
            raise HTTPException(status_code=409, detail="Cannot close a settled job")
        now = _now_iso()
        supabase.table(PRINT_JOBS_TABLE).update({"status": "closed", "updated_at": now}).eq("id", job_id).execute()
        supabase.table(PRINT_JOB_EVENTS_TABLE).insert({
            "job_id": job_id,
            "status": "closed",
            "note": "Closed by admin",
            "created_at": now
        }).execute()
        return {"ok": True, "job_id": job_id, "status": "closed"}
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to close job: {e}")


@print_router.get("/api/shop/jobs", summary="List jobs for my shop")
def shop_jobs(
    status: Optional[str] = Query(default=None),
    limit: int = Query(default=200, ge=1, le=1000),
    authorization: Optional[str] = Header(default=None),
):
    uid, _ = _get_auth_user(authorization)
    supabase = get_service_client()
    # Query shop id with retry to handle transient protocol disconnects
    shop_res = _supabase_retry(lambda: supabase
                               .table(PRINT_SHOPS_TABLE)
                               .select("id")
                               .eq("owner_user_id", uid)
                               .limit(1)
                               .execute())
    shop = (getattr(shop_res, 'data', []) or [{}])[0]
    if not shop:
        raise HTTPException(status_code=404, detail="No shop found")
    q = supabase.table(PRINT_JOBS_TABLE).select("*").eq("shop_id", shop.get("id"))
    if status:
        q = q.eq("status", status)
    # Order by newest first and cap result size to avoid large payload disconnects
    q = q.order("created_at", desc=True).limit(limit)
    try:
        res = _supabase_retry(lambda: q.execute())
    except Exception as e:
        # Retry once with a smaller window in case of upstream disconnects
        try:
            res = _supabase_retry(lambda: q.limit(min(limit, 100)).execute())
        except Exception:
            raise HTTPException(status_code=502, detail="Upstream query failed while fetching jobs")
    return {"jobs": getattr(res, 'data', []) or []}


def _set_status(job_id: str, new_status: str, note: str, authorization: Optional[str]):
    supabase = get_service_client()
    # Fetch job + verify ownership
    job_q = supabase.table(PRINT_JOBS_TABLE).select("id,shop_id,status,otp").eq("id", job_id).limit(1).execute()
    job = (getattr(job_q, 'data', []) or [{}])[0]
    if not job:
        raise HTTPException(status_code=404, detail="Job not found")
    _require_shop_owner(job.get("shop_id"), authorization)
    now = _now_iso()
    supabase.table(PRINT_JOBS_TABLE).update({"status": new_status, "updated_at": now}).eq("id", job_id).execute()
    supabase.table(PRINT_JOB_EVENTS_TABLE).insert({"job_id": job_id, "status": new_status, "note": note, "created_at": now}).execute()
    return {"ok": True}


@print_router.post("/api/shop/jobs/{job_id}/accept")
def shop_accept(job_id: str, authorization: Optional[str] = Header(default=None)):
    return _set_status(job_id, "accepted", "Accepted", authorization)


@print_router.post("/api/shop/jobs/{job_id}/reject")
def shop_reject(job_id: str, reason: Optional[str] = Form(default=None), authorization: Optional[str] = Header(default=None)):
    return _set_status(job_id, "cancelled", f"Rejected: {reason or ''}", authorization)


@print_router.post("/api/shop/jobs/{job_id}/printing")
def shop_printing(job_id: str, authorization: Optional[str] = Header(default=None)):
    return _set_status(job_id, "printing", "Printing", authorization)


@print_router.post("/api/shop/jobs/{job_id}/ready")
def shop_ready(job_id: str, authorization: Optional[str] = Header(default=None)):
    return _set_status(job_id, "ready", "Ready for pickup", authorization)


class ReleaseIn(BaseModel):
    otp: str


@print_router.post("/api/shop/jobs/{job_id}/release")
def shop_release(job_id: str, body: ReleaseIn, authorization: Optional[str] = Header(default=None)):
    supabase = get_service_client()
    job_q = supabase.table(PRINT_JOBS_TABLE).select("id,shop_id,status,otp").eq("id", job_id).limit(1).execute()
    job = (getattr(job_q, 'data', []) or [{}])[0]
    if not job:
        raise HTTPException(status_code=404, detail="Job not found")
    _require_shop_owner(job.get("shop_id"), authorization)
    if (job.get("otp") or "").strip() != (body.otp or "").strip():
        raise HTTPException(status_code=400, detail="Invalid OTP")
    return _set_status(job_id, "completed", "Released with OTP", authorization)

# --- Notes Marketplace API ---
marketplace_router = APIRouter()

# Storage directory for uploaded marketplace note files (PDF, images, etc.)
MARKETPLACE_STORAGE = Path(__file__).resolve().parent / "assets" / "notes_marketplace"
MARKETPLACE_STORAGE.mkdir(parents=True, exist_ok=True)

# Allow common document/image/presentation/archive types used for notes
ALLOWED_NOTE_EXTENSIONS = {
    ".pdf",
    ".md",
    ".txt",
    ".png",
    ".jpg",
    ".jpeg",
    ".doc",
    ".docx",
    ".ppt",
    ".pptx",
    ".zip",
}
MAX_NOTE_FILE_SIZE = 25 * 1024 * 1024  # 25 MB


def _require_auth_user_id(token: Optional[str]) -> str:
    """Resolve auth user id from bearer token using Supabase anon client."""
    if not token:
        raise HTTPException(status_code=401, detail="Missing or invalid auth token")
    anon = get_anon_client()
    if not anon:
        raise HTTPException(status_code=500, detail="Anon client not configured")
    try:
        user = anon.auth.get_user(token)  # type: ignore[attr-defined]
        uid = _get_user_id_from_auth_response(user)
        if not uid:
            raise HTTPException(status_code=401, detail="Invalid auth user")
        return uid
    except Exception:
        raise HTTPException(status_code=401, detail="Auth validation failed")


def _sanitize_filename(name: str) -> str:
    base = re.sub(r"[^a-zA-Z0-9_.-]+", "_", name.strip())[:120]
    return base or "note"


def _execute_supabase(builder, retries: int = 2, base_delay: float = 0.2):
    """Execute a Supabase query builder with simple retry on transport drops."""
    last_exc: Optional[Exception] = None
    for attempt in range(retries + 1):
        try:
            return builder.execute()
        except HTTPXRemoteProtocolError as exc:  # pragma: no cover - network dependent
            last_exc = exc
            time.sleep(base_delay * (attempt + 1))
            continue
    raise HTTPException(status_code=503, detail="Temporary Supabase connection issue. Please retry.") from last_exc


def _approximate_pages_from_words(words: int) -> Optional[int]:
    if words <= 0:
        return None
    # assume ~500 words per page, round up
    return max(1, math.ceil(words / 500))


def _compute_page_count(file_path: Path, original_name: Optional[str], mime_type: Optional[str]) -> Optional[int]:
    """Best-effort page/slide estimate using available libraries; returns None on failure."""
    if not file_path or not file_path.exists():
        return None
    ext = (Path(original_name or file_path.name).suffix or "").lower()
    if not ext and mime_type:
        mt = (mime_type or "").lower()
        if "pdf" in mt:
            ext = ".pdf"
        elif "word" in mt or "msword" in mt:
            ext = ".docx"
        elif "ppt" in mt:
            ext = ".pptx"
    try:
        if ext == ".pdf" and fitz:
            with fitz.open(file_path) as pdf:  # type: ignore[attr-defined]
                return int(pdf.page_count)
        if ext in {".docx"} and docx:
            document = docx.Document(str(file_path))
            words = sum(len((para.text or "").split()) for para in document.paragraphs)
            return _approximate_pages_from_words(words)
        if ext in {".pptx"} and Presentation:
            prs = Presentation(str(file_path))
            return len(prs.slides)
        if ext == ".doc" and textract:
            text = textract.process(str(file_path)).decode("utf-8", errors="ignore")
            words = len(text.split())
            return _approximate_pages_from_words(words)
        if ext == ".ppt" and textract:
            text = textract.process(str(file_path)).decode("utf-8", errors="ignore")
            slides = text.count("\f") or text.count("\x0c") or 0
            return slides or None
    except Exception:
        return None
    return None


def _store_marketplace_file(upload: UploadFile) -> tuple[str, int, str]:
    """Deprecated: legacy local storage path now replaced with Supabase Storage.
    Retained for compatibility but uploads to Supabase and returns public URL as stored_name.
    """
    suffix = Path(upload.filename or "").suffix.lower()
    if suffix not in ALLOWED_NOTE_EXTENSIONS:
        raise HTTPException(status_code=400, detail="Unsupported file type")
    blob = _read_upload_bytes(upload)
    size = len(blob)
    if size > MAX_NOTE_FILE_SIZE:
        raise HTTPException(status_code=400, detail="File too large (>25MB)")
    supabase = get_service_client()
    bucket = os.getenv("SUPABASE_BUCKET", "").strip()
    if not bucket:
        raise HTTPException(status_code=500, detail="Missing SUPABASE_BUCKET in environment")
    safe_ext = suffix or ".bin"
    dest = f"marketplace/notes/{uuid.uuid4().hex}{safe_ext}"
    _storage_upload_bytes(supabase, bucket, dest, blob, upload.content_type)
    public_url = _storage_public_url(supabase, bucket, dest)
    # Return public URL as stored_name for downstream callers
    return public_url, size, (upload.content_type or "application/octet-stream")


@marketplace_router.post("/api/marketplace/notes", summary="Upload a note to marketplace")
def mp_upload_note(
    title: str = Form(..., min_length=1, max_length=256),
    description: str = Form(""),
    subject: str = Form(""),
    subject_id: Optional[str] = Form(None),
    unit: str = Form(""),
    exam_type: str = Form(""),
    categories: str = Form(""),  # comma separated
    price_cents: int = Form(0, ge=0),
    college_id: Optional[str] = Form(None),
    degree_id: Optional[str] = Form(None),
    department_id: Optional[str] = Form(None),
    batch_id: Optional[str] = Form(None),
    semester: Optional[int] = Form(None),
    file: Optional[UploadFile] = File(None),
    files: Optional[List[UploadFile]] = File(None),
    # Remote storage (Supabase) fields to allow URL-only flow
    stored_path: Optional[str] = Form(None, description="If provided as http(s) URL, backend will not store locally."),
    url: Optional[str] = Form(None, description="Optional alias for stored_path (http URL)."),
    original_filename: Optional[str] = Form(None),
    mime_type: Optional[str] = Form(None),
    file_size: Optional[int] = Form(None),
    cover: Optional[UploadFile] = File(None),
    authorization: Optional[str] = Header(default=None),
):
    token = _parse_bearer_token(authorization)
    user_id = _require_auth_user_id(token)
    uploads: List[UploadFile] = []
    if files:
        uploads.extend([f for f in files if f is not None and getattr(f, 'filename', None)])
    if file is not None and getattr(file, 'filename', None):
        uploads.append(file)
    # If no binary uploads, accept URL-based create when stored_path/url is provided
    if not uploads:
        remote = url or stored_path
        if not remote or not str(remote).lower().startswith(("http://", "https://")):
            raise HTTPException(status_code=400, detail="No file(s) provided")
        supabase = get_service_client()
        cats = [c.strip() for c in (categories or "").split(",") if c.strip()]
        row: Dict[str, Any] = {
            "owner_user_id": user_id,
            "title": title.strip(),
            "description": description.strip() or None,
            "subject": subject.strip() or None,
            "unit": unit.strip() or None,
            "exam_type": exam_type.strip() or None,
            "categories": cats,
            "price_cents": price_cents,
            "original_filename": original_filename or None,
            "stored_path": str(remote),
            "mime_type": (mime_type or "application/octet-stream"),
            "file_size": int(file_size) if file_size is not None else None,
        }
        if url:
            # If the table has a 'url' column it will be persisted; otherwise ignored
            row["url"] = str(url)
        # Academic linkages
        if college_id:
            row["college_id"] = college_id
        if degree_id:
            row["degree_id"] = degree_id
        if department_id:
            row["department_id"] = department_id
        if batch_id:
            row["batch_id"] = batch_id
        if semester is not None:
            if semester < 1 or semester > 12:
                raise HTTPException(status_code=400, detail="semester must be between 1 and 12")
            row["semester"] = semester
        if subject_id:
            try:
                uuid.UUID(str(subject_id))
                row["subject_id"] = str(subject_id)
                row["subject_href"] = f"/api/syllabus/courses/{subject_id}"
            except Exception:
                pass
        res = supabase.table("marketplace_notes").insert(row).execute()
        if getattr(res, "error", None):
            raise HTTPException(status_code=500, detail=f"Supabase error (insert note): {res.error}")
        data_rows = res.data if isinstance(res.data, list) else ([res.data] if res.data else [row])
        return {"notes": data_rows}
    cover_name: Optional[str] = None
    if cover and cover.filename:
        try:
            # Upload cover to Supabase Storage
            ext = Path(cover.filename).suffix.lower()
            if ext not in {'.png', '.jpg', '.jpeg', '.webp', '.gif'}:
                raise HTTPException(status_code=400, detail="Unsupported cover image type")
            blob = _read_upload_bytes(cover)
            if len(blob) > 5 * 1024 * 1024:
                raise HTTPException(status_code=400, detail="Cover image too large (>5MB)")
            supabase = get_service_client()
            bucket = os.getenv("SUPABASE_BUCKET", "").strip()
            if not bucket:
                raise HTTPException(status_code=500, detail="Missing SUPABASE_BUCKET in environment")
            dest = f"marketplace/notes/covers/{uuid.uuid4().hex}{ext}"
            _storage_upload_bytes(supabase, bucket, dest, blob, cover.content_type or "image/png")
            cover_name = _storage_public_url(supabase, bucket, dest)
        except HTTPException:
            raise
        except Exception as e:  # pragma: no cover
            raise HTTPException(status_code=500, detail=f"Failed to store cover: {e}")
    cats = [c.strip() for c in (categories or "").split(",") if c.strip()]
    supabase = get_service_client()
    def build_row(upload: UploadFile, stored_name: str, size: int, mime: str) -> Dict[str, Any]:
        r: Dict[str, Any] = {
            "owner_user_id": user_id,
            "title": title.strip(),
            "description": description.strip() or None,
            "subject": subject.strip() or None,
            "unit": unit.strip() or None,
            "exam_type": exam_type.strip() or None,
            "categories": cats,
            "price_cents": price_cents,
            "original_filename": upload.filename,
            "stored_path": stored_name,
            "mime_type": mime,
            "file_size": size,
        }
        # Attach academic linkage if provided (light validation)
        if college_id:
            r["college_id"] = college_id
        if degree_id:
            r["degree_id"] = degree_id
        if department_id:
            r["department_id"] = department_id
        if batch_id:
            r["batch_id"] = batch_id
        if semester is not None:
            if semester < 1 or semester > 12:
                raise HTTPException(status_code=400, detail="semester must be between 1 and 12")
            r["semester"] = semester
        if cover_name:
            r["cover_path"] = cover_name
        if subject_id:
            try:
                uuid.UUID(str(subject_id))
                r["subject_id"] = str(subject_id)
                r["subject_href"] = f"/api/syllabus/courses/{subject_id}"
            except Exception:
                pass
        return r

    rows: List[Dict[str, Any]] = []
    for up in uploads:
        stored_name, size, mime = _store_marketplace_file(up)
        rows.append(build_row(up, stored_name, size, mime))
    res = supabase.table("marketplace_notes").insert(rows if len(rows) > 1 else rows[0]).execute()
    if getattr(res, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (insert note): {res.error}")
    data_rows = res.data if isinstance(res.data, list) else ([res.data] if res.data else rows)
    return {"notes": data_rows}


@marketplace_router.get("/api/marketplace/notes", summary="List marketplace notes")
def mp_list_notes(
    q: Optional[str] = Query(None),
    subject: Optional[str] = Query(None),
    exam_type: Optional[str] = Query(None),
    min_price: Optional[int] = Query(None, ge=0),
    max_price: Optional[int] = Query(None, ge=0),
    college_id: Optional[str] = Query(None),
    degree_id: Optional[str] = Query(None),
    department_id: Optional[str] = Query(None),
    batch_id: Optional[str] = Query(None),
    semester: Optional[int] = Query(None, ge=1, le=12),
    teachers_only: Optional[bool] = Query(False, description="If true, restrict to notes whose owners have role=teacher"),
    limit: int = Query(20, ge=1, le=100),
    offset: int = Query(0, ge=0),
):
    supabase = get_service_client()
    query = supabase.table("marketplace_notes").select("*").order("created_at", desc=True)
    # Basic filters happen client-side after fetch because supabase python client has limited chaining w/ dynamic filters.
    res = query.execute()
    if getattr(res, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (list notes): {res.error}")
    items = res.data or []
    # Collect unique owner_user_id values to enrich with profile display fields (user_profiles + teacher_profiles + roles).
    owner_ids = sorted({r.get("owner_user_id") for r in items if r.get("owner_user_id")})
    user_profiles_map: dict[str, dict] = {}
    teacher_profiles_map: dict[str, dict] = {}
    teacher_ids_role: set[str] = set()
    if owner_ids:
        CHUNK = 40
        for i in range(0, len(owner_ids), CHUNK):
            chunk = owner_ids[i:i+CHUNK]
            # Standard user profile enrichment
            try:
                prof_res = (
                    supabase.table("user_profiles")
                    .select("auth_user_id,name,profile_image_url")
                    .in_("auth_user_id", chunk)
                    .execute()
                )
                if not getattr(prof_res, "error", None):
                    for pr in prof_res.data or []:
                        pid = pr.get("auth_user_id")
                        if pid:
                            user_profiles_map[pid] = pr
            except Exception:  # pragma: no cover
                pass
            # Teacher profile enrichment (prefer these over user_profiles when present)
            try:
                tprof_res = (
                    supabase.table("teacher_profiles")
                    .select("auth_user_id,name,profile_image_url,college_id,department_id")
                    .in_("auth_user_id", chunk)
                    .execute()
                )
                if not getattr(tprof_res, "error", None):
                    for tr in tprof_res.data or []:
                        tid = tr.get("auth_user_id")
                        if tid:
                            teacher_profiles_map[tid] = tr
            except Exception:  # pragma: no cover
                pass
        # Roles lookup (single query if possible)
        try:
            role_res = (
                supabase.table("admin_roles")
                .select("auth_user_id,role")
                .in_("auth_user_id", owner_ids)
                .eq("role", "teacher")
                .execute()
            )
            if not getattr(role_res, "error", None):
                for rr in role_res.data or []:
                    rid = rr.get("auth_user_id")
                    if rid:
                        teacher_ids_role.add(rid)
        except Exception:  # pragma: no cover
            pass
    # Optionally restrict to teacher owners (role table lookup)
    if teachers_only:
        teacher_ids: set[str] = set()
        try:
            role_res = (
                supabase.table("admin_roles")
                .select("auth_user_id,role")
                .eq("role", "teacher")
                .execute()
            )
            if not getattr(role_res, "error", None):
                for r in role_res.data or []:
                    uid = r.get("auth_user_id")
                    if uid:
                        teacher_ids.add(uid)
        except Exception:
            teacher_ids = set()
        if teacher_ids:
            items = [r for r in items if r.get("owner_user_id") in teacher_ids]

    # Enrich each note with seller fields (short form) for UI consumption. Teacher profile takes precedence.
    for r in items:
        oid = r.get("owner_user_id")
        if not oid:
            continue
        tprof = teacher_profiles_map.get(oid)
        uprof = user_profiles_map.get(oid)
        is_teacher = oid in teacher_ids_role or bool(tprof)
        seller: dict[str, Any] = {"id": oid}
        if tprof:
            seller["name"] = tprof.get("name") or (uprof.get("name") if uprof else oid[:6] + "â€¦")
            if tprof.get("profile_image_url"):
                seller["avatar_url"] = tprof.get("profile_image_url")
        elif uprof:
            seller["name"] = uprof.get("name") or oid[:6] + "â€¦"
            if uprof.get("profile_image_url"):
                seller["avatar_url"] = uprof.get("profile_image_url")
        else:
            seller["name"] = oid[:6] + "â€¦"
        if is_teacher:
            seller["verified"] = True
            seller["is_teacher"] = True
            seller["profile_href"] = f"/ui/teacher_profile.html?user={oid}"
        r["seller"] = seller
    
    def _match(row: dict) -> bool:
        if q:
            txt = " ".join(str(row.get(k, "")) for k in ["title", "description", "subject", "unit"]).lower()
            if q.lower() not in txt:
                return False
        if subject and (row.get("subject") or "") != subject:
            return False
        if exam_type and (row.get("exam_type") or "") != exam_type:
            return False
        if college_id and (row.get("college_id") or "") != college_id:
            return False
        if degree_id and (row.get("degree_id") or "") != degree_id:
            return False
        if department_id and (row.get("department_id") or "") != department_id:
            return False
        if batch_id and (row.get("batch_id") or "") != batch_id:
            return False
        if semester is not None and row.get("semester") != semester:
            return False
        price = int(row.get("price_cents") or 0)
        if min_price is not None and price < min_price:
            return False
        if max_price is not None and price > max_price:
            return False
        return True
    filtered = [r for r in items if _match(r)]
    total = len(filtered)
    paged = filtered[offset: offset + limit]
    return {"items": paged, "total": total, "limit": limit, "offset": offset}


class TeacherNotesBatchIn(BaseModel):
    subject_ids: List[str] = Field(..., min_items=1, max_items=200)
    limit: int = Field(default=6, ge=1, le=50)


@marketplace_router.post("/api/marketplace/subjects/teacher-notes/batch", summary="Batch list teacher marketplace notes for subjects")
def mp_teacher_notes_by_subject_batch(payload: TeacherNotesBatchIn):
    raw_ids = [str(s).strip() for s in (payload.subject_ids or []) if s]
    cleaned = []
    seen = set()
    for sid in raw_ids:
        if not sid or sid.lower() in {"null", "undefined"}:
            continue
        if sid in seen:
            continue
        seen.add(sid)
        cleaned.append(sid)
    if not cleaned:
        return {"notes": {}, "count": 0, "limit": payload.limit}

    supabase = get_service_client()
    # Fetch a bounded window; we'll group and cap per-subject in app code
    max_total = min(500, max(1, payload.limit) * len(cleaned) * 2)
    notes_res = (
        supabase.table("marketplace_notes")
        .select("id,title,subject,subject_id,price_cents,owner_user_id,updated_at,created_at,semester,unit,exam_type")
        .in_("subject_id", cleaned)
        .order("updated_at", desc=True)
        .limit(max_total)
        .execute()
    )
    if getattr(notes_res, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (notes by subject batch): {notes_res.error}")

    notes: List[Dict[str, Any]] = notes_res.data or []
    if not notes:
        return {"notes": {}, "count": 0, "limit": payload.limit}

    owner_ids = {n.get("owner_user_id") for n in notes if n.get("owner_user_id")}
    if not owner_ids:
        return {"notes": {}, "count": 0, "limit": payload.limit}

    teacher_ids: Set[str] = set()
    try:
        role_res = (
            supabase.table("admin_roles")
            .select("auth_user_id,role")
            .in_("auth_user_id", list(owner_ids))
            .eq("role", "teacher")
            .execute()
        )
        if getattr(role_res, "error", None):
            teacher_ids = set(owner_ids)
        else:
            teacher_ids = {row.get("auth_user_id") for row in (role_res.data or []) if row.get("auth_user_id")}
    except Exception:
        teacher_ids = set(owner_ids)

    filtered_notes = [row for row in notes if row.get("owner_user_id") in teacher_ids]
    if not filtered_notes:
        return {"notes": {}, "count": 0, "limit": payload.limit}

    seller_ids = {row.get("owner_user_id") for row in filtered_notes if row.get("owner_user_id")}
    user_profiles_map: Dict[str, Dict[str, Any]] = {}
    teacher_profiles_map: Dict[str, Dict[str, Any]] = {}

    if seller_ids:
        try:
            prof_res = (
                supabase.table("user_profiles")
                .select("auth_user_id,name,profile_image_url")
                .in_("auth_user_id", list(seller_ids))
                .execute()
            )
            if not getattr(prof_res, "error", None):
                for row in prof_res.data or []:
                    uid = row.get("auth_user_id")
                    if uid:
                        user_profiles_map[uid] = row
        except Exception:
            pass
        try:
            tprof_res = (
                supabase.table("teacher_profiles")
                .select("auth_user_id,name,profile_image_url")
                .in_("auth_user_id", list(seller_ids))
                .execute()
            )
            if not getattr(tprof_res, "error", None):
                for row in tprof_res.data or []:
                    uid = row.get("auth_user_id")
                    if uid:
                        teacher_profiles_map[uid] = row
        except Exception:
            pass

    grouped: Dict[str, List[Dict[str, Any]]] = {}
    for row in filtered_notes:
        sid = row.get("subject_id")
        if not sid:
            continue
        key = str(sid)
        bucket = grouped.setdefault(key, [])
        if len(bucket) >= payload.limit:
            continue
        owner_id = row.get("owner_user_id")
        teacher_profile = teacher_profiles_map.get(owner_id)
        user_profile = user_profiles_map.get(owner_id)
        seller_name = None
        seller_avatar = None
        if teacher_profile and teacher_profile.get("name"):
            seller_name = teacher_profile.get("name")
        elif user_profile and user_profile.get("name"):
            seller_name = user_profile.get("name")
        elif owner_id:
            seller_name = owner_id[:6] + "..."
        if teacher_profile and teacher_profile.get("profile_image_url"):
            seller_avatar = teacher_profile.get("profile_image_url")
        elif user_profile and user_profile.get("profile_image_url"):
            seller_avatar = user_profile.get("profile_image_url")

        seller = {"id": owner_id, "is_teacher": True, "verified": True}
        if seller_name:
            seller["name"] = seller_name
        if seller_avatar:
            seller["avatar_url"] = seller_avatar
        if owner_id:
            seller["profile_href"] = f"/ui/teacher_profile.html?user={owner_id}"

        bucket.append(
            {
                "id": row.get("id"),
                "title": row.get("title"),
                "subject": row.get("subject"),
                "subject_id": row.get("subject_id"),
                "price_cents": int(row.get("price_cents") or 0),
                "owner_user_id": owner_id,
                "updated_at": row.get("updated_at") or row.get("created_at"),
                "created_at": row.get("created_at"),
                "semester": row.get("semester"),
                "unit": row.get("unit"),
                "exam_type": row.get("exam_type"),
                "seller": seller,
            }
        )

    total = sum(len(v) for v in grouped.values())
    return {"notes": grouped, "count": total, "limit": payload.limit}


@marketplace_router.get("/api/marketplace/subjects/{subject_id}/teacher-notes", summary="List teacher marketplace notes for a subject")
def mp_teacher_notes_by_subject(
    subject_id: str,
    limit: int = Query(20, ge=1, le=60),
    offset: int = Query(0, ge=0),
):
    subject_key = (subject_id or "").strip()
    if not subject_key or subject_key.lower() in {"null", "undefined"}:
        raise HTTPException(status_code=400, detail="A valid subject_id is required")

    supabase = get_service_client()
    notes_res = (
        supabase.table("marketplace_notes")
        .select("id,title,subject,subject_id,price_cents,owner_user_id,updated_at,created_at,semester,unit,exam_type")
        .eq("subject_id", subject_key)
        .order("updated_at", desc=True)
        .offset(offset)
        .limit(limit)
        .execute()
    )
    if getattr(notes_res, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (notes by subject): {notes_res.error}")

    notes: List[Dict[str, Any]] = notes_res.data or []
    if not notes:
        return {"notes": []}

    owner_ids = {n.get("owner_user_id") for n in notes if n.get("owner_user_id")}
    if not owner_ids:
        return {"notes": []}

    teacher_ids: Set[str] = set()
    try:
        role_res = (
            supabase.table("admin_roles")
            .select("auth_user_id,role")
            .in_("auth_user_id", list(owner_ids))
            .eq("role", "teacher")
            .execute()
        )
        if getattr(role_res, "error", None):
            teacher_ids = set(owner_ids)  # fall back to include all owners if role lookup fails
        else:
            teacher_ids = {row.get("auth_user_id") for row in (role_res.data or []) if row.get("auth_user_id")}
    except Exception:
        teacher_ids = set(owner_ids)

    filtered_notes = [row for row in notes if row.get("owner_user_id") in teacher_ids]
    if not filtered_notes:
        return {"notes": []}

    seller_ids = {row.get("owner_user_id") for row in filtered_notes if row.get("owner_user_id")}
    user_profiles_map: Dict[str, Dict[str, Any]] = {}
    teacher_profiles_map: Dict[str, Dict[str, Any]] = {}

    if seller_ids:
        try:
            prof_res = (
                supabase.table("user_profiles")
                .select("auth_user_id,name,profile_image_url")
                .in_("auth_user_id", list(seller_ids))
                .execute()
            )
            if not getattr(prof_res, "error", None):
                for row in prof_res.data or []:
                    uid = row.get("auth_user_id")
                    if uid:
                        user_profiles_map[uid] = row
        except Exception:
            pass
        try:
            tprof_res = (
                supabase.table("teacher_profiles")
                .select("auth_user_id,name,profile_image_url")
                .in_("auth_user_id", list(seller_ids))
                .execute()
            )
            if not getattr(tprof_res, "error", None):
                for row in tprof_res.data or []:
                    uid = row.get("auth_user_id")
                    if uid:
                        teacher_profiles_map[uid] = row
        except Exception:
            pass

    enriched: List[Dict[str, Any]] = []
    for row in filtered_notes:
        owner_id = row.get("owner_user_id")
        teacher_profile = teacher_profiles_map.get(owner_id)
        user_profile = user_profiles_map.get(owner_id)
        seller_name = None
        seller_avatar = None
        if teacher_profile and teacher_profile.get("name"):
            seller_name = teacher_profile.get("name")
        elif user_profile and user_profile.get("name"):
            seller_name = user_profile.get("name")
        elif owner_id:
            seller_name = owner_id[:6] + "..."
        if teacher_profile and teacher_profile.get("profile_image_url"):
            seller_avatar = teacher_profile.get("profile_image_url")
        elif user_profile and user_profile.get("profile_image_url"):
            seller_avatar = user_profile.get("profile_image_url")

        seller = {"id": owner_id, "is_teacher": True, "verified": True}
        if seller_name:
            seller["name"] = seller_name
        if seller_avatar:
            seller["avatar_url"] = seller_avatar
        if owner_id:
            seller["profile_href"] = f"/ui/teacher_profile.html?user={owner_id}"

        enriched.append(
            {
                "id": row.get("id"),
                "title": row.get("title"),
                "subject": row.get("subject"),
                "subject_id": row.get("subject_id"),
                "price_cents": int(row.get("price_cents") or 0),
                "owner_user_id": owner_id,
                "updated_at": row.get("updated_at") or row.get("created_at"),
                "created_at": row.get("created_at"),
                "semester": row.get("semester"),
                "unit": row.get("unit"),
                "exam_type": row.get("exam_type"),
                "seller": seller,
            }
        )

    return {"notes": enriched, "count": len(enriched), "limit": limit, "offset": offset}


@marketplace_router.get("/api/marketplace/notes/meta", summary="Distinct filter metadata for marketplace notes")
def mp_notes_meta(teachers_only: Optional[bool] = Query(False, description="If true, restrict to notes whose owners have role=teacher")):
    """Return distinct values useful for building client-side filters.

    This performs a single select * (bounded) and derives sets in app code because
    the Supabase python client lacks a simple DISTINCT helper across many columns.
    If the table grows large, replace with server-side RPC or dedicated materialized view.
    """
    supabase = get_service_client()
    # Fetch a reasonable window (latest 1000) â€“ adjust as needed or paginate later.
    res = supabase.table("marketplace_notes").select("*").order("created_at", desc=True).limit(1000).execute()
    if getattr(res, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (notes meta): {res.error}")
    rows: List[dict] = res.data or []
    if teachers_only:
        # build teacher id set
        try:
            role_res = supabase.table("admin_roles").select("auth_user_id,role").eq("role", "teacher").execute()
            if not getattr(role_res, "error", None):
                teacher_ids = {r.get("auth_user_id") for r in (role_res.data or []) if r.get("auth_user_id")}
                if teacher_ids:
                    rows = [r for r in rows if r.get("owner_user_id") in teacher_ids]
        except Exception:
            pass
    subjects: set[str] = set()
    exam_types: set[str] = set()
    semesters: set[int] = set()
    categories: set[str] = set()
    college_ids: set[str] = set()
    degree_ids: set[str] = set()
    department_ids: set[str] = set()
    batch_ids: set[str] = set()
    seller_ids: set[str] = set()
    prices: List[int] = []
    for r in rows:
        if r.get("subject"): subjects.add(str(r.get("subject")))
        if r.get("exam_type"): exam_types.add(str(r.get("exam_type")))
        if r.get("semester") is not None:
            try:
                semesters.add(int(r.get("semester")))
            except Exception:
                pass
        if isinstance(r.get("categories"), list):
            for c in r.get("categories"):
                if c: categories.add(str(c))
        if r.get("college_id"): college_ids.add(str(r.get("college_id")))
        if r.get("degree_id"): degree_ids.add(str(r.get("degree_id")))
        if r.get("department_id"): department_ids.add(str(r.get("department_id")))
        if r.get("batch_id"): batch_ids.add(str(r.get("batch_id")))
        if r.get("owner_user_id"): seller_ids.add(str(r.get("owner_user_id")))
        try:
            prices.append(int(r.get("price_cents") or 0))
        except Exception:
            pass
    # Enrich seller short names (best-effort)
    seller_map: Dict[str, Dict[str, Optional[str]]] = {}
    if seller_ids:
        try:
            prof = (
                supabase.table("user_profiles")
                .select("auth_user_id,name,profile_image_url")
                .in_("auth_user_id", list(seller_ids))
                .execute()
            )
            if not getattr(prof, "error", None):
                for row in prof.data or []:
                    uid = row.get("auth_user_id")
                    if uid:
                        seller_map[uid] = {
                            "id": uid,
                            "name": row.get("name") or uid[:6] + "â€¦",
                            "avatar_url": row.get("profile_image_url"),
                        }
        except Exception:
            pass
    price_min = min(prices) if prices else 0
    price_max = max(prices) if prices else 0
    return {
        "subjects": sorted(subjects),
        "exam_types": sorted(exam_types),
        "semesters": sorted(semesters),
        "categories": sorted(categories),
        "college_ids": sorted(college_ids),
        "degree_ids": sorted(degree_ids),
        "department_ids": sorted(department_ids),
        "batch_ids": sorted(batch_ids),
        "sellers": list(seller_map.values()),
        "price_range": {"min": price_min, "max": price_max},
        "count": len(rows),
    }


@marketplace_router.get("/api/marketplace/notes/{note_id}", summary="Get marketplace note detail")
def mp_get_note(note_id: uuid.UUID, authorization: Optional[str] = Header(default=None), token: Optional[str] = Query(None)):
    header_token = _parse_bearer_token(authorization)
    token = token or header_token
    supabase = get_service_client()
    note_query = supabase.table("marketplace_notes").select("*").eq("id", str(note_id)).limit(1)
    res = _execute_supabase(note_query)
    if getattr(res, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (get note): {res.error}")
    if not res.data:
        raise HTTPException(status_code=404, detail="Note not found")
    note = res.data[0]
    if not note.get("page_count"):
        stored_name = note.get("stored_path")
        if stored_name:
            file_path = MARKETPLACE_STORAGE / stored_name
            computed_pages = _compute_page_count(file_path, note.get("original_filename"), note.get("mime_type"))
            if computed_pages:
                note["page_count"] = computed_pages
                note["pages"] = computed_pages
    # Determine if user has access (owner or purchased or free)
    user_id = None
    if token:
        try:
            user_id = _require_auth_user_id(token)
        except HTTPException:
            user_id = None
    has_access = False
    if note.get("price_cents", 0) == 0:
        has_access = True
    elif user_id and user_id == note.get("owner_user_id"):
        has_access = True
    elif user_id:
        purchase_query = (
            supabase.table("marketplace_purchases")
            .select("id")
            .eq("note_id", str(note_id))
            .eq("buyer_user_id", user_id)
            .limit(1)
        )
        pur = _execute_supabase(purchase_query)
        if getattr(pur, "error", None):
            raise HTTPException(status_code=500, detail=f"Supabase error (check purchase): {pur.error}")
        if pur.data:
            has_access = True
    # Reviews
    reviews_query = (
        supabase.table("marketplace_reviews")
        .select("id,reviewer_user_id,rating,comment,created_at")
        .eq("note_id", str(note_id))
        .order("created_at", desc=True)
    )
    rev = _execute_supabase(reviews_query)
    reviews = rev.data or []
    # Collect user ids for enrichment (owner + reviewers)
    user_ids: set[str] = set()
    owner_id = note.get("owner_user_id")
    if owner_id:
        user_ids.add(owner_id)
    for r in reviews:
        rid = r.get("reviewer_user_id")
        if rid:
            user_ids.add(rid)
    profiles_map: dict[str, dict] = {}
    if user_ids:
        # Fetch profiles from user_profiles (auth_user_id mapping)
        try:
            prof_query = (
                supabase.table("user_profiles")
                .select("auth_user_id,name,profile_image_url")
                .in_("auth_user_id", list(user_ids))
            )
            prof_res = _execute_supabase(prof_query)
            if not getattr(prof_res, "error", None):
                for row in prof_res.data or []:
                    uid = row.get("auth_user_id")
                    if uid:
                        profiles_map[uid] = row
        except Exception:
            pass
    # Attach seller (prefer teacher_profiles for teachers)
    seller_obj: Dict[str, Any] = {"id": owner_id} if owner_id else {}
    teacher_profile_row = None
    if owner_id:
        try:
            tprof_query = supabase.table("teacher_profiles").select("auth_user_id,name,profile_image_url,college_id,department_id").eq("auth_user_id", owner_id).limit(1)
            tprof = _execute_supabase(tprof_query)
            if not getattr(tprof, "error", None) and tprof.data:
                teacher_profile_row = tprof.data[0]
        except Exception:
            teacher_profile_row = None
    if teacher_profile_row:
        seller_obj["name"] = teacher_profile_row.get("name") or (profiles_map.get(owner_id, {}).get("name") if owner_id in profiles_map else owner_id[:6] + "â€¦")
        if teacher_profile_row.get("profile_image_url"):
            seller_obj["avatar_url"] = teacher_profile_row.get("profile_image_url")
    if owner_id and not teacher_profile_row and owner_id in profiles_map:
        prow = profiles_map[owner_id]
        seller_obj.setdefault("name", prow.get("name") or owner_id[:6] + "â€¦")
        if prow.get("profile_image_url"):
            seller_obj["avatar_url"] = prow.get("profile_image_url")
    if owner_id and "name" not in seller_obj:
        seller_obj["name"] = owner_id[:6] + "â€¦"
    if owner_id:
        note["seller"] = seller_obj

    # Determine if owner has teacher role (verification badge)
    is_teacher_owner = False
    try:
        if owner_id:
            role_query = (
                supabase.table("admin_roles")
                .select("role")
                .eq("auth_user_id", owner_id)
                .eq("role", "teacher")
                .limit(1)
            )
            role_res = _execute_supabase(role_query)
            if not getattr(role_res, "error", None) and role_res.data:
                is_teacher_owner = True
    except Exception:
        # Non-fatal; silently ignore role lookup issues
        pass
    if note.get("seller"):
        note["seller"]["verified"] = is_teacher_owner
        note["seller"]["is_teacher"] = is_teacher_owner
        if is_teacher_owner:
            note["seller"]["profile_href"] = f"/ui/teacher_profile.html?user={owner_id}" if owner_id else None
    note["is_teacher_owner"] = is_teacher_owner
    # Enrich each review
    for r in reviews:
        rid = r.get("reviewer_user_id")
        prow = profiles_map.get(rid)
        if prow:
            r["reviewer"] = {
                "id": rid,
                "name": prow.get("name") or (rid[:6] + "â€¦" if rid else None),
                "avatar_url": prow.get("profile_image_url"),
            }
    # --- Academic metadata enrichment (names) ---
    # If the note row has academic foreign keys, attempt to resolve human-readable names.
    # This keeps the base schema flexible while giving the UI friendly labels.
    academic_name_map: dict[str, Optional[str]] = {
        "college_name": None,
        "degree_name": None,
        "department_name": None,
        "batch_range": None,
    }
    try:
        # We collect the needed ids first to minimize queries.
        college_id = note.get("college_id")
        degree_id = note.get("degree_id")
        department_id = note.get("department_id")
        batch_id = note.get("batch_id")
        # For each present id we fetch its table (single row).
        if college_id:
            rcol = supabase.table("colleges").select("id,name").eq("id", college_id).limit(1).execute()
            if not getattr(rcol, "error", None) and rcol.data:
                academic_name_map["college_name"] = rcol.data[0].get("name")
        if degree_id:
            rdeg = supabase.table("degrees").select("id,name").eq("id", degree_id).limit(1).execute()
            if not getattr(rdeg, "error", None) and rdeg.data:
                academic_name_map["degree_name"] = rdeg.data[0].get("name")
        if department_id:
            rdep = supabase.table("departments").select("id,name").eq("id", department_id).limit(1).execute()
            if not getattr(rdep, "error", None) and rdep.data:
                academic_name_map["department_name"] = rdep.data[0].get("name")
        if batch_id:
            rbat = supabase.table("batches").select("id,from_year,to_year").eq("id", batch_id).limit(1).execute()
            if not getattr(rbat, "error", None) and rbat.data:
                b = rbat.data[0]
                fy = b.get("from_year")
                ty = b.get("to_year")
                if fy and ty:
                    academic_name_map["batch_range"] = f"{fy}-{ty}"
                elif fy:
                    academic_name_map["batch_range"] = str(fy)
    except Exception:
        # Silently ignore enrichment errors; we don't want to block detail retrieval.
        pass
    # Attach only non-null values to the note object so the front-end can conditionally render.
    for k, v in academic_name_map.items():
        if v:
            note[k] = v
    return {"note": note, "has_access": has_access, "reviews": reviews}


@marketplace_router.get("/api/marketplace/notes/{note_id}/download", summary="Download note file (public)")
def mp_download_note(note_id: uuid.UUID):
    """Redirect to the remote file URL stored in Supabase (no local files)."""
    supabase = get_service_client()
    res = supabase.table("marketplace_notes").select("stored_path,url,original_filename").eq("id", str(note_id)).limit(1).execute()
    if getattr(res, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (get note): {res.error}")
    if not res.data:
        raise HTTPException(status_code=404, detail="Note not found")
    row = res.data[0]
    target = row.get("url") or row.get("stored_path") or ""
    if not target or not str(target).lower().startswith(("http://", "https://")):
        raise HTTPException(status_code=404, detail="Remote URL not available")
    return RedirectResponse(url=str(target))

@marketplace_router.get("/api/marketplace/notes/{note_id}/preview", summary="Inline preview for PDF or image")
def mp_preview_note(note_id: uuid.UUID):
    """Redirect to remote URL; front-end handles inline rendering based on type."""
    supabase = get_service_client()
    res = supabase.table("marketplace_notes").select("stored_path,url").eq("id", str(note_id)).limit(1).execute()
    if getattr(res, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (get note): {res.error}")
    if not res.data:
        raise HTTPException(status_code=404, detail="Note not found")
    row = res.data[0]
    target = row.get("url") or row.get("stored_path") or ""
    if not target or not str(target).lower().startswith(("http://", "https://")):
        raise HTTPException(status_code=404, detail="Remote URL not available")
    return RedirectResponse(url=str(target))

@marketplace_router.get("/api/marketplace/notes/{note_id}/cover", summary="Get cover image for a note")
def mp_cover_image(note_id: uuid.UUID):
    supabase = get_service_client()
    res = supabase.table("marketplace_notes").select("cover_path").eq("id", str(note_id)).limit(1).execute()
    if getattr(res, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (get cover): {res.error}")
    if not res.data:
        raise HTTPException(status_code=404, detail="Note not found")
    cover = res.data[0].get("cover_path")
    if not cover or not str(cover).lower().startswith(("http://", "https://")):
        raise HTTPException(status_code=404, detail="No remote cover URL set")
    return RedirectResponse(url=str(cover))


@marketplace_router.post("/api/marketplace/notes/{note_id}/purchase", summary="Purchase a paid note (mock payment)")
def mp_purchase_note(note_id: uuid.UUID, authorization: Optional[str] = Header(default=None), token: Optional[str] = Query(None)):
    header_token = _parse_bearer_token(authorization)
    token = token or header_token
    user_id = _require_auth_user_id(token)
    supabase = get_service_client()
    note_res = supabase.table("marketplace_notes").select("price_cents,owner_user_id").eq("id", str(note_id)).limit(1).execute()
    if getattr(note_res, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (get note): {note_res.error}")
    if not note_res.data:
        raise HTTPException(status_code=404, detail="Note not found")
    note = note_res.data[0]
    if note.get("owner_user_id") == user_id:
        raise HTTPException(status_code=400, detail="Cannot purchase your own note")
    price = int(note.get("price_cents") or 0)
    if price == 0:
        return {"status": "free", "message": "Note is free"}
    # Mock payment success: just record purchase if not exists
    existing = (
        supabase.table("marketplace_purchases")
        .select("id")
        .eq("note_id", str(note_id))
        .eq("buyer_user_id", user_id)
        .limit(1)
        .execute()
    )
    if getattr(existing, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (find purchase): {existing.error}")
    if existing.data:
        return {"status": "ok", "message": "Already purchased"}
    ins = supabase.table("marketplace_purchases").insert({
        "note_id": str(note_id),
        "buyer_user_id": user_id,
        "amount_cents": price,
    }).execute()
    if getattr(ins, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (insert purchase): {ins.error}")
    return {"status": "ok", "purchase": ins.data[0] if ins.data else None}


@marketplace_router.post("/api/marketplace/notes/{note_id}/review", summary="Add or update a review")
def mp_review_note(
    note_id: uuid.UUID,
    rating: int = Form(..., ge=1, le=5),
    comment: str = Form(""),
    authorization: Optional[str] = Header(default=None),
    token: Optional[str] = Query(None),
):
    header_token = _parse_bearer_token(authorization)
    token = token or header_token
    user_id = _require_auth_user_id(token)
    supabase = get_service_client()
    # Ensure note exists (access no longer required for reviews; any authenticated user may review)
    note_res = supabase.table("marketplace_notes").select("price_cents,owner_user_id").eq("id", str(note_id)).limit(1).execute()
    if getattr(note_res, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (get note): {note_res.error}")
    if not note_res.data:
        raise HTTPException(status_code=404, detail="Note not found")
    note = note_res.data[0]
    # Access rule relaxed: we no longer gate by purchase/free. Still only one review per user.
    # Upsert (one per user per note)
    existing = (
        supabase.table("marketplace_reviews")
        .select("id")
        .eq("note_id", str(note_id))
        .eq("reviewer_user_id", user_id)
        .limit(1)
        .execute()
    )
    if getattr(existing, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (find review): {existing.error}")
    now = datetime.utcnow().isoformat()
    if existing.data:
        rid = existing.data[0]["id"]
        upd = (
            supabase.table("marketplace_reviews")
            .update({"rating": rating, "comment": comment.strip() or None, "updated_at": now})
            .eq("id", rid)
            .execute()
        )
        if getattr(upd, "error", None):
            raise HTTPException(status_code=500, detail=f"Supabase error (update review): {upd.error}")
    else:
        ins = supabase.table("marketplace_reviews").insert({
            "note_id": str(note_id),
            "reviewer_user_id": user_id,
            "rating": rating,
            "comment": comment.strip() or None,
        }).execute()
        if getattr(ins, "error", None):
            raise HTTPException(status_code=500, detail=f"Supabase error (insert review): {ins.error}")
    # Recompute aggregates
    agg = supabase.rpc("exec", params={}).execute() if False else None  # placeholder for future RPC
    # manual aggregate
    revs = (
        supabase.table("marketplace_reviews").select("rating").eq("note_id", str(note_id)).execute()
    )
    if not getattr(revs, "error", None):
        ratings = [int(r.get("rating") or 0) for r in (revs.data or [])]
        if ratings:
            avg_rating = round(sum(ratings) / len(ratings), 2)
            supabase.table("marketplace_notes").update({
                "avg_rating": avg_rating,
                "rating_count": len(ratings),
                "updated_at": datetime.utcnow().isoformat(),
            }).eq("id", str(note_id)).execute()
    return {"status": "ok"}


@marketplace_router.put("/api/marketplace/notes/{note_id}", summary="Update own note metadata")
def mp_update_note(note_id: uuid.UUID, payload: dict = Body(...), authorization: Optional[str] = Header(default=None)):
    token = _parse_bearer_token(authorization)
    user_id = _require_auth_user_id(token)
    supabase = get_service_client()
    note_res = supabase.table("marketplace_notes").select("owner_user_id").eq("id", str(note_id)).limit(1).execute()
    if getattr(note_res, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (get note for update): {note_res.error}")
    if not note_res.data:
        raise HTTPException(status_code=404, detail="Note not found")
    owner_id = note_res.data[0].get("owner_user_id")
    if owner_id != user_id:
        raise HTTPException(status_code=403, detail="Not permitted to update this note")
    allowed_fields = {"title", "description", "subject", "subject_id", "semester", "price_cents", "unit", "exam_type", "categories"}
    updates: Dict[str, Any] = {}
    payload = payload or {}
    for field in allowed_fields:
        if field in payload:
            value = payload[field]
            if field == "categories" and isinstance(value, str):
                value = [c.strip() for c in value.split(",") if c.strip()]
            if field == "semester" and value not in (None, ""):
                try:
                    ivalue = int(value)
                    if ivalue < 1 or ivalue > 12:
                        raise ValueError
                    value = ivalue
                except ValueError:
                    raise HTTPException(status_code=400, detail="semester must be between 1 and 12")
            if field == "price_cents" and value not in (None, ""):
                try:
                    value = int(value)
                    if value < 0:
                        raise ValueError
                except ValueError:
                    raise HTTPException(status_code=400, detail="price_cents must be a non-negative integer")
            if field == "subject_id" and value:
                try:
                    uuid.UUID(str(value))
                    value = str(value)
                except Exception:
                    raise HTTPException(status_code=400, detail="subject_id must be a valid UUID")
            updates[field] = value if value != "" else None
    if not updates:
        return {"updated": False}
    updates["updated_at"] = datetime.utcnow().isoformat()
    upd = (
        supabase.table("marketplace_notes")
        .update(updates)
        .eq("id", str(note_id))
        .eq("owner_user_id", user_id)
        .execute()
    )
    if getattr(upd, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (update note): {upd.error}")
    data = None
    if isinstance(upd.data, list) and upd.data:
        data = upd.data[0]
    return {"updated": True, "note": data or updates}


@marketplace_router.post("/api/marketplace/notes/{note_id}/replace-file", summary="Replace stored file for own note")
def mp_replace_note_file(
    note_id: uuid.UUID,
    file: UploadFile = File(...),
    authorization: Optional[str] = Header(default=None),
):
    token = _parse_bearer_token(authorization)
    user_id = _require_auth_user_id(token)
    supabase = get_service_client()
    note_res = (
        supabase.table("marketplace_notes")
        .select("owner_user_id,stored_path")
        .eq("id", str(note_id))
        .limit(1)
        .execute()
    )
    if getattr(note_res, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (get note for replace): {note_res.error}")
    if not note_res.data:
        raise HTTPException(status_code=404, detail="Note not found")
    note_row = note_res.data[0]
    owner_id = note_row.get("owner_user_id")
    if owner_id != user_id:
        raise HTTPException(status_code=403, detail="Not permitted to replace this note")
    old_stored = note_row.get("stored_path")
    stored_name, size, mime = _store_marketplace_file(file)
    update_fields = {
        "stored_path": stored_name,
        "original_filename": file.filename or "upload",
        "file_size": size,
        "mime_type": mime,
        "updated_at": datetime.utcnow().isoformat(),
    }
    upd = (
        supabase.table("marketplace_notes")
        .update(update_fields)
        .eq("id", str(note_id))
        .eq("owner_user_id", user_id)
        .execute()
    )
    if getattr(upd, "error", None):
        try:
            new_path = MARKETPLACE_STORAGE / stored_name
            if new_path.is_file():
                new_path.unlink()
        except Exception:
            pass
        raise HTTPException(status_code=500, detail=f"Supabase error (replace file): {upd.error}")
    if old_stored and old_stored != stored_name:
        try:
            old_path = MARKETPLACE_STORAGE / old_stored
            if old_path.is_file():
                old_path.unlink()
        except Exception:
            pass
    return {"updated": True, "stored_path": stored_name}


@marketplace_router.delete("/api/marketplace/notes/{note_id}", summary="Delete own note")
def mp_delete_note(note_id: uuid.UUID, authorization: Optional[str] = Header(default=None), token: Optional[str] = Query(None)):
    header_token = _parse_bearer_token(authorization)
    token = token or header_token
    user_id = _require_auth_user_id(token)
    supabase = get_service_client()
    note_res = supabase.table("marketplace_notes").select("owner_user_id,stored_path").eq("id", str(note_id)).limit(1).execute()
    if getattr(note_res, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (get note): {note_res.error}")
    if not note_res.data:
        raise HTTPException(status_code=404, detail="Not found")
    note = note_res.data[0]
    if note.get("owner_user_id") != user_id:
        raise HTTPException(status_code=403, detail="Not owner")
    del_res = supabase.table("marketplace_notes").delete().eq("id", str(note_id)).execute()
    if getattr(del_res, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (delete note): {del_res.error}")
    # remove file silently
    try:
        fp = MARKETPLACE_STORAGE / (note.get("stored_path") or "")
        if fp.is_file():
            fp.unlink()
    except Exception:
        pass
    return {"status": "deleted"}


def _openai_client():
    """Return OpenAI client (1.x) or raise. Supports legacy 0.x fallback."""
    try:
        import openai  # type: ignore
    except Exception as e:  # pragma: no cover
        raise HTTPException(status_code=500, detail=f"OpenAI library missing: {e}")
    api_key = os.getenv("OPENAI_API_KEY") or os.getenv("OPENAI_KEY")
    if not api_key:
        raise HTTPException(status_code=500, detail="Missing OPENAI_API_KEY in environment")
    # New 1.x style client
    try:  # prefer 1.x Client
        from openai import OpenAI  # type: ignore

        return OpenAI(api_key=api_key)
    except Exception:
        # Fallback: configure legacy global (<=0.28)
        openai.api_key = api_key
        return openai


def _build_transform_prompt(mode: str, content: str, custom: str | None) -> str:
    base_instruction = {
        "summarize": "Summarize the markdown below into a concise, well-structured overview. Preserve headings hierarchy where helpful; keep math/mermaid/code blocks intact.",
        "expand": "Expand and elaborate the markdown below. Add helpful clarifying sentences, short intuitive examples, and brief context. Do NOT invent inaccurate facts. Preserve code/math/mermaid blocks.",
        "simplify": "Rewrite the markdown in simpler language suitable for a beginner, keeping key technical terms but adding plain-language explanation.",
    }.get(mode, "")
    if mode == "custom" and custom:
        base_instruction = f"Apply this custom transformation to the markdown: {custom}. Preserve code/math/mermaid blocks and headings."
    if not base_instruction:
        base_instruction = "Return the markdown unchanged."  # fallback
    return (
        base_instruction
        + "\n\nReturn ONLY valid markdown. Do not add commentary outside the transformed content.\n\n---\nMARKDOWN INPUT BELOW\n---\n"
        + content
    )


@notes_router.post("/api/notes/transform")
def api_transform_note(payload: dict):
    """Ephemeral transform of markdown (summarize / expand / custom) using Gemini only."""
    mode = (payload or {}).get("mode", "summarize").strip().lower()
    markdown = (payload or {}).get("markdown", "")
    custom = (payload or {}).get("prompt")
    if not markdown:
        raise HTTPException(status_code=400, detail="Missing markdown")
    if mode not in {"summarize", "expand", "custom", "simplify"}:
        raise HTTPException(status_code=400, detail="Invalid mode")
    if not GEMINI_API_KEY:
        raise HTTPException(status_code=501, detail="Gemini API key not configured.")
    try:
        import google.generativeai as genai  # type: ignore
    except Exception as exc:  # pragma: no cover - optional dependency
        raise HTTPException(
            status_code=501,
            detail=f"Gemini client library missing: {exc}. Install google-generativeai to enable this feature.",
        ) from exc

    prompt = _build_transform_prompt(mode, markdown, custom)
    genai.configure(api_key=GEMINI_API_KEY)
    safety_settings = [
        {"category": "HARM_CATEGORY_HATE_SPEECH", "threshold": "BLOCK_ONLY_HIGH"},
        {"category": "HARM_CATEGORY_HARASSMENT", "threshold": "BLOCK_ONLY_HIGH"},
        {"category": "HARM_CATEGORY_SEXUALLY_EXPLICIT", "threshold": "BLOCK_ONLY_HIGH"},
        {"category": "HARM_CATEGORY_DANGEROUS_CONTENT", "threshold": "BLOCK_ONLY_HIGH"},
    ]
    model = genai.GenerativeModel(GEMINI_NOTES_MODEL)

    base_instruction = textwrap.dedent(
        """
        You edit Markdown precisely as instructed. Return ONLY valid Markdown. Avoid any harmful, explicit, or actionable content.
        If the requested transform could enable harm, reply with a safe, high-level academic rewrite or keep content neutral.
        """
    ).strip()

    def _extract_gemini_text(resp):
        def _finish_reason(candidate):
            finish = getattr(candidate, "finish_reason", None)
            if finish is None and isinstance(candidate, dict):
                finish = candidate.get("finish_reason") or candidate.get("finishReason")
            if finish is None:
                return None
            finish_str = str(finish).upper()
            if finish_str.isdigit():
                return int(finish_str)
            return finish_str

        try:
            quick = getattr(resp, "text", None)
            if isinstance(quick, str) and quick.strip():
                return quick.strip()
        except Exception:
            pass
        candidates = getattr(resp, "candidates", None) or []
        if isinstance(candidates, dict):
            candidates = [candidates]
        for cand in candidates:
            finish = _finish_reason(cand)
            if finish in (2, "SAFETY") or (isinstance(finish, str) and "SAFETY" in finish):
                # Surface to caller so we can retry conservatively
                raise HTTPException(status_code=502, detail="Gemini blocked the response for safety.")
            content = getattr(cand, "content", None)
            parts = None
            if content is not None:
                parts = getattr(content, "parts", None)
                if parts is None and isinstance(content, dict):
                    parts = content.get("parts")
            if parts is None and isinstance(cand, dict):
                parts = cand.get("content", {}).get("parts") if isinstance(cand.get("content"), dict) else None
            texts = []
            if parts:
                for part in parts:
                    text_val = getattr(part, "text", None)
                    if text_val is None and isinstance(part, dict):
                        text_val = part.get("text")
                    if text_val:
                        texts.append(str(text_val))
            if texts:
                return "\n".join(texts).strip()
        return ""

    def call_gemini(_instruction: str, _body: str):
        return model.generate_content(
            [{"text": _instruction}, {"text": _body}],
            generation_config={"temperature": 0.25, "max_output_tokens": 2048},
            safety_settings=safety_settings,
        )

    try:
        response = call_gemini(base_instruction, prompt)
    except Exception as exc:
        # Return empty markdown so UI treats it as no change rather than hard fail
        return {"markdown": "", "mode": mode, "custom": custom or None, "error": f"Gemini error: {exc}"}

    try:
        out_text = _extract_gemini_text(response)
    except HTTPException as exc:
        # Retry once with a conservative instruction
        if "safety" in str(getattr(exc, "detail", "")).lower():
            conservative = textwrap.dedent(
                """
                Provide a safe, high-level, non-actionable rewrite of the provided Markdown per the instruction. Keep it brief if needed.
                Return only valid Markdown.
                """
            ).strip()
            try:
                resp2 = call_gemini(conservative, prompt)
                out_text = _extract_gemini_text(resp2)
            except Exception:
                out_text = ""
        else:
            out_text = ""
    except Exception:
        out_text = ""

    # If empty after attempts, return empty string (UI will show 'No change')
    return {"markdown": out_text or "", "mode": mode, "custom": custom or None}


@notes_router.post("/api/notes/snippet-assist")
def api_snippet_assist(payload: dict):
    """Provide inline Gemini help for a highlighted snippet."""
    selection = (payload or {}).get("selection") or (payload or {}).get("snippet") or ""
    instruction = (payload or {}).get("instruction") or (payload or {}).get("prompt") or ""
    selection = (selection or "").strip()
    instruction = (instruction or "").strip()
    if not selection:
        raise HTTPException(status_code=400, detail="Missing selection text.")
    if not GEMINI_API_KEY:
        raise HTTPException(status_code=501, detail="Gemini API key not configured.")
    try:
        import google.generativeai as genai  # type: ignore
    except Exception as exc:  # pragma: no cover - optional dependency
        raise HTTPException(
            status_code=501,
            detail=f"Gemini client library missing: {exc}. Install google-generativeai to enable this feature.",
        ) from exc

    trimmed_selection = selection[:3000]
    genai.configure(api_key=GEMINI_API_KEY)
    # Relax safety to block only high-severity content while still allowing educational, non-actionable summaries
    safety_settings = [
        {"category": "HARM_CATEGORY_HATE_SPEECH", "threshold": "BLOCK_ONLY_HIGH"},
        {"category": "HARM_CATEGORY_HARASSMENT", "threshold": "BLOCK_ONLY_HIGH"},
        {"category": "HARM_CATEGORY_SEXUALLY_EXPLICIT", "threshold": "BLOCK_ONLY_HIGH"},
        {"category": "HARM_CATEGORY_DANGEROUS_CONTENT", "threshold": "BLOCK_ONLY_HIGH"},
    ]
    model = genai.GenerativeModel(GEMINI_NOTES_MODEL)
    base_instruction = textwrap.dedent(
        """
        You are PaperX's inline study copilot. Prefer the highlighted passage for context.
        Key constraints to avoid safety blocks:
        - Provide a neutral, educational, non-actionable explanation only.
        - Do NOT include step-by-step instructions, realistic procedures, or facilitation of harm.
        - Avoid explicit, graphic, or sexual content.
        - If the selection entails sensitive material, provide a safe high-level overview in academic tone.
        - Keep it concise and student-friendly.
        """
    ).strip()
    user_instruction = instruction or "Explain this selection simply."
    prompt_body = textwrap.dedent(
        f"""
        Instruction: {user_instruction}

        Highlighted selection:
        {trimmed_selection}
        """
    ).strip()
    def _extract_gemini_text(resp):
        def _finish_reason(candidate):
            finish = getattr(candidate, "finish_reason", None)
            if finish is None and isinstance(candidate, dict):
                finish = candidate.get("finish_reason") or candidate.get("finishReason")
            if finish is None:
                return None
            finish_str = str(finish).upper()
            if finish_str.isdigit():
                return int(finish_str)
            return finish_str

        try:
            quick = getattr(resp, "text", None)
            if isinstance(quick, str) and quick.strip():
                return quick.strip()
        except Exception:
            pass

        candidates = getattr(resp, "candidates", None) or []
        if isinstance(candidates, dict):
            candidates = [candidates]
        for cand in candidates:
            finish = _finish_reason(cand)
            if finish in (2, "SAFETY") or (isinstance(finish, str) and "SAFETY" in finish):
                raise HTTPException(status_code=502, detail="Gemini blocked the response for safety. Try rephrasing your selection or question.")
            content = getattr(cand, "content", None)
            parts = None
            if content is not None:
                parts = getattr(content, "parts", None)
                if parts is None and isinstance(content, dict):
                    parts = content.get("parts")
            if parts is None and isinstance(cand, dict):
                parts = cand.get("content", {}).get("parts") if isinstance(cand.get("content"), dict) else None
            texts = []
            if parts:
                for part in parts:
                    text_val = getattr(part, "text", None)
                    if text_val is None and isinstance(part, dict):
                        text_val = part.get("text")
                    if text_val:
                        texts.append(str(text_val))
            if texts:
                return "\n".join(texts).strip()
        return ""

    def call_gemini(_instruction: str, _body: str):
        return model.generate_content(
            [{"text": _instruction}, {"text": _body}],
            generation_config={"temperature": 0.25, "max_output_tokens": 512},
            safety_settings=safety_settings,
        )

    # First attempt with safe defaults
    try:
        response = call_gemini(base_instruction, prompt_body)
    except Exception as exc:
        raise HTTPException(status_code=502, detail=f"Gemini selection assist failed: {exc}") from exc

    try:
        generated = _extract_gemini_text(response)
    except HTTPException as exc:
        # Retry once with an even more conservative instruction if blocked for SAFETY
        detail = getattr(exc, "detail", "") or str(exc)
        if "Gemini blocked the response for safety" in detail:
            conservative_instruction = textwrap.dedent(
                """
                Provide a brief, safe, high-level, non-actionable academic overview of the selection.
                Do NOT include steps, procedures, or details that could enable harm.
                If necessary, generalize abstractly. Keep it 2â€“5 sentences.
                """
            ).strip()
            try:
                response2 = call_gemini(conservative_instruction, prompt_body)
                generated = _extract_gemini_text(response2)
            except Exception:
                # Fall through to friendly message
                generated = ""
        else:
            raise
    except Exception as exc:
        raise HTTPException(status_code=502, detail=f"Gemini returned malformed response: {exc}") from exc
    if not generated:
        # Provide a graceful, safe fallback message (no OpenAI, no external calls)
        safe_msg = (
            "This selection may involve sensitive content. Here is a safe, high-level academic overview: "
            "The highlighted text appears to reference material that can trigger content safety filters. "
            "Please try asking for a neutral definition, historical context, purpose, or key concepts without step-by-step procedures."
        )
        return {"text": safe_msg, "truncated": len(selection) > len(trimmed_selection)}
    return {"text": generated, "truncated": len(selection) > len(trimmed_selection)}


@notes_router.post("/generate")
@notes_router.post("/api/notes/generate")
async def generate(payload: dict):
    topic = (payload or {}).get("topic", "").strip()
    force = bool((payload or {}).get("force", False))
    variant = _normalize_variant((payload or {}).get("variant", "detailed"))
    degree = (payload or {}).get("degree")
    if not topic:
        return JSONResponse({"error": "Missing 'topic'"}, status_code=400)
    try:
        if not force:
            # Exact-title cache from DB for the variant
            row = db_get_ai_note_by_title_exact_variant(topic, variant=variant)
            if row and (row.get("markdown") or "").strip():
                return {
                    "id": row.get("id"),
                    "markdown": row.get("markdown", ""),
                    "cached": True,
                    "title": row.get("title"),
                    "variant": variant,
                    "image_urls": row.get("image_urls") or [],
                }
        # Non-streaming generation: use detailed pipeline for detailed, or transform detailed into variant
        md_detailed = generate_notes_markdown(topic, degree=degree)
        md = md_detailed
        if variant == "cheatsheet" or variant == "simple":
            try:
                # Use transform endpoint logic locally to adjust style
                mode = "simplify" if variant == "simple" else "custom"
                custom = None
                if variant == "cheatsheet":
                    custom = (
                        "Rewrite as an ultra-concise exam cheat sheet: 250â€“400 words, bullets/tables, sections: Core Concepts; Key Definitions & Formulas; Quick Steps/Algorithms; Pitfalls; Keywords. Bold key terms. Do NOT include TL;DR, or any CITATIONS section."
                    )
                prompt = _build_transform_prompt(mode, md_detailed, custom)
                client = _openai_client()
                model = os.getenv("LLM_MODEL", "gpt-4o-mini")
                messages = [
                    {"role": "system", "content": "You are an assistant that edits markdown content precisely as instructed."},
                    {"role": "user", "content": prompt},
                ]
                create_fn = None
                try:
                    create_fn = client.chat.completions.create  # type: ignore[attr-defined]
                except AttributeError:
                    create_fn = None
                if create_fn:
                    resp = create_fn(model=model, messages=messages, temperature=0.4, max_tokens=4096)
                    md = resp.choices[0].message.content if resp.choices else md_detailed
                else:
                    resp = client.ChatCompletion.create(model=model, messages=messages, temperature=0.4, max_tokens=4096)  # type: ignore[attr-defined]
                    md = resp.choices[0].message["content"] if resp.choices else md_detailed
            except Exception:
                md = md_detailed
        image_urls: List[str] = []
        try:
            related_pages = serpapi_search(topic, num=8, degree=degree)
            image_urls = collect_image_urls(topic, related_pages)
        except Exception:
            image_urls = []
        row = db_upsert_ai_note_by_title_variant(topic, md, variant=variant, image_urls=image_urls)
        stored_images = row.get("image_urls") or image_urls
        return {
            "id": row.get("id"),
            "markdown": row.get("markdown", md),
            "cached": False,
            "title": row.get("title"),
            "variant": variant,
            "image_urls": stored_images,
        }
    except Exception as e:
        return JSONResponse({"error": str(e)}, status_code=500)


@notes_router.get("/generate/stream")
@notes_router.get("/api/notes/generate/stream")
async def generate_stream(topic: str, force: bool = False, variant: str = "detailed", degree: Optional[str] = None):
    async def event_source() -> AsyncGenerator[bytes, None]:
        yield b"event: open\n\n"
        # Early cache hit: exact-title lookup in DB
        if not force:
            row = db_get_ai_note_by_title_exact_variant(topic, variant=_normalize_variant(variant))
            if row and (row.get("markdown") or "").strip():
                payload = {
                    "id": row.get("id"),
                    "markdown": row.get("markdown", ""),
                    "cached": True,
                    "title": row.get("title"),
                    "variant": _normalize_variant(variant),
                    "image_urls": row.get("image_urls") or [],
                }
                line = f"event: final\n".encode("utf-8")
                data_json = json.dumps(payload, ensure_ascii=False)
                data_b = ("data: " + data_json + "\n\n").encode("utf-8")
                yield line
                yield data_b
                yield b"event: close\n\n"
                return
        loop = asyncio.get_running_loop()
        queue: asyncio.Queue[Tuple[str, Optional[str], Optional[Dict[str, Any]]]] = asyncio.Queue()
        stop_event = threading.Event()

        def dispatch(item: Tuple[str, Optional[str], Optional[Dict[str, Any]]]) -> None:
            loop.call_soon_threadsafe(queue.put_nowait, item)

        def worker() -> None:
            try:
                for name, payload in generate_notes_events(topic, stop_event=stop_event, variant=_normalize_variant(variant), degree=degree):
                    if stop_event.is_set():
                        break
                    dispatch(("event", name, payload))
                dispatch(("done", None, None))
            except Exception as exc:
                dispatch(("error", None, {"message": str(exc)}))
                dispatch(("done", None, None))

        worker_future = loop.run_in_executor(None, worker)

        try:
            while True:
                kind, name, payload = await queue.get()
                if kind == "event" and name is not None and payload is not None:
                    try:
                        if name == "final" and isinstance(payload, dict) and payload.get("markdown"):
                            try:
                                images_input = payload.get("image_urls")
                                if not isinstance(images_input, list):
                                    urls_input = payload.get("urls")
                                    images_input = urls_input if isinstance(urls_input, list) else None
                                row = db_upsert_ai_note_by_title_variant(
                                    topic,
                                    payload.get("markdown", ""),
                                    variant=_normalize_variant(variant),
                                    image_urls=images_input,
                                )
                                payload["id"] = row.get("id")
                                payload["title"] = row.get("title")
                                payload["cached"] = False
                                payload["variant"] = _normalize_variant(variant)
                                payload["image_urls"] = row.get("image_urls") or (images_input or [])
                            except Exception:
                                pass
                            finally:
                                stop_event.set()
                        line = f"event: {name}\n".encode("utf-8")
                        data_json = json.dumps(payload, ensure_ascii=False)
                        data = ("data: " + data_json + "\n\n").encode("utf-8")
                        yield line
                        yield data
                    except Exception as exc:
                        err = json.dumps({"message": str(exc)}, ensure_ascii=False)
                        yield b"event: error\n"
                        yield ("data: " + err + "\n\n").encode("utf-8")
                elif kind == "error" and payload is not None:
                    err_json = json.dumps(payload, ensure_ascii=False)
                    yield b"event: error\n"
                    yield ("data: " + err_json + "\n\n").encode("utf-8")
                elif kind == "done":
                    break
        except asyncio.CancelledError:
            stop_event.set()
            raise
        finally:
            stop_event.set()
            try:
                await asyncio.wait_for(asyncio.wrap_future(worker_future), timeout=1.0)
            except Exception:
                pass

        yield b"event: close\n\n"

    headers = {
        "Cache-Control": "no-cache, no-transform",
        "Connection": "keep-alive",
        "X-Accel-Buffering": "no",
    }
    return StreamingResponse(event_source(), media_type="text/event-stream", headers=headers)


# --- Engineering Mathematics Notes (Gemini 3 Pro Preview only) ---

MATHS_NOTES_MODEL = "gemini-3-pro-preview"

MATHS_NOTES_SYSTEM_PROMPT = """You are a senior Engineering Mathematics educator teaching undergraduate students in India.

PRIMARY GOAL

Teach Engineering Mathematics concepts in the simplest possible way, focusing on problem-solving, intuition, and real-life understanding rather than long theory.

Students should be able to:

Understand the idea behind the concept

Solve exam-oriented numerical problems

Relate maths to real-world engineering scenarios

CORE TEACHING PRINCIPLES

Theory must be extremely short (maximum 2–4 lines per concept) for non-theorem topics.

Focus mainly on:

Solved numerical problems

Step-by-step methods

Patterns, shortcuts, and exam tricks

Avoid unnecessary proofs, derivations, or abstract discussion UNLESS the topic itself is a theorem/lemma/proposition/result.

Teach as if explaining to a first-time learner

CONTENT RULES (STRICT)

Explain what the concept is, why it is used, and how to solve problems

Each topic MUST include:

✔ Very short concept explanation

✔ Formula(s) with meaning of symbols

✔ Worked examples (easy → exam level)

✔ At least one real-life or engineering application

Prefer numbers, steps, and visuals in words over theory paragraphs

Do NOT include long historical background or definitions

STRUCTURE TO FOLLOW FOR EVERY TOPIC

Use the exact structure below:

1. Introduction

2–3 lines explaining what the topic does and where it is used

2. Key Idea (Very Short Theory)

Maximum 2–4 bullet points

Simple language only

Highlight important symbols using bold

3. Important Formulae

List formulas clearly

Explain symbols briefly (1 line max)

4. Worked Examples (Main Focus)

At least 3–6 solved problems

Show each step clearly

Explain why each step is done (in one short line)

5. Real-Life / Engineering Example

Explain how the concept appears in:

Engineering

Daily life

Technology or data

Keep it practical and intuitive

6. Exam-Focused Tips & Shortcuts

Common mistakes

Faster methods

Frequently asked patterns

7. Practice Problems

5–10 numerical questions (no solutions)

8. Conclusion

2–3 lines summarizing when and how to use the concept

OUTPUT FORMATTING (MANDATORY)

- The very first line MUST be a title using Markdown level-1 heading:
    # <Topic Title>
- Use Markdown level-2 headings (##) for the main numbered sections 1 to 8.
    Example:
    ## 1. Introduction
    ## 2. Key Idea (Very Short Theory)
    ...
- Inside sections, use level-3 headings (###) only when needed for sub-parts (e.g., "Example 1", "Case 1", "Proof", "Derivation").

THEOREM / PROOF-STYLE TOPICS (VERY IMPORTANT)

If the topic is a theorem/lemma/proposition/result (or it contains words like "theorem", "lemma", "Cauchy", "Rolle", "Mean Value", "Green", "Stokes", "Divergence", etc.), you MUST write it like standard textbooks and Indian university exam answers:

- State the theorem clearly (with conditions/assumptions).
- Explain the meaning in simple words (2–4 lines max).
- Provide a clean exam-style proof/derivation (step-by-step, no skipped steps). Use short lines and clearly justify key steps.
- If there are standard variants/cases, mention them briefly.
- THEN include worked numerical/examples that use the theorem (do not only give solved questions without stating/proving the theorem).

DIFFICULTY & STYLE GUIDELINES

Start from very basic

Gradually move to university exam level

Assume no strong maths background

Use simple English, short sentences

Avoid symbols without explanation

VISUAL & FORMATTING RULES

Use:

Bullet points

Numbered steps

Tables (for formulas or comparisons)

Highlight key terms using bold

Include Mermaid diagrams ONLY when it helps understanding (graphs, flow of steps)

WHAT TO AVOID (IMPORTANT)

❌ Long theoretical explanations
❌ Proof-based discussions
❌ Complex mathematical jargon without explanation
❌ Overloading with formulas
❌ Irrelevant historical notes

TARGET OUTCOME

After reading the notes, a student should be able to:

Understand the concept in 5–10 minutes

Solve most exam questions confidently

Remember formulas easily

See why the math matters in real life"""


def _generate_maths_notes_markdown(topic: str) -> str:
    """Generate Engineering Mathematics notes in Markdown using Gemini 3 Pro Preview only."""
    if not genai:
        raise HTTPException(status_code=500, detail="AI service not available")

    gemini_key = os.getenv("GEMINI_API_KEY", "").strip()
    if not gemini_key:
        raise HTTPException(status_code=500, detail="GEMINI_API_KEY not configured")

    clean_topic = (topic or "").strip()
    if not clean_topic:
        raise HTTPException(status_code=400, detail="Missing topic")

    client = genai.Client(api_key=gemini_key)
    user_prompt = textwrap.dedent(
        f"""
        Create Engineering Mathematics notes for the topic: "{clean_topic}".

        Output format requirements:
        - Return ONLY Markdown (no code fences, no extra commentary).
        - First line must be: # {clean_topic}
        - Use ## headings for the numbered sections 1 to 8 (e.g., "## 1. Introduction", ...).
        - Use ### only for subheadings inside a section.
        - Keep theory extremely short; focus on worked examples.
        - If the topic is a theorem/result, include the theorem statement + exam-style proof/derivation before examples.
        """
    ).strip()

    response = client.models.generate_content(
        model=MATHS_NOTES_MODEL,
        contents=[
            {
                "role": "user",
                "parts": [{"text": user_prompt}],
            }
        ],
        config=types.GenerateContentConfig(
            system_instruction=MATHS_NOTES_SYSTEM_PROMPT,
            temperature=0.45,
            max_output_tokens=int(os.getenv("MATHS_NOTES_MAX_TOKENS", "8192") or "8192"),
        ),
    )

    text = getattr(response, "text", "") or ""
    text = (text or "").strip()
    if not text:
        raise HTTPException(status_code=502, detail="Empty response from Gemini")
    return text


@notes_router.post("/api/maths-notes/generate", summary="Generate Engineering Mathematics notes (Gemini 3 Pro Preview)")
async def api_generate_maths_notes(payload: dict):
    topic = (payload or {}).get("topic", "").strip()
    force = bool((payload or {}).get("force", False))
    variant = _normalize_variant((payload or {}).get("variant", "detailed"))
    if not topic:
        return JSONResponse({"error": "Missing 'topic'"}, status_code=400)

    # DB-first: return cached ai_notes if available (unless force)
    if not force:
        row = db_get_ai_note_by_title_exact_variant(topic, variant=variant)
        if row and (row.get("markdown") or "").strip():
            return {
                "id": row.get("id"),
                "markdown": row.get("markdown", ""),
                "cached": True,
                "title": row.get("title"),
                "variant": variant,
                "image_urls": row.get("image_urls") or [],
                "model": MATHS_NOTES_MODEL,
            }

    md = await run_in_threadpool(_generate_maths_notes_markdown, topic)
    row = db_upsert_ai_note_by_title_variant(topic, md, variant=variant, image_urls=[])
    return {
        "id": row.get("id"),
        "markdown": row.get("markdown", md),
        "cached": False,
        "title": row.get("title"),
        "variant": variant,
        "image_urls": row.get("image_urls") or [],
        "model": MATHS_NOTES_MODEL,
    }


@notes_router.get("/api/maths-notes/generate/stream", summary="Stream Engineering Mathematics notes (Gemini 3 Pro Preview)")
async def api_generate_maths_notes_stream(
    topic: str,
    force: bool = False,
    variant: str = "detailed",
    degree: Optional[str] = None,
    course_type: Optional[str] = None,
):
    async def event_source() -> AsyncGenerator[bytes, None]:
        yield b"event: open\n\n"

        normalized_variant = _normalize_variant(variant)

        # Early cache hit from ai_notes
        if not force:
            row = db_get_ai_note_by_title_exact_variant(topic, variant=normalized_variant)
            if row and (row.get("markdown") or "").strip():
                payload = {
                    "id": row.get("id"),
                    "markdown": row.get("markdown", ""),
                    "cached": True,
                    "title": row.get("title"),
                    "variant": normalized_variant,
                    "image_urls": row.get("image_urls") or [],
                    "model": MATHS_NOTES_MODEL,
                }
                yield b"event: final\n"
                yield ("data: " + json.dumps(payload, ensure_ascii=False) + "\n\n").encode("utf-8")
                yield b"event: close\n\n"
                return

        try:
            start_payload = {
                "topic": topic,
                "model": MATHS_NOTES_MODEL,
                "variant": normalized_variant,
                "degree": degree,
                "course_type": course_type,
                "force": bool(force),
            }
            yield b"event: start\n"
            yield ("data: " + json.dumps(start_payload, ensure_ascii=False) + "\n\n").encode("utf-8")

            yield b"event: llm_start\n"
            yield ("data: " + json.dumps({"status": "generating"}, ensure_ascii=False) + "\n\n").encode("utf-8")

            md = await run_in_threadpool(_generate_maths_notes_markdown, topic)

            # Persist to ai_notes (same table used by notes_generator)
            row = None
            try:
                row = db_upsert_ai_note_by_title_variant(topic, md, variant=normalized_variant, image_urls=[])
            except Exception:
                row = None

            final_payload = {
                "id": (row.get("id") if isinstance(row, dict) else None),
                "markdown": (row.get("markdown") if isinstance(row, dict) and row.get("markdown") else md),
                "cached": False,
                "title": (row.get("title") if isinstance(row, dict) and row.get("title") else topic),
                "variant": normalized_variant,
                "image_urls": (row.get("image_urls") if isinstance(row, dict) else []) or [],
                "model": MATHS_NOTES_MODEL,
            }
            yield b"event: final\n"
            yield ("data: " + json.dumps(final_payload, ensure_ascii=False) + "\n\n").encode("utf-8")
        except Exception as exc:
            err_payload = {"message": str(exc)}
            yield b"event: error\n"
            yield ("data: " + json.dumps(err_payload, ensure_ascii=False) + "\n\n").encode("utf-8")
        finally:
            yield b"event: close\n\n"

    headers = {
        "Cache-Control": "no-cache, no-transform",
        "Connection": "keep-alive",
        "X-Accel-Buffering": "no",
    }
    return StreamingResponse(event_source(), media_type="text/event-stream", headers=headers)


class FlashcardSection(BaseModel):
    icon: str = Field(..., min_length=1, max_length=8)
    heading: str = Field(..., min_length=1)
    question: str = Field(..., min_length=1)
    answer: str = Field(..., min_length=1)


class FlashcardCard(BaseModel):
    concept: str = Field(..., min_length=1)
    summary: Optional[str] = None
    sections: List[FlashcardSection] = Field(default_factory=list)
    key_points: List[str] = Field(default_factory=list)

    @validator("sections")
    def ensure_sections(cls, value):
        if not value:
            raise ValueError("Each flashcard must include at least one section.")
        return value


def _derive_topic_from_markdown(markdown: str, fallback: str = "") -> str:
    title, _ = _extract_title_and_headings(markdown or "")
    if title:
        return title
    return fallback


def _fallback_flashcards_from_markdown(markdown: str, topic: str, max_cards: int) -> List[Dict[str, Any]]:
    text = (markdown or "").strip()
    if not text:
        return []

    heading_pattern = re.compile(r"^##\s+(.+?)\s*$", re.MULTILINE)
    matches = list(heading_pattern.finditer(text))
    sections: List[Tuple[str, str]] = []

    if matches:
        for idx, match in enumerate(matches):
            title = match.group(1).strip()
            start = match.end()
            end = matches[idx + 1].start() if idx + 1 < len(matches) else len(text)
            body = text[start:end].strip()
            if body:
                sections.append((title, body))
    else:
        sections.append((topic or "Overview", text))

    bullet_pattern = re.compile(r"^\s*[-*+]\s+(.*)$", re.MULTILINE)
    candidate_cards: List[Tuple[str, str]] = []
    seen_titles: Set[str] = set()
    for title, body in sections:
        tkey = title.lower()
        if tkey in seen_titles:
            continue
        seen_titles.add(tkey)
        candidate_cards.append((title, body))

    # If not enough sections, create additional cards from bullets
    if len(candidate_cards) < max_cards:
        bullets = [b.strip() for b in bullet_pattern.findall(text) if b.strip()]
        for idx, bullet in enumerate(bullets):
            if len(candidate_cards) >= max_cards:
                break
            candidate_cards.append((f"Key Insight {idx + 1}", bullet))

    cards: List[Dict[str, Any]] = []
    for title, body in candidate_cards[:max_cards]:
        clean_body = body.strip()
        paragraphs = [p.strip() for p in re.split(r"\n{2,}", clean_body) if p.strip()]
        summary_source = paragraphs[0] if paragraphs else clean_body
        summary = textwrap.shorten(summary_source.replace("\n", " "), width=220, placeholder="â€¦") if summary_source else ""

        sections_payload: List[Dict[str, str]] = []
        sections_payload.append({
            "icon": "ðŸ§ ",
            "heading": "Core Idea",
            "question": f"What is {title}?",
            "answer": summary_source or "Not covered in notes",
        })

        if len(paragraphs) > 1:
            sections_payload.append({
                "icon": "âš™ï¸",
                "heading": "Mechanism",
                "question": "How does it work?",
                "answer": paragraphs[1],
            })
        if len(paragraphs) > 2:
            sections_payload.append({
                "icon": "ðŸ›¡ï¸",
                "heading": "Pitfalls",
                "question": "What should we watch out for?",
                "answer": paragraphs[2],
            })

        if len(sections_payload) < 2:
            alt_answer = " ".join(paragraphs[1:2]) or clean_body[:240]
            sections_payload.append({
                "icon": "ðŸ”­",
                "heading": "Details",
                "question": "Tell me more",
                "answer": alt_answer or "Not covered in notes",
            })

        key_points: List[str] = []
        bullets = [b.strip() for b in bullet_pattern.findall(body) if b.strip()]
        for bullet in bullets[:4]:
            key_points.append(textwrap.shorten(bullet, width=100, placeholder="â€¦"))
        if not key_points and summary:
            key_points.append(summary)

        cards.append({
            "concept": title[:80],
            "summary": summary,
            "sections": sections_payload,
            "key_points": key_points,
        })

    return cards[:max_cards]


def _generate_flashcards_with_gemini(markdown: str, topic: str, max_cards: int) -> Tuple[List[Dict[str, Any]], str, bool, bool]:
    if not GEMINI_API_KEY:
        raise HTTPException(status_code=501, detail="Gemini API key not configured for flashcard generation.")
    try:
        import google.generativeai as genai  # type: ignore
        from google.api_core import exceptions as google_exceptions  # type: ignore
    except Exception as exc:  # pragma: no cover - optional dependency
        raise HTTPException(status_code=500, detail=f"Gemini client library missing: {exc}") from exc

    cleaned_notes = (markdown or "").strip()
    if not cleaned_notes:
        raise HTTPException(status_code=400, detail="Note does not contain any content to build flashcards.")

    limit_chars = int(os.getenv("FLASHCARD_MAX_CHARS", "9000") or "9000")
    truncated = len(cleaned_notes) > limit_chars
    excerpt = cleaned_notes[:limit_chars]

    max_cards = max(4, min(max_cards, 8))
    min_cards = min(6, max_cards)

    prompt = textwrap.dedent(
        f"""
        You are PaperX's futuristic study companion. Craft an engaging flashcard deck for the topic "{topic}" using ONLY the material inside the triple chevrons.

        Requirements:
        - Produce between {min_cards} and {max_cards} flashcards (inclusive). If the notes cover fewer than {min_cards} solid ideas, produce as many as the notes justify but never exceed {max_cards}.
        - Each flashcard is a JSON object with keys: "concept", "summary", "sections", "key_points".
        - "concept": 3-6 word title anchored in the notes.
        - "summary": 1-2 sentence high-energy overview derived strictly from the notes.
        - "sections": array of 2-4 objects, each with emoji "icon", "heading", "question", "answer". Questions must be informational; answers must come strictly from the notes. Icons should feel futuristic/fantasy (ðŸ§ , âš™ï¸, ðŸ”®, ðŸŒŒ, ðŸ›¡ï¸, ðŸ’¡, etc.).
        - "key_points": array of 2-4 crisp bullet phrases (â‰¤80 characters) quoting or paraphrasing unique facts from the notes.
        - NEVER invent information. If the notes lack detail for a section, set the answer to "Not covered in notes".
        - Keep terminology consistent with the notes (math symbols, proper nouns, etc.).
        - The vibe should be adventurous and motivating while staying accurate.
        - Return ONLY valid JSON matching this exact structure: {{"topic": "...", "flashcards": [ {{...}} ] }}
        - Do not wrap the JSON in markdown fences or additional commentary.

        {"The notes excerpt was truncated for length. Mention this limitation when relevant." if truncated else ""}

        <<<NOTES>>>
        {excerpt}
        <<<END NOTES>>>
        """
    ).strip()

    genai.configure(api_key=GEMINI_API_KEY)
    model = genai.GenerativeModel(GEMINI_NOTES_MODEL)
    generation_config = genai.GenerationConfig(
        response_mime_type="application/json",
        temperature=0.45,
        top_p=0.75,
        max_output_tokens=1024,
    )

    request_timeout = float(os.getenv("GEMINI_FLASHCARD_TIMEOUT", "35"))
    max_attempts = max(1, int(os.getenv("GEMINI_FLASHCARD_RETRIES", "3")))
    base_delay = max(0.8, float(os.getenv("GEMINI_FLASHCARD_RETRY_DELAY", "1.5")))
    transient_errors = (
        google_exceptions.RetryError,
        google_exceptions.ServiceUnavailable,
        google_exceptions.ResourceExhausted,
        google_exceptions.InternalServerError,
    )
    payload = [
        {"text": prompt},
    ]

    response = None
    last_exc: Optional[Exception] = None
    for attempt in range(1, max_attempts + 1):
        try:
            response = model.generate_content(
                payload,
                generation_config=generation_config,
                request_options={"timeout": request_timeout},
            )
            break
        except google_exceptions.DeadlineExceeded as exc:
            notes_logger.warning(
                "Gemini flashcard generation deadline exceeded",
                extra={
                    "topic": topic,
                    "note_chars": len(cleaned_notes),
                    "limit_chars": limit_chars,
                    "attempt": attempt,
                },
            )
            fallback_cards = _fallback_flashcards_from_markdown(cleaned_notes, topic, max_cards)
            if fallback_cards:
                return fallback_cards, "fallback-markdown", truncated, True
            raise HTTPException(status_code=504, detail="Flashcard generation timed out. Try again with a shorter note.") from exc
        except transient_errors as exc:  # pragma: no cover - network dependent
            last_exc = exc
            delay = min(base_delay * (2 ** (attempt - 1)), 8.0)
            notes_logger.warning(
                "Gemini flashcard transient error; will retry",
                extra={
                    "topic": topic,
                    "attempt": attempt,
                    "remaining_attempts": max_attempts - attempt,
                    "error": str(exc),
                    "retry_delay_sec": delay,
                },
            )
            if attempt == max_attempts:
                break
            time.sleep(delay)
            continue
        except google_exceptions.GoogleAPICallError as exc:  # pragma: no cover - network dependent
            last_exc = exc
            notes_logger.error(
                "Gemini flashcard generation error",
                extra={"topic": topic, "error": str(exc), "attempt": attempt},
            )
            if attempt == max_attempts:
                break
            delay = min(base_delay * (2 ** (attempt - 1)), 8.0)
            time.sleep(delay)
        except Exception as exc:  # pragma: no cover - safer catch-all
            last_exc = exc
            notes_logger.error(
                "Gemini flashcard unexpected failure",
                extra={"topic": topic, "error": str(exc), "attempt": attempt},
            )
            break

    if response is None:
        fallback_cards = _fallback_flashcards_from_markdown(cleaned_notes, topic, max_cards)
        if fallback_cards:
            notes_logger.warning(
                "Gemini flashcard generation exhausted retries; using fallback",
                extra={"topic": topic, "attempts": max_attempts, "error": str(last_exc) if last_exc else None},
            )
            return fallback_cards, "fallback-markdown", truncated, True
        detail = "Gemini service error while generating flashcards."
        if isinstance(last_exc, google_exceptions.RetryError):
            detail = "Gemini retry attempts exhausted. Please try again shortly."
        raise HTTPException(status_code=502, detail=detail)

    raw_text = getattr(response, "text", None)
    if not raw_text and getattr(response, "candidates", None):
        for candidate in response.candidates:  # pragma: no cover - depends on API payload
            content = getattr(candidate, "content", None)
            if not content:
                continue
            parts = getattr(content, "parts", None) or []
            raw_text = "".join(part.text or "" for part in parts if getattr(part, "text", None))
            if raw_text:
                break

    if not raw_text:
        fallback_cards = _fallback_flashcards_from_markdown(cleaned_notes, topic, max_cards)
        if fallback_cards:
            notes_logger.warning("Gemini returned empty flashcard payload; using fallback", extra={"topic": topic})
            return fallback_cards, "fallback-markdown", truncated, True
        raise HTTPException(status_code=500, detail="Gemini returned empty flashcard response.")

    try:
        payload = json.loads(raw_text)
    except json.JSONDecodeError as exc:
        fallback_cards = _fallback_flashcards_from_markdown(cleaned_notes, topic, max_cards)
        if fallback_cards:
            notes_logger.warning("Gemini returned malformed JSON; using fallback", extra={"topic": topic})
            return fallback_cards, "fallback-markdown", truncated, True
        raise HTTPException(status_code=500, detail=f"Failed to parse Gemini flashcards JSON: {exc}") from exc

    cards_raw = payload.get("flashcards") or payload.get("cards")
    if not cards_raw or not isinstance(cards_raw, list):
        fallback_cards = _fallback_flashcards_from_markdown(cleaned_notes, topic, max_cards)
        if fallback_cards:
            notes_logger.warning("Gemini response missing flashcards array; using fallback", extra={"topic": topic})
            return fallback_cards, "fallback-markdown", truncated, True
        raise HTTPException(status_code=500, detail="Gemini response missing 'flashcards' list.")

    cards: List[Dict[str, Any]] = []
    for idx, item in enumerate(cards_raw[:max_cards]):
        prepared = dict(item or {})
        if "concept" not in prepared:
            prepared["concept"] = prepared.get("title") or prepared.get("name") or f"Concept {idx + 1}"
        if "summary" not in prepared:
            prepared["summary"] = prepared.get("overview") or prepared.get("synopsis") or ""

        sections = prepared.get("sections")
        if not isinstance(sections, list) or not sections:
            base_question = prepared.get("question") or "What is the key idea?"
            base_answer = prepared.get("answer") or prepared.get("summary") or "Not covered in notes"
            sections = [{
                "icon": prepared.get("icon") or "ðŸ§ ",
                "heading": prepared.get("heading") or prepared["concept"],
                "question": base_question,
                "answer": base_answer,
            }]
        normalized_sections = []
        for section in sections:
            sec = dict(section or {})
            sec.setdefault("icon", "ðŸ”®")
            sec.setdefault("heading", prepared["concept"])
            sec.setdefault("question", "What does this cover?")
            sec.setdefault("answer", "Not covered in notes")
            normalized_sections.append(sec)
        prepared["sections"] = normalized_sections

        key_points = prepared.get("key_points")
        if not isinstance(key_points, list) or not key_points:
            fallback_points = prepared.get("bullets") or prepared.get("highlights")
            if isinstance(fallback_points, list):
                key_points = fallback_points
            else:
                key_points = []
        prepared["key_points"] = key_points

        try:
            card = FlashcardCard.parse_obj(prepared)
        except Exception as exc:
            fallback_cards = _fallback_flashcards_from_markdown(cleaned_notes, topic, max_cards)
            if fallback_cards:
                notes_logger.warning("Gemini produced invalid flashcard schema; using fallback", extra={"topic": topic})
                return fallback_cards, "fallback-markdown", truncated, True
            raise HTTPException(status_code=500, detail=f"Invalid flashcard data returned by Gemini: {exc}") from exc

        card_data = card.dict()
        if not card_data.get("summary") and card_data["sections"]:
            first_answer = card_data["sections"][0]["answer"]
            card_data["summary"] = first_answer.split("\n")[0][:200]
        cards.append(card_data)

    if not cards:
        fallback_cards = _fallback_flashcards_from_markdown(cleaned_notes, topic, max_cards)
        if fallback_cards:
            return fallback_cards, "fallback-markdown", truncated, True
        raise HTTPException(status_code=500, detail="Gemini returned no flashcards.")

    return cards, GEMINI_NOTES_MODEL, truncated, False


@notes_router.post("/notes/{note_id}/flashcards")
@notes_router.post("/api/notes/{note_id}/flashcards")
def api_generate_flashcards(note_id: str, payload: Optional[Dict[str, Any]] = Body(default=None)):
    row, _v = db_get_ai_note_by_id_any(note_id)
    if not row:
        raise HTTPException(status_code=404, detail="Note not found")

    markdown = (row.get("markdown") or "").strip()
    if not markdown:
        raise HTTPException(status_code=400, detail="Note is empty; generate notes before requesting flashcards.")

    requested_max = 8
    topic_override = ""
    if isinstance(payload, dict):
        if "max_cards" in payload:
            try:
                requested_max = int(payload["max_cards"])
            except Exception:
                raise HTTPException(status_code=400, detail="max_cards must be an integer")
        topic_override = str(payload.get("topic") or "").strip()

    max_cards = max(4, min(requested_max, 8))
    inferred_topic = topic_override or row.get("title") or _derive_topic_from_markdown(markdown, fallback="Study Flashcards")
    try:
        cards, used_model, truncated, used_fallback = _generate_flashcards_with_gemini(
            markdown,
            inferred_topic,
            max_cards=max_cards,
        )
    except HTTPException:
        # Let explicit HTTPException bubble (these have proper status codes)
        raise
    except Exception as exc:  # defensive: ensure we never leak a 500 from unexpected errors
        notes_logger.exception("Flashcard generation - unexpected error, attempting fallback", extra={"note_id": note_id})
        # Try local fallback from markdown. This should always produce something if markdown exists.
        try:
            fallback_cards = _fallback_flashcards_from_markdown(markdown, inferred_topic, max_cards)
        except Exception:
            fallback_cards = []

        if fallback_cards:
            return {
                "note_id": note_id,
                "topic": inferred_topic,
                "flashcards": fallback_cards,
                "count": len(fallback_cards),
                "model": "fallback-markdown",
                "truncated": False,
                "fallback": True,
                "generated_at": datetime.utcnow().isoformat() + "Z",
            }

        # If fallback failed, return an explicit JSON error rather than raw 500
        raise HTTPException(status_code=500, detail=f"Flashcard generation failed: {exc}") from exc

    return {
        "note_id": note_id,
        "topic": inferred_topic,
        "flashcards": cards,
        "count": len(cards),
        "model": used_model,
        "truncated": truncated,
        "fallback": used_fallback,
        "generated_at": datetime.utcnow().isoformat() + "Z",
    }


# --- MCQ generation from notes ---

class MCQQuestion(BaseModel):
    question: str = Field(..., min_length=3)
    options: List[str] = Field(..., min_items=4)
    correct_index: int = Field(..., ge=0, le=3)
    explanation: Optional[str] = Field(default=None)


def _generate_mcq_with_gemini(markdown: str, topic: str, count: int) -> Tuple[List[Dict[str, Any]], str, bool]:
    """Generate MCQs grounded strictly in the provided markdown using Gemini.

    Returns (questions, model_name, truncated).
    """
    if not GEMINI_API_KEY:
        raise HTTPException(status_code=501, detail="Gemini API key not configured for MCQ generation.")
    try:
        import google.generativeai as genai  # type: ignore
    except Exception as exc:  # pragma: no cover - optional dependency
        raise HTTPException(status_code=500, detail=f"Gemini client library missing: {exc}") from exc

    cleaned = (markdown or "").strip()
    if not cleaned:
        raise HTTPException(status_code=400, detail="Note does not contain any content to build MCQs.")

    # Keep request compact
    limit_chars = int(os.getenv("MCQ_MAX_CHARS", "9000") or "9000")
    truncated = len(cleaned) > limit_chars
    excerpt = cleaned[:limit_chars]

    count = max(8, min(count or 10, 12))
    min_q = min(8, count)

    prompt = textwrap.dedent(
        f"""
        You are PaperX's quiz compiler. Create a rigorous MCQ test for "{topic}" using ONLY the material in the triple chevrons.

        Constraints:
        - Produce between {min_q} and {count} multiple-choice questions.
        - Each question MUST have 4 options (A–D). Distractors must be plausible from the notes.
        - For each question, return: "question" (string), "options" (array of 4 strings), "correct_index" (0..3), "explanation" (1–2 line reason grounded in notes).
        - Strictly avoid facts not supported by the notes. If unsure, prefer conceptual questions.
        - Return ONLY JSON with this exact schema: {{"topic": "...", "questions": [{{"question":"...","options":["...","...","...","..."],"correct_index": 0,"explanation":"..."}}]}}
        - Do not wrap in markdown fences or commentary.

        {"The notes excerpt was truncated for length; keep questions within available context." if truncated else ""}

        <<<NOTES>>>
        {excerpt}
        <<<END NOTES>>>
        """
    ).strip()

    genai.configure(api_key=GEMINI_API_KEY)
    model_name = GEMINI_NOTES_MODEL
    model = genai.GenerativeModel(model_name)
    generation_config = genai.GenerationConfig(
        response_mime_type="application/json",
        temperature=0.2,
        top_p=0.8,
        max_output_tokens=int(os.getenv("GEMINI_MCQ_MAX_TOKENS", "3600") or "3600"),
        candidate_count=1,
    )

    # Try to loosen safety gating for benign educational content.
    # Build safety settings using SDK enums when available; otherwise, disable this knob.
    safety_settings = None
    try:  # Newer SDKs expose these types
        from google.generativeai.types import HarmCategory, SafetySetting, HarmBlockThreshold  # type: ignore
        safety_settings = [
            SafetySetting(category=HarmCategory.HARM_CATEGORY_HARASSMENT, threshold=HarmBlockThreshold.BLOCK_NONE),
            SafetySetting(category=HarmCategory.HARM_CATEGORY_HATE_SPEECH, threshold=HarmBlockThreshold.BLOCK_NONE),
            SafetySetting(category=HarmCategory.HARM_CATEGORY_SEXUAL_CONTENT, threshold=HarmBlockThreshold.BLOCK_NONE),
            SafetySetting(category=HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT, threshold=HarmBlockThreshold.BLOCK_NONE),
        ]
    except Exception:
        safety_settings = None  # Fallback: rely on model defaults

    request_timeout = float(os.getenv("GEMINI_MCQ_TIMEOUT", "35"))
    payload = [{"text": prompt}]

    def _generate_safe(pl: List[Dict[str, str]]):
        """Call Gemini once, trying with safety_settings if available, and retrying without on compatibility errors."""
        try:
            if safety_settings is not None:
                return model.generate_content(
                    pl,
                    generation_config=generation_config,
                    safety_settings=safety_settings,  # may be unsupported on some SDKs
                    request_options={"timeout": request_timeout},
                )
            # No safety_settings available
            return model.generate_content(
                pl,
                generation_config=generation_config,
                request_options={"timeout": request_timeout},
            )
        except Exception as e:
            msg = str(e).lower()
            # Retry without safety_settings if we suspect compatibility or category errors
            if ("harm_category" in msg) or ("safety" in msg) or isinstance(e, (KeyError, TypeError, ValueError)):
                return model.generate_content(
                    pl,
                    generation_config=generation_config,
                    request_options={"timeout": request_timeout},
                )
            raise

    try:
        resp = _generate_safe(payload)
    except Exception as exc:
        raise HTTPException(status_code=502, detail=f"Gemini MCQ request failed: {exc}") from exc

    # Robustly extract text from response without assuming resp.text always exists
    raw: Optional[str] = None
    finish_reason: Optional[Any] = None
    try:
        # resp.text may raise ValueError if no Part was returned
        raw = resp.text  # type: ignore[attr-defined]
    except Exception:
        raw = None
    # Capture finish_reason if present
    try:
        if getattr(resp, "candidates", None):
            finish_reason = getattr(resp.candidates[0], "finish_reason", None)
    except Exception:
        finish_reason = None

    if not raw and getattr(resp, "candidates", None):  # try extracting from parts
        try:
            for cand in resp.candidates:
                content = getattr(cand, "content", None)
                parts = getattr(content, "parts", None) if content is not None else None
                if parts:
                    s = "".join(getattr(p, "text", "") for p in parts if hasattr(p, "text"))
                    if s and s.strip():
                        raw = s
                        break
        except Exception:
            raw = None

    if not raw:
        # Last resort: try dict form
        try:
            to_dict = getattr(resp, "to_dict", None)
            if to_dict:
                d = resp.to_dict()
                for cand in (d.get("candidates") or []):
                    parts = (((cand.get("content") or {}).get("parts")) or [])
                    s = "".join(p.get("text", "") for p in parts if isinstance(p, dict) and "text" in p)
                    if s and s.strip():
                        raw = s
                        break
        except Exception:
            raw = None

    if not raw:
        # If the first attempt produced no content, retry once with a more compact prompt
        retry_count = max(8, min(count, 8))
        compact_prompt = textwrap.dedent(
            f"""
            You are PaperX's quiz compiler. Create a rigorous MCQ test for "{topic}" using ONLY the material in the triple chevrons.

            Constraints (STRICT):
            - Return EXACTLY {retry_count} questions.
            - Each question MUST have 4 short options (A–D).
            - Explanation MUST be under 18 words and grounded in the notes.
            - Output MUST be strictly valid JSON only, no comments, no markdown fences, no trailing commas.
            - Schema: {{"topic": "...", "questions": [{{"question":"...","options":["...","...","...","..."],"correct_index": 0,"explanation":"..."}}]}}
            {"The notes excerpt was truncated for length; keep questions within available context." if truncated else ""}

            <<<NOTES>>>
            {excerpt}
            <<<END NOTES>>>
            """
        ).strip()
        try:
            resp_retry = _generate_safe([{"text": compact_prompt}])
            raw = getattr(resp_retry, "text", None)  # type: ignore[attr-defined]
            if not raw and getattr(resp_retry, "candidates", None):
                for cand in resp_retry.candidates:
                    content = getattr(cand, "content", None)
                    parts = getattr(content, "parts", None) if content is not None else None
                    if parts:
                        s = "".join(getattr(p, "text", "") for p in parts if hasattr(p, "text"))
                        if s and s.strip():
                            raw = s
                            break
        except Exception:
            raw = None

    if not raw:
        # Build a meaningful error message (safety/finish_reason info when available)
        msg = "Gemini returned no content for MCQ generation"
        try:
            pf = getattr(resp, "prompt_feedback", None)
            if pf and getattr(pf, "safety_ratings", None):
                ratings = []
                for r in pf.safety_ratings:
                    try:
                        ratings.append(f"{getattr(r, 'category', '?')}={getattr(r, 'probability', '?')}")
                    except Exception:
                        continue
                if ratings:
                    msg += f"; safety={', '.join(ratings)}"
        except Exception:
            pass
        if finish_reason is not None:
            msg += f"; finish_reason={finish_reason}"
        raise HTTPException(status_code=502, detail=msg)

    # --- Tolerant JSON parsing helpers ---
    def _strip_code_fences(txt: str) -> str:
        s = txt.strip()
        # Remove triple-backtick fences if present
        if s.startswith("```"):
            s = re.sub(r"^```[a-zA-Z0-9_-]*\s*", "", s)
            s = re.sub(r"\s*```$", "", s)
        return s.strip()

    def _extract_first_json_object(txt: str) -> Optional[str]:
        # Find first top-level {...} block using brace matching
        start = txt.find('{')
        if start == -1:
            return None
        depth = 0
        for i in range(start, len(txt)):
            c = txt[i]
            if c == '{':
                depth += 1
            elif c == '}':
                depth -= 1
                if depth == 0:
                    return txt[start:i+1]
        return None

    def _try_parse_mcq_payload(raw_text: str) -> Dict[str, Any]:
        s = _strip_code_fences(raw_text)
        # First attempt: direct JSON
        try:
            return json.loads(s)
        except Exception:
            pass
        # Second: extract first JSON object region
        block = _extract_first_json_object(s)
        if block:
            try:
                return json.loads(block)
            except Exception:
                pass
        # Third: python-literal fallback (handle 'true/false/null' and single quotes)
        t = s
        try:
            t = re.sub(r"\btrue\b", "True", t)
            t = re.sub(r"\bfalse\b", "False", t)
            t = re.sub(r"\bnull\b", "None", t)
            val = ast.literal_eval(t)
            if isinstance(val, dict):
                return val
        except Exception:
            pass
        # Give up
        raise ValueError("Unable to parse MCQ JSON")

    try:
        data = _try_parse_mcq_payload(raw)
    except Exception as exc_first:
        # Retry once with a more compact prompt and smaller count to avoid token/size truncation issues
        retry_count = max(8, min(count, 8))
        compact_prompt = textwrap.dedent(
            f"""
            You are PaperX's quiz compiler. Create a rigorous MCQ test for "{topic}" using ONLY the material in the triple chevrons.

            Constraints (STRICT):
            - Return EXACTLY {retry_count} questions.
            - Each question MUST have 4 short options (A–D).
            - Explanation MUST be under 18 words and grounded in the notes.
            - Output MUST be strictly valid JSON only, no comments, no markdown fences, no trailing commas.
            - Schema: {{"topic": "...", "questions": [{{"question":"...","options":["...","...","...","..."],"correct_index": 0,"explanation":"..."}}]}}
            {"The notes excerpt was truncated for length; keep questions within available context." if truncated else ""}

            <<<NOTES>>>
            {excerpt}
            <<<END NOTES>>>
            """
        ).strip()
        try:
            resp2 = model.generate_content(
                [{"text": compact_prompt}],
                generation_config=generation_config,
                request_options={"timeout": request_timeout},
            )
        except Exception as exc:
            # Give original parse error with context
            preview = (raw or "")[:480].replace("\n", " ")
            raise HTTPException(status_code=500, detail=f"Failed to parse MCQ JSON: {exc_first}; preview={preview}") from exc

        raw2: Optional[str] = None
        try:
            raw2 = resp2.text  # type: ignore[attr-defined]
        except Exception:
            raw2 = None
        if not raw2 and getattr(resp2, "candidates", None):
            try:
                for cand in resp2.candidates:
                    content = getattr(cand, "content", None)
                    parts = getattr(content, "parts", None) if content is not None else None
                    if parts:
                        s = "".join(getattr(p, "text", "") for p in parts if hasattr(p, "text"))
                        if s and s.strip():
                            raw2 = s
                            break
            except Exception:
                raw2 = None

        if not raw2:
            preview = (raw or "")[:480].replace("\n", " ")
            raise HTTPException(status_code=500, detail=f"Failed to parse MCQ JSON: {exc_first}; preview={preview}") from exc_first

        try:
            data = _try_parse_mcq_payload(raw2)
        except Exception as exc_second:
            preview1 = (raw or "")[:360].replace("\n", " ")
            preview2 = (raw2 or "")[:360].replace("\n", " ")
            raise HTTPException(status_code=500, detail=f"Failed to parse MCQ JSON after retry: {exc_second}; preview1={preview1}; preview2={preview2}") from exc_second

    qlist = data.get("questions")
    if not isinstance(qlist, list) or not qlist:
        raise HTTPException(status_code=500, detail="Gemini response missing 'questions' list.")

    out: List[Dict[str, Any]] = []
    for idx, item in enumerate(qlist[:count]):
        try:
            q = MCQQuestion.parse_obj(item)
        except Exception as exc:
            raise HTTPException(status_code=500, detail=f"Invalid MCQ at index {idx}: {exc}") from exc
        # Normalize options to exactly 4 entries when possible
        opts = list(q.options)[:4]
        if len(opts) < 4:
            # pad by repeating last safely
            while len(opts) < 4:
                opts.append(opts[-1] if opts else "")
        ci = int(q.correct_index)
        if ci < 0 or ci > 3:
            # clamp and ensure within bounds
            ci = max(0, min(ci, 3))
        # Enforce compact explanation length to reduce future overflows
        expl = (q.explanation or "").strip()
        if len(expl) > 200:
            expl = expl[:200].rstrip() + "…"
        out.append({
            "question": q.question.strip(),
            "options": [str(o).strip() for o in opts],
            "correct_index": ci,
            "explanation": expl,
        })

    return out, model_name, truncated


@notes_router.post("/notes/{note_id}/mcq")
@notes_router.post("/api/notes/{note_id}/mcq")
def api_generate_mcq(note_id: str, payload: Optional[Dict[str, Any]] = Body(default=None)):
    row, _v = db_get_ai_note_by_id_any(note_id)
    if not row:
        raise HTTPException(status_code=404, detail="Note not found")
    markdown = (row.get("markdown") or "").strip()
    if not markdown:
        raise HTTPException(status_code=400, detail="Note is empty; generate notes before requesting MCQs.")

    requested = 10
    topic_override = ""
    if isinstance(payload, dict):
        if "count" in payload:
            try:
                requested = int(payload["count"])  # type: ignore[index]
            except Exception:
                raise HTTPException(status_code=400, detail="count must be an integer")
        topic_override = str(payload.get("topic") or "").strip()

    inferred_topic = topic_override or row.get("title") or _derive_topic_from_markdown(markdown, fallback="MCQ Test")
    questions, used_model, truncated = _generate_mcq_with_gemini(markdown, inferred_topic, requested)
    return {
        "note_id": note_id,
        "topic": inferred_topic,
        "questions": questions,
        "count": len(questions),
        "model": used_model,
        "truncated": truncated,
        "generated_at": datetime.utcnow().isoformat() + "Z",
    }


@notes_router.get("/notes")
@notes_router.get("/api/notes")
def api_list_notes(variant: str = Query("detailed")):
    """List recent AI notes from DB (variant-specific)."""
    supabase = get_service_client()
    table = _table_for_variant(variant)
    try:
        res = supabase.table(table).select("id,title,created_at,updated_at").order("updated_at", desc=True).limit(50).execute()
        data = getattr(res, 'data', []) or []
        return {"items": data, "variant": _normalize_variant(variant)}
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to list notes: {e}")


@notes_router.get("/api/notes/topics/search", summary="Search AI notes topics for autocomplete")
def api_search_topics(
    q: str = Query("", min_length=0, max_length=120, description="Topic query string"),
    limit: int = Query(12, ge=1, le=50),
):
    """Return topic suggestions from DB as the user types.

    Uses Supabase service role client to query `ai_notes.title`.
    Results are ranked to prefer starts-with matches, then contains matches.
    """
    query = (q or "").strip()
    if not query:
        return {"query": "", "items": []}

    supabase = get_service_client()

    def _fetch_starts():
        return (
            supabase.table("ai_notes")
            .select("id,title")
            .ilike("title", f"{query}%")
            .order("title")
            .limit(limit)
            .execute()
        )

    def _fetch_contains():
        return (
            supabase.table("ai_notes")
            .select("id,title")
            .ilike("title", f"%{query}%")
            .order("title")
            .limit(max(50, limit * 4))
            .execute()
        )

    try:
        starts_res = _supabase_retry(_fetch_starts)
        if getattr(starts_res, "error", None):
            raise HTTPException(status_code=500, detail=f"Supabase error (topic starts): {starts_res.error}")

        contains_res = _supabase_retry(_fetch_contains)
        if getattr(contains_res, "error", None):
            raise HTTPException(status_code=500, detail=f"Supabase error (topic contains): {contains_res.error}")

        seen: Set[str] = set()
        items: List[Dict[str, Any]] = []

        def add_rows(rows: List[Dict[str, Any]]):
            for row in rows or []:
                topic = (row.get("title") or "").strip()
                if not topic:
                    continue
                key = topic.casefold()
                if key in seen:
                    continue
                seen.add(key)
                items.append({"id": row.get("id"), "topic": topic})
                if len(items) >= limit:
                    return

        add_rows(getattr(starts_res, "data", []) or [])
        if len(items) < limit:
            add_rows(getattr(contains_res, "data", []) or [])

        return {"query": query, "items": items}
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to search topics: {e}")


@notes_router.get("/api/notes/resolve", summary="Resolve AI note by exact title + variant")
def api_resolve_note(
    title: str = Query(..., min_length=1, max_length=200),
    variant: str = Query("detailed"),
):
    """Return a note if it exists in DB for an exact title+variant match.

    This is intended for client-side 'DB-first' behavior: only generate if we are
    definitively sure the note does not exist (404).
    """
    clean_title = (title or "").strip()
    if not clean_title:
        raise HTTPException(status_code=400, detail="Missing title")

    used_variant = _normalize_variant(variant)
    supabase = get_service_client()
    table = _table_for_variant(used_variant)

    try:
        res = (
            supabase.table(table)
            .select("id,title,markdown,updated_at,image_urls")
            .eq("title", clean_title)
            .order("updated_at", desc=True)
            .limit(1)
            .execute()
        )
        rows = getattr(res, "data", None) or []
        if not rows:
            return JSONResponse({"error": "Not found"}, status_code=404)
        row = rows[0] or {}
        return {
            "id": row.get("id"),
            "title": row.get("title"),
            "markdown": row.get("markdown", ""),
            "updated_at": row.get("updated_at"),
            "image_urls": row.get("image_urls") or [],
            "variant": used_variant,
        }
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to resolve note: {e}")


@notes_router.post("/notes")
@notes_router.post("/api/notes")
def api_create_note(payload: dict, variant: str = Query("detailed")):
    title = (payload or {}).get("topic", "").strip() or (payload or {}).get("title", "Untitled").strip() or "Untitled"
    markdown = (payload or {}).get("markdown", "")
    raw_images = (payload or {}).get("image_urls")
    image_urls = raw_images if isinstance(raw_images, list) else None
    row = db_upsert_ai_note_by_title_variant(title, markdown, variant=_normalize_variant(variant), image_urls=image_urls)
    return {
        "id": row.get("id"),
        "title": row.get("title"),
        "created_at": row.get("created_at"),
        "updated_at": row.get("updated_at"),
        "variant": _normalize_variant(variant),
        "image_urls": row.get("image_urls") or (image_urls or []),
    }


@notes_router.get("/notes/{note_id}")
@notes_router.get("/api/notes/{note_id}")
def api_read_note(note_id: str, variant: Optional[str] = Query(default=None)):
    row: Optional[Dict[str, Any]] = None
    used_variant: Optional[str] = None
    if variant:
        used_variant = _normalize_variant(variant)
        row = db_get_ai_note_by_id_variant(note_id, variant=used_variant)
    if not row:
        row, used_variant = db_get_ai_note_by_id_any(note_id)
    if not row:
        return JSONResponse({"error": "Not found"}, status_code=404)
    return {
        "id": row.get("id"),
        "title": row.get("title"),
        "markdown": row.get("markdown", ""),
        "updated_at": row.get("updated_at"),
        "variant": used_variant,
        "image_urls": row.get("image_urls") or [],
    }


@notes_router.put("/notes/{note_id}")
@notes_router.put("/api/notes/{note_id}")
def api_update_note(note_id: str, payload: dict, variant: Optional[str] = Query(default=None)):
    markdown = (payload or {}).get("markdown", "")
    raw_images = (payload or {}).get("image_urls")
    image_urls = raw_images if isinstance(raw_images, list) else None
    if variant:
        row = db_update_ai_note_markdown_variant(
            note_id,
            markdown,
            variant=_normalize_variant(variant),
            image_urls=image_urls,
        )
    else:
        found, found_variant = db_get_ai_note_by_id_any(note_id)
        if not found:
            raise HTTPException(status_code=404, detail="Not found")
        row = db_update_ai_note_markdown_variant(
            note_id,
            markdown,
            variant=found_variant,
            image_urls=image_urls,
        )
    return {
        "id": note_id,
        "updated_at": row.get("updated_at") if row else None,
        "image_urls": (row.get("image_urls") if row else None) or (image_urls or []),
    }


@notes_router.get("/notes/{note_id}/download")
@notes_router.get("/api/notes/{note_id}/download")
def api_download_note(note_id: str):
    row, _v = db_get_ai_note_by_id_any(note_id)
    if not row:
        return JSONResponse({"error": "Not found"}, status_code=404)
    content = row.get("markdown", "")
    filename = f"{note_id}.md"
    headers = {"Content-Disposition": f"attachment; filename={filename}"}
    return Response(content=content, media_type="text/markdown", headers=headers)


@notes_router.get("/notes/{note_id}/pdf")
@notes_router.get("/api/notes/{note_id}/pdf")
def api_note_pdf(note_id: str):
    row, _v = db_get_ai_note_by_id_any(note_id)
    if not row:
        return JSONResponse({"error": "Not found"}, status_code=404)
    md = row.get("markdown", "")
    try:
        pdf_bytes = render_pdf_from_markdown_via_headless(md, title=note_id)
    except Exception:
        try:
            pdf_bytes = render_pdf_from_markdown(md)
        except Exception as e:
            return JSONResponse({"error": f"PDF error: {e}"}, status_code=500)
    filename = f"{note_id}.pdf"
    headers = {"Content-Disposition": f"attachment; filename={filename}"}
    return Response(content=pdf_bytes, media_type="application/pdf", headers=headers)


# --- Per-user edited notes API ---

@notes_router.get("/api/notes/edited/check")
def api_check_user_edit(
    title: str = Query(..., min_length=1),
    variant: str = Query("detailed"),
    authorization: Optional[str] = Header(default=None),
):
    token = _parse_bearer_token(authorization)
    user_id = _require_auth_user_id(token)
    row = db_get_user_edit_by_title(user_id, title, variant=_normalize_variant(variant))
    if not row:
        return {"exists": False}
    return {
        "exists": True,
        "id": row.get("id"),
        "updated_at": row.get("updated_at"),
    }


@notes_router.get("/api/notes/edited")
def api_get_user_edit(
    title: str = Query(..., min_length=1),
    variant: str = Query("detailed"),
    authorization: Optional[str] = Header(default=None),
):
    token = _parse_bearer_token(authorization)
    user_id = _require_auth_user_id(token)
    row = db_get_user_edit_by_title(user_id, title, variant=_normalize_variant(variant))
    if not row:
        raise HTTPException(status_code=404, detail="Not found")
    return {
        "id": row.get("id"),
        "title": row.get("title"),
        "variant": row.get("variant"),
        "markdown": row.get("markdown", ""),
        "updated_at": row.get("updated_at"),
    }


@notes_router.post("/api/notes/edited")
def api_upsert_user_edit(
    payload: dict,
    variant: str = Query("detailed"),
    authorization: Optional[str] = Header(default=None),
):
    token = _parse_bearer_token(authorization)
    user_id = _require_auth_user_id(token)
    title = (payload or {}).get("title") or (payload or {}).get("topic") or "Untitled"
    markdown = (payload or {}).get("markdown") or ""
    if not str(title).strip():
        raise HTTPException(status_code=400, detail="Missing title")
    row = db_upsert_user_edit(user_id, str(title).strip(), markdown, variant=_normalize_variant(variant))
    return {
        "id": row.get("id"),
        "title": row.get("title"),
        "variant": row.get("variant"),
        "updated_at": row.get("updated_at"),
    }


@notes_router.post("/pdf")
@notes_router.post("/api/pdf")
def api_pdf_from_markdown(payload: dict):
    markdown = (payload or {}).get("markdown", "")
    title = (payload or {}).get("title", "notes").strip() or "notes"
    try:
        pdf_bytes = render_pdf_from_markdown_via_headless(markdown, title=title)
    except Exception:
        try:
            pdf_bytes = render_pdf_from_markdown(markdown)
        except Exception as e:
            return JSONResponse({"error": f"PDF error: {e}"}, status_code=500)
    safe = re.sub(r"[^a-zA-Z0-9_.-]+", "_", title)[:80]
    headers = {"Content-Disposition": f"attachment; filename={safe}.pdf"}
    return Response(content=pdf_bytes, media_type="application/pdf", headers=headers)


# --- Learning Tracks API ---


learning_tracks_router = APIRouter(prefix="/api/learning-tracks", tags=["learning tracks"])


def _upsert_learning_track_preferences(
    user_id: str,
    profile_id: str,
    payload: LearningTrackPlanRequest,
    companies: List[str],
) -> None:
    supabase = get_service_client()
    data = {
        "auth_user_id": user_id,
        "profile_id": profile_id,
        "language": payload.language,
        "stack": payload.stack,
        "goal": payload.goal,
        "companies": companies,
        "experience_level": payload.experience_level,
        "focus_areas": payload.focus_areas or [],
        "updated_at": datetime.utcnow().isoformat(),
    }
    res = (
        supabase.table(LEARNING_TRACK_GOALS_TABLE)
        .upsert(_supabase_payload(data), on_conflict="auth_user_id")
        .execute()
    )
    if getattr(res, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (learning goals): {res.error}")


def _upsert_learning_track_plan(
    user_id: str,
    profile_id: str,
    plan: Dict[str, Any],
    payload: LearningTrackPlanRequest,
    companies: List[str],
) -> None:
    supabase = get_service_client()
    now = datetime.utcnow().isoformat()
    data = {
        "plan_id": plan.get("plan_id"),
        "auth_user_id": user_id,
        "profile_id": profile_id,
        "language": plan.get("language"),
        "stack": plan.get("stack"),
        "goal": plan.get("goal"),
        "companies": companies,
        "experience_level": payload.experience_level,
        "focus_areas": payload.focus_areas or [],
        "generated_at": plan.get("generated_at"),
        "plan_json": plan,
        "updated_at": now,
    }
    res = (
        supabase.table(LEARNING_TRACK_PLANS_TABLE)
        .upsert(_supabase_payload(data), on_conflict="plan_id")
        .execute()
    )
    if getattr(res, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (learning plan): {res.error}")


def _upsert_learning_track_progress(
    user_id: str,
    profile_id: str,
    payload: "LearningTrackProgressUpdate",
) -> Dict[str, Any]:
    supabase = get_service_client()
    data = {
        "auth_user_id": user_id,
        "profile_id": profile_id,
        "plan_id": payload.plan_id,
        "topic_id": payload.topic_id,
        "status": payload.status,
        "score": payload.score,
        "updated_at": datetime.utcnow().isoformat(),
    }
    res = (
        supabase.table(LEARNING_TRACK_PROGRESS_TABLE)
        .upsert(
            _supabase_payload(data),
            on_conflict="auth_user_id,plan_id,topic_id",
            returning="representation",
        )
        .execute()
    )
    if getattr(res, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (learning progress): {res.error}")
    rows = getattr(res, "data", None) or []
    if rows:
        return rows[0]
    fetch = (
        supabase.table(LEARNING_TRACK_PROGRESS_TABLE)
        .select("auth_user_id,plan_id,topic_id,status,score,updated_at")
        .eq("auth_user_id", user_id)
        .eq("plan_id", payload.plan_id)
        .eq("topic_id", payload.topic_id)
        .limit(1)
        .execute()
    )
    if getattr(fetch, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (fetch learning progress): {fetch.error}")
    return (fetch.data or [data])[0]


def _fetch_latest_learning_plan(user_id: str) -> Optional[Dict[str, Any]]:
    supabase = get_service_client()
    res = (
        supabase.table(LEARNING_TRACK_PLANS_TABLE)
        .select(
            "plan_id, language, stack, goal, companies, experience_level, focus_areas, plan_json, updated_at, created_at"
        )
        .eq("auth_user_id", user_id)
        .order("updated_at", desc=True)
        .limit(1)
        .execute()
    )
    if getattr(res, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (fetch learning plan): {res.error}")
    data = getattr(res, "data", []) or []
    return data[0] if data else None


def _fetch_learning_goal(user_id: str) -> Optional[Dict[str, Any]]:
    supabase = get_service_client()
    res = (
        supabase.table(LEARNING_TRACK_GOALS_TABLE)
        .select("language, stack, goal, companies, experience_level, focus_areas, updated_at, created_at")
        .eq("auth_user_id", user_id)
        .order("updated_at", desc=True)
        .limit(1)
        .execute()
    )
    if getattr(res, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (fetch learning goal): {res.error}")
    data = getattr(res, "data", []) or []
    return data[0] if data else None


class LearningTrackPlanRequest(BaseModel):
    language: str = Field(..., min_length=2, max_length=16)
    stack: str = Field(..., min_length=2, max_length=64)
    goal: str = Field(..., min_length=2, max_length=64)
    companies: List[str] = Field(default_factory=list)
    experience_level: Optional[str] = Field(None, max_length=64)
    focus_areas: Optional[List[str]] = Field(default=None)


class LearningTrackPracticeRequest(BaseModel):
    topic_title: str = Field(..., min_length=2, max_length=256)
    language: str = Field(..., min_length=2, max_length=16)
    stack: str = Field(..., min_length=2, max_length=64)
    goal: str = Field(..., min_length=2, max_length=64)
    companies: List[str] = Field(default_factory=list)
    context: Optional[str] = Field(default=None, max_length=4000)


class LearningTrackMockRequest(BaseModel):
    stack: str = Field(..., min_length=2, max_length=64)
    goal: str = Field(..., min_length=2, max_length=64)
    companies: List[str] = Field(default_factory=list)
    focus_round: Optional[str] = Field(default=None, max_length=64)
    language: str = Field(..., min_length=2, max_length=16)
    recent_topics: Optional[List[str]] = Field(default=None)


class LearningTrackAnalyticsRequest(BaseModel):
    language: str = Field(..., min_length=2, max_length=16)
    stack: str = Field(..., min_length=2, max_length=64)
    goal: str = Field(..., min_length=2, max_length=64)
    companies: List[str] = Field(default_factory=list)
    metrics: Dict[str, Any]
    highlights: Optional[List[str]] = Field(default=None)
    blockers: Optional[List[str]] = Field(default=None)


class LearningTrackFacultyBriefRequest(BaseModel):
    language: str = Field(..., min_length=2, max_length=16)
    stack: str = Field(..., min_length=2, max_length=64)
    goal: str = Field(..., min_length=2, max_length=64)
    cohort_name: Optional[str] = Field(default=None, max_length=128)
    companies: List[str] = Field(default_factory=list)
    plan_outline: Dict[str, Any]
    progress_snapshot: Optional[Dict[str, Any]] = None


class LearningTrackCodeExecuteRequest(BaseModel):
    language_id: str = Field(..., min_length=1, max_length=32)
    source: str = Field(..., min_length=1, max_length=5000)
    stdin: Optional[str] = Field(default="", max_length=2000)


class LearningTrackCodeExplainRequest(BaseModel):
    language: str = Field(..., min_length=2, max_length=32)
    stack: str = Field(..., min_length=2, max_length=64)
    goal: str = Field(..., min_length=2, max_length=64)
    companies: List[str] = Field(default_factory=list)
    code: str = Field(..., min_length=1, max_length=6000)
    question: Optional[str] = Field(default=None, max_length=1000)
    language_preference: Optional[str] = Field(default=None, max_length=16)


class LearningTrackProgressUpdate(BaseModel):
    plan_id: str = Field(..., min_length=6, max_length=64)
    topic_id: str = Field(..., min_length=4, max_length=64)
    status: Literal["not_started", "in_progress", "completed"]
    score: Optional[float] = Field(default=None)


def _coerce_float(value: Any) -> Optional[float]:
    try:
        if value is None:
            return None
        return float(value)
    except Exception:
        return None


def _ensure_list_of_strings(value: Any) -> List[str]:
    if isinstance(value, list):
        out = []
        for item in value:
            text = str(item).strip()
            if text:
                out.append(text)
        return out
    if isinstance(value, str):
        stripped = value.strip()
        return [stripped] if stripped else []
    return []


def _normalize_activity_list(value: Any) -> List[Dict[str, Any]]:
    activities: List[Dict[str, Any]] = []
    if not isinstance(value, list):
        return activities
    for entry in value:
        if not isinstance(entry, dict):
            continue
        title = str(entry.get("title") or entry.get("name") or "").strip()
        if not title:
            continue
        activity = {
            "title": title,
            "type": str(entry.get("type") or entry.get("kind") or "activity").strip() or "activity",
            "description": (str(entry.get("description") or entry.get("summary") or "").strip() or None),
            "url": (str(entry.get("url") or entry.get("link") or "").strip() or None),
            "source": (str(entry.get("source") or "").strip() or None),
            "difficulty": (str(entry.get("difficulty") or entry.get("level") or "").strip() or None),
        }
        activities.append(activity)
    return activities


def _extract_json_payload(text: str) -> Dict[str, Any]:
    candidate = (text or "").strip()
    if not candidate:
        raise ValueError("Empty response")
    try:
        return json.loads(candidate)
    except json.JSONDecodeError:
        pass
    match = re.search(r"\{[\s\S]*\}", candidate)
    if match:
        try:
            return json.loads(match.group(0))
        except Exception:
            pass
    match = re.search(r"```json\s*([\s\S]*?)```", candidate)
    if match:
        snippet = match.group(1).strip()
        try:
            return json.loads(snippet)
        except Exception:
            pass
    raise ValueError("Failed to parse JSON from model response")


def _new_id(prefix: str) -> str:
    return f"{prefix}_{uuid.uuid4().hex[:12]}"


def _normalize_modules(raw_modules: Any, payload: LearningTrackPlanRequest) -> List[Dict[str, Any]]:
    modules: List[Dict[str, Any]] = []
    if not isinstance(raw_modules, list):
        return modules
    for idx, raw_module in enumerate(raw_modules):
        if not isinstance(raw_module, dict):
            continue
        m_title = str(raw_module.get("title") or raw_module.get("name") or f"Module {idx + 1}").strip()
        m_desc = str(raw_module.get("description") or raw_module.get("summary") or "").strip()
        module_id = _new_id("module")
        topics_data = raw_module.get("topics") or []
        module_topics: List[Dict[str, Any]] = []
        if isinstance(topics_data, list):
            for t_idx, raw_topic in enumerate(topics_data):
                if not isinstance(raw_topic, dict):
                    continue
                t_title = str(raw_topic.get("title") or raw_topic.get("name") or f"Topic {t_idx + 1}").strip()
                if not t_title:
                    continue
                topic_id = _new_id("topic")
                t_desc = str(raw_topic.get("description") or raw_topic.get("summary") or "").strip()
                objectives = _ensure_list_of_strings(raw_topic.get("objectives"))
                practice_items = _normalize_activity_list(raw_topic.get("practice"))
                activities = _normalize_activity_list(raw_topic.get("activities"))
                company_focus = _ensure_list_of_strings(raw_topic.get("company_focus")) or payload.companies
                references = _normalize_activity_list(raw_topic.get("references"))
                module_topics.append({
                    "topic_id": topic_id,
                    "title": t_title,
                    "description": t_desc,
                    "objectives": objectives,
                    "activities": activities,
                    "practice": practice_items,
                    "company_focus": company_focus,
                    "references": references,
                    "notes": [],
                })
        modules.append({
            "module_id": module_id,
            "title": m_title,
            "description": m_desc,
            "duration_hours": _coerce_float(raw_module.get("duration_hours")),
            "topics": module_topics,
        })
    return modules


def _collect_topic_notes(topic_title: str, stack: str, config: Dict[str, Any], *, limit: int = 3) -> List[Dict[str, Any]]:
    if not SERPAPI_API_KEY:
        raise HTTPException(status_code=501, detail="SerpAPI key not configured. Set SERPAPI_API_KEY.")
    query = f"{topic_title} {stack} tutorial"
    urls = serpapi_search(query, num=limit, allowed_domains=config.get("allowed_domains"))
    extracts: List[Dict[str, Any]] = []
    for url in urls[:limit]:
        try:
            html = fetch(url)
            page = extract_sections_from_html(url, html)
            sections: List[Dict[str, Any]] = []
            for section in page.sections[:4]:
                sections.append({
                    "heading": section.title,
                    "summary": section.text[:600],
                })
            extracts.append({
                "url": page.url,
                "title": page.title,
                "sections": sections,
            })
        except Exception as exc:
            learning_logger.warning("Learning track notes fetch failed", extra={"topic": topic_title, "url": url, "error": str(exc)})
    return extracts


def _run_learning_agent(agent_name: str, system_message: str, user_payload: Dict[str, Any]) -> Dict[str, Any]:
    assistant = AssistantAgent(
        agent_name,
        model_client=gemini_model_client,
        system_message=system_message,
    )
    result = _run_assistant_blocking(assistant, json.dumps(user_payload))
    if not result or not result.messages:
        raise HTTPException(status_code=502, detail="Model did not return a response")
    try:
        return _extract_json_payload(result.messages[-1].content)
    except ValueError as exc:
        raise HTTPException(status_code=502, detail=str(exc)) from exc


LEARNING_TRACK_PLANNER_SYSTEM = """You are PaperX's Learning Track planner. Respond ONLY with compact JSON.
Fields: plan_title (string), overview (string), modules (list of objects with keys title, description, duration_hours, topics).
Each topic is an object with keys title, description, objectives (list of strings), activities (list of objects title/type/description/url/source),
practice (list of objects title/type/description/url/source/difficulty), company_focus (list of strings), references (list of objects title/url/source/type).
Align content with Indian college learners preparing for company interviews and academic goals. Keep JSON under 6000 chars.
"""


LEARNING_TRACK_PRACTICE_SYSTEM = """You generate MCQs and flashcards for PaperX learners. Respond ONLY with JSON with keys mcqs (list) and flashcards (list).
Each mcq object: question, options (list of 4), answer, explanation, difficulty, source.
Each flashcard object: front, back, mnemonic, company_hint.
Localize the text when language != 'en'.
"""


LEARNING_TRACK_MOCK_SYSTEM = """You design mock interview drills for PaperX. Respond with JSON keys: warmup_questions, coding_round, system_design, behavioral.
Each should be a list of question objects {title, prompt, rubric, difficulty, estimated_minutes}.
"""


LEARNING_TRACK_ANALYTICS_SYSTEM = """You are an analytics coach. Return JSON with keys summary (string), wins (list of strings), risks (list of strings), recommendations (list of strings), scorecard (object with keys velocity, mastery, retention each 0-100).
Use the provided metrics to ground your analysis.
"""


LEARNING_TRACK_FACULTY_SYSTEM = """You brief faculty coordinators. Return JSON with keys overview (string), action_items (list of strings), at_risk_learners (list), support_requests (list of strings), upcoming_milestones (list of strings).
"""


def _validate_choice(value: str, allowed: List[str], label: str) -> str:
    if value not in allowed:
        raise HTTPException(status_code=400, detail=f"Unsupported {label}: {value}")
    return value


def _validate_companies(companies: List[str], allowed: List[str]) -> List[str]:
    valid = []
    for company in companies:
        c = company.strip()
        if c and c in allowed:
            valid.append(c)
    if not valid:
        return allowed[:2] if allowed else []
    return valid


def _map_language_runtime(language_id: str, config: Dict[str, Any]) -> Dict[str, str]:
    for entry in config.get("compiler_languages", []):
        if entry.get("id") == language_id:
            return entry
    raise HTTPException(status_code=400, detail=f"Unsupported compiler language: {language_id}")


@learning_tracks_router.get("/config")
def api_learning_tracks_config():
    config = load_learning_tracks_config()
    return {
        "languages": config["languages"],
        "stacks": config["stacks"],
        "goals": config["goals"],
        "companies": config["companies"],
        "allowed_domains": config["allowed_domains"],
        "planner_model": config["planner_model"],
        "notes_model": config["notes_model"],
        "mcq_model": config["mcq_model"],
        "flashcard_model": config["flashcard_model"],
        "code_explainer_model": config["code_explainer_model"],
        "compiler_languages": config["compiler_languages"],
    }


@learning_tracks_router.get("/plan/latest")
def api_learning_tracks_latest_plan(authorization: Optional[str] = Header(default=None)):
    token = _bearer_token_from_header(authorization)
    user_id, profile_id = _require_user_and_profile(token)
    plan_row = _fetch_latest_learning_plan(user_id)
    goal_row = _fetch_learning_goal(user_id)

    goal_payload: Optional[Dict[str, Any]] = None
    if goal_row:
        goal_payload = {
            "language": goal_row.get("language"),
            "stack": goal_row.get("stack"),
            "goal": goal_row.get("goal"),
            "companies": goal_row.get("companies") or [],
            "experience_level": goal_row.get("experience_level"),
            "focus_areas": goal_row.get("focus_areas") or [],
            "updated_at": goal_row.get("updated_at"),
        }

    if not plan_row:
        return {
            "plan": None,
            "plan_id": None,
            "filters": None,
            "goals": goal_payload,
        }

    raw_plan = plan_row.get("plan_json") or {}
    plan_json = dict(raw_plan) if isinstance(raw_plan, dict) else raw_plan
    if isinstance(plan_json, dict):
        plan_json.setdefault("plan_id", plan_row.get("plan_id"))
        plan_json.setdefault("language", plan_row.get("language"))
        plan_json.setdefault("stack", plan_row.get("stack"))
        plan_json.setdefault("goal", plan_row.get("goal"))
        if "companies" not in plan_json:
            plan_json["companies"] = plan_row.get("companies") or []

    filters = {
        "language": plan_row.get("language"),
        "stack": plan_row.get("stack"),
        "goal": plan_row.get("goal"),
        "companies": plan_row.get("companies") or [],
        "experience_level": plan_row.get("experience_level"),
        "focus_areas": plan_row.get("focus_areas") or [],
    }

    return {
        "plan_id": plan_row.get("plan_id"),
        "plan": plan_json,
        "filters": filters,
        "goals": goal_payload,
        "updated_at": plan_row.get("updated_at"),
    }


@learning_tracks_router.post("/path")
def api_learning_tracks_plan(payload: LearningTrackPlanRequest, authorization: Optional[str] = Header(default=None)):
    if not GEMINI_API_KEY:
        raise HTTPException(status_code=501, detail="Gemini API key not configured. Set GEMINI_API_KEY.")
    token = _bearer_token_from_header(authorization)
    user_id, profile_id = _require_user_and_profile(token)
    config = load_learning_tracks_config()
    language = _validate_choice(payload.language, config["languages"], "language")
    stack = _validate_choice(payload.stack, config["stacks"], "stack")
    goal = _validate_choice(payload.goal, config["goals"], "goal")
    companies = _validate_companies(payload.companies, config["companies"])

    planner_payload = {
        "language": language,
        "stack": stack,
        "goal": goal,
        "companies": companies,
        "experience_level": payload.experience_level,
        "focus_areas": payload.focus_areas or [],
        "model": config["planner_model"],
    }
    learning_logger.info("Learning track plan requested", extra={"stack": stack, "goal": goal, "companies": companies})
    try:
        plan_raw = _run_learning_agent("paperx_learning_planner", LEARNING_TRACK_PLANNER_SYSTEM, planner_payload)
    except HTTPException:
        raise
    except Exception as exc:
        raise HTTPException(status_code=502, detail=f"Planner model error: {exc}") from exc

    modules = _normalize_modules(plan_raw.get("modules"), payload)
    for module in modules:
        for topic in module.get("topics", []):
            topic["notes"] = _collect_topic_notes(topic["title"], stack, config)

    plan = {
        "plan_id": str(uuid.uuid4()),
        "generated_at": datetime.utcnow().isoformat() + "Z",
        "language": language,
        "stack": stack,
        "goal": goal,
        "companies": companies,
        "plan_title": plan_raw.get("plan_title") or f"{stack.title()} Track",
        "overview": plan_raw.get("overview") or "",
        "modules": modules,
        "planner_model": config["planner_model"],
        "serp_allowed_domains": config["allowed_domains"],
    }
    _upsert_learning_track_preferences(user_id, profile_id, payload, companies)
    _upsert_learning_track_plan(user_id, profile_id, plan, payload, companies)
    return plan


@learning_tracks_router.post("/topic/practice")
def api_learning_track_practice(payload: LearningTrackPracticeRequest):
    if not GEMINI_API_KEY:
        raise HTTPException(status_code=501, detail="Gemini API key not configured. Set GEMINI_API_KEY.")
    config = load_learning_tracks_config()
    language = _validate_choice(payload.language, config["languages"], "language")
    stack = _validate_choice(payload.stack, config["stacks"], "stack")
    goal = _validate_choice(payload.goal, config["goals"], "goal")
    companies = _validate_companies(payload.companies, config["companies"])

    practice_payload = {
        "topic": payload.topic_title,
        "language": language,
        "stack": stack,
        "goal": goal,
        "companies": companies,
        "context": (payload.context or "")[:3000],
        "model": config["mcq_model"],
    }
    try:
        practice = _run_learning_agent("paperx_learning_practice", LEARNING_TRACK_PRACTICE_SYSTEM, practice_payload)
    except HTTPException:
        raise
    except Exception as exc:
        raise HTTPException(status_code=502, detail=f"Practice model error: {exc}") from exc
    return practice


@learning_tracks_router.post("/mock")
def api_learning_track_mock(payload: LearningTrackMockRequest):
    if not GEMINI_API_KEY:
        raise HTTPException(status_code=501, detail="Gemini API key not configured. Set GEMINI_API_KEY.")
    config = load_learning_tracks_config()
    language = _validate_choice(payload.language, config["languages"], "language")
    stack = _validate_choice(payload.stack, config["stacks"], "stack")
    goal = _validate_choice(payload.goal, config["goals"], "goal")
    companies = _validate_companies(payload.companies, config["companies"])

    mock_payload = {
        "stack": stack,
        "goal": goal,
        "companies": companies,
        "focus_round": payload.focus_round or "",
        "language": language,
        "recent_topics": payload.recent_topics or [],
    }
    try:
        plan = _run_learning_agent("paperx_learning_mock", LEARNING_TRACK_MOCK_SYSTEM, mock_payload)
    except HTTPException:
        raise
    except Exception as exc:
        raise HTTPException(status_code=502, detail=f"Mock planner error: {exc}") from exc
    return plan


@learning_tracks_router.post("/analytics")
def api_learning_track_analytics(payload: LearningTrackAnalyticsRequest):
    if not GEMINI_API_KEY:
        raise HTTPException(status_code=501, detail="Gemini API key not configured. Set GEMINI_API_KEY.")
    config = load_learning_tracks_config()
    language = _validate_choice(payload.language, config["languages"], "language")
    stack = _validate_choice(payload.stack, config["stacks"], "stack")
    goal = _validate_choice(payload.goal, config["goals"], "goal")
    companies = _validate_companies(payload.companies, config["companies"])

    analytics_payload = {
        "language": language,
        "stack": stack,
        "goal": goal,
        "companies": companies,
        "metrics": payload.metrics,
        "highlights": payload.highlights or [],
        "blockers": payload.blockers or [],
    }
    try:
        analysis = _run_learning_agent("paperx_learning_analytics", LEARNING_TRACK_ANALYTICS_SYSTEM, analytics_payload)
    except HTTPException:
        raise
    except Exception as exc:
        raise HTTPException(status_code=502, detail=f"Analytics model error: {exc}") from exc
    return analysis


@learning_tracks_router.post("/faculty-brief")
def api_learning_track_faculty_brief(payload: LearningTrackFacultyBriefRequest):
    if not GEMINI_API_KEY:
        raise HTTPException(status_code=501, detail="Gemini API key not configured. Set GEMINI_API_KEY.")
    config = load_learning_tracks_config()
    language = _validate_choice(payload.language, config["languages"], "language")
    stack = _validate_choice(payload.stack, config["stacks"], "stack")
    goal = _validate_choice(payload.goal, config["goals"], "goal")
    companies = _validate_companies(payload.companies, config["companies"])

    brief_payload = {
        "language": language,
        "stack": stack,
        "goal": goal,
        "companies": companies,
        "cohort_name": payload.cohort_name or "",
        "plan_outline": payload.plan_outline,
        "progress_snapshot": payload.progress_snapshot or {},
    }
    try:
        brief = _run_learning_agent("paperx_learning_faculty", LEARNING_TRACK_FACULTY_SYSTEM, brief_payload)
    except HTTPException:
        raise
    except Exception as exc:
        raise HTTPException(status_code=502, detail=f"Faculty brief error: {exc}") from exc
    return brief


@learning_tracks_router.post("/code/execute")
def api_learning_track_execute(payload: LearningTrackCodeExecuteRequest):
    config = load_learning_tracks_config()
    language_entry = _map_language_runtime(payload.language_id, config)
    request_payload = {
        "language": language_entry["runtime"],
        "source": payload.source,
        "stdin": payload.stdin or "",
    }
    try:
        resp = requests.post(
            "https://emkc.org/api/v2/piston/execute",
            json=request_payload,
            timeout=25,
        )
    except Exception as exc:
        raise HTTPException(status_code=502, detail=f"Execution service error: {exc}") from exc
    if resp.status_code >= 400:
        raise HTTPException(status_code=resp.status_code, detail=resp.text)
    data = resp.json()
    return {
        "language": language_entry["id"],
        "runtime": language_entry["runtime"],
        "output": data.get("output") or data.get("stdout") or "",
        "stderr": data.get("stderr") or "",
        "exit_code": data.get("code") or data.get("exitCode"),
    }


@learning_tracks_router.post("/code/explain")
def api_learning_track_explain(payload: LearningTrackCodeExplainRequest):
    if not GEMINI_API_KEY:
        raise HTTPException(status_code=501, detail="Gemini API key not configured. Set GEMINI_API_KEY.")
    config = load_learning_tracks_config()
    language = _validate_choice(payload.language, config["languages"], "language")
    stack = _validate_choice(payload.stack, config["stacks"], "stack")
    goal = _validate_choice(payload.goal, config["goals"], "goal")
    companies = _validate_companies(payload.companies, config["companies"])

    explain_payload = {
        "language": language,
        "stack": stack,
        "goal": goal,
        "companies": companies,
        "code": payload.code,
        "question": payload.question or "",
        "model": config["code_explainer_model"],
        "language_preference": payload.language_preference or language,
    }
    system_message = "You explain source code to PaperX learners clearly. Respond with JSON {summary, complexity, suggestions (list), localized_explanation}."
    try:
        explanation = _run_learning_agent("paperx_learning_code", system_message, explain_payload)
    except HTTPException:
        raise
    except Exception as exc:
        raise HTTPException(status_code=502, detail=f"Code explanation error: {exc}") from exc
    return explanation


@learning_tracks_router.post("/progress")
def api_learning_track_progress_update(
    payload: LearningTrackProgressUpdate, authorization: Optional[str] = Header(default=None)
):
    token = _bearer_token_from_header(authorization)
    user_id, profile_id = _require_user_and_profile(token)
    record = _upsert_learning_track_progress(user_id, profile_id, payload)
    return {
        "plan_id": record.get("plan_id"),
        "topic_id": record.get("topic_id"),
        "status": record.get("status"),
        "score": record.get("score"),
        "updated_at": record.get("updated_at"),
    }


# --- YouTube transcript endpoints ---

youtube_transcript_router = APIRouter(prefix="/api/transcripts", tags=["youtube transcripts"])


def _structured_notes_from_transcript(transcript: str, *, title: str, lang: Optional[str]) -> Tuple[str, bool]:
    clean_text = (transcript or "").strip()
    if not clean_text:
        raise HTTPException(status_code=400, detail="Transcript text is empty.")

    if not GEMINI_API_KEY:
        raise HTTPException(
            status_code=501,
            detail="Gemini API key not configured. Set GEMINI_API_KEY to enable structured notes.",
        )

    try:
        import google.generativeai as genai  # type: ignore
    except Exception as exc:  # pragma: no cover - optional dependency
        raise HTTPException(
            status_code=501,
            detail=f"Gemini client library missing: {exc}. Install google-generativeai to enable this feature.",
        ) from exc

    max_chars = max(4000, MAX_TRANSCRIPT_CHARS_FOR_NOTES)
    truncated = False
    if len(clean_text) > max_chars:
        clean_text = clean_text[:max_chars]
        truncated = True

    notes_prompt = textwrap.dedent(
        """
        You are PaperX's academic note composer. Transform the provided YouTube transcript into polished,
        exam-ready study notes written in Markdown.

        Output requirements:
        - Start with a single H1 title using the video name.
        - Include these H2 sections, even if you must acknowledge limited information:
          2. Key Takeaways (bullets)
          3. Detailed Notes (use subsections or numbered steps when flow suggests)
          4. Examples & Analogies (bullets; add [not mentioned] if absent)
          5. Frameworks / Processes (tables or lists; include Mermaid diagrams when explaining flows)
          6. Reflection Questions
        - Bold critical vocabulary and formulas.
        - Prefer Markdown tables where comparing items.
        - Keep the tone clear, modern, and supportive for self-study.
        """
    ).strip()

    context_notice = ""
    if truncated:
        context_notice = (
            f"NOTE: Only the first {max_chars} characters of the transcript were available. "
            "Flag the notes as partial if key sections appear missing."
        )

    genai.configure(api_key=GEMINI_API_KEY)
    model = genai.GenerativeModel(GEMINI_NOTES_MODEL)
    payload = [
        {"text": notes_prompt},
        {
            "text": textwrap.dedent(
                f"""
                Video title: {title or 'Unknown YouTube Video'}
                Preferred output language: {lang or 'en'}
                {context_notice}

                Transcript:
                {clean_text}
                """
            ).strip()
        },
    ]

    try:
        response = model.generate_content(payload)
    except Exception as exc:
        raise HTTPException(
            status_code=502,
            detail=f"Gemini notes request failed: {exc}",
        ) from exc

    generated = getattr(response, "text", "") or ""
    if not generated:
        try:
            generated = response.candidates[0].content.parts[0].text  # type: ignore[index]
        except Exception:
            generated = ""

    generated = (generated or "").strip()
    if not generated:
        raise HTTPException(status_code=502, detail="Gemini did not return any content.")

    return generated, truncated


class YouTubeTranscriptRequest(BaseModel):
    url: HttpUrl
    lang: Optional[str] = "en"
    fallback_ytdlp: Optional[bool] = True
    use_whisper: Optional[bool] = False
    clean: Optional[bool] = True


class YouTubeTranscriptResponse(BaseModel):
    paragraph: str
    source: str


class YouTubeTranscriptNotesRequest(BaseModel):
    url: HttpUrl
    lang: Optional[str] = "en"
    fallback_ytdlp: Optional[bool] = True
    clean: Optional[bool] = True


class YouTubeTranscriptNotesResponse(BaseModel):
    notes_markdown: str
    model: str
    truncated: bool
    transcript_chars: int


class YouTubeMetaRequest(BaseModel):
    url: HttpUrl


class YouTubeMetaResponse(BaseModel):
    video_id: str
    embed_url: str
    channel_name: Optional[str] = None
    upload_date: Optional[str] = None  # ISO date string (YYYY-MM-DD) when available
    views: Optional[int] = None
    channel_url: Optional[str] = None
    channel_logo: Optional[str] = None


def _normalize_video_key(url: str) -> str:
    try:
        vid = extract_video_id(str(url))
        if vid:
            return vid
    except HTTPException:
        pass
    return str(url).strip()


@lru_cache(maxsize=512)
def _cached_youtube_meta(video_key: str) -> Dict[str, Any]:
    if YoutubeDL is None:  # pragma: no cover - optional dependency missing
        raise HTTPException(status_code=501, detail="yt-dlp is not available on the server.")

    ydl_opts = {
        "skip_download": True,
        "quiet": True,
        "no_warnings": True,
        "extract_flat": False,  # get full metadata for a single video
    }

    target_url = video_key
    if re.fullmatch(r"[A-Za-z0-9_-]{11}", video_key):
        target_url = f"https://www.youtube.com/watch?v={video_key}"

    with YoutubeDL(ydl_opts) as ydl:
        try:
            info = ydl.extract_info(str(target_url), download=False)
        except Exception as exc:
            raise HTTPException(status_code=400, detail=f"Could not fetch metadata: {exc}") from exc

    vid = str(info.get("id") or "").strip()
    if not vid:
        # Fallback to parser if present
        try:
            vid = extract_video_id(str(target_url))
        except HTTPException:
            pass
    if not vid:
        raise HTTPException(status_code=400, detail="Could not determine YouTube video ID from URL.")

    # Prefer channel name field, fallback to uploader
    channel_name = info.get("channel") or info.get("uploader") or None
    channel_url = info.get("channel_url") or info.get("uploader_url") or None

    # upload_date comes as YYYYMMDD; convert to YYYY-MM-DD if present
    up_raw = info.get("upload_date") or ""
    upload_date = None
    if isinstance(up_raw, str) and len(up_raw) == 8 and up_raw.isdigit():
        upload_date = f"{up_raw[0:4]}-{up_raw[4:6]}-{up_raw[6:8]}"

    views = info.get("view_count")
    try:
        views = int(views) if views is not None else None
    except Exception:
        views = None

    out = {
        "video_id": vid,
        "embed_url": f"https://www.youtube.com/embed/{vid}",
        "channel_name": channel_name,
        "upload_date": upload_date,
        "views": views,
    }
    # Best-effort channel logo resolution when channel_url is available
    try:
        if channel_url:
            logo = get_channel_logo(channel_url) or get_default_channel_logo()
            out["channel_url"] = channel_url
            if logo:
                out["channel_logo"] = logo
    except Exception:
        # Non-fatal; just omit logo on errors
        out["channel_url"] = channel_url
    return out


def _extract_youtube_meta(url: str) -> YouTubeMetaResponse:
    """Extract basic metadata for a YouTube video using yt-dlp without downloading.

    Returns video_id, embed_url, channel_name, upload_date (YYYY-MM-DD), and views.
    """
    video_key = _normalize_video_key(url)
    meta = _cached_youtube_meta(video_key)
    return YouTubeMetaResponse(**meta)


@youtube_transcript_router.post("/paragraph", response_model=YouTubeTranscriptResponse)
def api_transcript_paragraph(payload: YouTubeTranscriptRequest) -> YouTubeTranscriptResponse:
    text = fetch_transcript_paragraph(
        url_or_id=str(payload.url),
        lang=payload.lang or "en",
        fallback_ytdlp=bool(payload.fallback_ytdlp),
        use_whisper=bool(payload.use_whisper),
        clean=bool(payload.clean),
    ).strip()
    if not text:
        raise HTTPException(status_code=404, detail="Transcript is empty.")
    return YouTubeTranscriptResponse(paragraph=text, source=str(payload.url))


@youtube_transcript_router.post("/meta", response_model=YouTubeMetaResponse)
def api_youtube_meta(payload: YouTubeMetaRequest) -> YouTubeMetaResponse:
    """Return basic metadata (id, channel, upload date, views) for the given YouTube URL."""
    return _extract_youtube_meta(str(payload.url))


@youtube_transcript_router.post("/notes", response_model=YouTubeTranscriptNotesResponse)
def api_transcript_notes(payload: YouTubeTranscriptNotesRequest) -> YouTubeTranscriptNotesResponse:
    transcript_text = fetch_transcript_paragraph(
        url_or_id=str(payload.url),
        lang=payload.lang or "en",
        fallback_ytdlp=bool(payload.fallback_ytdlp),
        use_whisper=False,
        clean=bool(payload.clean),
    ).strip()
    if not transcript_text:
        raise HTTPException(status_code=404, detail="Transcript is empty.")

    try:
        video_id = extract_video_id(str(payload.url))
    except HTTPException:
        video_id = None
    video_title = f"YouTube Video {video_id}" if video_id else "YouTube Video"

    notes_markdown, truncated = _structured_notes_from_transcript(
        transcript_text,
        title=video_title,
        lang=payload.lang,
    )
    result = YouTubeTranscriptNotesResponse(
        notes_markdown=notes_markdown,
        model=GEMINI_NOTES_MODEL,
        truncated=truncated,
        transcript_chars=len(transcript_text),
    )

    # Best-effort: save generated notes for this video to Supabase for caching.
    try:
        if video_id:
            supabase = get_service_client()
            record = _supabase_payload({
                "video_id": video_id,
                "video_url": str(payload.url),
                "notes_markdown": result.notes_markdown,
                "model": result.model,
                "truncated": result.truncated,
                "transcript_chars": result.transcript_chars,
            })
            # Ignore failure; we don't want to block the response on storage errors.
            _ = supabase.table("youtube_ai_notes").insert(record).execute()
    except Exception:
        pass

    return result


@youtube_transcript_router.get("/saved/{video_id}")
def api_youtube_saved(video_id: str):
    """Return the latest saved notes for a given YouTube video ID if present; else 404."""
    if not video_id or not re.fullmatch(r"[A-Za-z0-9_-]{6,32}", video_id):
        raise HTTPException(status_code=400, detail="Invalid video ID.")
    supabase = get_service_client()
    try:
        def _run():
            return (
                supabase
                .table("youtube_ai_notes")
                .select("video_id,video_url,notes_markdown,model,truncated,transcript_chars,created_at")
                .eq("video_id", video_id)
                .order("created_at", desc=True)
                .limit(1)
                .execute()
            )
        res = _supabase_retry(_run)
        rows = getattr(res, "data", None) or []
        if not rows:
            raise HTTPException(status_code=404, detail="No saved notes found for this video.")
        # Return the row as-is; front-end expects at least notes_markdown and optionally model
        return rows[0]
    except APIError as exc:
        raise HTTPException(status_code=502, detail=f"Storage error: {exc}") from exc


# --- Tests & Assessments ---


class TestQuestionIn(BaseModel):
    prompt: str = Field(..., min_length=1, max_length=4000)
    options: List[str] = Field(..., min_items=2, max_items=8)
    correct_index: int = Field(..., ge=0)
    points: int = Field(1, ge=1, le=100)
    order: Optional[int] = Field(None, ge=0)

    @validator("options", pre=True)
    def _clean_options(cls, v):
        if not v:
            return []
        opts: List[str] = []
        for o in v:
            s = str(o or "").strip()
            if s:
                opts.append(s[:500])
        return opts

    @validator("correct_index")
    def _check_correct_index(cls, v, values):
        opts = values.get("options", []) or []
        if not opts:
            return v
        if v < 0 or v >= len(opts):
            raise ValueError("correct_index must point to an option")
        return v


class CreateTestIn(BaseModel):
    title: str = Field(..., min_length=1, max_length=200)
    description: Optional[str] = Field(None, max_length=2000)
    duration_seconds: Optional[int] = Field(None, ge=0, le=7200)
    class_id: Optional[str] = Field(None, description="Optional class/group association")
    questions: List[TestQuestionIn]
    accepting_submissions: Optional[bool] = True

    @validator("questions")
    def _must_have_questions(cls, v):
        if not v:
            raise ValueError("At least one question is required")
        return v


class SubmitAttemptIn(BaseModel):
    answers: List[int]
    elapsed_seconds: Optional[int] = Field(None, ge=0, le=7200)

    @validator("answers")
    def _non_empty(cls, v):
        if not v:
            raise ValueError("answers cannot be empty")
        return v


def _fetch_test_row(supabase, test_id: str) -> Dict[str, Any]:
    res = _supabase_retry(lambda: supabase.table("tests").select("*").eq("id", test_id).limit(1).execute())
    if getattr(res, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (get test): {res.error}")
    rows = getattr(res, "data", None) or []
    if not rows:
        raise HTTPException(status_code=404, detail="Test not found")
    return rows[0]


def _fetch_test_questions(supabase, test_id: str) -> List[Dict[str, Any]]:
    res = _supabase_retry(lambda: (
        supabase
        .table("test_questions")
        .select("id,prompt,options,correct_index,points,question_order")
        .eq("test_id", test_id)
        .order("question_order", desc=False)
        .order("id", desc=False)
        .execute()
    ), retries=5, base_delay=0.25)
    if getattr(res, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (get questions): {res.error}")
    return getattr(res, "data", None) or []


def _ensure_user_allowed_for_test(supabase, test_row: Dict[str, Any], user_id: str):
    """Gate test access based on class membership.

    - If test has class_id: only enrolled students of that class can access.
    - Owning teacher always has access.
    - If class_id is null: open to all signed-in users.
    """
    try:
        if not user_id:
            raise HTTPException(status_code=401, detail="Authentication required")
    except Exception:
        raise HTTPException(status_code=401, detail="Authentication required")

    if (test_row.get("teacher_user_id") or "") == user_id:
        return

    class_id = test_row.get("class_id")
    if not class_id:
        return

    try:
        # First fetch the class row to get matching criteria
        class_res = _supabase_retry(lambda: supabase.table("teacher_classes").select("*").eq("id", str(class_id)).limit(1).execute())
        if getattr(class_res, "error", None):
            raise HTTPException(status_code=500, detail=f"Supabase error (get class): {class_res.error}")
        if not class_res.data:
            # Class not found - deny access
            raise HTTPException(status_code=403, detail="This test is restricted to students of the selected class")
        class_row = class_res.data[0]

        # Get user's profile ID from auth_user_id
        prof_res = _supabase_retry(lambda: supabase.table("user_profiles").select("id").eq("auth_user_id", user_id).limit(1).execute())
        if getattr(prof_res, "error", None):
            raise HTTPException(status_code=500, detail=f"Supabase error (get profile): {prof_res.error}")
        if not prof_res.data:
            raise HTTPException(status_code=403, detail="This test is restricted to students of the selected class")
        profile_id = prof_res.data[0].get("id")

        # Build query to check if user's education record matches class criteria
        edu_query = supabase.table("user_education").select("id").eq("user_profile_id", profile_id)

        filter_count = 0
        for column, value in (
            ("batch_id", class_row.get("batch_id")),
            ("section", class_row.get("section")),
            ("current_semester", class_row.get("semester")),
            ("degree_id", class_row.get("degree_id")),
            ("department_id", class_row.get("department_id")),
            ("college_id", class_row.get("college_id")),
        ):
            if value not in {None, ""}:
                eq_value = str(value) if column.endswith("_id") or column == "batch_id" else value
                edu_query = edu_query.eq(column, eq_value)
                filter_count += 1

        if filter_count == 0:
            # No filters means class has no criteria - allow access
            return

        edu_res = _supabase_retry(lambda: edu_query.limit(1).execute())

        if getattr(edu_res, "error", None):
            raise HTTPException(status_code=500, detail=f"Supabase error (class access): {edu_res.error}")
        rows = getattr(edu_res, "data", None) or []
        if not rows:
            raise HTTPException(status_code=403, detail="This test is restricted to students of the selected class")
    except HTTPException:
        raise
    except Exception as exc:
        raise HTTPException(status_code=500, detail=f"Failed to validate class access: {exc}") from exc


class TeacherAIGenerateTestIn(BaseModel):
    # Academic test mode can build a multi-line prompt from many unit/topics.
    # Keep an upper bound to avoid accidental huge payloads.
    topic: str = Field(..., min_length=3, max_length=5000)
    count: int = Field(10, ge=1, le=30)


def _generate_topic_mcq_for_teacher(topic: str, count: int) -> Tuple[str, List[Dict[str, Any]], str]:
    """Generate MCQ questions from a topic for the teacher test builder.

    IMPORTANT: Per product requirement, this uses gemini-2.5-flash only.

    Returns (title, questions, model_name).
    """
    if not GEMINI_API_KEY:
        raise HTTPException(status_code=501, detail="Gemini API key not configured for AI test generation.")
    try:
        import google.generativeai as genai  # type: ignore
    except Exception as exc:  # pragma: no cover
        raise HTTPException(status_code=500, detail=f"Gemini client library missing: {exc}") from exc

    model_name = "gemini-2.5-flash"
    safe_count = int(max(1, min(int(count or 10), 30)))
    cleaned_topic = (topic or "").strip()
    if not cleaned_topic:
        raise HTTPException(status_code=400, detail="topic is required")

    # Keep the schema example short even if the actual topic prompt is long.
    cleaned_topic_one_line = re.sub(r"\s+", " ", cleaned_topic).strip()
    schema_title = (cleaned_topic_one_line[:80] + "…") if len(cleaned_topic_one_line) > 80 else cleaned_topic_one_line

    prompt = textwrap.dedent(
        f"""
        You are PaperX's quiz compiler.

        Create EXACTLY {safe_count} multiple-choice questions (MCQ) for the topic: "{cleaned_topic}".

        Constraints:
        - Each question MUST have 4 options.
        - Options must be plausible and unambiguous.
        - Provide: prompt, options (length 4), correct_index (0..3), explanation (<= 18 words).
        - Difficulty: mixed (basic to moderate), suitable for classroom assessment.
        - Return ONLY strict JSON (no markdown fences, no commentary).

        Schema (exact keys):
        {{"title":"...","description":"...","questions":[{{"prompt":"...","options":["...","...","...","..."],"correct_index":0,"explanation":"..."}}]}}
        """
    ).strip()

    compact_prompt = textwrap.dedent(
        f"""
        Return STRICT JSON ONLY.
        Make it small to avoid truncation.

        Topic: "{cleaned_topic}"

        Output EXACTLY {safe_count} questions.

        Hard limits:
        - prompt <= 120 chars
        - each option <= 70 chars
                - explanation <= 12 words

                Schema:
                {{"title":"{schema_title} MCQ","questions":[{{"prompt":"...","options":["...","...","...","..."],"correct_index":0,"explanation":"..."}}]}}
                """
        ).strip()

    genai.configure(api_key=GEMINI_API_KEY)
    model = genai.GenerativeModel(model_name)
    generation_config = genai.GenerationConfig(
        response_mime_type="application/json",
        temperature=0.25,
        top_p=0.9,
        max_output_tokens=int(os.getenv("GEMINI_TEACHER_TEST_MAX_TOKENS", "3600") or "3600"),
        candidate_count=1,
    )

    request_timeout = float(os.getenv("GEMINI_TEACHER_TEST_TIMEOUT", "35"))
    def _extract_text(resp_obj: Any) -> Optional[str]:
        try:
            t = resp_obj.text  # type: ignore[attr-defined]
            if t and str(t).strip():
                return str(t)
        except Exception:
            pass
        try:
            if getattr(resp_obj, "candidates", None):
                for cand in resp_obj.candidates:
                    content = getattr(cand, "content", None)
                    parts = getattr(content, "parts", None) if content is not None else None
                    if parts:
                        s = "".join(getattr(p, "text", "") for p in parts if hasattr(p, "text"))
                        if s and s.strip():
                            return s
        except Exception:
            pass
        return None

    def _call(prompt_text: str) -> str:
        try:
            r = model.generate_content(
                [{"text": prompt_text}],
                generation_config=generation_config,
                request_options={"timeout": request_timeout},
            )
        except Exception as exc:
            raise HTTPException(status_code=502, detail=f"Gemini request failed: {exc}") from exc
        txt = _extract_text(r)
        if not txt:
            raise HTTPException(status_code=502, detail="Gemini returned no content")
        return txt

    raw = _call(prompt)

    def _strip_code_fences(txt: str) -> str:
        s = txt.strip()
        if s.startswith("```"):
            s = re.sub(r"^```[a-zA-Z0-9_-]*\s*", "", s)
            s = re.sub(r"\s*```$", "", s)
        return s.strip()

    def _extract_first_json_object(txt: str) -> Optional[str]:
        start = txt.find('{')
        if start == -1:
            return None
        depth = 0
        for i in range(start, len(txt)):
            c = txt[i]
            if c == '{':
                depth += 1
            elif c == '}':
                depth -= 1
                if depth == 0:
                    return txt[start:i+1]
        return None

    def _try_parse_payload(raw_text: str) -> Dict[str, Any]:
        s = _strip_code_fences(raw_text)
        try:
            return json.loads(s)
        except Exception:
            pass
        block = _extract_first_json_object(s)
        if block:
            try:
                return json.loads(block)
            except Exception:
                pass
        # literal eval fallback
        t = s
        try:
            t = re.sub(r"\btrue\b", "True", t)
            t = re.sub(r"\bfalse\b", "False", t)
            t = re.sub(r"\bnull\b", "None", t)
            val = ast.literal_eval(t)
            if isinstance(val, dict):
                return val
        except Exception:
            pass
        raise ValueError("Unable to parse JSON")

    try:
        data = _try_parse_payload(raw)
    except Exception:
        # Retry once with compact prompt to reduce truncation risk
        raw2 = _call(compact_prompt)
        try:
            data = _try_parse_payload(raw2)
        except Exception as exc:
            preview1 = (raw or "")[:360].replace("\n", " ")
            preview2 = (raw2 or "")[:360].replace("\n", " ")
            raise HTTPException(status_code=500, detail=f"Failed to parse AI JSON after retry: {exc}; preview1={preview1}; preview2={preview2}") from exc

    title = str(data.get("title") or f"{cleaned_topic} MCQ").strip()[:200]
    qlist = data.get("questions")
    if not isinstance(qlist, list) or not qlist:
        raise HTTPException(status_code=500, detail="AI response missing 'questions' list")

    out_questions: List[Dict[str, Any]] = []
    for idx, item in enumerate(qlist[:safe_count]):
        if not isinstance(item, dict):
            continue
        prompt_text = str(item.get("prompt") or "").strip()
        options = item.get("options")
        correct_index = item.get("correct_index")
        if not isinstance(options, list):
            options = []
        norm_options = [str(o or "").strip()[:500] for o in options if str(o or "").strip()]
        norm_options = norm_options[:4]
        while len(norm_options) < 4:
            norm_options.append(norm_options[-1] if norm_options else "")
        try:
            ci = int(correct_index)
        except Exception:
            ci = 0
        ci = max(0, min(ci, 3))

        # Validate through existing schema
        try:
            q_in = TestQuestionIn(
                prompt=prompt_text,
                options=norm_options,
                correct_index=ci,
                points=1,
                order=idx,
            )
        except Exception:
            continue

        out_questions.append({
            "prompt": q_in.prompt,
            "options": q_in.options,
            "correct_index": q_in.correct_index,
            "points": q_in.points,
            "order": idx,
        })

    if len(out_questions) < max(1, min(3, safe_count)):
        raise HTTPException(status_code=500, detail="AI returned too few valid questions")

    return title, out_questions, model_name


@teacher_router.post("/api/teacher/tests/ai", summary="Teacher: generate MCQ questions with AI")
def api_teacher_generate_test_ai(payload: TeacherAIGenerateTestIn, authorization: Optional[str] = Header(default=None)):
    _ = _require_teacher(authorization)
    title, out_questions, model = _generate_topic_mcq_for_teacher(payload.topic, payload.count)
    return {
        "topic": payload.topic,
        "title": title,
        "description": f"AI generated MCQ test on {payload.topic}.",
        "questions": out_questions,
        "model": model,
        "count": len(out_questions),
        "generated_at": datetime.utcnow().isoformat() + "Z",
    }


@teacher_router.post("/api/teacher/tests", summary="Create a test (MCQ)")
def api_create_test(payload: CreateTestIn, authorization: Optional[str] = Header(default=None)):
    teacher_id = _require_teacher(authorization)
    supabase = get_service_client()

    max_score = sum(max(q.points, 1) for q in payload.questions)
    test_row = _supabase_payload({
        "title": payload.title,
        "description": payload.description,
        "duration_seconds": payload.duration_seconds,
        "class_id": payload.class_id,
        "teacher_user_id": teacher_id,
        "max_score": max_score,
        "accepting_submissions": True if payload.accepting_submissions is None else bool(payload.accepting_submissions),
    })

    res = supabase.table("tests").insert(test_row).execute()
    if getattr(res, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (create test): {res.error}")
    test_id = (getattr(res, "data", None) or [{}])[0].get("id")
    if not test_id:
        raise HTTPException(status_code=500, detail="Failed to create test")

    question_rows = []
    for idx, q in enumerate(payload.questions):
        question_rows.append(_supabase_payload({
            "test_id": test_id,
            "prompt": q.prompt,
            "options": q.options,
            "correct_index": q.correct_index,
            "points": q.points,
            "question_order": q.order if q.order is not None else idx,
        }))

    qres = supabase.table("test_questions").insert(question_rows).execute()
    if getattr(qres, "error", None):
        # Best-effort cleanup of orphan test
        try:
            supabase.table("tests").delete().eq("id", test_id).execute()
        except Exception:
            pass
        raise HTTPException(status_code=500, detail=f"Supabase error (create questions): {qres.error}")

    return {"id": test_id, "max_score": max_score}


@teacher_router.get("/api/teacher/tests/{test_id}/attempts", summary="Teacher: view attempts and scores")
def api_list_attempts(test_id: str, authorization: Optional[str] = Header(default=None)):
    teacher_id = _require_teacher(authorization)
    supabase = get_service_client()
    test_row = _fetch_test_row(supabase, test_id)
    if test_row.get("teacher_user_id") != teacher_id:
        raise HTTPException(status_code=403, detail="You do not own this test")

    res = (
        supabase
        .table("test_attempts")
        .select("id,student_user_id,score,elapsed_seconds,started_at,submitted_at")
        .eq("test_id", test_id)
        .order("submitted_at", desc=True)
        .execute()
    )
    if getattr(res, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (list attempts): {res.error}")
    return {
        "test": {"id": test_id, "title": test_row.get("title"), "max_score": test_row.get("max_score")},
        "attempts": getattr(res, "data", None) or [],
    }


@teacher_router.get("/api/teacher/tests", summary="Teacher: list my tests")
def api_list_tests(authorization: Optional[str] = Header(default=None)):
    teacher_id = _require_teacher(authorization)
    supabase = get_service_client()
    res = (
        supabase
        .table("tests")
        .select("id,title,description,class_id,duration_seconds,max_score,accepting_submissions,created_at,updated_at")
        .eq("teacher_user_id", teacher_id)
        .order("created_at", desc=True)
        .limit(200)
        .execute()
    )
    if getattr(res, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (list tests): {res.error}")
    return res.data or []


@teacher_router.delete("/api/teacher/tests/{test_id}", summary="Teacher: delete a test")
def api_delete_test(test_id: str, authorization: Optional[str] = Header(default=None)):
    teacher_id = _require_teacher(authorization)
    supabase = get_service_client()
    test_row = _fetch_test_row(supabase, test_id)
    if test_row.get("teacher_user_id") != teacher_id:
        raise HTTPException(status_code=403, detail="You do not own this test")
    res = supabase.table("tests").delete().eq("id", test_id).execute()
    if getattr(res, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (delete test): {res.error}")
    return {"deleted": True, "id": test_id}


class UpdateTestIn(CreateTestIn):
    accepting_submissions: Optional[bool] = None


@teacher_router.put("/api/teacher/tests/{test_id}", summary="Teacher: update a test")
def api_update_test(test_id: str, payload: UpdateTestIn, authorization: Optional[str] = Header(default=None)):
    teacher_id = _require_teacher(authorization)
    supabase = get_service_client()
    test_row = _fetch_test_row(supabase, test_id)
    if test_row.get("teacher_user_id") != teacher_id:
        raise HTTPException(status_code=403, detail="You do not own this test")

    max_score = sum(max(q.points, 1) for q in payload.questions)
    updates = _supabase_payload({
        "title": payload.title,
        "description": payload.description,
        "duration_seconds": payload.duration_seconds,
        "class_id": payload.class_id,
        "max_score": max_score,
    })
    if payload.accepting_submissions is not None:
        updates["accepting_submissions"] = bool(payload.accepting_submissions)

    up = supabase.table("tests").update(updates).eq("id", test_id).execute()
    if getattr(up, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (update test): {up.error}")

    # Replace questions: delete old then insert new
    delq = supabase.table("test_questions").delete().eq("test_id", test_id).execute()
    if getattr(delq, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (delete old questions): {delq.error}")

    question_rows = []
    for idx, q in enumerate(payload.questions):
        question_rows.append(_supabase_payload({
            "test_id": test_id,
            "prompt": q.prompt,
            "options": q.options,
            "correct_index": q.correct_index,
            "points": q.points,
            "question_order": q.order if q.order is not None else idx,
        }))
    qres = supabase.table("test_questions").insert(question_rows).execute()
    if getattr(qres, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (insert new questions): {qres.error}")

    return {"id": test_id, "max_score": max_score, "accepting_submissions": updates.get("accepting_submissions", test_row.get("accepting_submissions", True))}


@teacher_router.patch("/api/teacher/tests/{test_id}/accepting", summary="Teacher: toggle accepting submissions")
def api_toggle_accepting(test_id: str, accepting: bool = Query(...), authorization: Optional[str] = Header(default=None)):
    teacher_id = _require_teacher(authorization)
    supabase = get_service_client()
    test_row = _fetch_test_row(supabase, test_id)
    if test_row.get("teacher_user_id") != teacher_id:
        raise HTTPException(status_code=403, detail="You do not own this test")
    res = supabase.table("tests").update({"accepting_submissions": bool(accepting)}).eq("id", test_id).execute()
    if getattr(res, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (toggle accepting): {res.error}")
    return {"id": test_id, "accepting_submissions": bool(accepting)}


@teacher_router.get("/api/teacher/tests/{test_id}/results", summary="Teacher: results and participation")
def api_test_results(test_id: str, authorization: Optional[str] = Header(default=None)):
    teacher_id = _require_teacher(authorization)
    supabase = get_service_client()
    test_row = _fetch_test_row(supabase, test_id)
    if test_row.get("teacher_user_id") != teacher_id:
        raise HTTPException(status_code=403, detail="You do not own this test")

    attempts_res = (
        supabase
        .table("test_attempts")
        .select("id,student_user_id,score,elapsed_seconds,started_at,submitted_at")
        .eq("test_id", test_id)
        .order("submitted_at", desc=True)
        .limit(500)
        .execute()
    )
    if getattr(attempts_res, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (results attempts): {attempts_res.error}")
    attempts = attempts_res.data or []

    class_students: List[Dict[str, Any]] = []
    profile_by_id: Dict[str, Dict[str, Any]] = {}
    class_id = test_row.get("class_id")

    # Collect IDs for attempts and class roster to hydrate names in a single profile lookup
    attempted_ids = {a.get("student_user_id") for a in attempts if a.get("student_user_id")}
    enrolled_ids: Set[str] = set()

    if class_id:
        try:
            enrolled = supabase.table("teacher_class_students").select("student_user_id").eq("class_id", class_id).execute()
            if not getattr(enrolled, "error", None):
                enrolled_ids = {r.get("student_user_id") for r in (enrolled.data or []) if r.get("student_user_id")}
        except Exception:
            enrolled_ids = set()

    try:
        need_ids = list({sid for sid in (attempted_ids | enrolled_ids) if sid})
        if need_ids:
            prof_res = (
                supabase
                .table("user_profiles")
                .select("auth_user_id,name,email")
                .in_("auth_user_id", need_ids)
                .execute()
            )
            if not getattr(prof_res, "error", None):
                for p in prof_res.data or []:
                    pid = p.get("auth_user_id")
                    if pid:
                        profile_by_id[pid] = {"name": p.get("name"), "email": p.get("email")}
    except Exception:
        profile_by_id = {}

    for attempt in attempts:
        sid = attempt.get("student_user_id")
        prof = profile_by_id.get(sid or "")
        if prof:
            attempt["name"] = prof.get("name")
            attempt["email"] = prof.get("email")

    students_not_attempted: List[Dict[str, Any]] = []
    if class_id and enrolled_ids:
        try:
            missing_ids = list(enrolled_ids - attempted_ids)
            if missing_ids:
                missing_profiles = [profile_by_id.get(mid) for mid in missing_ids]
                students_not_attempted = [
                    {"auth_user_id": mid, "name": (mp or {}).get("name"), "email": (mp or {}).get("email")}
                    for mid, mp in zip(missing_ids, missing_profiles)
                ]
        except Exception:
            students_not_attempted = []

    # Build full class roster with status when class is linked
    if class_id and enrolled_ids:
        attempt_map = {a.get("student_user_id"): a for a in attempts if a.get("student_user_id")}
        class_students = []
        for sid in enrolled_ids:
            att = attempt_map.get(sid)
            prof = profile_by_id.get(sid, {})
            status = "pending"
            if att:
                status = "in_progress" if not att.get("submitted_at") else "completed"
            class_students.append({
                "auth_user_id": sid,
                "name": prof.get("name"),
                "email": prof.get("email"),
                "status": status,
                "score": att.get("score") if att else None,
                "submitted_at": att.get("submitted_at") if att else None,
            })
        # Optional: keep deterministic order by name/email
        class_students.sort(key=lambda r: (r.get("name") or "", r.get("email") or "", r.get("auth_user_id") or ""))

    completed = len([a for a in attempts if a.get("submitted_at")])
    return {
        "test": {
            "id": test_id,
            "title": test_row.get("title"),
            "class_id": test_row.get("class_id"),
            "max_score": test_row.get("max_score"),
            "accepting_submissions": test_row.get("accepting_submissions", True),
        },
        "completed_count": completed,
        "attempts": attempts,
        "students_not_attempted": students_not_attempted,
        "class_students": class_students,
    }


@academics_router.get("/api/tests/{test_id}", summary="Fetch a test for taking")
def api_get_test(test_id: str, authorization: Optional[str] = Header(default=None)):
    user_id, _ = _get_auth_user(authorization)
    supabase = get_service_client()
    test_row = _fetch_test_row(supabase, test_id)
    _ensure_user_allowed_for_test(supabase, test_row, user_id)
    questions = _fetch_test_questions(supabase, test_id)

    attempt_res = _supabase_retry(lambda: (
        supabase
        .table("test_attempts")
        .select("id,submitted_at,score")
        .eq("test_id", test_id)
        .eq("student_user_id", user_id)
        .limit(1)
        .execute()
    ))
    if getattr(attempt_res, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (attempt check): {attempt_res.error}")
    attempt = (getattr(attempt_res, "data", None) or [None])[0]

    is_owner = test_row.get("teacher_user_id") == user_id
    sanitized_questions = []
    for q in questions:
        entry = {
            "id": q.get("id"),
            "prompt": q.get("prompt"),
            "options": q.get("options"),
            "points": q.get("points", 1),
            "order": q.get("question_order", 0),
        }
        if is_owner:
            entry["correct_index"] = q.get("correct_index")
        sanitized_questions.append(entry)

    return {
        "id": test_row.get("id"),
        "title": test_row.get("title"),
        "description": test_row.get("description"),
        "duration_seconds": test_row.get("duration_seconds"),
        "max_score": test_row.get("max_score"),
        "accepting_submissions": test_row.get("accepting_submissions", True),
        "attempt": attempt,
        "questions": sanitized_questions,
    }


@academics_router.post("/api/tests/{test_id}/start", summary="Start a test attempt (one per student)")
def api_start_attempt(test_id: str, authorization: Optional[str] = Header(default=None)):
    user_id, _ = _get_auth_user(authorization)
    supabase = get_service_client()
    test_row = _fetch_test_row(supabase, test_id)
    _ensure_user_allowed_for_test(supabase, test_row, user_id)
    if not test_row.get("accepting_submissions", True):
        raise HTTPException(status_code=410, detail="Test submissions are closed")

    def _attempt_lookup():
        return (
            supabase
            .table("test_attempts")
            .select("id,submitted_at,started_at")
            .eq("test_id", test_id)
            .eq("student_user_id", user_id)
            .limit(1)
            .execute()
        )

    check = _supabase_retry(_attempt_lookup, retries=5, base_delay=0.25)
    if getattr(check, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (attempt check): {check.error}")
    existing = (getattr(check, "data", None) or [])
    if existing:
        att = existing[0]
        if att.get("submitted_at"):
            raise HTTPException(status_code=409, detail="You already submitted this test")
        return {"id": att.get("id"), "started_at": att.get("started_at")}

    def _attempt_insert():
        return supabase.table("test_attempts").insert({
            "test_id": test_id,
            "student_user_id": user_id,
        }).execute()

    try:
        ins = _supabase_retry(_attempt_insert, retries=5, base_delay=0.25)
    except RETRYABLE_EXCEPTIONS as e:  # pragma: no cover - network timing dependent
        # If the insert actually succeeded server-side but the connection closed, a follow-up
        # lookup will return the row; otherwise we bubble a retryable error to the client.
        verify = _supabase_retry(_attempt_lookup, retries=5, base_delay=0.25)
        existing = (getattr(verify, "data", None) or [])
        if existing:
            att = existing[0]
            if att.get("submitted_at"):
                raise HTTPException(status_code=409, detail="You already submitted this test")
            return {"id": att.get("id"), "started_at": att.get("started_at")}
        raise HTTPException(status_code=503, detail="Temporary connection issue starting attempt; please retry") from e

    if getattr(ins, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (start attempt): {ins.error}")
    row = (getattr(ins, "data", None) or [{}])[0]
    return {"id": row.get("id"), "started_at": row.get("started_at")}


@academics_router.post("/api/tests/{test_id}/submit", summary="Submit a test attempt and score")
def api_submit_attempt(test_id: str, payload: SubmitAttemptIn, authorization: Optional[str] = Header(default=None)):
    user_id, _ = _get_auth_user(authorization)
    supabase = get_service_client()
    test_row = _fetch_test_row(supabase, test_id)
    _ensure_user_allowed_for_test(supabase, test_row, user_id)
    if not test_row.get("accepting_submissions", True):
        raise HTTPException(status_code=410, detail="Test submissions are closed")
    questions = _fetch_test_questions(supabase, test_id)

    # Fetch or create attempt record
    att_res = (
        supabase
        .table("test_attempts")
        .select("id,submitted_at")
        .eq("test_id", test_id)
        .eq("student_user_id", user_id)
        .limit(1)
        .execute()
    )
    if getattr(att_res, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (attempt lookup): {att_res.error}")
    att_rows = getattr(att_res, "data", None) or []
    if att_rows and att_rows[0].get("submitted_at"):
        raise HTTPException(status_code=409, detail="You already submitted this test")
    attempt_id = att_rows[0].get("id") if att_rows else None

    if not attempt_id:
        ins = supabase.table("test_attempts").insert({
            "test_id": test_id,
            "student_user_id": user_id,
        }).execute()
        if getattr(ins, "error", None):
            raise HTTPException(status_code=500, detail=f"Supabase error (start attempt late): {ins.error}")
        attempt_id = (getattr(ins, "data", None) or [{}])[0].get("id")

    if len(payload.answers) != len(questions):
        raise HTTPException(status_code=400, detail="Answer count does not match questions")

    score = 0
    for q, ans in zip(questions, payload.answers):
        try:
            ans_int = int(ans)
        except Exception:
            ans_int = -1
        if ans_int == q.get("correct_index"):
            score += int(q.get("points", 1))

    upd = supabase.table("test_attempts").update({
        "answers": payload.answers,
        "score": score,
        "elapsed_seconds": payload.elapsed_seconds,
        "submitted_at": datetime.utcnow().isoformat(),
    }).eq("id", attempt_id).execute()
    if getattr(upd, "error", None):
        raise HTTPException(status_code=500, detail=f"Supabase error (submit attempt): {upd.error}")

    return {
        "test_id": test_id,
        "attempt_id": attempt_id,
        "score": score,
        "max_score": test_row.get("max_score"),
    }


# --- YouTube Video Search endpoints ---

youtube_search_router = APIRouter(prefix="/api/youtube", tags=["youtube search"])


@youtube_search_router.get("/search")
def api_youtube_search(
    query: str = Query(..., description="Search query for YouTube videos"),
    num: int = Query(8, ge=1, le=20, description="Number of videos to return")
):
    """Search YouTube videos using SerpAPI and return metadata including thumbnails, views, channel info."""
    if not query or not query.strip():
        raise HTTPException(status_code=400, detail="Query parameter is required")
    
    try:
        videos = search_youtube_videos(query.strip(), num=num)
        return videos
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"YouTube search failed: {str(e)}")


@youtube_search_router.get("/channel-logo")
def api_youtube_channel_logo(
    channel_url: str = Query(..., description="Full YouTube channel URL")
):
    if not channel_url or not channel_url.strip():
        raise HTTPException(status_code=400, detail="channel_url is required")

    try:
        logo = get_channel_logo(channel_url.strip()) or get_default_channel_logo()
        return {"logo": logo}
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Channel logo lookup failed: {str(e)}")


# --- Streak API ---

class StreakResponse(BaseModel):
    current_streak: int = 0
    longest_streak: int = 0
    last_activity_date: Optional[str] = None
    next_milestone: int = 7
    prev_milestone: int = 0
    milestone_progress: int = 0
    days_completed: int = 0
    week_data: List[dict] = []

STREAK_MILESTONES = [7, 14, 21, 30, 60, 90, 180, 365]

def _get_next_milestone(current: int) -> int:
    for m in STREAK_MILESTONES:
        if current < m:
            return m
    return STREAK_MILESTONES[-1] + 100

def _get_prev_milestone(current: int) -> int:
    for i in range(len(STREAK_MILESTONES) - 1, -1, -1):
        if current >= STREAK_MILESTONES[i]:
            return STREAK_MILESTONES[i]
    return 0

def _compute_current_streak(active_dates, today: date) -> int:
    streak = 0
    day = today
    while day in active_dates:
        streak += 1
        day = day - timedelta(days=1)
    return streak

@academics_router.get("/api/streak", response_model=StreakResponse, summary="Get user streak data")
def get_user_streak(authorization: Optional[str] = Header(default=None)):
    token = _bearer_token_from_header(authorization)
    if not token:
        raise HTTPException(status_code=401, detail="Missing authorization")
    
    try:
        user_id = _get_user_id_with_retry(token)
    except Exception:
        raise HTTPException(status_code=401, detail="Invalid token")
    
    # Retry logic for transient Supabase connection errors
    max_retries = 3
    retry_delay = 0.5
    last_error = None
    
    for attempt in range(max_retries):
        try:
            supabase = get_service_client()
            today = date.today()
            yesterday = today - timedelta(days=1)
            
            # Get user profile ID
            prof_q = supabase.table("user_profiles").select("id").eq("auth_user_id", user_id).limit(1).execute()
            if not prof_q.data:
                raise HTTPException(status_code=404, detail="Profile not found")
            profile_id = prof_q.data[0]["id"]
            
            # Get or create streak record
            streak_q = supabase.table("notex_streak").select("*").eq("user_profile_id", profile_id).limit(1).execute()
            
            if not streak_q.data:
                # Create new streak record
                new_streak = {
                    "user_profile_id": profile_id,
                    "current_streak": 1,
                    "longest_streak": 1,
                    "last_activity_date": today.isoformat()
                }
                supabase.table("notex_streak").insert(new_streak).execute()
                streak_data = new_streak
            else:
                streak_data = streak_q.data[0]

            # Pull recent activity logs to compute streak and week data
            lookback_start = today - timedelta(days=400)
            activity_q = supabase.table("notex_activity_logs").select("activity_date").eq("user_profile_id", profile_id).gte("activity_date", lookback_start.isoformat()).order("activity_date", desc=True).limit(400).execute()
            active_dates = set()
            for a in (activity_q.data or []):
                if a.get("activity_date"):
                    try:
                        active_dates.add(date.fromisoformat(str(a["activity_date"])))
                    except Exception:
                        pass

            # Backfill with stored last_activity_date if logs are missing
            last_date = None
            last_date_str = streak_data.get("last_activity_date")
            if last_date_str:
                try:
                    last_date = date.fromisoformat(str(last_date_str))
                    active_dates.add(last_date)
                except Exception:
                    last_date = None

            # Count this visit as activity so streak continues for today
            active_dates.add(today)

            current = _compute_current_streak(active_dates, today)
            longest = max(int(streak_data.get("longest_streak") or 0), current)

            if current != int(streak_data.get("current_streak") or 0) or (streak_data.get("last_activity_date") != today.isoformat()):
                supabase.table("notex_streak").update({
                    "current_streak": current,
                    "longest_streak": longest,
                    "last_activity_date": today.isoformat(),
                    "updated_at": datetime.utcnow().isoformat()
                }).eq("user_profile_id", profile_id).execute()
                streak_data["current_streak"] = current
                streak_data["longest_streak"] = longest
                streak_data["last_activity_date"] = today.isoformat()

            # Calculate milestones
            next_m = _get_next_milestone(current)
            prev_m = _get_prev_milestone(current)
            range_val = next_m - prev_m
            progress = round(((current - prev_m) / range_val) * 100) if range_val > 0 else 0

            # Build week data
            week_start = today - timedelta(days=today.weekday())  # Monday
            day_names = ["M", "T", "W", "T", "F", "S", "S"]
            today_index = today.weekday()
            week_data = []
            for i in range(7):
                day_date = week_start + timedelta(days=i)
                is_today = i == today_index
                is_future = i > today_index
                is_active = day_date in active_dates
                week_data.append({
                    "day": day_names[i],
                    "date": day_date.day,
                    "is_today": is_today,
                    "is_active": is_active,
                    "is_future": is_future
                })

            days_completed = len([d for d in week_data if d["is_active"]])
            
            return StreakResponse(
                current_streak=current,
                longest_streak=longest,
                last_activity_date=streak_data.get("last_activity_date"),
                next_milestone=next_m,
                prev_milestone=prev_m,
                milestone_progress=progress,
                days_completed=days_completed,
                week_data=week_data
            )
        
        except Exception as e:
            last_error = e
            error_msg = str(e).lower()
            # Only retry on connection/protocol errors
            if "disconnect" in error_msg or "connection" in error_msg or "remote" in error_msg:
                import time
                print(f"[Streak] Retry {attempt + 1}/{max_retries} after error: {e}")
                time.sleep(retry_delay)
                continue
            else:
                raise
    
    # If all retries failed
    print(f"[Streak] All {max_retries} retries failed")
    raise HTTPException(status_code=500, detail=f"Service temporarily unavailable: {last_error}")

@academics_router.post("/api/streak/ping", summary="Record user activity for streak")
def ping_streak(authorization: Optional[str] = Header(default=None)):
    """Call this endpoint when user is active to maintain streak."""
    token = _bearer_token_from_header(authorization)
    if not token:
        raise HTTPException(status_code=401, detail="Missing authorization")
    
    try:
        user_id = _get_user_id_with_retry(token)
    except Exception:
        raise HTTPException(status_code=401, detail="Invalid token")
    
    supabase = get_service_client()
    today = date.today()
    
    # Get user profile ID
    prof_q = supabase.table("user_profiles").select("id").eq("auth_user_id", user_id).limit(1).execute()
    if not prof_q.data:
        raise HTTPException(status_code=404, detail="Profile not found")
    profile_id = prof_q.data[0]["id"]
    
    # Record activity log for today (upsert)
    try:
        # Check if already logged today
        existing = supabase.table("notex_activity_logs").select("id").eq("user_profile_id", profile_id).eq("activity_date", today.isoformat()).limit(1).execute()
        if not existing.data:
            supabase.table("notex_activity_logs").insert({
                "user_profile_id": profile_id,
                "activity_date": today.isoformat()
            }).execute()
    except Exception:
        pass  # Ignore duplicate key errors
    
    return {"status": "ok", "date": today.isoformat()}


# --- FastAPI app ---

def create_app() -> FastAPI:
    app = FastAPI(title="PaperX Unified API", version="1.0.0")

    # CORS: allow dev origins and support Authorization header
    # Note: Using allow_origin_regex to correctly echo Origin when credentials are enabled.
    app.add_middleware(
        CORSMiddleware,
        allow_origins=[
            "https://paperx.tech",
            "https://www.paperx.tech",
            "https://squid-app-6jvdq.ondigitalocean.app",
            "https://uppzpkmpxgyipjzcskva.supabase.co",
            "http://127.0.0.1:5500",
            "http://127.0.0.1:8000",
            "http://localhost:5500",
            "http://localhost:8000",
            "http://localhost",
        ],
        allow_credentials=True,
        allow_methods=["*"],
        allow_headers=["*"],
    )

    # Compress large HTML/CSS/JS/JSON responses to reduce bandwidth
    app.add_middleware(GZipMiddleware, minimum_size=1000, compresslevel=6)

    # Cache static assets to reduce repeated downloads
    STATIC_CACHE_EXTENSIONS = {
        ".css", ".js", ".mjs", ".png", ".jpg", ".jpeg", ".webp", ".gif", ".svg",
        ".ico", ".woff2", ".woff", ".ttf", ".mp4", ".webm"
    }
    HTML_EXTENSIONS = {".html", ".htm"}

    @app.middleware("http")
    async def add_static_cache_headers(request: Request, call_next):  # type: ignore[override]
        response = await call_next(request)
        if request.method in ("GET", "HEAD"):
            path = request.url.path or ""
            if path.startswith("/ui/") or path.startswith("/assets/"):
                ext = Path(path).suffix.lower()
                if ext in HTML_EXTENSIONS:
                    response.headers.setdefault("Cache-Control", "public, max-age=120")
                elif ext in STATIC_CACHE_EXTENSIONS:
                    response.headers.setdefault("Cache-Control", "public, max-age=604800, immutable")
                else:
                    response.headers.setdefault("Cache-Control", "public, max-age=3600")
        return response

    app.include_router(projects_router)
    app.include_router(notes_router)
    app.include_router(learning_tracks_router)
    app.include_router(print_router)
    app.include_router(academics_router)
    app.include_router(marketplace_router)
    app.include_router(teacher_router)
    app.include_router(youtube_transcript_router)
    app.include_router(youtube_search_router)
    app.include_router(yt_transcript_router, prefix="/api/youtube", tags=["youtube transcripts (raw)"])
    app.include_router(tunex_router.router)

    # Simple request logger to aid debugging 405/OPTIONS/CORS issues
    @app.middleware("http")
    async def log_requests(request: Request, call_next):  # type: ignore[override]
        # Use only path (no query params) and truncate if too long
        path = request.url.path
        if len(path) > 60:
            path = path[:57] + "..."
        try:
            print(f"[REQ] {request.method} {path}")
        except Exception:
            pass
        try:
            response = await call_next(request)
        except Exception as e:
            try:
                print(f"[ERR] {request.method} {path} -> {e}")
            except Exception:
                pass
            raise
        try:
            print(f"[RES] {request.method} {path} {response.status_code}")
        except Exception:
            pass
        return response

    @app.get("/")
    def root():
        return {
            "message": "PaperX API running",
            "notes_ui": "/ui/notes_generator.html",
            "transcripts_ui": "/ui/youtube-transcript.html",
            "youtube_videos_ui": "/ui/youtube_videos.html",
        }

    @app.get("/health", tags=["system"])
    def health():
        """Lightweight health probe used by the Render load balancer."""
        return {"status": "ok", "timestamp": datetime.utcnow().isoformat() + "Z"}

    ui_dir = Path(__file__).resolve().parent / "ui"
    if ui_dir.is_dir():
        app.mount("/ui", StaticFiles(directory=ui_dir), name="ui")
    # Static assets (images, uploaded teacher ID cards, etc.)
    assets_dir = Path(__file__).resolve().parent / "assets"
    if assets_dir.is_dir():
        # Serve at /assets so front-end references like ../assets/... resolve
        app.mount("/assets", StaticFiles(directory=assets_dir), name="assets")

    return app


app = create_app()

# ==========================================
# ANALYTICS MODULE (Injected)
# ==========================================
from datetime import timedelta

class AnalyticsSessionStart(BaseModel):
    user_id: Optional[str] = None
    user_agent: Optional[str] = None

class AnalyticsHeartbeat(BaseModel):
    session_id: str

class AnalyticsEventModel(BaseModel):
    session_id: str
    user_id: Optional[str] = None
    event_type: str
    event_data: Dict[str, Any] = {}

class TopicFeedbackModel(BaseModel):
    user_id: str
    topic_id: str
    is_helpful: bool
    comment: Optional[str] = None

analytics_router = APIRouter(prefix="/analytics", tags=["analytics"])

@analytics_router.post("/session/start")
async def start_session(req: AnalyticsSessionStart, request: Request):
    supabase = get_service_client()
    
    user_agent = req.user_agent or request.headers.get("user-agent")
    client_ip = request.client.host if request.client else None
    
    uid = req.user_id
    if uid and not uid.strip():
        uid = None

    data = {
        "user_id": uid,
        "user_agent": user_agent,
        "ip": client_ip,
        "last_seen_at": datetime.utcnow().isoformat()
    }
    
    try:
        res = supabase.table("user_sessions").insert(data).execute()
        if res.data:
            return {"session_id": res.data[0]["id"]}
        # Fallback if no data returned (RLS or otherwise), generate UUID
        return {"session_id": str(uuid.uuid4())} 
    except Exception as e:
        print(f"Analytics Error: {e}")
        return {"session_id": str(uuid.uuid4())} # Graceful degradation

@analytics_router.post("/session/heartbeat")
async def analytics_heartbeat(payload: AnalyticsHeartbeat):
    try:
        supabase = get_service_client()
        supabase.table("user_sessions").update({
            "last_seen_at": datetime.utcnow().isoformat()
        }).eq("id", payload.session_id).execute()
    except Exception:
        pass
    return {"status": "ok"}

@analytics_router.post("/event")
async def analytics_track_event(payload: AnalyticsEventModel):
    try:
        supabase = get_service_client()
        data = {
            "session_id": payload.session_id,
            "user_id": payload.user_id if payload.user_id else None,
            "event_type": payload.event_type,
            "event_data": payload.event_data
        }
        if not data["user_id"]: del data["user_id"]
        supabase.table("analytics_events").insert(data).execute()
    except Exception as e:
        print(f"Analytics Event Error: {e}")
    return {"status": "ok"}

@analytics_router.post("/feedback/topic")
async def analytics_topic_feedback(payload: TopicFeedbackModel):
    try:
        supabase = get_service_client()
        data = {
            "user_id": payload.user_id,
            "topic_id": payload.topic_id,
            "is_helpful": payload.is_helpful,
            "comment": payload.comment
        }
        supabase.table("topic_feedback").insert(data).execute()
    except Exception:
        pass
    return {"status": "ok"}

@analytics_router.get("/dashboard")
async def analytics_dashboard_metrics():
    try:
        supabase = get_service_client()
        now = datetime.utcnow()
        day_ago = (now - timedelta(days=1)).isoformat()
        week_ago = (now - timedelta(days=7)).isoformat()

        # 1. Active Users (DAU/WAU)
        sessions_24h_res = supabase.table("user_sessions").select("user_id").gte("started_at", day_ago).execute()
        sessions_7d_res = supabase.table("user_sessions").select("user_id", "started_at").gte("started_at", week_ago).execute()
        
        sessions_24h = getattr(sessions_24h_res, 'data', []) or []
        sessions_7d = getattr(sessions_7d_res, 'data', []) or []

        dau = len({s["user_id"] for s in sessions_24h if s.get("user_id")})
        wau_users = {s["user_id"] for s in sessions_7d if s.get("user_id")}
        wau = len(wau_users)

        # 2. Avg Study Time
        # Needs last_seen - started_at. We need to fetch times.
        # Re-fetch 24h with times
        sessions_times_res = supabase.table("user_sessions").select("started_at,last_seen_at").gte("started_at", day_ago).execute()
        sessions_times = getattr(sessions_times_res, 'data', []) or []
        
        total_minutes = 0.0
        for s in sessions_times:
            if s.get("started_at") and s.get("last_seen_at"):
                try:
                    start = datetime.fromisoformat(s["started_at"].replace('Z', '+00:00'))
                    end = datetime.fromisoformat(s["last_seen_at"].replace('Z', '+00:00'))
                    diff = (end - start).total_seconds() / 60
                    if 0 < diff < 480: # Cap at 8h to ignore stuck sessions
                        total_minutes += diff
                except: pass
        
        avg_study_time = (total_minutes / dau) if dau > 0 else 0
        avg_sessions = (len(sessions_7d) / wau) if wau > 0 else 0

        # 3. Feature Usage
        events_7d_res = supabase.table("analytics_events").select("event_type,event_data,user_id").gte("created_at", week_ago).execute()
        events_7d = getattr(events_7d_res, 'data', []) or []

        note_viewers = set()
        blink_viewers = set()
        lab_users = set()
        
        counts = {}

        for e in events_7d:
            et = e.get("event_type")
            counts[et] = counts.get(et, 0) + 1
            uid = e.get("user_id")
            if not uid: continue
            
            if et == "note_viewed":
                data = e.get("event_data") or {}
                # Handle both dict and string if Supabase returns weirdly, but usually dict
                if isinstance(data, str):
                     try: data = json.loads(data)
                     except: data = {}
                
                variant = data.get("variant", "detailed")
                if variant == "cheatsheet":
                    blink_viewers.add(uid)
                else:
                    note_viewers.add(uid)
            elif et == "lab_started":
                lab_users.add(uid)

        # Calc Feature Volume (Counts)
        notex_count = counts.get("note_viewed", 0)
        
        # Blink count (cheatsheet variant of note_viewed)
        blink_count = 0
        for e in events_7d:
             et = e.get("event_type")
             if et == "note_viewed":
                 data = e.get("event_data") or {}
                 if isinstance(data, str):
                     try: data = json.loads(data)
                     except: data = {}
                 if data.get("variant") == "cheatsheet":
                     blink_count += 1
        
        labx_count = counts.get("lab_started", 0)

        # 4. Learning Metrics / Funnel (Simplistic)
        # Topic opened -> Note viewed -> Lab started
        topic_opens = counts.get("topic_opened", 0)
        note_views = counts.get("note_viewed", 0)
        lab_starts = counts.get("lab_started", 0)
        
        # Funnel metrics
        raw_funnel_1 = (note_views / topic_opens * 100) if topic_opens > 0 else 0
        raw_funnel_2 = (lab_starts / note_views * 100) if note_views > 0 else 0
        
        funnel_1 = round(min(raw_funnel_1, 100.0), 2)
        funnel_2 = round(min(raw_funnel_2, 100.0), 2)

        # 5. Advanced Learning Metrics (Real Logic)
        
        # Note Completion: % of note users who have a 'scroll_depth' event >= 70
        scroll_users = set()
        completed_users = set()
        blink_counts = {} # uid -> count of cheatsheet views
        
        for e in events_7d:
            et = e.get("event_type")
            uid = e.get("user_id")
            if not uid: continue
            
            if et == "scroll_depth":
                data = e.get("event_data") or {}
                if isinstance(data, str):
                    try: data = json.loads(data)
                    except: data = {}
                depth = int(data.get("depth", 0))
                # Only consider it a "note scroll" if on relevant pages
                page = data.get("page", "")
                if "notes_generator" in page or "note_detail" in page:
                     scroll_users.add(uid)
                     if depth >= 70:
                         completed_users.add(uid)
            
            elif et == "note_viewed":
                data = e.get("event_data") or {}
                if isinstance(data, str):
                    try: data = json.loads(data)
                    except: data = {}
                variant = data.get("variant")
                if variant == "cheatsheet":
                    blink_counts[uid] = blink_counts.get(uid, 0) + 1

        # Note Completion Rate
        note_completion_rate = round((len(completed_users) / len(scroll_users) * 100), 1) if scroll_users else 0
        
        # Blink Revisit Rate: % of blink users who viewed >= 2 blinks (or same blink 2x, simpler to just count total views > 1)
        revisit_users = [u for u, c in blink_counts.items() if c >= 2]
        blink_total_users = len(blink_counts)
        blink_revisit = round((len(revisit_users) / blink_total_users * 100), 1) if blink_total_users > 0 else 0

        # 6. User Activity Details (User List)
        users_map = {} # uid -> { notex, blink, labx, sessions, last_seen }
        
        # Init from WAU list
        for uid in wau_users:
            users_map[uid] = { "notex": 0, "blink": 0, "labx": 0, "sessions": 0, "last_seen": "", "id": uid }

        # Aggregation
        for e in events_7d:
            uid = e.get("user_id")
            if not uid or uid not in users_map: continue
            
            et = e.get("event_type")
            data = e.get("event_data") or {}
            if isinstance(data, str):
                 try: data = json.loads(data)
                 except: data = {}
            
            if et == "note_viewed":
                if data.get("variant") == "cheatsheet":
                    users_map[uid]["blink"] += 1
                else:
                    users_map[uid]["notex"] += 1
            elif et == "lab_started":
                users_map[uid]["labx"] += 1
        
        # Session counts + Last active
        for s in sessions_7d:
            uid = s.get("user_id")
            if not uid or uid not in users_map: continue
            users_map[uid]["sessions"] += 1
            # Update last_seen if fresher
            curr = users_map[uid]["last_seen"]
            started = s.get("started_at")
            if started and (not curr or started > curr):
                users_map[uid]["last_seen"] = started

        # Fetch Profiles
        users_list = []
        profile_id_map = {}  # auth_user_id -> profile id
        if wau_users:
            try:
                prof_res = supabase.table("user_profiles").select("id, auth_user_id, name, email, profile_image_url").in_("auth_user_id", list(wau_users)).execute()
                profiles = getattr(prof_res, 'data', []) or []
                for p in profiles:
                    uid = p.get("auth_user_id")
                    if uid in users_map:
                        users_map[uid]["name"] = p.get("name")
                        users_map[uid]["email"] = p.get("email")
                        users_map[uid]["image"] = p.get("profile_image_url")
                        users_map[uid]["streak"] = 0  # default
                        profile_id_map[p.get("id")] = uid
            except: pass
        
        # Fetch Streak data from notex_streak table
        if profile_id_map:
            try:
                streak_res = supabase.table("notex_streak").select("user_profile_id, current_streak").in_("user_profile_id", list(profile_id_map.keys())).execute()
                streaks = getattr(streak_res, 'data', []) or []
                for s in streaks:
                    profile_id = s.get("user_profile_id")
                    if profile_id in profile_id_map:
                        uid = profile_id_map[profile_id]
                        users_map[uid]["streak"] = s.get("current_streak", 0)
            except: pass
        
        # Convert to list
        for uid, stats in users_map.items():
            users_list.append(stats)
        
        # Sort by most active (sessions or last active)
        users_list.sort(key=lambda x: x.get("last_seen", ""), reverse=True)

        return {
            "kpi": {
                "dau": dau,
                "wau": wau,
                "avg_study_time": round(avg_study_time, 1),
                "sessions_per_user": round(avg_sessions, 1),
                "retention_7d": 85 # Dummy/Placeholder for now as logic is complex
            },
            "features": {
                "notex": notex_count,
                "blink": blink_count,
                "labx": labx_count,
                "avg_time_note": 12 # Placeholder
            },
            "learning": {
                "note_completion_rate": note_completion_rate,
                "blink_revisit": blink_revisit,
                "revisited_topics": len(revisit_users)
            },
            "funnel": {
                "topic_to_note": funnel_1,
                "note_to_lab": funnel_2
            },
            "users": users_list
        }
    except Exception as e:
        print(f"Dashboard Error: {e}")
        return {"error": str(e)}


def _parse_iso_datetime_maybe(value: Optional[str]) -> Optional[datetime]:
    if not value:
        return None
    try:
        s = str(value)
        # Supabase typically returns ISO8601 with Z.
        if s.endswith("Z"):
            s = s[:-1] + "+00:00"
        dt = datetime.fromisoformat(s)
        if dt.tzinfo is None:
            dt = dt.replace(tzinfo=timezone.utc)
        return dt
    except Exception:
        return None


@analytics_router.get("/active-users", summary="Active users in last 24h")
async def analytics_active_users_last_24h(limit: int = Query(200, ge=1, le=2000)):
    """
    Returns users with at least one analytics event in the last 24 hours.
    Sorted by most recent activity (descending).

    For each user:
    - last_active_24h: most recent event in last 24h
    - first_active_today / last_active_today: min/max event time for UTC 'today'
    - interactions_today: count of events for UTC 'today'
    """
    supabase = get_service_client()

    # Use server-local timezone for "today" so the UI matches what you expect (morning vs now).
    supabase = get_service_client()

    local_now = datetime.now().astimezone()
    local_tz = local_now.tzinfo or timezone.utc
    now_utc = local_now.astimezone(timezone.utc)
    since_24h_utc = now_utc - timedelta(hours=24)
    since_24h = since_24h_utc.isoformat()

    today_start_local_dt = local_now.replace(hour=0, minute=0, second=0, microsecond=0)
    today_start = today_start_local_dt.isoformat()
    today_date_local = today_start_local_dt.date()

    # Note: limit is on users; caps below are on rows (sessions/events).
    sessions_cap = max(5000, min(50000, limit * 250))
    events_cap = max(5000, min(50000, limit * 250))

    # 1) Sessions: used for accurate first/last activity today
    s_res = supabase.table("user_sessions").select("user_id,started_at,last_seen_at").gte("last_seen_at", since_24h).order("last_seen_at", desc=True).limit(sessions_cap).execute()
    sessions = getattr(s_res, "data", []) or []

    # 2) Events: used for interaction counts (and as fallback signal)
    ev_res = supabase.table("analytics_events").select("user_id,created_at").gte("created_at", since_24h).order("created_at", desc=True).limit(events_cap).execute()
    events = getattr(ev_res, "data", []) or []

    users: Dict[str, Dict[str, Any]] = {}

    def _ensure(uid: str) -> Dict[str, Any]:
        st = users.get(uid)
        if st is None:
            st = {
                "user_id": uid,
                "last_active_24h": None,
                "first_active_today": None,
                "last_active_today": None,
                "interactions_today": 0,
            }
            users[uid] = st
        return st

    # Aggregate sessions
    for s in sessions:
        uid = s.get("user_id")
        if not uid:
            continue
        uid = str(uid)
        started_dt = _parse_iso_datetime_maybe(s.get("started_at"))
        last_seen_dt = _parse_iso_datetime_maybe(s.get("last_seen_at"))
        if last_seen_dt is None and started_dt is None:
            continue

        stats = _ensure(uid)

        # last_active_24h from last_seen
        if last_seen_dt is not None:
            last_seen_utc = last_seen_dt.astimezone(timezone.utc)
            curr_last = stats.get("last_active_24h")
            if curr_last is None or last_seen_utc > curr_last:
                stats["last_active_24h"] = last_seen_utc

        # today's first/last: use both started_at and last_seen_at if they fall on local 'today'
        for dt in (started_dt, last_seen_dt):
            if dt is None:
                continue
            dt_local = dt.astimezone(local_tz)
            if dt_local.date() != today_date_local:
                continue
            dt_utc = dt.astimezone(timezone.utc)
            fa = stats.get("first_active_today")
            la = stats.get("last_active_today")
            if fa is None or dt_utc < fa:
                stats["first_active_today"] = dt_utc
            if la is None or dt_utc > la:
                stats["last_active_today"] = dt_utc

    # Aggregate events (interactions + fallback timestamps)
    for e in events:
        uid = e.get("user_id")
        if not uid:
            continue
        uid = str(uid)
        created_dt = _parse_iso_datetime_maybe(e.get("created_at"))
        if created_dt is None:
            continue
        created_utc = created_dt.astimezone(timezone.utc)
        created_local = created_dt.astimezone(local_tz)

        stats = _ensure(uid)

        curr_last = stats.get("last_active_24h")
        if curr_last is None or created_utc > curr_last:
            stats["last_active_24h"] = created_utc

        if created_local.date() == today_date_local:
            stats["interactions_today"] += 1
            fa = stats.get("first_active_today")
            la = stats.get("last_active_today")
            if fa is None or created_utc < fa:
                stats["first_active_today"] = created_utc
            if la is None or created_utc > la:
                stats["last_active_today"] = created_utc

    user_ids = list(users.keys())

    # Enrich: profiles (name/email + ids for clg/dept + sem)
    profiles_by_uid: Dict[str, Dict[str, Any]] = {}
    profile_id_by_uid: Dict[str, str] = {}
    if user_ids:
        try:
            prof_res = supabase.table("user_profiles").select(
                "id,auth_user_id,name,email,semester,college_id,department_id,batch_id"
            ).in_("auth_user_id", user_ids).execute()
            profiles = getattr(prof_res, "data", []) or []
            for p in profiles:
                puid = p.get("auth_user_id")
                pid = p.get("id")
                if puid:
                    profiles_by_uid[str(puid)] = p
                    if pid:
                        profile_id_by_uid[str(puid)] = str(pid)
        except Exception:
            profiles_by_uid = {}
            profile_id_by_uid = {}

    # Enrich: latest education row per profile (section + possible overrides)
    edu_by_profile_id: Dict[str, Dict[str, Any]] = {}
    profile_ids = [pid for pid in profile_id_by_uid.values() if pid]
    if profile_ids:
        try:
            edu_limit = min(5000, max(500, len(profile_ids) * 10))
            edu_res = supabase.table("user_education").select(
                "user_profile_id,current_semester,section,college_id,department_id,updated_at"
            ).in_("user_profile_id", profile_ids).order("updated_at", desc=True).limit(edu_limit).execute()
            edu_rows = getattr(edu_res, "data", []) or []
            for r in edu_rows:
                upid = r.get("user_profile_id")
                if not upid:
                    continue
                upid = str(upid)
                if upid not in edu_by_profile_id:
                    edu_by_profile_id[upid] = r
        except Exception:
            edu_by_profile_id = {}

    # Resolve college/department names
    college_ids: Set[str] = set()
    dept_ids: Set[str] = set()
    for uid in user_ids:
        p = profiles_by_uid.get(uid) or {}
        pid = profile_id_by_uid.get(uid)
        edu = edu_by_profile_id.get(pid or "") if pid else None
        cid = (edu.get("college_id") if edu else None) or p.get("college_id")
        did = (edu.get("department_id") if edu else None) or p.get("department_id")
        if cid:
            college_ids.add(str(cid))
        if did:
            dept_ids.add(str(did))

    colleges_map: Dict[str, str] = {}
    if college_ids:
        try:
            c_res = supabase.table("colleges").select("id,name").in_("id", list(college_ids)).execute()
            for c in (getattr(c_res, "data", []) or []):
                if c.get("id"):
                    colleges_map[str(c["id"])] = c.get("name")
        except Exception:
            colleges_map = {}

    depts_map: Dict[str, str] = {}
    if dept_ids:
        try:
            d_res = supabase.table("departments").select("id,name").in_("id", list(dept_ids)).execute()
            for d in (getattr(d_res, "data", []) or []):
                if d.get("id"):
                    depts_map[str(d["id"])] = d.get("name")
        except Exception:
            depts_map = {}

    rows: List[Dict[str, Any]] = []
    for uid, stats in users.items():
        prof = profiles_by_uid.get(uid) or {}
        pid = profile_id_by_uid.get(uid)
        edu = edu_by_profile_id.get(pid or "") if pid else None

        sem = None
        if edu and edu.get("current_semester") is not None:
            sem = edu.get("current_semester")
        else:
            sem = prof.get("semester")

        section_val = (edu.get("section") if edu else None)
        cid = (edu.get("college_id") if edu else None) or prof.get("college_id")
        did = (edu.get("department_id") if edu else None) or prof.get("department_id")

        rows.append({
            "user_id": uid,
            "name": prof.get("name"),
            "email": prof.get("email"),
            "college": colleges_map.get(str(cid)) if cid else None,
            "department": depts_map.get(str(did)) if did else None,
            "semester": sem,
            "section": section_val,
            "last_active_24h": (stats.get("last_active_24h").isoformat() if stats.get("last_active_24h") else None),
            "first_active_today": (stats.get("first_active_today").isoformat() if stats.get("first_active_today") else None),
            "last_active_today": (stats.get("last_active_today").isoformat() if stats.get("last_active_today") else None),
            "interactions_today": int(stats.get("interactions_today") or 0),
        })

    rows.sort(key=lambda r: (r.get("last_active_24h") or ""), reverse=True)
    rows = rows[:limit]

    return {
        "count_24h": len(users),
        "since_24h": since_24h,
        "today_start": today_start,
        "timezone": str(local_tz),
        "users": rows,
    }

app.include_router(analytics_router)

# -------------------- Lcoding Learning Tracks --------------------

class LcodingLanguageBase(BaseModel):
    name: str
    # slug removed
    description: Optional[str] = None
    logo_url: Optional[str] = None

class LcodingLanguageCreate(LcodingLanguageBase):
    pass

class LcodingLanguage(LcodingLanguageBase):
    id: str
    created_at: str
    updated_at: str

    class Config:
        orm_mode = True

class LcodingLevelBase(BaseModel):
    title: str
    order_index: int = 0

class LcodingLevelCreate(LcodingLevelBase):
    pass

class LcodingLevel(LcodingLevelBase):
    id: str
    language_id: str
    created_at: str
    updated_at: str

class LcodingSectionBase(BaseModel):
    title: str
    order_index: int = 0

class LcodingSectionCreate(LcodingSectionBase):
    pass

class LcodingSection(LcodingSectionBase):
    id: str
    level_id: str
    created_at: str
    updated_at: str

class LcodingTopicBase(BaseModel):
    title: str
    order_index: int = 0

class LcodingTopicCreate(LcodingTopicBase):
    pass

class LcodingTopic(LcodingTopicBase):
    id: str
    section_id: str
    created_at: str
    updated_at: str



class LcodingTopicUpdate(BaseModel):
    title: Optional[str] = None
    order_index: Optional[int] = None

lcoding_router = APIRouter(prefix="/api/lcoding", tags=["lcoding"])

@lcoding_router.get("/languages", response_model=List[LcodingLanguage])
def get_lcoding_languages():
    supabase = get_service_client()
    res = supabase.table("lcoding_languages").select("*").order("name").execute()
    return res.data or []

@lcoding_router.post("/languages", response_model=LcodingLanguage)
async def create_lcoding_language(
    name: str = Form(...),
    description: Optional[str] = Form(None),
    logo: Optional[UploadFile] = File(None)
):
    supabase = get_service_client()
    
    logo_url = None
    if logo:
        try:
            file_content = await logo.read()
            file_ext = logo.filename.split(".")[-1] if "." in logo.filename else "png"
            file_name = f"lcoding-logos/{uuid.uuid4()}.{file_ext}"
            
            # Upload to 'tunex' bucket
            res = supabase.storage.from_("tunex").upload(
                path=file_name,
                file=file_content,
                file_options={"content-type": logo.content_type}
            )
            
            # Get public URL
            logo_url = supabase.storage.from_("tunex").get_public_url(file_name)
        except Exception as e:
            print(f"Logo upload failed: {e}")
            
    payload = {
        "name": name,
        "description": description,
        "logo_url": logo_url,
        "updated_at": datetime.utcnow().isoformat()
    }
    
    res = supabase.table("lcoding_languages").insert(payload).execute()
    if not res.data:
        raise HTTPException(status_code=500, detail="Failed to create language")
    return res.data[0]

@lcoding_router.put("/languages/{lang_id}", response_model=LcodingLanguage)
async def update_lcoding_language(
    lang_id: str,
    name: str = Form(...),
    description: Optional[str] = Form(None),
    logo: Optional[UploadFile] = File(None)
):
    supabase = get_service_client()
    
    existing = supabase.table("lcoding_languages").select("id, logo_url").eq("id", lang_id).single().execute()
    if not existing.data:
        raise HTTPException(status_code=404, detail="Language not found")

    payload = {
        "name": name,
        "description": description,
        "updated_at": datetime.utcnow().isoformat()
    }

    if logo:
        try:
            file_content = await logo.read()
            file_ext = logo.filename.split(".")[-1] if "." in logo.filename else "png"
            file_name = f"lcoding-logos/{uuid.uuid4()}.{file_ext}"
            
            res = supabase.storage.from_("tunex").upload(
                path=file_name,
                file=file_content,
                file_options={"content-type": logo.content_type}
            )
            logo_url = supabase.storage.from_("tunex").get_public_url(file_name)
            payload["logo_url"] = logo_url
        except Exception as e:
            print(f"Logo upload failed: {e}")
            
    res = supabase.table("lcoding_languages").update(payload).eq("id", lang_id).execute()
    if not res.data:
         raise HTTPException(status_code=500, detail="Failed to update language")
    return res.data[0]

@lcoding_router.get("/languages/{lang_id}", response_model=LcodingLanguage)
def get_lcoding_language(lang_id: str):
    supabase = get_service_client()
    res = supabase.table("lcoding_languages").select("*").eq("id", lang_id).single().execute()
    if not res.data:
        raise HTTPException(status_code=404, detail="Language not found")
    return res.data

# --- Levels ---

@lcoding_router.get("/languages/{lang_id}/levels", response_model=List[LcodingLevel])
def get_lcoding_levels(lang_id: str):
    supabase = get_service_client()
    res = supabase.table("lcoding_levels").select("*").eq("language_id", lang_id).order("order_index").execute()
    return res.data or []

@lcoding_router.post("/languages/{lang_id}/levels", response_model=LcodingLevel)
def create_lcoding_level(lang_id: str, level: LcodingLevelCreate):
    supabase = get_service_client()
    payload = level.dict()
    payload["language_id"] = lang_id
    payload["updated_at"] = datetime.utcnow().isoformat()
    res = supabase.table("lcoding_levels").insert(payload).execute()
    if not res.data:
        raise HTTPException(status_code=500, detail="Failed to create level")
    return res.data[0]
    
@lcoding_router.get("/levels/{level_id}", response_model=LcodingLevel)
def get_lcoding_level(level_id: str):
    supabase = get_service_client()
    res = supabase.table("lcoding_levels").select("*").eq("id", level_id).single().execute()
    if not res.data:
        raise HTTPException(status_code=404, detail="Level not found")
    return res.data

@lcoding_router.put("/levels/{level_id}", response_model=LcodingLevel)
def update_lcoding_level(level_id: str, level: LcodingLevelCreate):
    supabase = get_service_client()
    payload = level.dict()
    payload["updated_at"] = datetime.utcnow().isoformat()
    # Ensure we don't accidentally wipe out language_id if pydantic excludes it, 
    # but since it's an update, Supabase handles partials for us if we sent them, 
    # but here we are sending full payload. Ideally we only update what changed.
    # LcodingLevelCreate has title and order_index.
    
    res = supabase.table("lcoding_levels").update(payload).eq("id", level_id).execute()
    if not res.data:
        raise HTTPException(status_code=500, detail="Failed to update level")
    return res.data[0]

@lcoding_router.delete("/levels/{level_id}")
def delete_lcoding_level(level_id: str):
    supabase = get_service_client()
    res = supabase.table("lcoding_levels").delete().eq("id", level_id).execute()
    if not res.data:
        raise HTTPException(status_code=404, detail="Level not found or failed to delete")
    return {"message": "Level deleted successfully"}

# --- Sections (now under Levels) ---

@lcoding_router.get("/levels/{level_id}/sections", response_model=List[LcodingSection])
def get_lcoding_sections(level_id: str):
    supabase = get_service_client()
    res = supabase.table("lcoding_sections").select("*").eq("level_id", level_id).order("order_index").execute()
    return res.data or []

@lcoding_router.post("/levels/{level_id}/sections", response_model=LcodingSection)
def create_lcoding_section(level_id: str, section: LcodingSectionCreate):
    supabase = get_service_client()
    payload = section.dict()
    payload["level_id"] = level_id
    payload["updated_at"] = datetime.utcnow().isoformat()
    res = supabase.table("lcoding_sections").insert(payload).execute()
    if not res.data:
        raise HTTPException(status_code=500, detail="Failed to create section")
    return res.data[0]

@lcoding_router.get("/sections/{section_id}", response_model=LcodingSection)
def get_lcoding_section(section_id: str):
    supabase = get_service_client()
    res = supabase.table("lcoding_sections").select("*").eq("id", section_id).single().execute()
    if not res.data:
        raise HTTPException(status_code=404, detail="Section not found")
    return res.data

@lcoding_router.get("/sections/{section_id}/topics", response_model=List[LcodingTopic])
def get_lcoding_topics(section_id: str):
    supabase = get_service_client()
    res = supabase.table("lcoding_topics").select("*").eq("section_id", section_id).order("order_index").execute()
    return res.data or []

@lcoding_router.post("/sections/{section_id}/topics", response_model=LcodingTopic)
def create_lcoding_topic(section_id: str, topic: LcodingTopicCreate):
    supabase = get_service_client()
    payload = topic.dict()
    payload["section_id"] = section_id
    payload["updated_at"] = datetime.utcnow().isoformat()
    res = supabase.table("lcoding_topics").insert(payload).execute()
    if not res.data:
        raise HTTPException(status_code=500, detail="Failed to create topic")
    return res.data[0]

@lcoding_router.get("/topics/{topic_id}", response_model=LcodingTopic)
def get_lcoding_topic(topic_id: str):
    supabase = get_service_client()
    res = supabase.table("lcoding_topics").select("*").eq("id", topic_id).single().execute()
    if not res.data:
        raise HTTPException(status_code=404, detail="Topic not found")
    return res.data

@lcoding_router.patch("/topics/{topic_id}", response_model=LcodingTopic)
def update_lcoding_topic(topic_id: str, topic: LcodingTopicUpdate):
    supabase = get_service_client()
    payload = topic.dict(exclude_unset=True)
    if not payload:
        raise HTTPException(status_code=400, detail="No fields to update")
    payload["updated_at"] = datetime.utcnow().isoformat()
    res = supabase.table("lcoding_topics").update(payload).eq("id", topic_id).execute()
    if not res.data:
        raise HTTPException(status_code=404, detail="Topic not found or failed to update")
    return res.data[0]

@lcoding_router.delete("/topics/{topic_id}")
def delete_lcoding_topic(topic_id: str):
    supabase = get_service_client()
    res = supabase.table("lcoding_topics").delete().eq("id", topic_id).execute()
    # Note: Supabase delete returns the deleted rows. If empty, it might mean not found OR already deleted.
    if not res.data:
         # Check if it existed? Or just return success.
         # For safety, let's assume if it returns nothing, it wasn't there.
         raise HTTPException(status_code=404, detail="Topic not found or failed to delete")
    return {"message": "Topic deleted successfully"}

app.include_router(lcoding_router)
app.include_router(problems_api.router) # Problem Solver Routes

if __name__ == "__main__":
    import uvicorn

    uvicorn.run(app, host="0.0.0.0", port=int(os.getenv("PORT", "8000")))

# ... (existing code)

# --- Python Compiler Endpoint ---
from packages.python_compiler import execute_python_code
from packages.java_compiler import execute_java_code

class CompilerRequest(BaseModel):
    code: str

@app.post("/api/tunex/compiler/run")
async def run_python_compiler(request: CompilerRequest):
    """
    Executes python code sent from the frontend.
    """
    result = execute_python_code(request.code)
    return result


@app.post("/api/tunex/compiler/java/run")
async def run_java_compiler(request: CompilerRequest):
    """Compiles and runs Java code sent from the frontend."""
    result = execute_java_code(request.code)
    return result

# ------------------------------------------------------------------------------
# Blink Generation Endpoint (Gemini 3 Pro + Supabase)
# ------------------------------------------------------------------------------

class BlinkRequest(BaseModel):
    topic: Optional[str] = None
    topic_id: Optional[str] = None
    note_content: Optional[str] = None # Optional override

@app.get("/api/blink/links")
async def get_blink_links(topics: str = Query(..., description="Comma-separated list of topic names")):
    """
    Fetch existing blink links for given topic names.
    Returns a map of topic_name -> blink_link for topics that have blinks.
    """
    supabase = get_service_client()
    topics_list = [t.strip().lower() for t in topics.split(",") if t.strip()]
    
    if not topics_list:
        return {"links": {}}
    
    try:
        res = supabase.table(AI_NOTES_TABLE).select("title, title_ci, blink_link").in_("title_ci", topics_list).execute()
        data = getattr(res, 'data', []) or []
        
        links = {}
        for row in data:
            blink_link = row.get("blink_link")
            if blink_link and blink_link.strip():
                # Use original title as key for better matching
                links[row.get("title_ci") or row.get("title", "").lower()] = blink_link
        
        return {"links": links}
    except Exception as e:
        print(f"[Blink] Error fetching links: {e}")
        return {"links": {}}

@app.post("/api/blink/generate")
async def generate_blink_endpoint(
    req: BlinkRequest, 
    user_id: str = "00000000-0000-0000-0000-000000000000" 
):
    """
    Generate a 'Blink' (landscape illustration) for a given topic or note content.
    """
    
    # 1. Get content
    content_to_illustrate = req.note_content
    
    # If content not provided directly, try to fetch from DB
    if not content_to_illustrate and (req.topic or req.topic_id):
        supabase = get_service_client()
        try:
            # Attempt 1: Try by ID if provided
            data = []
            if req.topic_id:
                print(f"[Blink] Looking up note by ID: {req.topic_id}")
                res = supabase.table(AI_NOTES_TABLE).select("markdown, id").eq("id", req.topic_id).limit(1).execute()
                data = getattr(res, 'data', [])
            
            # Attempt 2: If no data yet and topic provided, try by title
            if not data and req.topic:
                print(f"[Blink] No note found by ID or no ID. Looking up by title: {req.topic}")
                res = supabase.table(AI_NOTES_TABLE).select("markdown, id").eq("title_ci", req.topic.strip().lower()).limit(1).execute()
                data = getattr(res, 'data', [])

            if data:
                content_to_illustrate = data[0].get("markdown", "")
                # If we found it by title but didn't have the ID (or it differed), update req.topic_id so we update the correct row later
                real_id = data[0].get("id")
                if real_id:
                     req.topic_id = real_id
            else:
                print("[Blink] Note not found in DB.")
        except Exception as e:
            print(f"Error fetching note: {e}")

    if not content_to_illustrate:
         raise HTTPException(status_code=404, detail="Note content not found for this topic. Please generate notes first.")

    # 2. Generate Image
    if not os.getenv("GEMINI_API_KEY"):
        raise HTTPException(status_code=500, detail="GEMINI_API_KEY not configured.")
        
    if genai is None:
         # raise HTTPException(status_code=500, detail="google-genai library not installed.")
         print("Warning: google-genai not imported. Simulating or failing.")

    prompt = f"""now,create me an one landscape illustration for the below notes so that just by looking this one illustration, they can understand the entire notes completely i want the ilustratio to be professional and white bg, content ={content_to_illustrate[:8000]}"""  

    print(f"Generating Blink for topic '{req.topic or req.topic_id}'...")
    
    try:
        def _generate_bytes_sync():
            # Check if genai is available
            if not genai:
                raise RuntimeError("google-genai library not available")

            client_g = genai.Client(api_key=os.getenv("GEMINI_API_KEY"))
            
            # Using user-provided structure specifically:
            chat = client_g.chats.create(
                model="gemini-3-pro-image-preview", 
                config=types.GenerateContentConfig(
                    response_modalities=['TEXT', 'IMAGE'],
                    tools=[{"google_search": {}}]
                )
            )

            resp = chat.send_message(prompt,
                config=types.GenerateContentConfig(
                    image_config=types.ImageConfig(
                        aspect_ratio="16:9",
                        image_size="2K"
                    ),
            ))
            
            for part in resp.parts:
                if part.as_image():
                    return part.as_image().image_bytes
                elif part.text:
                    print(f"[Blink] Model returned text: {part.text}")
            
            raise RuntimeError(f"No image part in response. Response text: {resp.text if hasattr(resp, 'text') else 'Unknown'}")

        image_bytes = await run_in_threadpool(_generate_bytes_sync)
        
        # 3. Upload to Supabase
        filename = f"gen_blink_{uuid.uuid4().hex[:8]}.png"
        bucket_name = "blink"
        
        supabase = get_service_client()
        
        def _upload_sync():
            res = supabase.storage.from_(bucket_name).upload(
                path=filename,
                file=image_bytes,
                file_options={"content-type": "image/png"}
            )
            pub = supabase.storage.from_(bucket_name).get_public_url(filename)
            return pub

        public_url = await run_in_threadpool(_upload_sync)
        
        # 4. Update DB
        if req.topic_id:
             supabase.table(AI_NOTES_TABLE).update({"blink_link": public_url}).eq("id", req.topic_id).execute()
        
        return {
            "success": True,
            "url": public_url,
            "filename": filename
        }

    except Exception as e:
        import traceback
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=str(e))

class BlinkRemoveRequest(BaseModel):
    topic: str = Field(..., min_length=1, description="Topic name to remove blink for")

@app.post("/api/blink/remove")
async def remove_blink_endpoint(req: BlinkRemoveRequest):
    """
    Remove the blink_link from ai_notes for a given topic.
    """
    supabase = get_service_client()
    topic_ci = req.topic.strip().lower()
    
    try:
        # Update ai_notes to set blink_link to null
        res = supabase.table(AI_NOTES_TABLE).update({"blink_link": None}).eq("title_ci", topic_ci).execute()
        
        return {
            "success": True,
            "topic": req.topic,
            "message": "Blink link removed"
        }
    except Exception as e:
        print(f"[Blink Remove] Error: {e}")
        raise HTTPException(status_code=500, detail=str(e))

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)


# ==========================================
# LABX - EXPLORABLE EXPLANATION GENERATOR
# With Database Caching for instant retrieval
# ==========================================

LABX_EXPLANATIONS_TABLE = "labx_explanations"

labx_router = APIRouter(prefix="/api/labx", tags=["labx"])

class LabXGenerateRequest(BaseModel):
    topic: str = Field(..., min_length=1, max_length=500, description="Topic to generate explorable explanation for")
    force_regenerate: bool = Field(default=False, description="Force regeneration even if cached")

class LabXRegenerateRequest(BaseModel):
    topic: str = Field(..., min_length=1, max_length=500, description="Topic to regenerate")

LABX_PROMPT_TEMPLATE = """You are a world-class "Explorable Explanation" Designer and Senior Creative Developer. You blend the storytelling of Vox, the interactivity of Bret Victor, and the aesthetics of Apple. Your task: produce a single-file, production-ready, fully working HTML deep-dive for the topic {topic}.

Output exactly one file: a single self-contained HTML document. Include Tailwind via CDN, GSAP via CDN, Lucide Icons via CDN, and Three.js via CDN only if 3D is required. No external assets (images may be inline SVG). The file must run locally when opened in a browser.

Follow this exact narrative flow and sectioning:
A. Kitchen Table Analogy (Hook) — plain-language, everyday metaphor with prominent editorial typography and a large SVG or Lucide icon.
B. Toy Model (Interactive Core) — state-driven interactive simulator with controls (sliders, toggles, inputs), direct manipulation, and immediate animated feedback using GSAP.
C. Technical Breakdown (Under the Hood) — real technical terms, formulas, and concrete mapping using a responsive Bento Grid (CSS Grid).

Use PaperX palette: primary #9E4B8A, secondary #4C2A59, deep #1E1E2F. Implement dark/light themes with toggle. Use Tailwind CSS CDN, GSAP for animations, state-driven architecture with visible JSON debug panel.

Include 4 named presets (Simple, Balanced, Extreme, Real-world example), export/import state JSON control, and download buttons.

Return ONLY the raw HTML starting with <!DOCTYPE html>. No markdown, no code fences, no explanatory text."""


def _clean_html_response(html_content: str) -> str:
    """Remove markdown code fences from AI response if present."""
    if html_content.startswith("```html"):
        html_content = html_content[7:]
    elif html_content.startswith("```"):
        html_content = html_content[3:]
    if html_content.endswith("```"):
        html_content = html_content[:-3]
    return html_content.strip()


def _get_cached_explanation(topic_ci: str) -> Optional[dict]:
    """Check if topic exists in database, return cached data or None."""
    try:
        supabase = get_service_client()
        result = supabase.table(LABX_EXPLANATIONS_TABLE).select(
            "id, topic, html_content, created_at, generation_time_ms, view_count"
        ).eq("topic_ci", topic_ci).limit(1).execute()
        
        if result.data and len(result.data) > 0:
            return result.data[0]
        return None
    except Exception as e:
        print(f"[LabX] Cache lookup error: {e}")
        return None


def _increment_view_count(explanation_id: str):
    """Increment view count for a cached explanation."""
    try:
        supabase = get_service_client()
        # Get current view count and increment
        current = supabase.table(LABX_EXPLANATIONS_TABLE).select("view_count").eq("id", explanation_id).limit(1).execute()
        if current.data:
            new_count = (current.data[0].get("view_count") or 0) + 1
            supabase.table(LABX_EXPLANATIONS_TABLE).update({"view_count": new_count}).eq("id", explanation_id).execute()
    except Exception as e:
        print(f"[LabX] View count update error: {e}")


def _save_explanation_to_db(topic: str, html_content: str, generation_time_ms: int) -> Optional[str]:
    """Save generated explanation to database. Returns the ID or None on error."""
    try:
        supabase = get_service_client()
        topic_ci = topic.strip().lower()
        
        # Upsert - update if exists, insert if not
        payload = {
            "topic": topic.strip(),
            "topic_ci": topic_ci,
            "html_content": html_content,
            "generation_time_ms": generation_time_ms,
            "updated_at": datetime.utcnow().isoformat() + "Z",
            "view_count": 1
        }
        
        result = supabase.table(LABX_EXPLANATIONS_TABLE).upsert(
            payload, 
            on_conflict="topic_ci"
        ).execute()
        
        if result.data and len(result.data) > 0:
            return result.data[0].get("id")
        return None
    except Exception as e:
        print(f"[LabX] Save to DB error: {e}")
        return None


@labx_router.post("/generate", summary="Generate explorable explanation for a topic")
async def generate_labx(req: LabXGenerateRequest):
    """
    Generate a self-contained, interactive HTML explorable explanation for the given topic.
    Uses database caching - returns cached version instantly if available.
    """
    topic = req.topic.strip()
    topic_ci = topic.lower()
    
    # Check cache first (unless force_regenerate is True)
    if not req.force_regenerate:
        cached = _get_cached_explanation(topic_ci)
        if cached:
            # Increment view count in background
            _increment_view_count(cached["id"])
            return JSONResponse(content={
                "success": True,
                "topic": cached["topic"],
                "html": cached["html_content"],
                "cached": True,
                "created_at": cached.get("created_at"),
                "generation_time_ms": cached.get("generation_time_ms"),
                "view_count": (cached.get("view_count") or 0) + 1
            })
    
    # Not cached or force regenerate - generate with Gemini
    if genai is None:
        raise HTTPException(status_code=500, detail="google-genai library not installed")
    
    gemini_key = os.getenv("GEMINI_API_KEY", "").strip()
    if not gemini_key:
        raise HTTPException(status_code=500, detail="GEMINI_API_KEY not configured")
    
    prompt = LABX_PROMPT_TEMPLATE.format(topic=topic)
    
    try:
        start_time = time.time()
        
        def _generate_sync():
            client = genai.Client(api_key=gemini_key)
            response = client.models.generate_content(
                model="gemini-3-pro-preview",
                contents=prompt,
            )
            return response.text
        
        html_content = await run_in_threadpool(_generate_sync)
        html_content = _clean_html_response(html_content)
        
        generation_time_ms = int((time.time() - start_time) * 1000)
        
        # Save to database
        saved_id = _save_explanation_to_db(topic, html_content, generation_time_ms)
        
        return JSONResponse(content={
            "success": True,
            "topic": topic,
            "html": html_content,
            "cached": False,
            "generation_time_ms": generation_time_ms,
            "saved_to_db": saved_id is not None
        })
        
    except Exception as e:
        import traceback
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=f"Generation failed: {str(e)}")


@labx_router.post("/regenerate", summary="Force regenerate an explanation")
async def regenerate_labx(req: LabXRegenerateRequest):
    """Force regenerate an explanation even if it exists in cache."""
    new_req = LabXGenerateRequest(topic=req.topic, force_regenerate=True)
    return await generate_labx(new_req)


@labx_router.get("/list", summary="List all cached explanations")
async def list_labx_explanations():
    """Get a list of all cached explanation topics."""
    try:
        supabase = get_service_client()
        result = supabase.table(LABX_EXPLANATIONS_TABLE).select(
            "id, topic, created_at, updated_at, generation_time_ms, view_count"
        ).order("view_count", desc=True).limit(100).execute()
        
        return JSONResponse(content={
            "success": True,
            "count": len(result.data) if result.data else 0,
            "explanations": result.data or []
        })
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to list explanations: {str(e)}")


@labx_router.get("/check-batch", summary="Check multiple topics at once")
async def check_labx_cache_batch(topics: str = Query(..., description="Comma-separated list of topic names")):
    """Check which topics have cached explanations. Returns a map of topic_name -> exists boolean."""
    topics_list = [t.strip().lower() for t in topics.split(",") if t.strip()]
    
    if not topics_list:
        return JSONResponse(content={"cached": {}})
    
    try:
        supabase = get_service_client()
        # Fetch all matching topics in one query
        result = supabase.table(LABX_EXPLANATIONS_TABLE).select(
            "topic_ci"
        ).in_("topic_ci", topics_list).execute()
        
        # Build map of which topics exist
        cached_set = set()
        if result.data:
            for row in result.data:
                cached_set.add(row.get("topic_ci", "").lower())
        
        cached_map = {topic: topic in cached_set for topic in topics_list}
        
        return JSONResponse(content={"cached": cached_map})
    except Exception as e:
        print(f"[LabX] Batch check error: {e}")
        return JSONResponse(content={"cached": {}})


@labx_router.get("/check/{topic}", summary="Check if a topic is cached")
async def check_labx_cache(topic: str):
    """Check if an explanation exists in cache without returning the full HTML."""
    topic_ci = topic.strip().lower()
    cached = _get_cached_explanation(topic_ci)
    
    if cached:
        return JSONResponse(content={
            "exists": True,
            "topic": cached["topic"],
            "created_at": cached.get("created_at"),
            "view_count": cached.get("view_count")
        })
    return JSONResponse(content={"exists": False})


@labx_router.get("/get/{topic}", summary="Get cached explanation without regenerating")
async def get_labx_cached(topic: str):
    """Retrieve cached HTML content for a topic. Does NOT regenerate if not found."""
    topic_ci = topic.strip().lower()
    cached = _get_cached_explanation(topic_ci)
    
    if cached:
        # Increment view count
        _increment_view_count(cached["id"])
        return JSONResponse(content={
            "success": True,
            "topic": cached["topic"],
            "html": cached["html_content"],
            "cached": True,
            "created_at": cached.get("created_at"),
            "view_count": cached.get("view_count", 0) + 1
        })
    
    return JSONResponse(
        status_code=404,
        content={"success": False, "error": "LabX not found for this topic"}
    )


@labx_router.delete("/{topic}", summary="Delete a cached explanation")
async def delete_labx_explanation(topic: str):
    """Delete a cached explanation to allow regeneration."""
    try:
        supabase = get_service_client()
        topic_ci = topic.strip().lower()
        result = supabase.table(LABX_EXPLANATIONS_TABLE).delete().eq("topic_ci", topic_ci).execute()
        
        deleted = len(result.data) > 0 if result.data else False
        return JSONResponse(content={
            "success": True,
            "deleted": deleted,
            "topic": topic
        })
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to delete: {str(e)}")


@labx_router.post("/generate-stream", summary="Generate explorable explanation with streaming")
async def generate_labx_stream(req: LabXGenerateRequest):
    """
    Generate with streaming. Note: Streaming bypasses cache for fresh generation.
    For cached results, use the regular /generate endpoint.
    """
    if genai is None:
        raise HTTPException(status_code=500, detail="google-genai library not installed")
    
    gemini_key = os.getenv("GEMINI_API_KEY", "").strip()
    if not gemini_key:
        raise HTTPException(status_code=500, detail="GEMINI_API_KEY not configured")
    
    topic = req.topic.strip()
    prompt = LABX_PROMPT_TEMPLATE.format(topic=topic)
    
    async def stream_generator():
        collected_html = []
        start_time = time.time()
        
        try:
            client = genai.Client(api_key=gemini_key)
            response_stream = client.models.generate_content_stream(
                model="gemini-3-pro-preview",
                contents=prompt,
            )
            
            first_chunk = True
            for chunk in response_stream:
                if chunk.text:
                    text = chunk.text
                    if first_chunk:
                        if text.startswith("```html"):
                            text = text[7:]
                        elif text.startswith("```"):
                            text = text[3:]
                        first_chunk = False
                    collected_html.append(text)
                    yield text
            
            # Save to DB after streaming completes
            full_html = "".join(collected_html)
            if full_html.endswith("```"):
                full_html = full_html[:-3]
            generation_time_ms = int((time.time() - start_time) * 1000)
            _save_explanation_to_db(topic, full_html.strip(), generation_time_ms)
                    
        except Exception as e:
            yield f"<!-- Error: {str(e)} -->"
    
    return StreamingResponse(stream_generator(), media_type="text/html", headers={"X-Topic": topic})


app.include_router(labx_router)


# ============================================================================
# STUDYAI - Academic AI Chatbot
# ============================================================================

studyai_router = APIRouter(prefix="/api/studyai", tags=["StudyAI Chatbot"])

AI_CHAT_CONVERSATIONS_TABLE = "ai_chat_conversations"
AI_CHAT_MESSAGES_TABLE = "ai_chat_messages"


class ChatConversationCreate(BaseModel):
    title: Optional[str] = "New Chat"


class ChatConversationUpdate(BaseModel):
    title: Optional[str] = None
    is_public: Optional[bool] = None
    is_anonymous: Optional[bool] = None


class ChatMessageCreate(BaseModel):
    content: str = Field(..., min_length=1)
    attachments: Optional[List[Dict[str, Any]]] = []
    study_mode: Optional[str] = None  # explain, summarize, quiz, solve, compare, outline, flashcards, cite


class ChatMessageBookmark(BaseModel):
    is_bookmarked: bool


# System prompt for academic assistant
STUDYAI_SYSTEM_PROMPT = """You are StudyAI, an advanced academic AI assistant designed specifically for students. Your primary goal is to help students with their academic preparation and learning.

## Core Capabilities:
1. **Explain concepts** - Break down complex topics into understandable parts with examples
2. **Summarize content** - Condense long texts into key points and main ideas
3. **Generate quizzes** - Create MCQ questions to test understanding
4. **Solve problems** - Show step-by-step solutions with explanations
5. **Compare concepts** - Highlight similarities and differences between topics
6. **Create outlines** - Structure content for better organization
7. **Generate flashcards** - Create study flashcards with Q&A format
8. **Help with citations** - Assist with proper referencing and citations

## Response Guidelines:
- Use clear, educational language appropriate for students
- Include examples when explaining concepts
- Format responses with proper markdown (headings, lists, code blocks, LaTeX for math)
- For math equations, use LaTeX notation: $inline$ or $$block$$
- For code, specify the programming language in code blocks
- Be encouraging and supportive
- Ask clarifying questions when the query is ambiguous
- Cite sources when providing factual information

## Study Mode Behaviors:
When a specific study mode is requested, adapt your response accordingly:
- **explain**: Provide detailed explanations with examples and analogies
- **summarize**: Create concise bullet points of key concepts
- **quiz**: Generate 5 MCQ questions with answers and explanations
- **solve**: Show step-by-step problem solving
- **compare**: Create a comparison table or structured comparison
- **outline**: Create a hierarchical outline structure
- **flashcards**: Format as Q: / A: pairs for easy studying
- **cite**: Help format citations in common styles (APA, MLA, etc.)
"""


def _generate_share_code() -> str:
    """Generate a unique 8-character share code."""
    import secrets
    import string
    chars = string.ascii_lowercase + string.digits
    return ''.join(secrets.choice(chars) for _ in range(8))


def _build_study_mode_prompt(mode: str, content: str) -> str:
    """Build a prompt based on the study mode."""
    mode_prompts = {
        "explain": f"Please explain the following concept in detail with examples:\n\n{content}",
        "summarize": f"Please summarize the following content into key points:\n\n{content}",
        "quiz": f"Please generate 5 multiple choice quiz questions based on the following topic. Include the correct answer and brief explanation for each:\n\n{content}",
        "solve": f"Please solve the following problem step by step, showing all work:\n\n{content}",
        "compare": f"Please compare and contrast the following concepts, highlighting similarities and differences:\n\n{content}",
        "outline": f"Please create a detailed study outline for the following topic:\n\n{content}",
        "flashcards": f"Please create 10 flashcards (Q&A pairs) for studying the following topic:\n\n{content}",
        "cite": f"Please help format a proper citation for the following source. Ask clarifying questions if needed:\n\n{content}",
    }
    return mode_prompts.get(mode, content)


async def _generate_ai_response(messages: List[Dict[str, str]], study_mode: Optional[str] = None) -> str:
    """Generate AI response using Gemini 2.5 Flash."""
    if not genai:
        raise HTTPException(status_code=500, detail="AI service not available")
    
    gemini_key = os.getenv("GEMINI_API_KEY", "").strip()
    if not gemini_key:
        raise HTTPException(status_code=500, detail="AI API key not configured")
    
    def _call_gemini():
        client = genai.Client(api_key=gemini_key)
        
        # Build conversation history
        gemini_messages = []
        for msg in messages:
            role = "user" if msg["role"] == "user" else "model"
            gemini_messages.append({
                "role": role,
                "parts": [{"text": msg["content"]}]
            })
        
        # Create chat session with system instruction
        response = client.models.generate_content(
            model="gemini-2.5-flash",
            contents=gemini_messages,
            config=types.GenerateContentConfig(
                system_instruction=STUDYAI_SYSTEM_PROMPT,
                temperature=0.7,
                max_output_tokens=4096,
            )
        )
        
        return response.text if hasattr(response, 'text') else str(response)
    
    try:
        result = await run_in_threadpool(_call_gemini)
        return result
    except Exception as e:
        print(f"[StudyAI] Error generating response: {e}")
        raise HTTPException(status_code=500, detail=f"Failed to generate AI response: {str(e)}")


@studyai_router.post("/conversations")
async def create_conversation(
    payload: ChatConversationCreate,
    authorization: Optional[str] = Header(default=None)
):
    """Create a new chat conversation."""
    user_id = get_user_id_from_token(authorization)
    if not user_id:
        raise HTTPException(status_code=401, detail="Authentication required")
    
    supabase = get_service_client()
    
    conversation_data = {
        "user_id": user_id,
        "title": (payload.title or "New Chat").strip()[:100],
    }
    
    try:
        result = supabase.table(AI_CHAT_CONVERSATIONS_TABLE).insert(conversation_data).execute()
        if not result.data:
            raise HTTPException(status_code=500, detail="Failed to create conversation")
        return result.data[0]
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Database error: {str(e)}")


@studyai_router.get("/conversations")
async def list_conversations(
    limit: int = Query(default=50, ge=1, le=100),
    offset: int = Query(default=0, ge=0),
    authorization: Optional[str] = Header(default=None)
):
    """List user's chat conversations."""
    user_id = get_user_id_from_token(authorization)
    if not user_id:
        raise HTTPException(status_code=401, detail="Authentication required")
    
    supabase = get_service_client()
    
    try:
        # Get conversations with latest message preview
        result = supabase.table(AI_CHAT_CONVERSATIONS_TABLE).select(
            "id, title, share_code, is_public, created_at, updated_at"
        ).eq("user_id", user_id).order("updated_at", desc=True).range(offset, offset + limit - 1).execute()
        
        conversations = result.data or []
        
        # Get message count for each conversation
        for conv in conversations:
            msg_count = supabase.table(AI_CHAT_MESSAGES_TABLE).select(
                "id", count="exact"
            ).eq("conversation_id", conv["id"]).execute()
            conv["message_count"] = msg_count.count if hasattr(msg_count, 'count') else len(msg_count.data or [])
        
        return {"conversations": conversations, "total": len(conversations)}
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Database error: {str(e)}")


@studyai_router.get("/conversations/{conversation_id}")
async def get_conversation(
    conversation_id: str,
    authorization: Optional[str] = Header(default=None)
):
    """Get a conversation with all its messages."""
    user_id = get_user_id_from_token(authorization)
    if not user_id:
        raise HTTPException(status_code=401, detail="Authentication required")
    
    supabase = get_service_client()
    
    try:
        # Get conversation
        conv_result = supabase.table(AI_CHAT_CONVERSATIONS_TABLE).select("*").eq(
            "id", conversation_id
        ).eq("user_id", user_id).limit(1).execute()
        
        if not conv_result.data:
            raise HTTPException(status_code=404, detail="Conversation not found")
        
        conversation = conv_result.data[0]
        
        # Get messages
        msg_result = supabase.table(AI_CHAT_MESSAGES_TABLE).select("*").eq(
            "conversation_id", conversation_id
        ).order("created_at").execute()
        
        conversation["messages"] = msg_result.data or []
        
        return conversation
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Database error: {str(e)}")


@studyai_router.patch("/conversations/{conversation_id}")
async def update_conversation(
    conversation_id: str,
    payload: ChatConversationUpdate,
    authorization: Optional[str] = Header(default=None)
):
    """Update conversation settings (title, sharing options)."""
    user_id = get_user_id_from_token(authorization)
    if not user_id:
        raise HTTPException(status_code=401, detail="Authentication required")
    
    supabase = get_service_client()
    
    # Verify ownership
    conv_check = supabase.table(AI_CHAT_CONVERSATIONS_TABLE).select("id").eq(
        "id", conversation_id
    ).eq("user_id", user_id).limit(1).execute()
    
    if not conv_check.data:
        raise HTTPException(status_code=404, detail="Conversation not found")
    
    update_data = {"updated_at": datetime.utcnow().isoformat()}
    if payload.title is not None:
        update_data["title"] = payload.title.strip()[:100]
    if payload.is_public is not None:
        update_data["is_public"] = payload.is_public
    if payload.is_anonymous is not None:
        update_data["is_anonymous"] = payload.is_anonymous
    
    try:
        result = supabase.table(AI_CHAT_CONVERSATIONS_TABLE).update(update_data).eq(
            "id", conversation_id
        ).execute()
        return result.data[0] if result.data else {"success": True}
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Database error: {str(e)}")


@studyai_router.delete("/conversations/{conversation_id}")
async def delete_conversation(
    conversation_id: str,
    authorization: Optional[str] = Header(default=None)
):
    """Delete a conversation and all its messages."""
    user_id = get_user_id_from_token(authorization)
    if not user_id:
        raise HTTPException(status_code=401, detail="Authentication required")
    
    supabase = get_service_client()
    
    # Verify ownership
    conv_check = supabase.table(AI_CHAT_CONVERSATIONS_TABLE).select("id").eq(
        "id", conversation_id
    ).eq("user_id", user_id).limit(1).execute()
    
    if not conv_check.data:
        raise HTTPException(status_code=404, detail="Conversation not found")
    
    try:
        # Delete conversation (messages will cascade delete)
        supabase.table(AI_CHAT_CONVERSATIONS_TABLE).delete().eq("id", conversation_id).execute()
        return {"success": True}
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Database error: {str(e)}")


@studyai_router.post("/conversations/{conversation_id}/messages")
async def send_message(
    conversation_id: str,
    payload: ChatMessageCreate,
    authorization: Optional[str] = Header(default=None)
):
    """Send a message and get AI response."""
    user_id = get_user_id_from_token(authorization)
    if not user_id:
        raise HTTPException(status_code=401, detail="Authentication required")
    
    supabase = get_service_client()
    
    # Verify ownership
    conv_check = supabase.table(AI_CHAT_CONVERSATIONS_TABLE).select("id, title").eq(
        "id", conversation_id
    ).eq("user_id", user_id).limit(1).execute()
    
    if not conv_check.data:
        raise HTTPException(status_code=404, detail="Conversation not found")
    
    conversation = conv_check.data[0]
    
    # Get existing messages for context
    existing_msgs = supabase.table(AI_CHAT_MESSAGES_TABLE).select(
        "role, content"
    ).eq("conversation_id", conversation_id).order("created_at").limit(20).execute()
    
    messages_history = existing_msgs.data or []
    
    # Build the user message with study mode if provided
    user_content = payload.content
    if payload.study_mode:
        user_content = _build_study_mode_prompt(payload.study_mode, payload.content)
    
    # Save user message
    user_msg_data = {
        "conversation_id": conversation_id,
        "role": "user",
        "content": payload.content,  # Store original content
        "attachments": payload.attachments or [],
        "metadata": {"study_mode": payload.study_mode} if payload.study_mode else {},
    }
    
    try:
        user_msg_result = supabase.table(AI_CHAT_MESSAGES_TABLE).insert(user_msg_data).execute()
        user_message = user_msg_result.data[0] if user_msg_result.data else None
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to save message: {str(e)}")
    
    # Build conversation for AI
    ai_messages = [{"role": m["role"], "content": m["content"]} for m in messages_history]
    ai_messages.append({"role": "user", "content": user_content})
    
    # Generate AI response
    try:
        ai_response = await _generate_ai_response(ai_messages, payload.study_mode)
    except HTTPException:
        raise
    except Exception as e:
        ai_response = f"I apologize, but I encountered an error while processing your request. Please try again. Error: {str(e)}"
    
    # Save AI response
    ai_msg_data = {
        "conversation_id": conversation_id,
        "role": "assistant",
        "content": ai_response,
        "attachments": [],
        "metadata": {"model": "gemini-2.5-flash"},
    }
    
    try:
        ai_msg_result = supabase.table(AI_CHAT_MESSAGES_TABLE).insert(ai_msg_data).execute()
        ai_message = ai_msg_result.data[0] if ai_msg_result.data else None
    except Exception as e:
        print(f"[StudyAI] Failed to save AI response: {e}")
    
    # Update conversation title if it's the first message and title is default
    if len(messages_history) == 0 and conversation.get("title") == "New Chat":
        # Auto-generate title from first message
        new_title = payload.content[:50] + ("..." if len(payload.content) > 50 else "")
        try:
            supabase.table(AI_CHAT_CONVERSATIONS_TABLE).update({
                "title": new_title,
                "updated_at": datetime.utcnow().isoformat()
            }).eq("id", conversation_id).execute()
        except Exception:
            pass
    else:
        # Just update timestamp
        try:
            supabase.table(AI_CHAT_CONVERSATIONS_TABLE).update({
                "updated_at": datetime.utcnow().isoformat()
            }).eq("id", conversation_id).execute()
        except Exception:
            pass
    
    return {
        "user_message": user_message,
        "ai_message": ai_message,
    }


@studyai_router.post("/conversations/{conversation_id}/share")
async def generate_share_link(
    conversation_id: str,
    authorization: Optional[str] = Header(default=None)
):
    """Generate a shareable link for a conversation."""
    user_id = get_user_id_from_token(authorization)
    if not user_id:
        raise HTTPException(status_code=401, detail="Authentication required")
    
    supabase = get_service_client()
    
    # Verify ownership and get current share_code
    conv_check = supabase.table(AI_CHAT_CONVERSATIONS_TABLE).select(
        "id, share_code, is_public"
    ).eq("id", conversation_id).eq("user_id", user_id).limit(1).execute()
    
    if not conv_check.data:
        raise HTTPException(status_code=404, detail="Conversation not found")
    
    conversation = conv_check.data[0]
    
    # Generate new share code if none exists
    share_code = conversation.get("share_code")
    if not share_code:
        share_code = _generate_share_code()
        try:
            supabase.table(AI_CHAT_CONVERSATIONS_TABLE).update({
                "share_code": share_code,
                "is_public": True,
                "updated_at": datetime.utcnow().isoformat()
            }).eq("id", conversation_id).execute()
        except Exception as e:
            raise HTTPException(status_code=500, detail=f"Failed to generate share link: {str(e)}")
    
    return {"share_code": share_code, "is_public": True}


@studyai_router.get("/shared/{share_code}")
async def get_shared_conversation(share_code: str):
    """Get a shared conversation (public access)."""
    supabase = get_service_client()
    
    try:
        # Get conversation by share code
        conv_result = supabase.table(AI_CHAT_CONVERSATIONS_TABLE).select(
            "id, title, is_anonymous, created_at, user_id"
        ).eq("share_code", share_code).eq("is_public", True).limit(1).execute()
        
        if not conv_result.data:
            raise HTTPException(status_code=404, detail="Shared conversation not found")
        
        conversation = conv_result.data[0]
        
        # Get user info if not anonymous
        owner_name = None
        if not conversation.get("is_anonymous"):
            profile = supabase.table("user_profiles").select("name").eq(
                "auth_user_id", conversation["user_id"]
            ).limit(1).execute()
            if profile.data:
                owner_name = profile.data[0].get("name")
        
        # Remove user_id from response
        del conversation["user_id"]
        conversation["owner_name"] = owner_name if not conversation.get("is_anonymous") else "Anonymous"
        
        # Get messages
        msg_result = supabase.table(AI_CHAT_MESSAGES_TABLE).select(
            "id, role, content, attachments, created_at"
        ).eq("conversation_id", conversation["id"]).order("created_at").execute()
        
        conversation["messages"] = msg_result.data or []
        
        return conversation
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Database error: {str(e)}")


@studyai_router.patch("/messages/{message_id}/bookmark")
async def toggle_message_bookmark(
    message_id: str,
    payload: ChatMessageBookmark,
    authorization: Optional[str] = Header(default=None)
):
    """Toggle bookmark status of a message."""
    user_id = get_user_id_from_token(authorization)
    if not user_id:
        raise HTTPException(status_code=401, detail="Authentication required")
    
    supabase = get_service_client()
    
    # Verify message belongs to user's conversation
    msg_check = supabase.table(AI_CHAT_MESSAGES_TABLE).select(
        "id, conversation_id"
    ).eq("id", message_id).limit(1).execute()
    
    if not msg_check.data:
        raise HTTPException(status_code=404, detail="Message not found")
    
    conv_id = msg_check.data[0]["conversation_id"]
    
    conv_check = supabase.table(AI_CHAT_CONVERSATIONS_TABLE).select("id").eq(
        "id", conv_id
    ).eq("user_id", user_id).limit(1).execute()
    
    if not conv_check.data:
        raise HTTPException(status_code=403, detail="Access denied")
    
    try:
        result = supabase.table(AI_CHAT_MESSAGES_TABLE).update({
            "is_bookmarked": payload.is_bookmarked
        }).eq("id", message_id).execute()
        return {"success": True, "is_bookmarked": payload.is_bookmarked}
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Database error: {str(e)}")


@studyai_router.get("/conversations/{conversation_id}/export/md")
async def export_conversation_markdown(
    conversation_id: str,
    authorization: Optional[str] = Header(default=None)
):
    """Export conversation as Markdown."""
    user_id = get_user_id_from_token(authorization)
    if not user_id:
        raise HTTPException(status_code=401, detail="Authentication required")
    
    supabase = get_service_client()
    
    # Verify ownership
    conv_check = supabase.table(AI_CHAT_CONVERSATIONS_TABLE).select(
        "id, title, created_at"
    ).eq("id", conversation_id).eq("user_id", user_id).limit(1).execute()
    
    if not conv_check.data:
        raise HTTPException(status_code=404, detail="Conversation not found")
    
    conversation = conv_check.data[0]
    
    # Get messages
    msg_result = supabase.table(AI_CHAT_MESSAGES_TABLE).select(
        "role, content, created_at"
    ).eq("conversation_id", conversation_id).order("created_at").execute()
    
    messages = msg_result.data or []
    
    # Build markdown
    md_lines = [
        f"# {conversation['title']}",
        f"*Exported from StudyAI on {datetime.utcnow().strftime('%Y-%m-%d %H:%M')} UTC*",
        "",
        "---",
        ""
    ]
    
    for msg in messages:
        role = "**You:**" if msg["role"] == "user" else "**StudyAI:**"
        md_lines.append(role)
        md_lines.append("")
        md_lines.append(msg["content"])
        md_lines.append("")
        md_lines.append("---")
        md_lines.append("")
    
    markdown_content = "\n".join(md_lines)
    
    return Response(
        content=markdown_content,
        media_type="text/markdown",
        headers={
            "Content-Disposition": f'attachment; filename="{conversation["title"][:40]}.md"'
        }
    )


app.include_router(studyai_router)


# ============================================================================
# GROUP CHAT - WebRTC Signaling Server
# ============================================================================

# In-memory room storage (for production, use Redis or database)
group_chat_rooms: Dict[str, Dict[str, Any]] = {}
group_chat_connections: Dict[str, Dict[str, WebSocket]] = {}  # room_id -> {user_id: websocket}

DEFAULT_GROUP_CHAT_SETTINGS: Dict[str, Any] = {
    "host_management_enabled": True,
    "chat_enabled": True,
    "screen_share_enabled": True,
    "reactions_enabled": True,
    "participants_can_unmute": True,
    "participants_can_start_video": True,
    "lock_meeting": False,
    "waiting_room_enabled": False,
    "access_type": "open",  # open|trusted
    "allow_ask_to_join": True,
    "host_must_join_first": False,
}


def _merge_settings(current: Dict[str, Any], patch: Dict[str, Any]) -> Dict[str, Any]:
    merged = dict(current or {})
    for k, v in (patch or {}).items():
        if k in DEFAULT_GROUP_CHAT_SETTINGS:
            merged[k] = v
    # ensure all defaults exist
    for k, v in DEFAULT_GROUP_CHAT_SETTINGS.items():
        merged.setdefault(k, v)
    # normalize
    if merged.get("access_type") not in ("open", "trusted"):
        merged["access_type"] = "open"
    return merged


def _is_uuid(value: str) -> bool:
    try:
        uuid.UUID(value)
        return True
    except Exception:
        return False


def _is_moderator(room: Dict[str, Any], user_id: str) -> bool:
    if not room or not user_id:
        return False
    if user_id == room.get("host_id"):
        return True
    return user_id in set(room.get("cohosts", set()) or set())


def _participant_role(room: Dict[str, Any], user_id: str) -> str:
    if user_id == room.get("host_id"):
        return "host"
    if user_id in set(room.get("cohosts", set()) or set()):
        return "cohost"
    return "participant"


def _participants_payload(room: Dict[str, Any]) -> List[Dict[str, Any]]:
    muted: Set[str] = set(room.get("muted_user_ids") or set())
    out: List[Dict[str, Any]] = []
    for p in (room.get("participants") or {}).values():
        item = dict(p)
        item["muted"] = str(item.get("user_id")) in muted
        out.append(item)
    return out


async def _persist_group_call_settings(room_id: str, settings: Dict[str, Any]) -> None:
    try:
        supabase = get_service_client()
        # Upsert settings
        supabase.table("group_call_settings").upsert({
            "call_id": room_id,
            "settings": settings,
        }).execute()
    except Exception as e:
        print(f"[GroupChat] Settings persist error: {e}")


async def _log_group_call_event(room_id: str, actor_user_id: Optional[str], event_type: str, event_data: Dict[str, Any]) -> None:
    try:
        if actor_user_id and not _is_uuid(actor_user_id):
            actor_user_id = None
        supabase = get_service_client()
        supabase.table("group_call_events").insert({
            "call_id": room_id,
            "actor_user_id": actor_user_id,
            "event_type": event_type,
            "event_data": event_data or {},
        }).execute()
    except Exception:
        # Best-effort audit log; never break the call.
        pass

class GroupChatMessage(BaseModel):
    type: str  # "join", "leave", "offer", "answer", "ice-candidate", "whiteboard-draw", "screen-share-started", "screen-share-stopped"
    data: Optional[Dict[str, Any]] = None
    target_user_id: Optional[str] = None  # For directed messages (offers, answers, ice-candidates)

def generate_room_id() -> str:
    """Generate a unique room ID."""
    return str(uuid.uuid4())[:8].upper()

def generate_user_id() -> str:
    """Generate a unique user ID for anonymous participants."""
    return f"user-{str(uuid.uuid4())[:6]}"

@app.post("/api/group-chat/create")
async def create_group_chat_room(
    name: str = Body("Group Chat", embed=True),
    authorization: Optional[str] = Header(None),
):
    """Create a new group chat room. Returns room ID and join link."""
    room_id = generate_room_id()
    
    # Try to get user info from token
    host_id = None
    host_name = "Host"
    if authorization and authorization.startswith("Bearer "):
        token = authorization.split(" ")[1]
        try:
            supabase = get_service_client()
            user_resp = supabase.auth.get_user(token)
            if user_resp and user_resp.user:
                host_id = user_resp.user.id
                # Try to get profile name
                profile = supabase.table("profiles").select("name").eq("user_id", host_id).maybe_single().execute()
                if profile.data and profile.data.get("name"):
                    host_name = profile.data["name"]
        except:
            pass
    
    if not host_id:
        host_id = generate_user_id()
    
    group_chat_rooms[room_id] = {
        "id": room_id,
        "name": name,
        "host_id": host_id,
        "host_name": host_name,
        "created_at": datetime.now(timezone.utc).isoformat(),
        "participants": {},  # user_id -> {name, joined_at, is_host}
        "pending": {},  # user_id -> {name, requested_at}
        "cohosts": set(),
        "muted_user_ids": set(),
        "settings": dict(DEFAULT_GROUP_CHAT_SETTINGS),
        "whiteboard_history": [],  # Drawing history for late joiners
        "screen_share_active": False,
        "screen_sharer_id": None,
        "status": "active"
    }
    group_chat_connections[room_id] = {}

    # Store in Database
    try:
        supabase = get_service_client()
        supabase.table("group_calls").insert({
            "id": room_id,
            "room_name": name,
            "host_user_id": host_id,
            "status": "active"
        }).execute()

        # Default settings + role bootstrap (best-effort)
        try:
            supabase.table("group_call_settings").upsert({
                "call_id": room_id,
                "settings": dict(DEFAULT_GROUP_CHAT_SETTINGS),
            }).execute()
        except Exception:
            pass

        try:
            if _is_uuid(host_id):
                # store cohosts only; host is already on group_calls
                pass
        except Exception:
            pass
    except Exception as e:
        print(f"[GroupChat] DB Insert Error: {e}")
    
    return JSONResponse(content={
        "room_id": room_id,
        "name": name,
        "host_id": host_id,
        "join_url": f"/ui/groupChat/group_chat.html?room={room_id}",
    })

@app.post("/api/group-chat/end")
async def end_group_chat_room(
    room_id: str = Body(..., embed=True),
    authorization: Optional[str] = Header(None)
):
    """End a group chat room."""
    # Verify user is host
    user_id = None
    if authorization and authorization.startswith("Bearer "):
        token = authorization.split(" ")[1]
        try:
            supabase = get_service_client()
            user_resp = supabase.auth.get_user(token)
            if user_resp and user_resp.user:
                user_id = user_resp.user.id
        except:
            raise HTTPException(status_code=401, detail="Invalid token")
            
    if not user_id:
         raise HTTPException(status_code=401, detail="Unauthorized")

    room = group_chat_rooms.get(room_id)
    
    # If not in memory, check DB
    if not room:
         supabase = get_service_client()
         res = supabase.table("group_calls").select("*").eq("id", room_id).execute()
         if res.data:
             db_room = res.data[0]
             if db_room["host_user_id"] != user_id:
                  raise HTTPException(status_code=403, detail="Only host can end the call")
             # Mark ended in DB
             supabase.table("group_calls").update({
                 "status": "ended",
                 "ended_at": datetime.now(timezone.utc).isoformat()
             }).eq("id", room_id).execute()
             return {"success": True, "message": "Call ended (was not active in memory)"}
         else:
             raise HTTPException(status_code=404, detail="Room not found")

    if room["host_id"] != user_id:
        raise HTTPException(status_code=403, detail="Only host can end the call")
        
    # Mark as ended in memory
    room["status"] = "ended"
    
    # Notify all users
    await broadcast_to_room(room_id, {
        "type": "room-ended",
        "data": {"reason": "Host ended the call"}
    })
    
    # Close all connections
    connections = group_chat_connections.get(room_id, {})
    for ws in list(connections.values()):
        await ws.close()
    
    # Clean up memory
    if room_id in group_chat_rooms:
        del group_chat_rooms[room_id]
    if room_id in group_chat_connections:
        del group_chat_connections[room_id]
        
    # Update DB
    try:
        supabase = get_service_client()
        supabase.table("group_calls").update({
            "status": "ended",
            "ended_at": datetime.now(timezone.utc).isoformat()
        }).eq("id", room_id).execute()
    except Exception as e:
        print(f"[GroupChat] DB Update Error: {e}")
        
    return {"success": True}

@app.get("/api/group-chat/history")
async def get_group_chat_history(authorization: Optional[str] = Header(None)):
    """Get call history for the user."""
    if not authorization or not authorization.startswith("Bearer "):
         raise HTTPException(status_code=401, detail="Unauthorized")
         
    token = authorization.split(" ")[1]
    supabase = get_service_client()
    try:
        user_resp = supabase.auth.get_user(token)
        if not user_resp or not user_resp.user:
             raise HTTPException(status_code=401, detail="Invalid token")
        user_id = user_resp.user.id
    except:
         raise HTTPException(status_code=401, detail="Invalid token")
         
    # 1. Active calls created by me
    active_calls = []
    try:
        res = supabase.table("group_calls").select("*").eq("host_user_id", user_id).eq("status", "active").order("created_at", desc=True).execute()
        active_calls = res.data or []
    except Exception as e:
        print(f"Error fetching active calls: {e}")

    # 2. Past calls (created or attended)
    past_calls = []
    try:
        # Calls I created (ended)
        res_hosted = supabase.table("group_calls").select("*").eq("host_user_id", user_id).neq("status", "active").order("created_at", desc=True).limit(20).execute()
        hosted = res_hosted.data or []
        
        # Calls I attended (via participants table)
        # Note: This requires a join or two queries.
        res_attended = supabase.table("group_call_participants").select("call_id, group_calls(*)").eq("user_id", user_id).order("joined_at", desc=True).limit(20).execute()
        attended = []
        if res_attended.data:
            for item in res_attended.data:
                call = item.get("group_calls")
                if call and call.get("host_user_id") != user_id: # Avoid duplicates if I hosted it
                    attended.append(call)
        
        # Merge and sort
        all_past = hosted + attended
        # Deduplicate by id
        seen = set()
        unique_past = []
        for c in all_past:
             if c["id"] not in seen:
                 unique_past.append(c)
                 seen.add(c["id"])
                 
        # Sort by created_at desc
        unique_past.sort(key=lambda x: x["created_at"], reverse=True)
        past_calls = unique_past[:50]
        
    except Exception as e:
         print(f"Error fetching past calls: {e}")
         
    return {
        "active_created": active_calls,
        "history": past_calls
    }


@app.get("/api/group-chat/{room_id}")
async def get_group_chat_room(room_id: str):
    """Get room information."""
    room = group_chat_rooms.get(room_id)

    # If not in memory, attempt to restore if active in DB (server restart / memory eviction).
    if not room:
        supabase = get_service_client()
        try:
            res = supabase.table("group_calls").select("*").eq("id", room_id).execute()
            if res.data:
                db_room = res.data[0]
                if db_room.get("status") != "active":
                    raise HTTPException(status_code=400, detail="This call has ended")

                host_id = db_room.get("host_user_id")
                host_name = "Host"
                try:
                    if host_id:
                        p_res = supabase.table("profiles").select("name").eq("user_id", host_id).maybe_single().execute()
                        if p_res.data and p_res.data.get("name"):
                            host_name = p_res.data["name"]
                except Exception:
                    pass

                group_chat_rooms[room_id] = {
                    "id": room_id,
                    "name": db_room.get("room_name") or "Group Chat",
                    "host_id": host_id or generate_user_id(),
                    "host_name": host_name,
                    "created_at": db_room.get("created_at") or datetime.now(timezone.utc).isoformat(),
                    "participants": {},
                    "pending": {},
                    "cohosts": set(),
                    "muted_user_ids": set(),
                    "settings": dict(DEFAULT_GROUP_CHAT_SETTINGS),
                    "whiteboard_history": [],
                    "screen_share_active": False,
                    "screen_sharer_id": None,
                    "status": "active",
                }
                group_chat_connections.setdefault(room_id, {})
                room = group_chat_rooms[room_id]

                # Restore settings + cohosts (best-effort)
                try:
                    s_res = supabase.table("group_call_settings").select("settings").eq("call_id", room_id).maybe_single().execute()
                    if s_res.data and isinstance(s_res.data.get("settings"), dict):
                        room["settings"] = _merge_settings(dict(DEFAULT_GROUP_CHAT_SETTINGS), s_res.data["settings"])
                except Exception:
                    pass
                try:
                    r_res = supabase.table("group_call_roles").select("user_id, role").eq("call_id", room_id).execute()
                    cohosts: Set[str] = set()
                    if r_res.data:
                        for item in r_res.data:
                            if item.get("role") == "cohost" and item.get("user_id"):
                                cohosts.add(str(item["user_id"]))
                    room["cohosts"] = cohosts
                except Exception:
                    pass
        except HTTPException:
            raise
        except Exception:
            pass

    if not room:
        raise HTTPException(status_code=404, detail="Room not found or ended")

    
    return JSONResponse(content={
        "id": room["id"],
        "name": room["name"],
        "host_id": room["host_id"],
        "host_name": room["host_name"],
        "participant_count": len(room["participants"]),
        "participants": list(room["participants"].values()),
        "settings": room.get("settings") or dict(DEFAULT_GROUP_CHAT_SETTINGS),
        "screen_share_active": room.get("screen_share_active", False),
        "screen_sharer_id": room.get("screen_sharer_id"),
    })


@app.get("/api/rtc-config")
async def get_rtc_config() -> Dict[str, Any]:
    """Return ICE server configuration for WebRTC.

    For reliable audio/video across networks you need a working TURN server.
    Configure via env:
      - PAPERX_TURN_URLS: comma-separated (e.g. turns:turn.example.com:443?transport=tcp,turn:turn.example.com:3478?transport=udp)
      - PAPERX_TURN_USERNAME
      - PAPERX_TURN_CREDENTIAL
    """
    require_turn = (os.getenv("PAPERX_REQUIRE_TURN") or "").strip().lower() in {"1", "true", "yes"}

    ice_servers: List[Dict[str, Any]] = [
        {"urls": "stun:stun.l.google.com:19302"},
        {"urls": "stun:stun1.l.google.com:19302"},
        {"urls": "stun:stun.cloudflare.com:3478"},
    ]

    turn_urls = (os.getenv("PAPERX_TURN_URLS") or "").strip()
    turn_user = (os.getenv("PAPERX_TURN_USERNAME") or "").strip()
    turn_cred = (os.getenv("PAPERX_TURN_CREDENTIAL") or "").strip()
    turn_configured = bool(turn_urls and turn_user and turn_cred)
    if turn_configured:
        urls = [u.strip() for u in turn_urls.split(",") if u.strip()]
        if urls:
            ice_servers.append({
                "urls": urls,
                "username": turn_user,
                "credential": turn_cred,
            })

    if require_turn and not turn_configured:
        raise HTTPException(
            status_code=503,
            detail=(
                "TURN is required but not configured. Set PAPERX_TURN_URLS, PAPERX_TURN_USERNAME, PAPERX_TURN_CREDENTIAL. "
                "(This prevents unreliable WebRTC behavior on NAT/mobile networks.)"
            ),
        )

    # When TURN is configured, instruct clients to force relay. This is the most reliable option across NATs.
    ice_transport_policy = "relay" if turn_configured else "all"
    return {
        "iceServers": ice_servers,
        "iceTransportPolicy": ice_transport_policy,
        "turnConfigured": turn_configured,
        "requireTurn": require_turn,
    }

async def broadcast_to_room(room_id: str, message: dict, exclude_user_id: Optional[str] = None):
    """Broadcast a message to all participants in a room."""
    connections = group_chat_connections.get(room_id, {})
    room = group_chat_rooms.get(room_id)
    participants = set((room or {}).get("participants", {}).keys())
    for user_id, ws in list(connections.items()):
        if participants and user_id not in participants:
            continue
        if exclude_user_id and user_id == exclude_user_id:
            continue
        try:
            await ws.send_json(message)
        except Exception as e:
            print(f"[GroupChat] Failed to send to {user_id}: {e}")

async def send_to_user(room_id: str, target_user_id: str, message: dict):
    """Send a message to a specific user in a room."""
    connections = group_chat_connections.get(room_id, {})
    ws = connections.get(target_user_id)
    if ws:
        try:
            await ws.send_json(message)
        except Exception as e:
            print(f"[GroupChat] Failed to send to {target_user_id}: {e}")

@app.websocket("/ws/group-chat/{room_id}")
async def group_chat_websocket(websocket: WebSocket, room_id: str):
    """WebSocket endpoint for group chat signaling."""
    await websocket.accept()
    
    room = group_chat_rooms.get(room_id)
    if not room:
        # Try to restore from DB if not in memory (e.g. server restart)
        try:
            supabase = get_service_client()
            res = supabase.table("group_calls").select("*").eq("id", room_id).execute()
            if res.data and res.data[0]["status"] == "active":
                db_room = res.data[0]
                host_id = db_room["host_user_id"]
                
                # Try to fetch host name
                host_name = "Host"
                try:
                    p_res = supabase.table("profiles").select("name").eq("user_id", host_id).maybe_single().execute()
                    if p_res.data and p_res.data.get("name"):
                        host_name = p_res.data["name"]
                except:
                    pass

                # Restore to memory
                group_chat_rooms[room_id] = {
                    "id": room_id,
                    "name": db_room["room_name"],
                    "host_id": host_id,
                    "host_name": host_name,
                    "created_at": db_room.get("created_at") or datetime.now(timezone.utc).isoformat(),
                    "participants": {},
                    "pending": {},
                    "cohosts": set(),
                    "muted_user_ids": set(),
                    "settings": dict(DEFAULT_GROUP_CHAT_SETTINGS),
                    "whiteboard_history": [],
                    "screen_share_active": False,
                    "screen_sharer_id": None,
                    "status": "active"
                }
                group_chat_connections[room_id] = {}
                room = group_chat_rooms[room_id]
                print(f"[GroupChat] Restored room {room_id} from DB")

                # Restore settings + cohosts (best-effort)
                try:
                    s_res = supabase.table("group_call_settings").select("settings").eq("call_id", room_id).maybe_single().execute()
                    if s_res.data and isinstance(s_res.data.get("settings"), dict):
                        room["settings"] = _merge_settings(dict(DEFAULT_GROUP_CHAT_SETTINGS), s_res.data["settings"])
                except Exception:
                    pass

                try:
                    r_res = supabase.table("group_call_roles").select("user_id, role").eq("call_id", room_id).execute()
                    cohosts: Set[str] = set()
                    if r_res.data:
                        for item in r_res.data:
                            if item.get("role") == "cohost" and item.get("user_id"):
                                cohosts.add(str(item["user_id"]))
                    room["cohosts"] = cohosts
                except Exception:
                    pass
        except Exception as e:
            print(f"[GroupChat] Error restoring room: {e}")

    if not room:
        await websocket.send_json({"type": "error", "message": "Room not found"})
        await websocket.close()
        return
    
    user_id = None
    user_name = "Anonymous"
    
    try:
        # First message should be join with user info
        join_data = await websocket.receive_json()
        
        if join_data.get("type") != "join":
            await websocket.send_json({"type": "error", "message": "First message must be 'join'"})
            await websocket.close()
            return
        
        user_id = join_data.get("data", {}).get("user_id") or generate_user_id()
        user_name = join_data.get("data", {}).get("name") or "Anonymous"

        # If a token is provided, trust auth-derived identity over client-provided fields.
        join_token = join_data.get("data", {}).get("token")
        if join_token and isinstance(join_token, str) and join_token.strip():
            try:
                supabase = get_service_client()
                user_resp = supabase.auth.get_user(join_token.strip())
                if user_resp and user_resp.user:
                    user_id = str(user_resp.user.id)
                    # prefer profile name
                    try:
                        p = supabase.table("profiles").select("name").eq("user_id", user_id).maybe_single().execute()
                        if p.data and p.data.get("name"):
                            user_name = p.data["name"]
                    except Exception:
                        pass
            except Exception:
                # Token invalid -> keep client-supplied identity
                pass

        # Enforce bans for authenticated users
        if _is_uuid(user_id):
            try:
                supabase = get_service_client()
                ban = supabase.table("group_call_bans").select("id").eq("call_id", room_id).eq("user_id", user_id).maybe_single().execute()
                if ban.data:
                    await websocket.send_json({"type": "join-denied", "data": {"reason": "banned"}})
                    await websocket.close()
                    return
            except Exception:
                pass

        is_host = user_id == room["host_id"]
        can_moderate = _is_moderator(room, user_id)

        # Access type enforcement (trusted requires authenticated UUID)
        settings = room.get("settings") or dict(DEFAULT_GROUP_CHAT_SETTINGS)
        settings = _merge_settings(dict(DEFAULT_GROUP_CHAT_SETTINGS), settings)
        room["settings"] = settings

        if settings.get("access_type") == "trusted" and not _is_uuid(user_id):
            await websocket.send_json({"type": "join-denied", "data": {"reason": "trusted_only"}})
            await websocket.close()
            return

        # Invite-only mode: trusted + no ask-to-join -> only host/co-host may enter
        if settings.get("access_type") == "trusted" and not settings.get("allow_ask_to_join", True) and not can_moderate:
            await websocket.send_json({"type": "join-denied", "data": {"reason": "invite_only"}})
            await websocket.close()
            return

        # Lock meeting enforcement
        if settings.get("lock_meeting") and not can_moderate:
            await websocket.send_json({"type": "join-denied", "data": {"reason": "locked"}})
            await websocket.close()
            return

        # Waiting room / host-must-join-first: keep connection in pending mode until admitted.
        pending_mode = False
        if not can_moderate:
            host_present = room.get("host_id") in set((room.get("participants") or {}).keys())
            if settings.get("waiting_room_enabled") or (settings.get("host_must_join_first") and not host_present):
                pending_mode = True
                room.setdefault("pending", {})[user_id] = {
                    "user_id": user_id,
                    "name": user_name,
                    "requested_at": datetime.now(timezone.utc).isoformat(),
                }
                group_chat_connections[room_id][user_id] = websocket

                await websocket.send_json({
                    "type": "waiting-room",
                    "data": {
                        "room_id": room_id,
                        "message": "Waiting for the host to admit you",
                    },
                })

                await broadcast_to_room(room_id, {
                    "type": "join-request",
                    "data": {"user_id": user_id, "name": user_name},
                })
        
        if not pending_mode:
            # Add to room
            room["participants"][user_id] = {
                "user_id": user_id,
                "name": user_name,
                "joined_at": datetime.now(timezone.utc).isoformat(),
                "is_host": is_host,
                "role": _participant_role(room, user_id),
            }
            group_chat_connections[room_id][user_id] = websocket

            # Send room info and existing participants to new user
            rtc_cfg = await get_rtc_config()
            await websocket.send_json({
                "type": "room-info",
                "data": {
                    "room_id": room_id,
                    "name": room["name"],
                    "your_user_id": user_id,
                    "is_host": is_host,
                    "can_moderate": can_moderate,
                    "role": _participant_role(room, user_id),
                    "settings": room.get("settings") or dict(DEFAULT_GROUP_CHAT_SETTINGS),
                    "pending": list((room.get("pending") or {}).values()) if can_moderate else [],
                    "participants": _participants_payload(room),
                    # Send full in-memory history (capped when stored) so refresh/new joiners
                    # can fully reconstruct the whiteboard.
                    "whiteboard_history": room.get("whiteboard_history", []),
                    "screen_share_active": room.get("screen_share_active", False),
                    "screen_sharer_id": room.get("screen_sharer_id"),
                    "rtc_config": rtc_cfg,
                }
            })

            # Notify others about new participant
            await broadcast_to_room(room_id, {
                "type": "user-joined",
                "data": {
                    "user_id": user_id,
                    "name": user_name,
                    "is_host": is_host,
                    "role": _participant_role(room, user_id),
                }
            }, exclude_user_id=user_id)
        
        # ---------------------------
        # Record Attendance in DB
        # ---------------------------
        try:
            supabase = get_service_client()
            # Check if executing in a way that allows us to check DB
            # We want to record (call_id, user_id)
            # Check if already recorded to avoid dups if re-joining?
            # The table has a PK, so we can just Insert and ignore specific errors active
            
            # Since user_id in memory might be "user-xxxx", we should only record if it is a real UUID (authenticated)
            # OR we just store whatever user_id we have (the schema defined uuid, so we must check)
            
            is_valid_uuid = False
            try:
                uuid.UUID(user_id)
                is_valid_uuid = True
            except:
                is_valid_uuid = False
                
            if is_valid_uuid:
                # Check if already exists?
                exists = supabase.table("group_call_participants").select("id").eq("call_id", room_id).eq("user_id", user_id).execute()
                if not exists.data:
                     try:
                        supabase.table("group_call_participants").insert({
                            "call_id": room_id,
                            "user_id": user_id
                        }).execute()
                     except Exception as ex:
                          print(f"Error inserting participant: {ex}")

        except Exception as e:
            print(f"Error recording attendance: {e}")

        
        # Main message loop
        while True:
            message = await websocket.receive_json()
            msg_type = message.get("type")
            msg_data = message.get("data", {})
            target_user_id = message.get("target_user_id")

            # If this user was pending and got admitted, allow normal message flow.
            if pending_mode and user_id in (room.get("participants") or {}):
                pending_mode = False

            if pending_mode:
                # Pending users can't send signaling/messages until admitted.
                if msg_type in ("leave",):
                    break
                continue

            # Moderator controls
            if msg_type == "host-set-settings":
                if not _is_moderator(room, user_id):
                    continue
                new_settings = _merge_settings(room.get("settings") or dict(DEFAULT_GROUP_CHAT_SETTINGS), msg_data)
                room["settings"] = new_settings
                await _persist_group_call_settings(room_id, new_settings)
                await _log_group_call_event(room_id, user_id if _is_uuid(user_id) else None, "host_set_settings", msg_data)
                await broadcast_to_room(room_id, {
                    "type": "settings-updated",
                    "data": {"settings": new_settings},
                })
                continue

            if msg_type == "host-mute-all":
                if not _is_moderator(room, user_id):
                    continue
                await _log_group_call_event(room_id, user_id if _is_uuid(user_id) else None, "host_mute_all", {})
                muted_set: Set[str] = set(room.get("muted_user_ids") or set())
                targets: List[str] = []
                for pid in list((room.get("participants") or {}).keys()):
                    if not pid or pid == user_id:
                        continue
                    # Don't force-mute host/cohost by default
                    if _is_moderator(room, pid):
                        continue
                    muted_set.add(pid)
                    targets.append(pid)
                room["muted_user_ids"] = muted_set
                for pid in targets:
                    await send_to_user(room_id, pid, {
                        "type": "host-mute",
                        "data": {"scope": "user", "by": user_id},
                    })
                await broadcast_to_room(room_id, {
                    "type": "participants-updated",
                    "data": {"participants": _participants_payload(room)},
                })
                continue

            if msg_type == "host-mute-user":
                if not _is_moderator(room, user_id) or not target_user_id:
                    continue
                room.setdefault("muted_user_ids", set()).add(target_user_id)
                await _log_group_call_event(room_id, user_id if _is_uuid(user_id) else None, "host_mute_user", {"target": target_user_id})
                await send_to_user(room_id, target_user_id, {
                    "type": "host-mute",
                    "data": {"scope": "user", "by": user_id},
                })
                await broadcast_to_room(room_id, {
                    "type": "participants-updated",
                    "data": {"participants": _participants_payload(room)},
                })
                continue

            if msg_type == "host-unmute-user":
                if not _is_moderator(room, user_id) or not target_user_id:
                    continue
                try:
                    room.setdefault("muted_user_ids", set()).discard(target_user_id)
                except Exception:
                    room["muted_user_ids"] = set()
                await _log_group_call_event(room_id, user_id if _is_uuid(user_id) else None, "host_unmute_user", {"target": target_user_id})
                await send_to_user(room_id, target_user_id, {
                    "type": "host-unmute",
                    "data": {"scope": "user", "by": user_id},
                })
                await broadcast_to_room(room_id, {
                    "type": "participants-updated",
                    "data": {"participants": _participants_payload(room)},
                })
                continue

            if msg_type == "host-kick":
                if not _is_moderator(room, user_id) or not target_user_id:
                    continue
                # never allow kicking the host
                if target_user_id == room.get("host_id"):
                    continue
                # remove from pending if present
                try:
                    (room.get("pending") or {}).pop(target_user_id, None)
                except Exception:
                    pass
                await _log_group_call_event(room_id, user_id if _is_uuid(user_id) else None, "host_kick", {"target": target_user_id})
                await send_to_user(room_id, target_user_id, {
                    "type": "kicked",
                    "data": {"by": user_id, "reason": msg_data.get("reason") or "removed"},
                })
                try:
                    t_ws = group_chat_connections.get(room_id, {}).get(target_user_id)
                    if t_ws:
                        await t_ws.close()
                except Exception:
                    pass
                continue

            if msg_type == "host-set-role":
                if not _is_moderator(room, user_id) or not target_user_id:
                    continue
                role = (msg_data or {}).get("role")
                if role not in ("cohost", "participant"):
                    continue
                if target_user_id == room.get("host_id"):
                    continue
                if role == "cohost":
                    room.setdefault("cohosts", set()).add(target_user_id)
                    # Persist if uuid
                    if _is_uuid(target_user_id):
                        try:
                            supabase = get_service_client()
                            supabase.table("group_call_roles").upsert({
                                "call_id": room_id,
                                "user_id": target_user_id,
                                "role": "cohost",
                            }).execute()
                        except Exception:
                            pass
                else:
                    room.setdefault("cohosts", set()).discard(target_user_id)
                    if _is_uuid(target_user_id):
                        try:
                            supabase = get_service_client()
                            supabase.table("group_call_roles").delete().eq("call_id", room_id).eq("user_id", target_user_id).execute()
                        except Exception:
                            pass

                # Update participant roles in memory
                if target_user_id in room.get("participants", {}):
                    room["participants"][target_user_id]["role"] = _participant_role(room, target_user_id)
                # Broadcast refreshed participants list
                await broadcast_to_room(room_id, {
                    "type": "participants-updated",
                    "data": {"participants": _participants_payload(room)},
                })
                continue

            if msg_type == "host-admit":
                if not _is_moderator(room, user_id) or not target_user_id:
                    continue
                pending = (room.get("pending") or {}).pop(target_user_id, None)
                if not pending:
                    continue
                # admit: add to participants and send room-info
                role = _participant_role(room, target_user_id)
                room.setdefault("participants", {})[target_user_id] = {
                    "user_id": target_user_id,
                    "name": pending.get("name") or "Anonymous",
                    "joined_at": datetime.now(timezone.utc).isoformat(),
                    "is_host": target_user_id == room.get("host_id"),
                    "role": role,
                }
                await send_to_user(room_id, target_user_id, {
                    "type": "admitted",
                    "data": {
                        "room_id": room_id,
                        "settings": room.get("settings") or dict(DEFAULT_GROUP_CHAT_SETTINGS),
                        "participants": _participants_payload(room),
                    },
                })
                await broadcast_to_room(room_id, {
                    "type": "user-joined",
                    "data": {
                        "user_id": target_user_id,
                        "name": pending.get("name") or "Anonymous",
                        "is_host": target_user_id == room.get("host_id"),
                        "role": role,
                    },
                }, exclude_user_id=target_user_id)
                continue
            
            if msg_type == "offer":
                # Forward SDP offer to target user
                if target_user_id:
                    await send_to_user(room_id, target_user_id, {
                        "type": "offer",
                        "data": msg_data,
                        "from_user_id": user_id,
                        "from_name": user_name,
                    })
            
            elif msg_type == "answer":
                # Forward SDP answer to target user
                if target_user_id:
                    await send_to_user(room_id, target_user_id, {
                        "type": "answer",
                        "data": msg_data,
                        "from_user_id": user_id,
                    })
            
            elif msg_type == "ice-candidate":
                # Forward ICE candidate to target user
                if target_user_id:
                    await send_to_user(room_id, target_user_id, {
                        "type": "ice-candidate",
                        "data": msg_data,
                        "from_user_id": user_id,
                    })
            
            elif msg_type == "whiteboard-draw":
                # Store in history and broadcast drawing data
                stroke_data = {**msg_data, "user_id": user_id, "timestamp": time.time()}
                if len(room.get("whiteboard_history", [])) < 5000:  # Limit history
                    room.setdefault("whiteboard_history", []).append(stroke_data)
                
                await broadcast_to_room(room_id, {
                    "type": "whiteboard-draw",
                    "data": stroke_data,
                }, exclude_user_id=user_id)
            
            elif msg_type == "whiteboard-clear":
                # Clear whiteboard history
                room["whiteboard_history"] = []
                await broadcast_to_room(room_id, {
                    "type": "whiteboard-clear",
                    "data": {"cleared_by": user_id},
                })

            elif msg_type == "annotation-draw":
                # Ephemeral screen-share annotations (not persisted)
                await broadcast_to_room(room_id, {
                    "type": "annotation-draw",
                    "data": msg_data,
                    "from_user_id": user_id,
                }, exclude_user_id=user_id)

            elif msg_type == "annotation-clear":
                await broadcast_to_room(room_id, {
                    "type": "annotation-clear",
                    "data": {"cleared_by": user_id},
                    "from_user_id": user_id,
                })
            
            elif msg_type == "screen-share-started":
                # Host started screen sharing
                settings = room.get("settings") or dict(DEFAULT_GROUP_CHAT_SETTINGS)
                if not settings.get("host_management_enabled", True):
                    allowed = True
                else:
                    allowed = settings.get("screen_share_enabled", True)
                if _is_moderator(room, user_id) or allowed:
                    room["screen_share_active"] = True
                    room["screen_sharer_id"] = user_id
                    await broadcast_to_room(room_id, {
                        "type": "screen-share-started",
                        "data": {"sharer_id": user_id, "sharer_name": user_name},
                    }, exclude_user_id=user_id)
            
            elif msg_type == "screen-share-stopped":
                # Screen sharing stopped
                if room.get("screen_sharer_id") == user_id:
                    room["screen_share_active"] = False
                    room["screen_sharer_id"] = None
                    await broadcast_to_room(room_id, {
                        "type": "screen-share-stopped",
                        "data": {"sharer_id": user_id},
                    }, exclude_user_id=user_id)
            
            elif msg_type == "chat-message":
                # Text chat message
                settings = room.get("settings") or dict(DEFAULT_GROUP_CHAT_SETTINGS)
                if not settings.get("host_management_enabled", True):
                    allowed = True
                else:
                    allowed = settings.get("chat_enabled", True)
                if allowed or _is_moderator(room, user_id):
                    await broadcast_to_room(room_id, {
                        "type": "chat-message",
                        "data": {
                            "user_id": user_id,
                            "name": user_name,
                            "message": msg_data.get("message", ""),
                            "timestamp": datetime.now(timezone.utc).isoformat(),
                        }
                    })

            elif msg_type == "reaction":
                # Broadcast reaction to others
                settings = room.get("settings") or dict(DEFAULT_GROUP_CHAT_SETTINGS)
                if not settings.get("host_management_enabled", True):
                    allowed = True
                else:
                    allowed = settings.get("reactions_enabled", True)
                if allowed or _is_moderator(room, user_id):
                    await broadcast_to_room(room_id, {
                        "type": "reaction",
                        "data": msg_data,
                        "from_user_id": user_id,
                    }, exclude_user_id=user_id)

            elif msg_type == "hand-raised":
                await broadcast_to_room(room_id, {
                    "type": "hand-raised",
                    "data": msg_data,
                    "from_user_id": user_id,
                    "name": user_name
                }) # Don't exclude sender so they see confirmation if needed (though UI handles local)

            elif msg_type == "hand-lowered":
                 await broadcast_to_room(room_id, {
                    "type": "hand-lowered",
                    "data": msg_data,
                    "from_user_id": user_id,
                    "name": user_name
                })
    
    except WebSocketDisconnect:
        print(f"[GroupChat] User {user_id} disconnected from room {room_id}")
    except Exception as e:
        print(f"[GroupChat] Error: {e}")
    finally:
        # Clean up on disconnect
        if user_id and room_id:
            # Remove from connections
            if room_id in group_chat_connections:
                group_chat_connections[room_id].pop(user_id, None)

            # Remove from pending
            if room_id in group_chat_rooms:
                try:
                    (group_chat_rooms[room_id].get("pending") or {}).pop(user_id, None)
                except Exception:
                    pass
            
            # Remove from participants
            if room_id in group_chat_rooms and user_id in group_chat_rooms[room_id]["participants"]:
                del group_chat_rooms[room_id]["participants"][user_id]
                
                # Notify others
                await broadcast_to_room(room_id, {
                    "type": "user-left",
                    "data": {"user_id": user_id, "name": user_name}
                })
                
                # Stop screen share if sharer left
                if group_chat_rooms[room_id].get("screen_sharer_id") == user_id:
                    group_chat_rooms[room_id]["screen_share_active"] = False
                    group_chat_rooms[room_id]["screen_sharer_id"] = None
                    await broadcast_to_room(room_id, {
                        "type": "screen-share-stopped",
                        "data": {"sharer_id": user_id, "reason": "sharer_left"}
                    })
                
                # Clean up empty rooms after 5 minutes
                if not group_chat_rooms[room_id]["participants"]:
                    async def cleanup_room():
                        await asyncio.sleep(300)  # 5 minutes
                        if room_id in group_chat_rooms and not group_chat_rooms[room_id]["participants"]:
                            del group_chat_rooms[room_id]
                            group_chat_connections.pop(room_id, None)
                            print(f"[GroupChat] Cleaned up empty room {room_id}")
                    asyncio.create_task(cleanup_room())


# ============================================
# ASSIGNMENTS PORTAL
# ============================================


import hashlib

def get_supabase() -> Client:
    """Get or create Supabase client"""
    # Try to find global client first
    if 'supabase' in globals():
        return globals()['supabase']
    
    # Fallback: create new client (should ideally reuse global)
    url = os.environ.get("SUPABASE_URL")
    key = os.environ.get("SUPABASE_SERVICE_ROLE_KEY") or os.environ.get("SUPABASE_KEY")
    if not url or not key:
        raise ValueError("Supabase credentials not found")
    return create_client(url, key)

def get_user_id_from_token(authorization: Optional[str]) -> Optional[str]:
    """Extract user ID from Authorization header"""
    if not authorization:
        return None
        
    try:
        token = authorization.replace("Bearer ", "").strip()
        if not token:
            return None
            
        supabase = get_supabase()
        user = supabase.auth.get_user(token)
        return user.user.id if user and user.user else None
    except Exception as e:
        print(f"[Auth] Error validating token: {e}")
        return None


# Pydantic models for assignments
class AssignmentCreate(BaseModel):
    class_id: Optional[str] = None
    title: str
    description: Optional[str] = None
    instructions_md: Optional[str] = None
    resource_links: Optional[List[str]] = []
    assignment_type: Optional[str] = "individual"
    max_team_size: Optional[int] = 1
    max_marks: Optional[int] = 100
    allowed_file_types: Optional[List[str]] = ["pdf", "jpg", "jpeg", "png", "doc", "docx", "ppt", "pptx"]
    max_files: Optional[int] = 5
    max_file_size_mb: Optional[int] = 10
    due_date: str
    allow_late_submission: Optional[bool] = False
    late_penalty_percent: Optional[int] = 0
    grace_period_hours: Optional[int] = 0
    status: Optional[str] = "draft"
    rubrics: Optional[List[Dict[str, Any]]] = []

class AssignmentUpdate(BaseModel):
    title: Optional[str] = None
    description: Optional[str] = None
    instructions_md: Optional[str] = None
    resource_links: Optional[List[str]] = None
    assignment_type: Optional[str] = None
    max_team_size: Optional[int] = None
    max_marks: Optional[int] = None
    allowed_file_types: Optional[List[str]] = None
    max_files: Optional[int] = None
    max_file_size_mb: Optional[int] = None
    due_date: Optional[str] = None
    allow_late_submission: Optional[bool] = None
    late_penalty_percent: Optional[int] = None
    grace_period_hours: Optional[int] = None
    status: Optional[str] = None

class SubmissionCreate(BaseModel):
    file_urls: Optional[List[Dict[str, Any]]] = []
    text_content: Optional[str] = None
    is_draft: Optional[bool] = False

class GradeSubmission(BaseModel):
    total_marks: Optional[int] = None
    rubric_scores: Optional[Dict[str, int]] = {}
    feedback: Optional[str] = None

class ExtensionRequest(BaseModel):
    requested_date: str
    reason: str

class ExtensionResponse(BaseModel):
    status: str  # approved, denied
    new_due_date: Optional[str] = None


# --- Staff Assignment Management ---

@app.post("/api/assignments")
def create_assignment(payload: AssignmentCreate, authorization: Optional[str] = Header(default=None)):
    """Create a new assignment"""
    user_id = get_user_id_from_token(authorization)
    if not user_id:
        raise HTTPException(status_code=401, detail="Authentication required")
    
    supabase = get_supabase()
    
    # Build assignment row
    assignment_data = {
        "teacher_user_id": user_id,
        "class_id": payload.class_id,
        "title": payload.title,
        "description": payload.description,
        "instructions_md": payload.instructions_md,
        "resource_links": payload.resource_links or [],
        "assignment_type": payload.assignment_type or "individual",
        "max_team_size": payload.max_team_size or 1,
        "max_marks": payload.max_marks or 100,
        "allowed_file_types": payload.allowed_file_types or ["pdf", "jpg", "jpeg", "png", "doc", "docx"],
        "max_files": payload.max_files or 5,
        "max_file_size_mb": payload.max_file_size_mb or 10,
        "due_date": payload.due_date,
        "allow_late_submission": payload.allow_late_submission or False,
        "late_penalty_percent": payload.late_penalty_percent or 0,
        "grace_period_hours": payload.grace_period_hours or 0,
        "status": payload.status or "draft",
    }
    
    if payload.status == "published":
        assignment_data["published_at"] = datetime.now(timezone.utc).isoformat()
    
    result = supabase.table("assignments").insert(assignment_data).execute()
    if not result.data:
        raise HTTPException(status_code=500, detail="Failed to create assignment")
    
    assignment = result.data[0]
    
    # Insert rubrics if provided
    if payload.rubrics:
        for idx, rubric in enumerate(payload.rubrics):
            supabase.table("assignment_rubrics").insert({
                "assignment_id": assignment["id"],
                "criterion": rubric.get("criterion", ""),
                "max_points": rubric.get("max_points", 0),
                "description": rubric.get("description", ""),
                "order_index": idx
            }).execute()
    
    return {"success": True, "assignment": assignment}


@app.get("/api/assignments")
def list_assignments(
    class_id: Optional[str] = None,
    status: Optional[str] = None,
    authorization: Optional[str] = Header(default=None)
):
    """List assignments created by the teacher"""
    user_id = get_user_id_from_token(authorization)
    if not user_id:
        raise HTTPException(status_code=401, detail="Authentication required")
    
    supabase = get_supabase()
    query = supabase.table("assignments").select("*").eq("teacher_user_id", user_id)
    
    if class_id:
        query = query.eq("class_id", class_id)
    if status:
        query = query.eq("status", status)
    
    query = query.order("created_at", desc=True)
    result = query.execute()
    
    assignments = result.data or []
    if not assignments:
        return {"assignments": []}
    
    # Batch fetch submissions to avoid N+1
    assign_ids = [a["id"] for a in assignments]
    
    # Fetch all submissions for these assignments (just status is enough)
    sub_query = supabase.table("assignment_submissions").select("assignment_id,status").in_("assignment_id", assign_ids)
    sub_res = sub_query.execute()
    all_subs = sub_res.data or []
    
    # Group by assignment_id
    stats_map = {}
    for s in all_subs:
        aid = s.get("assignment_id")
        if aid not in stats_map:
            stats_map[aid] = {"submission_count": 0, "graded_count": 0, "pending_count": 0}
        
        stats_map[aid]["submission_count"] += 1
        if s.get("status") == "graded":
            stats_map[aid]["graded_count"] += 1
        elif s.get("status") == "pending":
            stats_map[aid]["pending_count"] += 1
            
    # Attach stats
    for a in assignments:
        stats = stats_map.get(a["id"], {})
        a["submission_count"] = stats.get("submission_count", 0)
        a["graded_count"] = stats.get("graded_count", 0)
        a["pending_count"] = stats.get("pending_count", 0)
    
    return {"assignments": assignments}


@app.get("/api/assignments/{assignment_id}")
def get_assignment(assignment_id: str, authorization: Optional[str] = Header(default=None)):
    """Get assignment details with rubrics"""
    user_id = get_user_id_from_token(authorization)
    if not user_id:
        raise HTTPException(status_code=401, detail="Authentication required")
    
    supabase = get_supabase()
    
    result = supabase.table("assignments").select("*").eq("id", assignment_id).limit(1).execute()
    if not result.data:
        raise HTTPException(status_code=404, detail="Assignment not found")
    
    assignment = result.data[0]
    
    # Fetch rubrics
    rubrics = supabase.table("assignment_rubrics").select("*").eq(
        "assignment_id", assignment_id
    ).order("order_index").execute()
    
    assignment["rubrics"] = rubrics.data or []
    
    return {"assignment": assignment}


@app.put("/api/assignments/{assignment_id}")
def update_assignment(
    assignment_id: str,
    payload: AssignmentUpdate,
    authorization: Optional[str] = Header(default=None)
):
    """Update an assignment"""
    user_id = get_user_id_from_token(authorization)
    if not user_id:
        raise HTTPException(status_code=401, detail="Authentication required")
    
    supabase = get_supabase()
    
    # Verify ownership
    existing = supabase.table("assignments").select("teacher_user_id").eq("id", assignment_id).limit(1).execute()
    if not existing.data or existing.data[0].get("teacher_user_id") != user_id:
        raise HTTPException(status_code=403, detail="Not authorized")
    
    updates = {k: v for k, v in payload.dict().items() if v is not None}
    updates["updated_at"] = datetime.now(timezone.utc).isoformat()
    
    if updates.get("status") == "published":
        updates["published_at"] = datetime.now(timezone.utc).isoformat()
    
    result = supabase.table("assignments").update(updates).eq("id", assignment_id).execute()
    
    return {"success": True, "assignment": result.data[0] if result.data else None}


@app.delete("/api/assignments/{assignment_id}")
def delete_assignment(assignment_id: str, authorization: Optional[str] = Header(default=None)):
    """Delete an assignment"""
    user_id = get_user_id_from_token(authorization)
    if not user_id:
        raise HTTPException(status_code=401, detail="Authentication required")
    
    supabase = get_supabase()
    
    # Verify ownership
    existing = supabase.table("assignments").select("teacher_user_id").eq("id", assignment_id).limit(1).execute()
    if not existing.data or existing.data[0].get("teacher_user_id") != user_id:
        raise HTTPException(status_code=403, detail="Not authorized")
    
    supabase.table("assignments").delete().eq("id", assignment_id).execute()
    
    return {"success": True}


@app.post("/api/assignments/{assignment_id}/publish")
def publish_assignment(assignment_id: str, authorization: Optional[str] = Header(default=None)):
    """Publish a draft assignment"""
    user_id = get_user_id_from_token(authorization)
    if not user_id:
        raise HTTPException(status_code=401, detail="Authentication required")
    
    supabase = get_supabase()
    
    result = supabase.table("assignments").update({
        "status": "published",
        "published_at": datetime.now(timezone.utc).isoformat(),
        "updated_at": datetime.now(timezone.utc).isoformat()
    }).eq("id", assignment_id).eq("teacher_user_id", user_id).execute()
    
    return {"success": True, "assignment": result.data[0] if result.data else None}


@app.post("/api/assignments/{assignment_id}/close")
def close_assignment(assignment_id: str, authorization: Optional[str] = Header(default=None)):
    """Close submissions for an assignment"""
    user_id = get_user_id_from_token(authorization)
    if not user_id:
        raise HTTPException(status_code=401, detail="Authentication required")
    
    supabase = get_supabase()
    
    result = supabase.table("assignments").update({
        "status": "closed",
        "updated_at": datetime.now(timezone.utc).isoformat()
    }).eq("id", assignment_id).eq("teacher_user_id", user_id).execute()
    
    return {"success": True}


# --- Submissions ---

@app.get("/api/assignments/{assignment_id}/submissions")
def get_submissions(assignment_id: str, authorization: Optional[str] = Header(default=None)):
    """Get all submissions for an assignment (teacher only)"""
    user_id = get_user_id_from_token(authorization)
    if not user_id:
        raise HTTPException(status_code=401, detail="Authentication required")
    
    supabase = get_supabase()
    
    # Verify teacher owns assignment
    assignment = supabase.table("assignments").select("*").eq("id", assignment_id).limit(1).execute()
    if not assignment.data or assignment.data[0].get("teacher_user_id") != user_id:
        raise HTTPException(status_code=403, detail="Not authorized")
    
    submissions = supabase.table("assignment_submissions").select("*").eq(
        "assignment_id", assignment_id
    ).order("submitted_at", desc=True).execute()
    
    # Fetch student info for each submission
    student_ids = [s.get("student_user_id") for s in (submissions.data or [])]
    student_info = {}
    if student_ids:
        profiles = supabase.table("user_profiles").select("auth_user_id,name").in_(
            "auth_user_id", student_ids
        ).execute()
        for p in (profiles.data or []):
            student_info[p.get("auth_user_id")] = p.get("name", "Unknown")
    
    for sub in (submissions.data or []):
        sub["student_name"] = student_info.get(sub.get("student_user_id"), "Unknown")
    
    return {"submissions": submissions.data or [], "assignment": assignment.data[0]}


@app.get("/api/assignments/{assignment_id}/duplicates")
def get_duplicate_submissions(assignment_id: str, authorization: Optional[str] = Header(default=None)):
    """Get duplicate file submissions based on file hash"""
    user_id = get_user_id_from_token(authorization)
    if not user_id:
        raise HTTPException(status_code=401, detail="Authentication required")
    
    supabase = get_supabase()
    
    # Verify teacher owns assignment
    assignment = supabase.table("assignments").select("teacher_user_id").eq("id", assignment_id).limit(1).execute()
    if not assignment.data or assignment.data[0].get("teacher_user_id") != user_id:
        raise HTTPException(status_code=403, detail="Not authorized")
    
    # Find duplicate files by hash
    files = supabase.table("assignment_submission_files").select(
        "id,submission_id,student_user_id,file_name,file_hash,file_size_bytes"
    ).eq("assignment_id", assignment_id).execute()
    
    # Group by hash
    hash_groups = {}
    for f in (files.data or []):
        file_hash = f.get("file_hash")
        if not file_hash:
            continue
        if file_hash not in hash_groups:
            hash_groups[file_hash] = []
        hash_groups[file_hash].append(f)
    
    # Filter to only duplicates (2+ files with same hash)
    duplicates = []
    for file_hash, file_list in hash_groups.items():
        if len(file_list) > 1:
            student_ids = [f.get("student_user_id") for f in file_list]
            # Fetch student names
            profiles = supabase.table("user_profiles").select("auth_user_id,full_name").in_(
                "auth_user_id", student_ids
            ).execute()
            name_map = {p.get("auth_user_id"): p.get("full_name") for p in (profiles.data or [])}
            
            for f in file_list:
                f["student_name"] = name_map.get(f.get("student_user_id"), "Unknown")
            
            duplicates.append({
                "file_hash": file_hash,
                "file_size_bytes": file_list[0].get("file_size_bytes"),
                "count": len(file_list),
                "submissions": file_list
            })
    
    return {"duplicates": duplicates}


@app.put("/api/assignments/{assignment_id}/submissions/{submission_id}/grade")
def grade_submission(
    assignment_id: str,
    submission_id: str,
    payload: GradeSubmission,
    authorization: Optional[str] = Header(default=None)
):
    """Grade a submission"""
    user_id = get_user_id_from_token(authorization)
    if not user_id:
        raise HTTPException(status_code=401, detail="Authentication required")
    
    supabase = get_supabase()
    
    # Verify teacher owns assignment
    assignment = supabase.table("assignments").select("teacher_user_id").eq("id", assignment_id).limit(1).execute()
    if not assignment.data or assignment.data[0].get("teacher_user_id") != user_id:
        raise HTTPException(status_code=403, detail="Not authorized")
    
    updates = {
        "total_marks": payload.total_marks,
        "rubric_scores": payload.rubric_scores or {},
        "feedback": payload.feedback,
        "graded_by": user_id,
        "graded_at": datetime.now(timezone.utc).isoformat(),
        "status": "graded",
        "updated_at": datetime.now(timezone.utc).isoformat()
    }
    
    result = supabase.table("assignment_submissions").update(updates).eq("id", submission_id).execute()
    
    return {"success": True, "submission": result.data[0] if result.data else None}


# --- Student Endpoints ---

@app.get("/api/student/assignments")
def get_student_assignments(authorization: Optional[str] = Header(default=None)):
    """Get assignments for the student's class"""
    user_id = get_user_id_from_token(authorization)
    if not user_id:
        raise HTTPException(status_code=401, detail="Authentication required")
    
    supabase = get_supabase()
    
    # Get student's education info to determine class
    profile = supabase.table("user_profiles").select("id").eq("auth_user_id", user_id).limit(1).execute()
    if not profile.data:
        return {"assignments": []}
    
    profile_id = profile.data[0].get("id")
    education = supabase.table("user_education").select("*").eq("user_profile_id", profile_id).limit(1).execute()
    
    if not education.data:
        return {"assignments": []}
    
    edu = education.data[0]
    
    # Find matching classes
    class_query = supabase.table("teacher_classes").select("id")
    for col, val in [
        ("batch_id", edu.get("batch_id")),
        ("department_id", edu.get("department_id")),
        ("semester", edu.get("current_semester")),
        ("section", edu.get("section")),
    ]:
        if val:
            class_query = class_query.eq(col, val)
    
    classes = class_query.execute()
    class_ids = [c.get("id") for c in (classes.data or [])]
    
    if not class_ids:
        return {"assignments": []}
    
    # Get published/closed assignments for these classes
    assignments = supabase.table("assignments").select("*").in_(
        "class_id", class_ids
    ).in_("status", ["published", "closed"]).order("due_date").execute()
    
    # Get student's submissions
    submissions = supabase.table("assignment_submissions").select(
        "assignment_id,status,total_marks,submitted_at"
    ).eq("student_user_id", user_id).execute()
    
    sub_map = {s.get("assignment_id"): s for s in (submissions.data or [])}
    
    for a in (assignments.data or []):
        sub = sub_map.get(a.get("id"))
        a["submission"] = sub
        a["is_submitted"] = bool(sub and sub.get("status") != "pending")
        a["is_graded"] = bool(sub and sub.get("status") == "graded")
    
    return {"assignments": assignments.data or []}


@app.get("/api/student/assignments/{assignment_id}")
def get_student_assignment(assignment_id: str, authorization: Optional[str] = Header(default=None)):
    """Get assignment details for student"""
    user_id = get_user_id_from_token(authorization)
    if not user_id:
        raise HTTPException(status_code=401, detail="Authentication required")
    
    supabase = get_supabase()
    
    assignment = supabase.table("assignments").select("*").eq("id", assignment_id).limit(1).execute()
    if not assignment.data:
        raise HTTPException(status_code=404, detail="Assignment not found")
    
    a = assignment.data[0]
    
    # Fetch rubrics
    rubrics = supabase.table("assignment_rubrics").select("*").eq(
        "assignment_id", assignment_id
    ).order("order_index").execute()
    a["rubrics"] = rubrics.data or []
    
    # Fetch student's submission if exists
    submission = supabase.table("assignment_submissions").select("*").eq(
        "assignment_id", assignment_id
    ).eq("student_user_id", user_id).limit(1).execute()
    a["my_submission"] = submission.data[0] if submission.data else None
    
    return {"assignment": a}


@app.post("/api/student/assignments/{assignment_id}/submit")
def submit_assignment(
    assignment_id: str,
    payload: SubmissionCreate,
    authorization: Optional[str] = Header(default=None)
):
    """Submit or update assignment submission"""
    user_id = get_user_id_from_token(authorization)
    if not user_id:
        raise HTTPException(status_code=401, detail="Authentication required")
    
    supabase = get_supabase()
    
    # Check assignment exists and is open
    assignment = supabase.table("assignments").select("*").eq("id", assignment_id).limit(1).execute()
    if not assignment.data:
        raise HTTPException(status_code=404, detail="Assignment not found")
    
    a = assignment.data[0]
    
    # Check deadline
    due_date = datetime.fromisoformat(a.get("due_date").replace("Z", "+00:00"))
    now = datetime.now(timezone.utc)
    
    if now > due_date and not a.get("allow_late_submission"):
        # Check for extension
        extension = supabase.table("assignment_extensions").select("new_due_date").eq(
            "assignment_id", assignment_id
        ).eq("student_user_id", user_id).eq("status", "approved").limit(1).execute()
        
        if extension.data:
            ext_date = datetime.fromisoformat(extension.data[0].get("new_due_date").replace("Z", "+00:00"))
            if now > ext_date:
                raise HTTPException(status_code=400, detail="Submission deadline has passed")
        else:
            raise HTTPException(status_code=400, detail="Submission deadline has passed")
    
    # Check if submission exists
    existing = supabase.table("assignment_submissions").select("id,version").eq(
        "assignment_id", assignment_id
    ).eq("student_user_id", user_id).limit(1).execute()
    
    submission_data = {
        "file_urls": payload.file_urls or [],
        "text_content": payload.text_content,
        "is_draft": payload.is_draft or False,
        "status": "pending" if payload.is_draft else "submitted",
        "updated_at": datetime.now(timezone.utc).isoformat()
    }
    
    if not payload.is_draft:
        submission_data["submitted_at"] = datetime.now(timezone.utc).isoformat()
    
    if existing.data:
        # Update existing
        submission_data["version"] = (existing.data[0].get("version") or 1) + 1
        result = supabase.table("assignment_submissions").update(submission_data).eq(
            "id", existing.data[0].get("id")
        ).execute()
        submission_id = existing.data[0].get("id")
    else:
        # Create new
        submission_data["assignment_id"] = assignment_id
        submission_data["student_user_id"] = user_id
        submission_data["version"] = 1
        result = supabase.table("assignment_submissions").insert(submission_data).execute()
        submission_id = result.data[0].get("id") if result.data else None
    
    # Store file metadata with hash for duplicate detection
    if submission_id and payload.file_urls:
        # Clear old files
        supabase.table("assignment_submission_files").delete().eq("submission_id", submission_id).execute()
        
        # Insert new file records
        for file_info in payload.file_urls:
            file_record = {
                "submission_id": submission_id,
                "assignment_id": assignment_id,
                "student_user_id": user_id,
                "file_url": file_info.get("url", ""),
                "file_name": file_info.get("name", ""),
                "file_size_bytes": file_info.get("size", 0),
                "file_type": file_info.get("type", ""),
                "file_hash": file_info.get("hash", "")  # Hash calculated on frontend
            }
            supabase.table("assignment_submission_files").insert(file_record).execute()
    
    return {"success": True, "submission": result.data[0] if result.data else None}


# --- File Upload ---

@app.post("/api/assignments/upload")
async def upload_assignment_file(
    file: UploadFile = File(...),
    assignment_id: str = Form(...),
    authorization: Optional[str] = Header(default=None)
):
    """Upload file to assignments bucket and return URL with hash"""
    user_id = get_user_id_from_token(authorization)
    if not user_id:
        raise HTTPException(status_code=401, detail="Authentication required")
    
    supabase = get_supabase()
    
    # Read file content
    content = await file.read()
    
    # Calculate SHA-256 hash for duplicate detection
    # Default to binary content hash
    file_hash = hashlib.sha256(content).hexdigest()
    
    # Try text-based hashing for PDFs (robust against metadata/filename changes)
    if (file.filename or "").lower().endswith(".pdf"):
        try:
            import fitz  # PyMuPDF
            with fitz.open(stream=content, filetype="pdf") as doc:
                text = ""
                for page in doc:
                    text += page.get_text()
                
                # Only use text hash if we extracted significant text
                if len(text.strip()) > 50:
                    # Normalize text (remove whitespace noise)
                    clean_text = "".join(text.split())
                    file_hash = hashlib.sha256(clean_text.encode("utf-8")).hexdigest()
                    print(f"[Duplicate Check] Used text hash for {file.filename}")
        except Exception as e:
            print(f"[Duplicate Check] Text extraction failed: {e}, falling back to binary hash")
    
    # Generate unique filename
    ext = os.path.splitext(file.filename)[1] if file.filename else ""
    unique_name = f"{assignment_id}/{user_id}/{uuid.uuid4()}{ext}"
    
    # Upload to Supabase bucket
    try:
        result = supabase.storage.from_("assignments").upload(
            unique_name,
            content,
            {"content-type": file.content_type or "application/octet-stream"}
        )
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Upload failed: {str(e)}")
    
    # Get public URL
    public_url = supabase.storage.from_("assignments").get_public_url(unique_name)
    
    return {
        "success": True,
        "file": {
            "url": public_url,
            "name": file.filename,
            "size": len(content),
            "type": file.content_type,
            "hash": file_hash
        }
    }


# --- Extensions ---

@app.post("/api/student/assignments/{assignment_id}/request-extension")
def request_extension(
    assignment_id: str,
    payload: ExtensionRequest,
    authorization: Optional[str] = Header(default=None)
):
    """Request deadline extension"""
    user_id = get_user_id_from_token(authorization)
    if not user_id:
        raise HTTPException(status_code=401, detail="Authentication required")
    
    supabase = get_supabase()
    
    # Check if request already exists
    existing = supabase.table("assignment_extensions").select("id").eq(
        "assignment_id", assignment_id
    ).eq("student_user_id", user_id).limit(1).execute()
    
    if existing.data:
        raise HTTPException(status_code=400, detail="Extension request already exists")
    
    result = supabase.table("assignment_extensions").insert({
        "assignment_id": assignment_id,
        "student_user_id": user_id,
        "requested_date": payload.requested_date,
        "reason": payload.reason,
        "status": "pending"
    }).execute()
    
    return {"success": True, "extension": result.data[0] if result.data else None}


@app.get("/api/assignments/{assignment_id}/extensions")
def get_extensions(assignment_id: str, authorization: Optional[str] = Header(default=None)):
    """Get extension requests for an assignment (teacher only)"""
    user_id = get_user_id_from_token(authorization)
    if not user_id:
        raise HTTPException(status_code=401, detail="Authentication required")
    
    supabase = get_supabase()
    
    extensions = supabase.table("assignment_extensions").select("*").eq(
        "assignment_id", assignment_id
    ).order("created_at", desc=True).execute()
    
    # Fetch student names
    student_ids = [e.get("student_user_id") for e in (extensions.data or [])]
    if student_ids:
        profiles = supabase.table("user_profiles").select("auth_user_id,full_name").in_(
            "auth_user_id", student_ids
        ).execute()
        name_map = {p.get("auth_user_id"): p.get("full_name") for p in (profiles.data or [])}
        for e in (extensions.data or []):
            e["student_name"] = name_map.get(e.get("student_user_id"), "Unknown")
    
    return {"extensions": extensions.data or []}


@app.put("/api/assignments/extensions/{extension_id}")
def respond_to_extension(
    extension_id: str,
    payload: ExtensionResponse,
    authorization: Optional[str] = Header(default=None)
):
    """Approve or deny extension request (teacher only)"""
    user_id = get_user_id_from_token(authorization)
    if not user_id:
        raise HTTPException(status_code=401, detail="Authentication required")
    
    supabase = get_supabase()
    
    updates = {
        "status": payload.status,
        "approved_by": user_id if payload.status == "approved" else None,
    }
    
    if payload.new_due_date:
        updates["new_due_date"] = payload.new_due_date
    
    result = supabase.table("assignment_extensions").update(updates).eq("id", extension_id).execute()
    
    return {"success": True, "extension": result.data[0] if result.data else None}


# --- Comments ---

@app.get("/api/assignments/{assignment_id}/comments")
def get_comments(
    assignment_id: str,
    submission_id: Optional[str] = None,
    authorization: Optional[str] = Header(default=None)
):
    """Get comments for assignment or submission"""
    user_id = get_user_id_from_token(authorization)
    if not user_id:
        raise HTTPException(status_code=401, detail="Authentication required")
    
    supabase = get_supabase()
    
    query = supabase.table("assignment_comments").select("*").eq("assignment_id", assignment_id)
    
    if submission_id:
        query = query.eq("submission_id", submission_id)
    
    comments = query.order("created_at").execute()
    
    # Fetch user names
    user_ids = list(set([c.get("user_id") for c in (comments.data or [])]))
    if user_ids:
        profiles = supabase.table("user_profiles").select("auth_user_id,full_name").in_(
            "auth_user_id", user_ids
        ).execute()
        name_map = {p.get("auth_user_id"): p.get("full_name") for p in (profiles.data or [])}
        for c in (comments.data or []):
            c["user_name"] = name_map.get(c.get("user_id"), "Unknown")
    
    return {"comments": comments.data or []}


class CommentCreate(BaseModel):
    content: str
    submission_id: Optional[str] = None
    is_private: Optional[bool] = False


@app.post("/api/assignments/{assignment_id}/comments")
def add_comment(
    assignment_id: str,
    payload: CommentCreate,
    authorization: Optional[str] = Header(default=None)
):
    """Add comment to assignment or submission"""
    user_id = get_user_id_from_token(authorization)
    if not user_id:
        raise HTTPException(status_code=401, detail="Authentication required")
    
    supabase = get_supabase()
    
    result = supabase.table("assignment_comments").insert({
        "assignment_id": assignment_id,
        "submission_id": payload.submission_id,
        "user_id": user_id,
        "content": payload.content,
        "is_private": payload.is_private or False
    }).execute()
    
    return {"success": True, "comment": result.data[0] if result.data else None}


# --- Analytics ---

@app.get("/api/assignments/{assignment_id}/analytics")
def get_assignment_analytics(assignment_id: str, authorization: Optional[str] = Header(default=None)):
    """Get analytics for an assignment"""
    user_id = get_user_id_from_token(authorization)
    if not user_id:
        raise HTTPException(status_code=401, detail="Authentication required")
    
    supabase = get_supabase()
    
    # Get assignment and class info
    assignment = supabase.table("assignments").select("*").eq("id", assignment_id).limit(1).execute()
    if not assignment.data:
        raise HTTPException(status_code=404, detail="Assignment not found")
    
    a = assignment.data[0]
    
    # Get all submissions
    submissions = supabase.table("assignment_submissions").select("*").eq(
        "assignment_id", assignment_id
    ).execute()
    
    subs = submissions.data or []
    
    # Calculate stats
    total_submitted = len([s for s in subs if s.get("status") != "pending"])
    total_graded = len([s for s in subs if s.get("status") == "graded"])
    total_pending = len([s for s in subs if s.get("status") == "pending"])
    
    grades = [s.get("total_marks") for s in subs if s.get("total_marks") is not None]
    avg_grade = sum(grades) / len(grades) if grades else 0
    max_grade = max(grades) if grades else 0
    min_grade = min(grades) if grades else 0
    
    # Late submissions
    due_date = datetime.fromisoformat(a.get("due_date").replace("Z", "+00:00"))
    late_count = 0
    for s in subs:
        if s.get("submitted_at"):
            sub_date = datetime.fromisoformat(s.get("submitted_at").replace("Z", "+00:00"))
            if sub_date > due_date:
                late_count += 1
    
    # Grade distribution
    grade_ranges = {"0-40": 0, "41-60": 0, "61-80": 0, "81-100": 0}
    max_marks = a.get("max_marks", 100)
    for g in grades:
        pct = (g / max_marks) * 100 if max_marks else 0
        if pct <= 40:
            grade_ranges["0-40"] += 1
        elif pct <= 60:
            grade_ranges["41-60"] += 1
        elif pct <= 80:
            grade_ranges["61-80"] += 1
        else:
            grade_ranges["81-100"] += 1
    
    return {
        "analytics": {
            "total_submitted": total_submitted,
            "total_graded": total_graded,
            "total_pending": total_pending,
            "late_submissions": late_count,
            "average_grade": round(avg_grade, 2),
            "max_grade": max_grade,
            "min_grade": min_grade,
            "grade_distribution": grade_ranges
        }
    }


# --- Templates ---

@app.get("/api/assignment-templates")
def list_templates(authorization: Optional[str] = Header(default=None)):
    """List assignment templates"""
    user_id = get_user_id_from_token(authorization)
    if not user_id:
        raise HTTPException(status_code=401, detail="Authentication required")
    
    supabase = get_supabase()
    
    templates = supabase.table("assignment_templates").select("*").eq(
        "teacher_user_id", user_id
    ).order("created_at", desc=True).execute()
    
    return {"templates": templates.data or []}


class TemplateCreate(BaseModel):
    title: str
    template_data: Dict[str, Any]


@app.post("/api/assignment-templates")
def create_template(payload: TemplateCreate, authorization: Optional[str] = Header(default=None)):
    """Save assignment as template"""
    user_id = get_user_id_from_token(authorization)
    if not user_id:
        raise HTTPException(status_code=401, detail="Authentication required")
    
    supabase = get_supabase()
    
    result = supabase.table("assignment_templates").insert({
        "teacher_user_id": user_id,
        "title": payload.title,
        "template_data": payload.template_data
    }).execute()
    
    return {"success": True, "template": result.data[0] if result.data else None}


@app.delete("/api/assignment-templates/{template_id}")
def delete_template(template_id: str, authorization: Optional[str] = Header(default=None)):
    """Delete a template"""
    user_id = get_user_id_from_token(authorization)
    if not user_id:
        raise HTTPException(status_code=401, detail="Authentication required")
    
    supabase = get_supabase()
    
    supabase.table("assignment_templates").delete().eq("id", template_id).eq(
        "teacher_user_id", user_id
    ).execute()
    
    return {"success": True}


# =============================================
# STUDENT FEEDBACK COLLECTION SYSTEM APIs
# =============================================

import secrets

class FeedbackFormCreate(BaseModel):
    title: str
    description: Optional[str] = None
    class_id: Optional[str] = None
    is_anonymous_display: bool = True
    settings: Optional[Dict[str, Any]] = None
    questions: List[Dict[str, Any]] = []

class FeedbackFormUpdate(BaseModel):
    title: Optional[str] = None
    description: Optional[str] = None
    class_id: Optional[str] = None
    is_anonymous_display: Optional[bool] = None
    settings: Optional[Dict[str, Any]] = None
    questions: Optional[List[Dict[str, Any]]] = None

class FeedbackSubmission(BaseModel):
    answers: List[Dict[str, Any]]

class FeedbackAIGenerateRequest(BaseModel):
    topic: str
    requirements: Optional[str] = None
    count: int = 8
    mix: str = "balanced"  # balanced, rating, text, mcq

def generate_share_code() -> str:
    """Generate a short unique share code (8 chars)"""
    return secrets.token_urlsafe(6)[:8]


@app.post("/api/feedback/generate-questions")
def generate_feedback_questions(payload: FeedbackAIGenerateRequest, authorization: Optional[str] = Header(default=None)):
    """Generate feedback questions using Gemini 2.5 Flash AI"""
    user_id = get_user_id_from_token(authorization)
    if not user_id:
        raise HTTPException(status_code=401, detail="Authentication required")
    
    # Build prompt based on mix type
    mix_instructions = {
        "balanced": "Include a balanced mix of question types: 2-3 rating questions (1-5 stars), 2-3 scale questions (1-10), 1-2 multiple choice, and 2-3 text/textarea questions.",
        "rating": "Focus mostly on star rating (1-5) and scale (1-10) questions. Include 1-2 text questions for open feedback.",
        "text": "Focus mostly on open-ended text and textarea questions. Include 1-2 rating questions.",
        "mcq": "Focus mostly on multiple choice (mcq_single) and yes/no questions. Include 1-2 text questions."
    }
    
    mix_guide = mix_instructions.get(payload.mix, mix_instructions["balanced"])
    
    prompt = f"""You are an expert at creating student feedback survey questions. Generate exactly {payload.count} feedback questions for the following topic:

TOPIC: {payload.topic}

{"ADDITIONAL REQUIREMENTS: " + payload.requirements if payload.requirements else ""}

{mix_guide}

AVAILABLE QUESTION TYPES:
- "text": Short text answer (single line)
- "textarea": Long text answer (multiple lines) 
- "rating": Star rating 1-5
- "scale": Numeric scale 1-10
- "mcq_single": Single choice (provide 3-5 options)
- "mcq_multiple": Multiple choice (provide 3-5 options)
- "yes_no": Yes/No question (no options needed)
- "dropdown": Dropdown select (provide 3-5 options)

RESPOND WITH ONLY A VALID JSON ARRAY. Each question object must have:
- "question_type": one of the types above
- "question_text": the question text
- "options": array of option strings (only for mcq_single, mcq_multiple, dropdown)

Example:
[
  {{"question_type": "rating", "question_text": "How would you rate the overall teaching quality?", "options": null}},
  {{"question_type": "mcq_single", "question_text": "How often did you attend the classes?", "options": ["Always", "Most of the time", "Sometimes", "Rarely", "Never"]}},
  {{"question_type": "textarea", "question_text": "What improvements would you suggest?", "options": null}}
]

Generate {payload.count} professional, insightful feedback questions. Output ONLY the JSON array, no other text."""

    try:
        import google.generativeai as genai
        
        # Use Gemini 2.5 Flash as specified
        model = genai.GenerativeModel("gemini-2.5-flash")
        response = model.generate_content(prompt)
        
        response_text = response.text.strip()
        
        # Clean up response - extract JSON array
        if "```json" in response_text:
            response_text = response_text.split("```json")[1].split("```")[0].strip()
        elif "```" in response_text:
            response_text = response_text.split("```")[1].split("```")[0].strip()
        
        # Find the JSON array
        start_idx = response_text.find("[")
        end_idx = response_text.rfind("]") + 1
        if start_idx != -1 and end_idx > start_idx:
            response_text = response_text[start_idx:end_idx]
        
        questions = json.loads(response_text)
        
        # Validate and clean questions
        valid_types = ["text", "textarea", "rating", "scale", "mcq_single", "mcq_multiple", "yes_no", "dropdown"]
        cleaned_questions = []
        
        for q in questions:
            if not isinstance(q, dict):
                continue
            q_type = q.get("question_type", "text")
            if q_type not in valid_types:
                q_type = "text"
            
            cleaned = {
                "question_type": q_type,
                "question_text": q.get("question_text", ""),
                "options": q.get("options") if q_type in ["mcq_single", "mcq_multiple", "dropdown"] else None
            }
            
            if cleaned["question_text"]:
                cleaned_questions.append(cleaned)
        
        return {"questions": cleaned_questions[:payload.count], "model": "gemini-2.5-flash"}
        
    except json.JSONDecodeError as e:
        raise HTTPException(status_code=500, detail=f"Failed to parse AI response: {str(e)}")
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"AI generation failed: {str(e)}")


@app.get("/api/feedback/forms")
def list_feedback_forms(authorization: Optional[str] = Header(default=None)):
    """List all feedback forms for the authenticated teacher"""
    user_id = get_user_id_from_token(authorization)
    if not user_id:
        raise HTTPException(status_code=401, detail="Authentication required")
    
    supabase = get_supabase()
    
    # Get forms with response count
    forms = supabase.table("feedback_forms").select("*").eq(
        "teacher_id", user_id
    ).order("created_at", desc=True).execute()
    
    form_data = forms.data or []
    
    # Get response counts for each form
    for form in form_data:
        resp_count = supabase.table("feedback_responses").select(
            "id", count="exact"
        ).eq("form_id", form["id"]).execute()
        form["response_count"] = resp_count.count or 0
    
    return {"forms": form_data}


@app.post("/api/feedback/forms")
def create_feedback_form(payload: FeedbackFormCreate, authorization: Optional[str] = Header(default=None)):
    """Create a new feedback form with questions"""
    user_id = get_user_id_from_token(authorization)
    if not user_id:
        raise HTTPException(status_code=401, detail="Authentication required")
    
    supabase = get_supabase()
    
    # Create the form
    form_data = {
        "teacher_id": user_id,
        "title": payload.title,
        "description": payload.description,
        "class_id": payload.class_id,
        "is_anonymous_display": payload.is_anonymous_display,
        "settings": payload.settings or {},
        "status": "draft"
    }
    
    result = supabase.table("feedback_forms").insert(form_data).execute()
    
    if not result.data:
        raise HTTPException(status_code=500, detail="Failed to create form")
    
    form = result.data[0]
    form_id = form["id"]
    
    # Add questions
    if payload.questions:
        questions_data = []
        for idx, q in enumerate(payload.questions):
            questions_data.append({
                "form_id": form_id,
                "question_type": q.get("question_type", "text"),
                "question_text": q.get("question_text", ""),
                "description": q.get("description"),
                "options": q.get("options"),
                "settings": q.get("settings", {}),
                "display_order": idx
            })
        
        if questions_data:
            supabase.table("feedback_questions").insert(questions_data).execute()
    
    return {"success": True, "form": form}


@app.get("/api/feedback/forms/{form_id}")
def get_feedback_form(form_id: str, authorization: Optional[str] = Header(default=None)):
    """Get a feedback form with its questions"""
    user_id = get_user_id_from_token(authorization)
    if not user_id:
        raise HTTPException(status_code=401, detail="Authentication required")
    
    supabase = get_supabase()
    
    # Get form
    form_res = supabase.table("feedback_forms").select("*").eq(
        "id", form_id
    ).eq("teacher_id", user_id).execute()
    
    if not form_res.data:
        raise HTTPException(status_code=404, detail="Form not found")
    
    form = form_res.data[0]
    
    # Get questions
    questions = supabase.table("feedback_questions").select("*").eq(
        "form_id", form_id
    ).order("display_order").execute()
    
    form["questions"] = questions.data or []
    
    # Get response count
    resp_count = supabase.table("feedback_responses").select(
        "id", count="exact"
    ).eq("form_id", form_id).execute()
    form["response_count"] = resp_count.count or 0
    
    return form


@app.put("/api/feedback/forms/{form_id}")
def update_feedback_form(form_id: str, payload: FeedbackFormUpdate, authorization: Optional[str] = Header(default=None)):
    """Update a feedback form and its questions"""
    user_id = get_user_id_from_token(authorization)
    if not user_id:
        raise HTTPException(status_code=401, detail="Authentication required")
    
    supabase = get_supabase()
    
    # Verify ownership
    existing = supabase.table("feedback_forms").select("id, status").eq(
        "id", form_id
    ).eq("teacher_id", user_id).execute()
    
    if not existing.data:
        raise HTTPException(status_code=404, detail="Form not found")
    
    # Update form fields
    update_data = {"updated_at": "now()"}
    if payload.title is not None:
        update_data["title"] = payload.title
    if payload.description is not None:
        update_data["description"] = payload.description
    if payload.class_id is not None:
        update_data["class_id"] = payload.class_id
    if payload.is_anonymous_display is not None:
        update_data["is_anonymous_display"] = payload.is_anonymous_display
    if payload.settings is not None:
        update_data["settings"] = payload.settings
    
    supabase.table("feedback_forms").update(update_data).eq("id", form_id).execute()
    
    # Update questions if provided
    if payload.questions is not None:
        # Delete existing questions
        supabase.table("feedback_questions").delete().eq("form_id", form_id).execute()
        
        # Insert new questions
        if payload.questions:
            questions_data = []
            for idx, q in enumerate(payload.questions):
                questions_data.append({
                    "form_id": form_id,
                    "question_type": q.get("question_type", "text"),
                    "question_text": q.get("question_text", ""),
                    "description": q.get("description"),
                    "options": q.get("options"),
                    "settings": q.get("settings", {}),
                    "display_order": idx
                })
            
            supabase.table("feedback_questions").insert(questions_data).execute()
    
    return {"success": True}


@app.delete("/api/feedback/forms/{form_id}")
def delete_feedback_form(form_id: str, authorization: Optional[str] = Header(default=None)):
    """Delete a feedback form"""
    user_id = get_user_id_from_token(authorization)
    if not user_id:
        raise HTTPException(status_code=401, detail="Authentication required")
    
    supabase = get_supabase()
    
    supabase.table("feedback_forms").delete().eq("id", form_id).eq(
        "teacher_id", user_id
    ).execute()
    
    return {"success": True}


@app.post("/api/feedback/forms/{form_id}/publish")
def publish_feedback_form(form_id: str, authorization: Optional[str] = Header(default=None)):
    """Publish a feedback form and generate a share code"""
    user_id = get_user_id_from_token(authorization)
    if not user_id:
        raise HTTPException(status_code=401, detail="Authentication required")
    
    supabase = get_supabase()
    
    # Verify ownership
    existing = supabase.table("feedback_forms").select("id, share_code, status").eq(
        "id", form_id
    ).eq("teacher_id", user_id).execute()
    
    if not existing.data:
        raise HTTPException(status_code=404, detail="Form not found")
    
    form = existing.data[0]
    
    # Generate share code if not exists
    share_code = form.get("share_code") or generate_share_code()
    
    supabase.table("feedback_forms").update({
        "status": "published",
        "share_code": share_code,
        "updated_at": "now()"
    }).eq("id", form_id).execute()
    
    return {"success": True, "share_code": share_code}


@app.post("/api/feedback/forms/{form_id}/close")
def close_feedback_form(form_id: str, authorization: Optional[str] = Header(default=None)):
    """Close a feedback form to stop accepting submissions"""
    user_id = get_user_id_from_token(authorization)
    if not user_id:
        raise HTTPException(status_code=401, detail="Authentication required")
    
    supabase = get_supabase()
    
    supabase.table("feedback_forms").update({
        "status": "closed",
        "updated_at": "now()"
    }).eq("id", form_id).eq("teacher_id", user_id).execute()
    
    return {"success": True}


# --- Public/Student facing endpoints ---

@app.get("/api/feedback/f/{share_code}")
def get_feedback_form_public(share_code: str, authorization: Optional[str] = Header(default=None)):
    """Get a published feedback form for student submission"""
    supabase = get_supabase()
    
    # Get form by share code
    form_res = supabase.table("feedback_forms").select("*").eq(
        "share_code", share_code
    ).execute()
    
    if not form_res.data:
        raise HTTPException(status_code=404, detail="Form not found")
    
    form = form_res.data[0]
    
    if form["status"] != "published":
        raise HTTPException(status_code=400, detail="Form is not accepting submissions")
    
    # Get questions
    questions = supabase.table("feedback_questions").select(
        "id, question_type, question_text, description, options, settings, display_order"
    ).eq("form_id", form["id"]).order("display_order").execute()
    
    # Get teacher info
    teacher = supabase.table("teacher_profiles").select("name").eq(
        "auth_user_id", form["teacher_id"]
    ).execute()
    
    teacher_name = teacher.data[0]["name"] if teacher.data else "Teacher"
    
    # Check if user already submitted
    already_submitted = False
    user_id = get_user_id_from_token(authorization)
    if user_id:
        existing = supabase.table("feedback_responses").select("id").eq(
            "form_id", form["id"]
        ).eq("student_id", user_id).execute()
        already_submitted = bool(existing.data)
    
    return {
        "id": form["id"],
        "title": form["title"],
        "description": form["description"],
        "is_anonymous_display": form["is_anonymous_display"],
        "teacher_name": teacher_name,
        "questions": questions.data or [],
        "already_submitted": already_submitted
    }


@app.post("/api/feedback/f/{share_code}/submit")
def submit_feedback(share_code: str, payload: FeedbackSubmission, authorization: Optional[str] = Header(default=None)):
    """Submit a feedback response"""
    supabase = get_supabase()
    
    # Get form
    form_res = supabase.table("feedback_forms").select("id, status").eq(
        "share_code", share_code
    ).execute()
    
    if not form_res.data:
        raise HTTPException(status_code=404, detail="Form not found")
    
    form = form_res.data[0]
    
    if form["status"] != "published":
        raise HTTPException(status_code=400, detail="Form is not accepting submissions")
    
    # Get student info (names stored even if displayed as anonymous)
    user_id = get_user_id_from_token(authorization)
    student_name = None
    student_email = None
    
    if user_id:
        # Check if already submitted
        existing = supabase.table("feedback_responses").select("id").eq(
            "form_id", form["id"]
        ).eq("student_id", user_id).execute()
        
        if existing.data:
            raise HTTPException(status_code=400, detail="You have already submitted feedback for this form")
        
        # Get student profile
        profile = supabase.table("user_profiles").select("name, email").eq(
            "auth_user_id", user_id
        ).execute()
        
        if profile.data:
            student_name = profile.data[0].get("name")
            student_email = profile.data[0].get("email")
    
    # Create response
    response_data = {
        "form_id": form["id"],
        "student_id": user_id,
        "student_name": student_name,
        "student_email": student_email
    }
    
    response_res = supabase.table("feedback_responses").insert(response_data).execute()
    
    if not response_res.data:
        raise HTTPException(status_code=500, detail="Failed to submit feedback")
    
    response_id = response_res.data[0]["id"]
    
    # Insert answers
    if payload.answers:
        answers_data = []
        for answer in payload.answers:
            answers_data.append({
                "response_id": response_id,
                "question_id": answer.get("question_id"),
                "answer_text": answer.get("answer_text"),
                "answer_options": answer.get("answer_options"),
                "answer_rating": answer.get("answer_rating")
            })
        
        if answers_data:
            supabase.table("feedback_answers").insert(answers_data).execute()
    
    return {"success": True, "message": "Thank you for your feedback!"}


# --- Teacher responses view ---

@app.get("/api/feedback/forms/{form_id}/responses")
def get_feedback_responses(form_id: str, authorization: Optional[str] = Header(default=None)):
    """Get all responses for a feedback form (teacher only)"""
    user_id = get_user_id_from_token(authorization)
    if not user_id:
        raise HTTPException(status_code=401, detail="Authentication required")
    
    supabase = get_supabase()
    
    # Verify ownership
    form = supabase.table("feedback_forms").select("id, title").eq(
        "id", form_id
    ).eq("teacher_id", user_id).execute()
    
    if not form.data:
        raise HTTPException(status_code=404, detail="Form not found")
    
    # Get all responses
    responses = supabase.table("feedback_responses").select("*").eq(
        "form_id", form_id
    ).order("submitted_at", desc=True).execute()
    
    # Get answers for each response
    response_data = []
    for resp in (responses.data or []):
        answers = supabase.table("feedback_answers").select("*").eq(
            "response_id", resp["id"]
        ).execute()
        resp["answers"] = answers.data or []
        response_data.append(resp)
    
    # Get questions for context
    questions = supabase.table("feedback_questions").select("*").eq(
        "form_id", form_id
    ).order("display_order").execute()
    
    return {
        "form": form.data[0],
        "questions": questions.data or [],
        "responses": response_data,
        "total_responses": len(response_data)
    }


@app.get("/api/feedback/forms/{form_id}/analytics")
def get_feedback_analytics(form_id: str, authorization: Optional[str] = Header(default=None)):
    """Get analytics for a feedback form"""
    user_id = get_user_id_from_token(authorization)
    if not user_id:
        raise HTTPException(status_code=401, detail="Authentication required")
    
    supabase = get_supabase()
    
    # Verify ownership
    form = supabase.table("feedback_forms").select("*").eq(
        "id", form_id
    ).eq("teacher_id", user_id).execute()
    
    if not form.data:
        raise HTTPException(status_code=404, detail="Form not found")
    
    # Get questions
    questions = supabase.table("feedback_questions").select("*").eq(
        "form_id", form_id
    ).order("display_order").execute()
    
    # Get all answers
    responses = supabase.table("feedback_responses").select("id").eq(
        "form_id", form_id
    ).execute()
    
    total_responses = len(responses.data or [])
    response_ids = [r["id"] for r in (responses.data or [])]
    
    analytics = {
        "total_responses": total_responses,
        "questions": []
    }
    
    for q in (questions.data or []):
        q_analytics = {
            "id": q["id"],
            "question_text": q["question_text"],
            "question_type": q["question_type"]
        }
        
        # Get answers for this question
        if response_ids:
            answers = supabase.table("feedback_answers").select("*").eq(
                "question_id", q["id"]
            ).in_("response_id", response_ids).execute()
            
            answer_list = answers.data or []
            
            if q["question_type"] in ["rating", "scale"]:
                # Calculate average rating
                ratings = [a["answer_rating"] for a in answer_list if a.get("answer_rating") is not None]
                if ratings:
                    q_analytics["average"] = round(sum(ratings) / len(ratings), 2)
                    q_analytics["distribution"] = {}
                    for r in ratings:
                        q_analytics["distribution"][str(r)] = q_analytics["distribution"].get(str(r), 0) + 1
                else:
                    q_analytics["average"] = 0
                    
            elif q["question_type"] in ["mcq_single", "mcq_multiple", "yes_no", "dropdown"]:
                # Calculate option distribution
                options = q.get("options") or []
                distribution = {str(i): 0 for i in range(len(options))}
                
                for a in answer_list:
                    opts = a.get("answer_options") or []
                    for opt_idx in opts:
                        if str(opt_idx) in distribution:
                            distribution[str(opt_idx)] += 1
                
                q_analytics["distribution"] = distribution
                q_analytics["options"] = options
                
            elif q["question_type"] in ["text", "textarea"]:
                # Return text responses
                q_analytics["responses"] = [a.get("answer_text", "") for a in answer_list if a.get("answer_text")]
        
        analytics["questions"].append(q_analytics)
    
    return analytics
